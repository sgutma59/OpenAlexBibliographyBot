title,year,authors,venue,citation_count,openalex_id,doi,url,open_access_pdf,concepts,type,is_oa,abstract
Guiding Principles to Address the Impact of Algorithm Bias on Racial and Ethnic Disparities in Health and Health Care,2023,Marshall H. Chin; Nasim Afsarmanesh; Arlene S. Bierman; Christine Chang; Caleb J. Colón-Rodríguez; Prashila Dullabh; Deborah G. Duran; Malika Fair; Tina Hernandez‐Boussard; Maia Hightower; Anjali Jain; William B. Jordan; Stephen Konya; Roslyn Holliday Moore; Tamra Tyree Moore; Richard Rodriguez; Gauher Shaheen; Lynne Page Snyder; Mithuna Srinivasan; Craig A. Umscheid; Lucila Ohno‐Machado,JAMA Network Open,156,W4389794809,10.1001/jamanetworkopen.2023.45050,https://openalex.org/W4389794809,https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2812958/chin_2023_sc_230007_1702050468.82841.pdf,Health equity; Health care; Algorithm; Ethnic group; Health policy,article,True,"Importance Health care algorithms are used for diagnosis, treatment, prognosis, risk stratification, and allocation of resources. Bias in the development and use of algorithms can lead to worse outcomes for racial and ethnic minoritized groups and other historically marginalized populations such as individuals with lower income. Objective To provide a conceptual framework and guiding principles for mitigating and preventing bias in health care algorithms to promote health and health care equity. Evidence Review The Agency for Healthcare Research and Quality and the National Institute for Minority Health and Health Disparities convened a diverse panel of experts to review evidence, hear from stakeholders, and receive community feedback. Findings The panel developed a conceptual framework to apply guiding principles across an algorithm’s life cycle, centering health and health care equity for patients and communities as the goal, within the wider context of structural racism and discrimination. Multiple stakeholders can mitigate and prevent bias at each phase of the algorithm life cycle, including problem formulation (phase 1); data selection, assessment, and management (phase 2); algorithm development, training, and validation (phase 3); deployment and integration of algorithms in intended settings (phase 4); and algorithm monitoring, maintenance, updating, or deimplementation (phase 5). Five principles should guide these efforts: (1) promote health and health care equity during all phases of the health care algorithm life cycle; (2) ensure health care algorithms and their use are transparent and explainable; (3) authentically engage patients and communities during all phases of the health care algorithm life cycle and earn trustworthiness; (4) explicitly identify health care algorithmic fairness issues and trade-offs; and (5) establish accountability for equity and fairness in outcomes from health care algorithms. Conclusions and Relevance Multiple stakeholders must partner to create systems, processes, regulations, incentives, standards, and policies to mitigate and prevent algorithmic bias. Reforms should implement guiding principles that support promotion of health and health care equity in all phases of the algorithm life cycle as well as transparency and explainability, authentic community engagement and ethical partnerships, explicit identification of fairness issues and trade-offs, and accountability for equity and fairness."
An adversarial training framework for mitigating algorithmic biases in clinical machine learning,2023,Jenny Yang; Andrew A. S. Soltan; David W. Eyre; Yang Yang; David A. Clifton,npj Digital Medicine,106,W4361285150,10.1038/s41746-023-00805-y,https://openalex.org/W4361285150,https://www.nature.com/articles/s41746-023-00805-y.pdf,Adversarial system; Odds; Task (project management); Machine learning; Computer science,article,True,"Abstract Machine learning is becoming increasingly prominent in healthcare. Although its benefits are clear, growing attention is being given to how these tools may exacerbate existing biases and disparities. In this study, we introduce an adversarial training framework that is capable of mitigating biases that may have been acquired through data collection. We demonstrate this proposed framework on the real-world task of rapidly predicting COVID-19, and focus on mitigating site-specific (hospital) and demographic (ethnicity) biases. Using the statistical definition of equalized odds, we show that adversarial training improves outcome fairness, while still achieving clinically-effective screening performances (negative predictive values &gt;0.98). We compare our method to previous benchmarks, and perform prospective and external validation across four independent hospital cohorts. Our method can be generalized to any outcomes, models, and definitions of fairness."
"Risk and the future of AI: Algorithmic bias, data colonialism, and marginalization",2023,Anmol Arora; M. Barrett; Euisin Lee; Eivor Oborn; Karl Prince,Information and Organization,79,W4386131004,10.1016/j.infoandorg.2023.100478,https://openalex.org/W4386131004,,Colonialism; Data science; Computer science; History; Archaeology,article,False,
Bias in artificial intelligence algorithms and recommendations for mitigation,2023,Lama Nazer; Razan Zatarah; Shai Waldrip; Janny Xue Chen Ke; Mira Moukheiber; Ashish K. Khanna; Rachel S. Hicklen; Lama Moukheiber; Dana Moukheiber; Haobo Ma; Piyush Mathur,PLOS Digital Health,407,W4381716616,10.1371/journal.pdig.0000278,https://openalex.org/W4381716616,https://journals.plos.org/digitalhealth/article/file?id=10.1371/journal.pdig.0000278&type=printable,Health care; Computer science; Preprocessor; Artificial intelligence; Data science,review,True,"The adoption of artificial intelligence (AI) algorithms is rapidly increasing in healthcare. Such algorithms may be shaped by various factors such as social determinants of health that can influence health outcomes. While AI algorithms have been proposed as a tool to expand the reach of quality healthcare to underserved communities and improve health equity, recent literature has raised concerns about the propagation of biases and healthcare disparities through implementation of these algorithms. Thus, it is critical to understand the sources of bias inherent in AI-based algorithms. This review aims to highlight the potential sources of bias within each step of developing AI algorithms in healthcare, starting from framing the problem, data collection, preprocessing, development, and validation, as well as their full implementation. For each of these steps, we also discuss strategies to mitigate the bias and disparities. A checklist was developed with recommendations for reducing bias during the development and implementation stages. It is important for developers and users of AI-based algorithms to keep these important considerations in mind to advance health equity for all populations."
A Comprehensive Review of AI Techniques for Addressing Algorithmic Bias in Job Hiring,2024,Elham Albaroudi; Taha Mansouri; Ali Alameer,AI,56,W4391598857,10.3390/ai5010019,https://openalex.org/W4391598857,https://www.mdpi.com/2673-2688/5/1/19/pdf?version=1707285598,Computer science; Data science; Political science,review,True,"The study comprehensively reviews artificial intelligence (AI) techniques for addressing algorithmic bias in job hiring. More businesses are using AI in curriculum vitae (CV) screening. While the move improves efficiency in the recruitment process, it is vulnerable to biases, which have adverse effects on organizations and the broader society. This research aims to analyze case studies on AI hiring to demonstrate both successful implementations and instances of bias. It also seeks to evaluate the impact of algorithmic bias and the strategies to mitigate it. The basic design of the study entails undertaking a systematic review of existing literature and research studies that focus on artificial intelligence techniques employed to mitigate bias in hiring. The results demonstrate that the correction of the vector space and data augmentation are effective natural language processing (NLP) and deep learning techniques for mitigating algorithmic bias in hiring. The findings underscore the potential of artificial intelligence techniques in promoting fairness and diversity in the hiring process with the application of artificial intelligence techniques. The study contributes to human resource practice by enhancing hiring algorithms’ fairness. It recommends the need for collaboration between machines and humans to enhance the fairness of the hiring process. The results can help AI developers make algorithmic changes needed to enhance fairness in AI-driven tools. This will enable the development of ethical hiring tools, contributing to fairness in society."
Addressing AI Algorithmic Bias in Health Care,2024,Raj M. Ratwani; Karey M. Sutton; Jessica E. Galarraga,JAMA,48,W4402223957,10.1001/jama.2024.13486,https://openalex.org/W4402223957,,Medicine; Health care; Applications of artificial intelligence; MEDLINE; Artificial intelligence,editorial,False,"This Viewpoint discusses the bias that exists in artificial intelligence (AI) algorithms used in health care despite recent federal rules to prohibit discriminatory outcomes from AI and recommends ways in which health care facilities, AI developers, and regulators could share responsibilities and actions to address bias."
Advancing algorithmic bias management capabilities in AI-driven marketing analytics research,2023,Shahriar Akter; Saida Sultana; Marcello M. Mariani; Samuel Fosso Wamba; Konstantina Spanaki; Yogesh K. Dwivedi,Industrial Marketing Management,42,W4386300838,10.1016/j.indmarman.2023.08.013,https://openalex.org/W4386300838,https://doi.org/10.1016/j.indmarman.2023.08.013,Customer lifetime value; Analytics; Equity (law); Marketing; Data science,article,True,"Algorithms in the age of artificial intelligence (AI) constantly transform customer behaviour, marketing programs, and marketing strategies in industrial markets. However, algorithms often fail to perform as expected due to various data, model, and market biases. Motivated by this challenge, this study presents a framework of algorithmic bias management capabilities for industrial markets that contribute to customer equity in terms of value, brand and relationship equity. Drawing on the dynamic capability theory, this study fills this gap by conducting a literature review, thematic analysis, and two rounds of surveys (n=200 analytics professionals and n=200 business customers) in the financial service industry in Australia. The findings show that algorithmic bias management capability consists of three primary dimensions (data, model, and deployment capabilities) and nine subdimensions. These findings have important implications for scholars and managers interested in developing algorithmic bias management capabilities to influence customer equity in industrial markets."
The Potential of Diverse Youth as Stakeholders in Identifying and Mitigating Algorithmic Bias for a Future of Fairer AI,2023,Jaemarie Solyst; Ellia Yang; Shixian Xie; Amy Ogan; Jessica Hammer; Motahhare Eslami,Proceedings of the ACM on Human-Computer Interaction,45,W4387344911,10.1145/3610213,https://openalex.org/W4387344911,https://dl.acm.org/doi/pdf/10.1145/3610213,Harm; Diversity (politics); Injustice; Inclusion (mineral); Psychology,article,True,"Youth regularly use technology driven by artificial intelligence (AI). However, it is increasingly well-known that AI can cause harm on small and large scales, especially for those underrepresented in tech fields. Recently, users have played active roles in surfacing and mitigating harm from algorithmic bias. Despite being frequent users of AI, youth have been under-explored as potential contributors and stakeholders to the future of AI. We consider three notions that may be at the root of youth facing barriers to playing an active role in responsible AI, which are youth (1) cannot understand the technical aspects of AI, (2) cannot understand the ethical issues around AI, and (3) need protection from serious topics related to bias and injustice. In this study, we worked with youth (N = 30) in first through twelfth grade and parents (N = 6) to explore how youth can be part of identifying algorithmic bias and designing future systems to address problematic technology behavior. We found that youth are capable of identifying and articulating algorithmic bias, often in great detail. Participants suggested different ways users could give feedback for AI that reflects their values of diversity and inclusion. Youth who may have less experience with computing or exposure to societal structures can be supported by peers or adults with more of this knowledge, leading to critical conversations about fairer AI. This work illustrates youths' insights, suggesting that they should be integrated in building a future of responsible AI."
Tackling algorithmic bias and promoting transparency in health datasets: the STANDING Together consensus recommendations,2024,Joseph Alderman; Joanne Palmer; Elinor Laws; Melissa D. McCradden; Johan Ordish; Marzyeh Ghassemi; Stephen Pfohl; Negar Rostamzadeh; Heather Cole-Lewis; Ben Glocker; Melanie Calvert; Tom Pollard; J. M. Gill; Jacqui Gath; Ade Adebajo; Jude Beng; Cheuk Wing Leung; Stephanie Kuku; Lesley-Anne Farmer; Rubeta Matin; Bilal A. Mateen; Francis McKay; Katherine Heller; Alan Karthikesalingam; Darren Treanor; Maxine Mackintosh; Lauren Oakden‐Rayner; Russell J. Pearson; Arjun K. Manrai; Puja Myles; Judit Kumuthini; Zoher Kapacee; Neil J. Sebire; Lama Nazer; Jarrel Seah; Ashley Akbari; Lewis E. Berman; Judy Wawira Gichoya; Lorenzo Righetto; D. V. K. Samuel; William Wasswa; Maria Charalambides; Anmol Arora; Sameer Pujari; Charlotte Summers; Elizabeth Sapey; S P Wilkinson; Vishal Thakker; Alastair K. Denniston; Xiaoxuan Liu,The Lancet Digital Health,45,W4405534231,10.1016/s2589-7500(24)00224-3,https://openalex.org/W4405534231,https://doi.org/10.1016/s2589-7500(24)00224-3,Transparency (behavior); Data science; Computer science; Political science; Computer security,review,True,
"AI Algorithmic Bias: Understanding its Causes, Ethical and Social Implications",2023,Lakshitha R Jain; Vineetha Menon,,28,W4389988494,10.1109/ictai59109.2023.00073,https://openalex.org/W4389988494,,Computer science; Artificial intelligence; Selection bias; Machine learning; Apprehension,article,False,"The escalating usage of artificial intelligence (AI) and machine learning algorithms across diverse fields has prompted apprehension regarding the propagation of algorithmic bias, which may exacerbate instances of discrimination and inequality. Algorithmic bias in AI and machine learning (ML) techniques manifests in real-world applications as a result of either insufficient data variation or augmentation availability in the AI/ML training data, or a flawed learning policy. This leads to the accidental propagation of AI bias as an unjust treatment of particular groups of individuals, owing to their race, gender [1], age, or other distinguishing attributes in practical applications. This paper offers a comprehensive analysis of algorithmic bias, encompassing its origins, ethical and social ramifications, and possible remediations. In addition, this paper introduces an innovative methodology for identifying and measuring algorithmic bias that integrates statistical analysis with input from users and domain specialists. This exposition examines distinct forms of algorithmic biases, such as selection bias, confirmation bias, and measurement bias, and examines underlying catalysts for algorithmic bias, encompassing data integrity concerns, decisions regarding algorithmic design, and institutional prejudgments. The adverse ramifications of algorithmic bias, including the perpetuation of social inequality and the impeding of societal advancement, are the focus of our examination. The present study seeks to make a contribution to the advancement of impartial [2] and equitable AI systems with the potential to foster societal progress and benefit individuals across diverse demographics by identifying the sources and repercussions of algorithmic bias and recommending efficacious interventions."
"Practical, epistemic and normative implications of algorithmic bias in healthcare artificial intelligence: a qualitative study of multidisciplinary expert perspectives",2023,Yves Saint James Aquino; Stacy M. Carter; Nehmat Houssami; Annette Braunack‐Mayer; Khin Than Win; Chris Degeling; Lei Wang; Wendy Rogers,Journal of Medical Ethics,43,W4321610264,10.1136/jme-2022-108850,https://openalex.org/W4321610264,https://jme.bmj.com/content/medethics/early/2023/02/22/jme-2022-108850.full.pdf,Health care; Normative; Inclusion (mineral); Multidisciplinary approach; Psychology,article,True,"Background There is a growing concern about artificial intelligence (AI) applications in healthcare that can disadvantage already under-represented and marginalised groups (eg, based on gender or race). Objectives Our objectives are to canvas the range of strategies stakeholders endorse in attempting to mitigate algorithmic bias, and to consider the ethical question of responsibility for algorithmic bias. Methodology The study involves in-depth, semistructured interviews with healthcare workers, screening programme managers, consumer health representatives, regulators, data scientists and developers. Results Findings reveal considerable divergent views on three key issues. First, views on whether bias is a problem in healthcare AI varied, with most participants agreeing bias is a problem (which we call the bias-critical view), a small number believing the opposite (the bias-denial view), and some arguing that the benefits of AI outweigh any harms or wrongs arising from the bias problem (the bias-apologist view). Second, there was a disagreement on the strategies to mitigate bias, and who is responsible for such strategies. Finally, there were divergent views on whether to include or exclude sociocultural identifiers (eg, race, ethnicity or gender-diverse identities) in the development of AI as a way to mitigate bias. Conclusion/significance Based on the views of participants, we set out responses that stakeholders might pursue, including greater interdisciplinary collaboration, tailored stakeholder engagement activities, empirical studies to understand algorithmic bias and strategies to modify dominant approaches in AI development such as the use of participatory methods, and increased diversity and inclusion in research teams and research participant recruitment and selection."
Data’s Impact on Algorithmic Bias,2023,Donghee Shin; Emily Y. Shin,Computer,16,W4377818787,10.1109/mc.2023.3262909,https://openalex.org/W4377818787,https://ieeexplore.ieee.org/ielx7/2/10132019/10132055.pdf,Computer science; Column (typography); Artificial intelligence; Machine learning; Algorithm,article,True,"Algorithmic bias refers to systematic and structured errors in an artificial intelligence system that generate unfair results and inequalities. This column discusses how bias in algorithms appears, amplifies over time, and shapes people's thinking, potentially leading to discrimination."
Disambiguating Algorithmic Bias: From Neutrality to Justice,2023,Elizabeth Edenberg; Alexandra Wood,,20,W4386242313,10.1145/3600211.3604695,https://openalex.org/W4386242313,https://doi.org/10.1145/3600211.3604695,Neutrality; Injustice; Prejudice (legal term); Implicit bias; Economic Justice,article,True,"As algorithms have become ubiquitous in consequential domains, societal concerns about the potential for discriminatory outcomes have prompted urgent calls to address algorithmic bias. In response, a rich literature across computer science, law, and ethics is rapidly proliferating to advance approaches to designing fair algorithms. Yet computer scientists, legal scholars, and ethicists are often not speaking the same language when using the term 'bias.' Debates concerning whether society can or should tackle the problem of algorithmic bias are hampered by conflations of various understandings of bias, ranging from neutral deviations from a standard to morally problematic instances of injustice due to prejudice, discrimination, and disparate treatment. This terminological confusion impedes efforts to address clear cases of discrimination."
Misinformation and Algorithmic Bias,2024,Donghee Shin,,12,W4393191699,10.1007/978-3-031-52569-8_2,https://openalex.org/W4393191699,,Misinformation; Internet privacy; Computer science; Computer security; Psychology,book-chapter,False,
Navigating algorithm bias in AI: ensuring fairness and trust in Africa,2024,Notice Pasipamire; Abton Muroyiwa,Frontiers in Research Metrics and Analytics,20,W4403745416,10.3389/frma.2024.1486600,https://openalex.org/W4403745416,https://doi.org/10.3389/frma.2024.1486600,Transparency (behavior); Compromise; Perspective (graphical); Sustainable development; Public relations,article,True,"This article presents a perspective on the impact of algorithmic bias on information fairness and trust in artificial intelligence (AI) systems within the African context. The author's personal experiences and observations, combined with relevant literature, formed the basis of this article. The authors demonstrate why algorithm bias poses a substantial challenge in Africa, particularly regarding fairness and the integrity of AI applications. This perspective underscores the urgent need to address biases that compromise the fairness of information dissemination and undermine public trust. The authors advocate for the implementation of strategies that promote inclusivity, enhance cultural sensitivity, and actively engage local communities in the development of AI systems. By prioritizing ethical practices and transparency, stakeholders can mitigate the risks associated with bias, thereby fostering trust and ensuring equitable access to technology. Additionally, the article explores the potential consequences of inaction, including exacerbated social disparities, diminished confidence in public institutions, and economic stagnation. Ultimately, this work argues for a collaborative approach to AI that positions Africa as a leader in responsible development, ensuring that technology serves as a catalyst for sustainable development and social justice."
Auditing Work: Exploring the New York City algorithmic bias audit regime,2024,Lara Groves; Jacob Metcalf; Alayna Kennedy; Briana Vecchione; Andrew Strait,,30,W4399362708,10.1145/3630106.3658959,https://openalex.org/W4399362708,https://dl.acm.org/doi/pdf/10.1145/3630106.3658959,Audit; Work (physics); Computer science; Accounting; Business,article,True,"In July 2023, New York City (NYC) implemented the first attempt to create an algorithm auditing regime for commercial machine-learning systems. Local Law 144 (LL 144), requires NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to be subject to annual bias audits by an independent auditor. In this paper, we analyse what lessons can be learned from LL 144 for other national attempts to create algorithm auditing regimes. Using qualitative interviews with 17 experts and practitioners working within the regime, we find LL 144 has failed to create an effective auditing regime: the law fails to clearly define key aspects like AEDTs and what constitutes an independent auditor, leaving auditors, vendors who create AEDTs, and companies using AEDTs to define the law's practical implementation in ways that failed to protect job applicants. Several factors contribute to this: first, the law was premised on a faulty transparency-driven theory of change that fails to stop biased AEDTs from being used by employers. Second, industry lobbying led to the definition of what constitutes an AEDT being narrowed to the point where most companies considered their tools exempt. Third, we find auditors face enormous practical and cultural challenges gaining access to data from employers and vendors building these tools. Fourth, we find wide disagreement over what constitutes a legitimate auditor and identify four different kinds of 'auditor roles' that serve different functions and offer different kinds of services. We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes that use clearer definitions and metrics and more accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, and can provide guidance for policymakers seeking to create similar regimes."
Algorithmic bias in artificial intelligence is a problem—And the root issue is power,2023,Rae Walker; Jess Dillard‐Wright; Favorite Iradukunda,Nursing Outlook,25,W4385788498,10.1016/j.outlook.2023.102023,https://openalex.org/W4385788498,http://www.nursingoutlook.org/article/S0029655423001288/pdf,Injustice; Health care; Power (physics); Root (linguistics); Curriculum,article,True,"Its neither feasible nor just to expect individual nurses to counter systemic injustice in health care through individual actions, more technocentric curricula, or industry partnerships. We need disciplinary supports for collective action to renegotiate power for AI tech."
ALGORITHMIC BIAS IN LAW: THE DISCRIMINATORY POTENTIAL AND LEGAL LIABILITY OF AI-BASED DECISION SUPPORT SYSTEMS,2024,Abdulmecit Nuredin,,25,W4414126478,10.55843/isc2024conf105n,https://openalex.org/W4414126478,,,article,False,"This study examines the impact of artificial intelligence (AI)-based algorithmic decision-making systems on human rights through a multidimensional legal and empirical approach. Specifically, it evaluates the structural inequalities resulting from algorithmic bias in critical sectors such as criminal justice, social rights, public services, and private sector operations. Through content analysis and comparative case studies, the article investigates a range of international examples— including the COMPAS and PredPol systems in the United States, the SyRI and Ofqual algorithms in Europe, and immigration and welfare tools deployed in countries like Canada and Australia. The article is structured into four main sections. First, it explores how algorithmic systems operate based on biased datasets and the implications of such processes for marginalized social groups. The second section discusses how algorithmic tools have contributed to the reproduction of inequality in public service delivery. The third section analyzes how AI technologies used in education, healthcare, and immigration procedures may yield outcomes that conflict with fundamental human rights. Lastly, the article focuses on digital discrimination in the private sector and the emerging threats to consumer protection and equality. The study argues that algorithmic justice is not merely a technical challenge but also an ethical, legal, and institutional one. In its concluding section, the article proposes holistic solutions such as fair machine learning practices, principles of algorithmic transparency, mandatory ethical impact assessments, and the establishment of independent oversight bodies. The findings underscore the need for a multidisciplinary, normatively grounded, and transparent governance framework to ensure that algorithmic systems are designed and implemented in accordance with international human rights standards."
Investigating algorithmic bias in student progress monitoring,2024,Jamiu Adekunle Idowu; Adriano Koshiyama; Philip Treleaven,Computers and Education Artificial Intelligence,15,W4400767613,10.1016/j.caeai.2024.100267,https://openalex.org/W4400767613,https://doi.org/10.1016/j.caeai.2024.100267,Computer science; Learning analytics; Grading (engineering); Analytics; Data science,article,True,"This research investigates bias in AI algorithms used for monitoring student progress, specifically focusing on bias related to age, disability, and gender. The study is motivated by incidents such as the UK A-level grading controversy, which demonstrated the real-world implications of biased algorithms. Using the Open University Learning Analytics Dataset, the research evaluates fairness with metrics like ABROCA, Average Odds Difference, and Equality of Opportunity Difference. The analysis is structured into three experiments. The first experiment examines fairness as an attribute of the data sources and reveals that institutional data is the primary contributor to model discrimination, followed by Virtual Learning Environment data, while assessment data is the least biased. In the second experiment, the research introduces the Optimal Time Index, which pinpoints Day 60 of an average 255-day course as the optimal time for predicting student outcomes, balancing timely interventions, model accuracy, and efficient resource allocation. The third experiment implements bias mitigation strategies throughout the model's life cycle, achieving fairness without compromising accuracy. Finally, this study introduces the Student Progress Card, designed to provide actionable personalized feedback for each student."
Inside the Black Box: Detecting and Mitigating Algorithmic Bias Across Racialized Groups in College Student-Success Prediction,2024,Denisa Gándara; Hadis Anahideh; Matthew P. Ison; Lorenzo Picchiarini,AERA Open,25,W4400485150,10.1177/23328584241258741,https://openalex.org/W4400485150,https://journals.sagepub.com/doi/pdf/10.1177/23328584241258741,Racial bias; Implicit bias; Racism; Psychological intervention; Higher education,article,True,"Colleges and universities are increasingly turning to algorithms that predict college-student success to inform various decisions, including those related to admissions, budgeting, and student-success interventions. Because predictive algorithms rely on historical data, they capture societal injustices, including racism. In this study, we examine how the accuracy of college student success predictions differs between racialized groups, signaling algorithmic bias. We also evaluate the utility of leading bias-mitigating techniques in addressing this bias. Using nationally representative data from the Education Longitudinal Study of 2002 and various machine learning modeling approaches, we demonstrate how models incorporating commonly used features to predict college-student success are less accurate when predicting success for racially minoritized students. Common approaches to mitigating algorithmic bias are generally ineffective at eliminating disparities in prediction outcomes and accuracy between racialized groups."
Artificial Intelligence and Algorithmic Bias,2023,Natasha H. Williams,The International Library of Bioethics,11,W4390476661,10.1007/978-3-031-48262-5_1,https://openalex.org/W4390476661,,Section (typography); Computer science; Artificial intelligence; Applications of artificial intelligence; Health care,book-chapter,False,
ALGORITHMIC BIAS IN AI-ENHANCED EDUCATION: CULTURAL DIMENSIONS AND PEDAGOGICAL IMPACT,2025,Fadil HOCA; Abdulmecit Nuredin,,20,W4414179319,10.55843/isl2025symp163h,https://openalex.org/W4414179319,,,article,False,"Artificial Intelligence, has become a transformative force in education, enabling personalized learning, adaptive assessment, and data-driven pedagogy. Yet, its integration poses significant risks, notably algorithmic—particularly cultural—bias, which can exacerbate systemic inequities. This study analyzes the origins, manifestations, and educational impacts of such bias, drawing on literature from computer science, education, ethics, and law. It highlights how unrepresentative datasets, biased model architectures, and sociocultural blind spots lead to discrimination in assessment, learning recommendations, and resource allocation. Using a critical synthesis of empirical research and policy analysis, supported by international case studies, the study finds that cultural underrepresentation, opaque decision-making, and weak governance frameworks undermine fairness and equity in AI-driven education. Such biases distort performance evaluations, reinforce stereotypes—such as gendered career guidance in STEM— and widen disparities. The paper recommends diverse datasets, transparent and explainable AI (XAI), institutionalized fairness audits, and the integration of ethical AI principles in education. It underscores the role of educators, policymakers, and international bodies in establishing accountability, inclusivity, and cultural adaptability. Achieving equitable AI-supported education demands sustained interdisciplinary collaboration, combining innovation with robust ethical governance."
Automating Automaticity: How the Context of Human Choice Affects the Extent of Algorithmic Bias,2023,Amanda Agan; Diag Davenport; J. Ludwig; Sendhil Mullainathan,,20,W4321366289,10.3386/w30981,https://openalex.org/W4321366289,https://doi.org/10.3386/w30981,Automaticity; Context (archaeology); Computer science; Cognitive psychology; Psychology,report,True,"Consumer choices are increasingly mediated by algorithms, which use data on those past choices to infer consumer preferences and then curate future choice sets.Behavioral economics suggests one reason these algorithms so often fail: choices can systematically deviate from preferences.For example, research shows that prejudice can arise not just from preferences and beliefs, but also from the context in which people choose.When people behave automatically, biases creep in; snap decisions are typically more prejudiced than slow, deliberate ones, and can lead to behaviors that users themselves do not consciously want or intend.As a result, algorithms trained on automatic behaviors can misunderstand the prejudice of users: the more automatic the behavior, the greater the error.We empirically test these ideas in a lab experiment, and find that more automatic behavior does indeed seem to lead to more biased algorithms.We then explore the large-scale consequences of this idea by carrying out algorithmic audits of Facebook in its two biggest markets, the US and India, focusing on two algorithms that differ in how users engage with them: News Feed (people interact with friends' posts fairly automatically) and People You May Know (people choose friends fairly deliberately).We find significant out-group bias in the News Feed algorithm (e.g., whites are less likely to be shown Black friends' posts, and Muslims less likely to be shown Hindu friends' posts), but no detectable bias in the PYMK algorithm.Together, these results suggest a need to rethink how large-scale algorithms use data on human behavior, especially in online contexts where so much of the measured behavior might be quite automatic."
Are algorithms biased in education? Exploring racial bias in predicting community college student success,2024,Kelli Bird; Benjamin Castleman; Yifeng Song,Journal of Policy Analysis and Management,14,W4391402561,10.1002/pam.22569,https://openalex.org/W4391402561,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/pam.22569,Racial bias; Community college; Psychology; Computer science; Implicit bias,article,True,"Abstract Predictive analytics are increasingly pervasive in higher education. However, algorithmic bias has the potential to reinforce racial inequities in postsecondary success. We provide a comprehensive and translational investigation of algorithmic bias in two separate prediction models—one predicting course completion, the second predicting degree completion. We show that if either model were used to target additional supports for “at‐risk” students, then the algorithmic bias would lead to fewer marginal Black students receiving these resources. We also find the magnitude of algorithmic bias varies within the distribution of predicted success. With the degree completion model, the amount of bias is over 5 times higher when we define at‐risk using the bottom decile than when we focus on students in the bottom half of predicted scores; in the course completion model, the reverse is true. These divergent patterns emphasize the contextual nature of algorithmic bias and attempts to mitigate it. Our results moreover suggest that algorithmic bias is due in part to currently‐available administrative data being relatively less useful at predicting Black student success, particularly for new students; this suggests that additional data collection efforts have the potential to mitigate bias."
Enhancing children’s understanding of algorithmic biases in and with text-to-image generative AI,2024,Henriikka Vartiainen; Juho Kahila; Matti Tedre; Sonsoles López‐Pernas; Nicolas Pope,New Media & Society,13,W4397034407,10.1177/14614448241252820,https://openalex.org/W4397034407,https://journals.sagepub.com/doi/pdf/10.1177/14614448241252820,Generative grammar; Image (mathematics); Computer science; Generative model; Psychology,article,True,"Despite the growing concerns surrounding algorithmic biases in generative AI (artificial intelligence), there is a noticeable lack of research on how to facilitate children and young people’s awareness and understanding of them. This study aimed to address this gap by conducting hands-on workshops with fourth- and seventh-grade students in Finland, and by focusing on students’ ( N = 209) evolving explanations of the potential causes of algorithmic biases within text-to-image generative models. Statistically significant progress in children’s data-driven explanations was observed on a written reasoning test, which was administered prior to and after the intervention, as well as in their responses to the worksheets they filled out during a lesson that focused on algorithmic biases. The article concludes with a discussion on the development and facilitation of children’s understanding of algorithmic biases."
Trapped in the search box: An examination of algorithmic bias in search engine autocomplete predictions,2023,Cong Lin; Yuxin Gao; Na Ta; Kaiyu Li; Hongyao Fu,Telematics and Informatics,12,W4388010320,10.1016/j.tele.2023.102068,https://openalex.org/W4388010320,,Computer science; Search engine; Disadvantaged; Black box; Debiasing,article,False,
"Algorithmic bias, data ethics, and governance: Ensuring fairness, transparency and compliance in AI-powered business analytics applications",2025,Julien Kiesse Bahangulu; Louis Owusu-Berko,World Journal of Advanced Research and Reviews,18,W4407857156,10.30574/wjarr.2025.25.2.0571,https://openalex.org/W4407857156,,Transparency (behavior); Compliance (psychology); Corporate governance; Analytics; Business ethics,article,False,"The widespread adoption of AI-powered business analytics applications has revolutionized decision-making, yet it has also introduced significant challenges related to algorithmic bias, data ethics, and governance. As organizations increasingly rely on machine learning and big data analytics for customer profiling, credit scoring, hiring decisions, and predictive analytics, concerns about fairness, transparency, and compliance have intensified. Algorithmic biases—often stemming from biased training data, flawed model assumptions, and insufficient diversity in datasets—can result in discriminatory outcomes, reinforcing societal inequalities and reputational risks for businesses. To address these concerns, robust data ethics frameworks must be integrated into AI governance strategies. Ethical AI principles emphasize accountability, explainability, and bias mitigation techniques, ensuring that decision-making algorithms are transparent and justifiable. Organizations must implement bias detection methods, fairness-aware machine learning models, and continuous audits to minimize unintended consequences. Additionally, regulatory frameworks such as GDPR, CCPA, and AI-specific compliance laws necessitate stringent governance practices to protect consumer rights and data privacy. Beyond compliance, fostering public trust in AI-powered analytics requires organizations to adopt ethical data stewardship, ensuring that AI models align with corporate social responsibility (CSR) initiatives and stakeholder expectations. The intersection of data ethics, algorithmic accountability, and regulatory compliance presents both challenges and opportunities for businesses seeking to leverage AI responsibly. This paper examines key strategies for mitigating algorithmic bias, establishing ethical AI governance models, and ensuring fairness in data-driven business applications, providing a roadmap for organizations to enhance transparency, compliance, and equitable AI adoption."
Investigating Algorithmic Bias on Bayesian Knowledge Tracing and Carelessness Detectors,2024,Andres Felipe Zambrano; Jiayi Zhang; Ryan S. Baker,,10,W4392445451,10.1145/3636555.3636890,https://openalex.org/W4392445451,,Carelessness; Computer science; Demographics; Debiasing; Bayesian probability,article,False,"In today's data-driven educational technologies, algorithms have a pivotal impact on student experiences and outcomes. Therefore, it is critical to take steps to minimize biases, to avoid perpetuating or exacerbating inequalities. In this paper, we investigate the degree to which algorithmic biases are present in two learning analytics models: knowledge estimates based on Bayesian Knowledge Tracing (BKT) and carelessness detectors. Using data from a learning platform used across the United States at scale, we explore algorithmic bias following three different approaches: 1) analyzing the performance of the models on every demographic group in the sample, 2) comparing performance across intersectional groups of these demographics, and 3) investigating whether the models trained using specific groups can be transferred to demographics that were not observed during the training process. Our experimental results show that the performance of these models is close to equal across all the demographic and intersectional groups. These findings establish the feasibility of validating educational algorithms for intersectional groups and indicate that these algorithms can be fairly used for diverse students at scale."
Algorithmic bias in educational systems: Examining the impact of AI-driven decision making in modern education,2025,Obed Boateng; Bright Boateng,World Journal of Advanced Research and Reviews,14,W4407040219,10.30574/wjarr.2025.25.1.0253,https://openalex.org/W4407040219,https://doi.org/10.30574/wjarr.2025.25.1.0253,Computer science; Mathematics education; Psychology; Data science; Artificial intelligence,article,True,"The increasing integration of artificial intelligence and algorithmic systems in educational settings has raised critical concerns about their impact on educational equity. This paper examines the manifestation and implications of algorithmic bias across various educational domains, including admissions processes, assessment systems, and learning management platforms. Through analysis of current research and studies, we investigate how these biases can perpetuate or exacerbate existing educational disparities, particularly affecting students from marginalized communities. The study reveals that algorithmic bias in education operates through multiple channels, from data collection and algorithm design to implementation practices and institutional policies. Our findings indicate that biased algorithms can significantly impact students' educational trajectories, creating new forms of systemic barriers in education. We propose a comprehensive framework for addressing these challenges, combining technical solutions with policy reforms and institutional guidelines. This research contributes to the growing discourse on ethical AI in education and provides practical strategies for creating more equitable educational systems in an increasingly digitized world."
EXplainable Artificial Intelligence (XAI) for facilitating recognition of algorithmic bias: An experiment from imposed users’ perspectives,2024,Ching‐Hua Chuan; Ruoyu Sun; Shiyun Tian; Wan‐Hsiu Sunny Tsai,Telematics and Informatics,14,W4396793783,10.1016/j.tele.2024.102135,https://openalex.org/W4396793783,,Raising (metalworking); Computer science; Perception; Artificial intelligence; Cognitive psychology,article,False,
Mitigating Algorithmic Bias in AI-Driven Cardiovascular Imaging for Fairer Diagnostics,2024,Md Abu Sufian; Lujain Alsadder; Wahiba Hamzi; Sadia Zaman; A. S. M. Sharifuzzaman Sagar; Boumediene Hamzi,Diagnostics,10,W4404756939,10.3390/diagnostics14232675,https://openalex.org/W4404756939,https://doi.org/10.3390/diagnostics14232675,Computer science; Artificial intelligence; Data science,article,True,"Background/Objectives: The research addresses algorithmic bias in deep learning models for cardiovascular risk prediction, focusing on fairness across demographic and socioeconomic groups to mitigate health disparities. It integrates fairness-aware algorithms, susceptible carrier-infected-recovered (SCIR) models, and interpretability frameworks to combine fairness with actionable AI insights supported by robust segmentation and classification metrics. Methods: The research utilised quantitative 3D/4D heart magnetic resonance imaging and tabular datasets from the Cardiac Atlas Project’s (CAP) open challenges to explore AI-driven methodologies for mitigating algorithmic bias in cardiac imaging. The SCIR model, known for its robustness, was adapted with the Capuchin algorithm, adversarial debiasing, Fairlearn, and post-processing with equalised odds. The robustness of the SCIR model was further demonstrated in the fairness evaluation metrics, which included demographic parity, equal opportunity difference (0.037), equalised odds difference (0.026), disparate impact (1.081), and Theil Index (0.249). For interpretability, YOLOv5, Mask R-CNN, and ResNet18 were implemented with LIME and SHAP. Bias mitigation improved disparate impact (0.80 to 0.95), reduced equal opportunity difference (0.20 to 0.05), and decreased false favourable rates for males (0.0059 to 0.0033) and females (0.0096 to 0.0064) through balanced probability adjustment. Results: The SCIR model outperformed the SIR model (recovery rate: 1.38 vs 0.83) with a −10% transmission bias impact. Parameters (β=0.5, δ=0.2, γ=0.15) reduced susceptible counts to 2.53×10−12 and increased recovered counts to 9.98 by t=50. YOLOv5 achieved high Intersection over Union (IoU) scores (94.8%, 93.7%, 80.6% for normal, severe, and abnormal cases). Mask R-CNN showed 82.5% peak confidence, while ResNet demonstrated a 10.4% accuracy drop under noise. Performance metrics (IoU: 0.91–0.96, Dice: 0.941–0.980, Kappa: 0.95) highlighted strong predictive accuracy and reliability. Conclusions: The findings validate the effectiveness of fairness-aware algorithms in addressing cardiovascular predictive model biases. The integration of fairness and explainable AI not only promotes equitable diagnostic precision but also significantly reduces diagnostic disparities across vulnerable populations. This reduction in disparities is a key outcome of the research, enhancing clinical trust in AI-driven systems. The promising results of this study pave the way for future work that will explore scalability in real-world clinical settings and address limitations such as computational complexity in large-scale data processing."
Measuring algorithmic bias to analyze the reliability of AI tools that predict depression risk using smartphone sensed-behavioral data,2024,Daniel A. Adler; Caitlin A. Stamatis; Jonah Meyerhoff; David C. Mohr; Fei Wang; Gabriel J. Aranovich; Srijan Sen; Tanzeem Choudhury,npj Mental Health Research,17,W4395010721,10.1038/s44184-024-00057-y,https://openalex.org/W4395010721,https://www.nature.com/articles/s44184-024-00057-y.pdf,Generalizability theory; Depression (economics); Reliability (semiconductor); Mental health; Computer science,article,True,"Abstract AI tools intend to transform mental healthcare by providing remote estimates of depression risk using behavioral data collected by sensors embedded in smartphones. While these tools accurately predict elevated depression symptoms in small, homogenous populations, recent studies show that these tools are less accurate in larger, more diverse populations. In this work, we show that accuracy is reduced because sensed-behaviors are unreliable predictors of depression across individuals: sensed-behaviors that predict depression risk are inconsistent across demographic and socioeconomic subgroups. We first identified subgroups where a developed AI tool underperformed by measuring algorithmic bias, where subgroups with depression were incorrectly predicted to be at lower risk than healthier subgroups. We then found inconsistencies between sensed-behaviors predictive of depression across these subgroups. Our findings suggest that researchers developing AI tools predicting mental health from sensed-behaviors should think critically about the generalizability of these tools, and consider tailored solutions for targeted populations."
Algorithmic Bias: A Challenge for Ethical Artificial Intelligence (AI),2023,Divya Dwivedi,,7,W4390444195,10.1007/978-981-99-8834-1_5,https://openalex.org/W4390444195,,Exploit; Computer science; Artificial intelligence; Ethical issues; Cognition,book-chapter,False,
Cognitively Biased Users Interacting with Algorithmically Biased Results in Whole-Session Search on Debated Topics,2024,Ben Wang; Jiqun Liu,,10,W4393284545,10.1145/3664190.3672520,https://openalex.org/W4393284545,https://doi.org/10.1145/3664190.3672520,Session (web analytics); Crowdsourcing; Openness to experience; Confirmation bias; Computer science,preprint,True,"When interacting with information retrieval (IR) systems, users, affected by confirmation biases, tend to select search results that confirm their existing beliefs on socially significant contentious issues. To understand the judgments and attitude changes of users searching online, our study examined how cognitively biased users interact with algorithmically biased search engine result pages (SERPs). We designed three-query search sessions on debated topics under various bias conditions. We recruited 1,321 crowdsourcing participants and explored their attitude changes, search interactions, and the effects of confirmation bias. Three key findings emerged: 1) most attitude changes occur in the initial query of a search session; 2) Confirmation bias and result presentation on SERPs affect the number and depth of clicks in the current query and perceived familiarity with clicked results in subsequent queries; 3) The bias position also affects attitude changes of users with lower perceived openness to conflicting opinions. Our study goes beyond traditional simulation-based evaluation settings and simulated rational users, sheds light on the mixed effects of human biases and algorithmic biases in information retrieval tasks on debated topics, and can inform the design of bias-aware user models, human-centered bias mitigation techniques, and socially responsible intelligent IR systems."
Algorithmic bias and research integrity; the role of nonhuman authors in shaping scientific knowledge with respect to artificial intelligence: a perspective,2023,Malik Olatunde Oduoye; Binish Javed; Nikhil Gupta; Che Mbali Valentina Sih,International Journal of Surgery,18,W4380714834,10.1097/js9.0000000000000552,https://openalex.org/W4380714834,https://doi.org/10.1097/js9.0000000000000552,Sophistication; Prejudice (legal term); Perspective (graphical); Transparency (behavior); Computer science,article,True,"Artificial intelligence technologies were developed to assist authors in bettering the organization and caliber of their published papers, which are both growing in quantity and sophistication. Even though the usage of artificial intelligence tools in particular ChatGPT’s natural language processing systems has been shown to be beneficial in research, there are still concerns about accuracy, responsibility, and transparency when it comes to the norms regarding authorship credit and contributions. Genomic algorithms quickly examine large amounts of genetic data to identify potential disease-causing mutations. By analyzing millions of medications for potential therapeutic benefits, they can quickly and relatively economically find novel approaches to treatment. Researchers from several fields can collaborate on difficult tasks with the assistance of nonhuman writers, promoting interdisciplinary research. Sadly, there are a number of significant disadvantages associated with employing nonhuman authors, including the potential for algorithmic prejudice. Biased data may be reinforced by the algorithm since machine learning algorithms can only be as objective as the data they are trained on. It is overdue that scholars bring forth basic moral concerns in the fight against algorithmic prejudice. Overall, even if the use of nonhuman authors has the potential to significantly improve scientific research, it is crucial for scientists to be aware of these drawbacks and take precautions to avoid bias and limits. To provide accurate and objective results, algorithms must be carefully designed and implemented, and researchers need to be mindful of the larger ethical ramifications of their usage."
Reflecting on Algorithmic Bias With Design Fiction: The MiniCoDe Workshops,2024,Tommaso Turchi; Alessio Malizia; Simone Borsci,IEEE Intelligent Systems,6,W4390806593,10.1109/mis.2024.3352977,https://openalex.org/W4390806593,https://ieeexplore.ieee.org/ielx7/9670/5196652/10388369.pdf,Transparency (behavior); Computer science; Accountability; Economic Justice; Implementation,article,True,"In an increasingly complex everyday life, algorithms—often learned from data, i.e., machine learning (ML)—are used to make or assist with operational decisions. However, developers and designers usually are not entirely aware of how to reflect on social justice while designing ML algorithms and applications. Algorithmic social justice—i.e., designing algorithms including fairness, transparency, and accountability—aims at helping expose, counterbalance, and remedy bias and exclusion in future ML-based decision-making applications. How might we entice people to engage in more reflective practices that examine the ethical consequences of ML algorithmic bias in society? We developed and tested a design-fiction-driven methodology to enable multidisciplinary teams to perform intense, workshop-like gatherings to let potential ethical issues emerge and mitigate bias through a series of guided steps. With this contribution, we present an original and innovative use of design fiction as a method to reduce algorithmic bias in co-design activities."
Mitigating Algorithmic Bias with Limited Annotations,2023,Guanchu Wang; Mengnan Du; Ninghao Liu; Na Zou; Xia Hu,Lecture notes in computer science,5,W4386804585,10.1007/978-3-031-43415-0_15,https://openalex.org/W4386804585,,Computer science; Annotation; Benchmark (surveying); Bounding overwatch; Baseline (sea),book-chapter,False,
Algorithmic bias and racial inequality: a critical review,2024,Maximilian Kasy,Oxford Review of Economic Policy,6,W4404610377,10.1093/oxrep/grae031,https://openalex.org/W4404610377,,Inequality; Economics; Neoclassical economics; Mathematical economics; Econometrics,review,False,"Abstract Most definitions of algorithmic bias and fairness encode decision-maker interests, such as profits, rather than the interests of disadvantaged groups (e.g. racial minorities): bias is defined as a deviation from profit maximization. Future research should instead focus on the causal effect of automated decisions on the distribution of welfare, both across and within groups. The literature emphasizes some apparent contradictions between different notions of fairness, and between fairness and profits. These contradictions vanish, however, when profits are maximized. Existing work involves conceptual slippages between statistical notions of bias and misclassification errors, economic notions of profit, and normative notions of bias and fairness. Notions of bias nonetheless carry some interest within the welfare paradigm that I advocate for, if we understand bias and discrimination as mechanisms and potential points of intervention."
Technology assisted research assessment: algorithmic bias and transparency issues,2023,Mike Thelwall; Kayvan Kousha,Aslib Journal of Information Management,8,W4387204488,10.1108/ajim-04-2023-0119,https://openalex.org/W4387204488,http://hdl.handle.net/2436/625315,Transparency (behavior); Scope (computer science); Originality; Computer science; Data science,article,True,"Purpose Technology is sometimes used to support assessments of academic research in the form of automatically generated bibliometrics for reviewers to consult during their evaluations or by replacing some or all human judgements. With artificial intelligence (AI), there is increasing scope to use technology to assist research assessment processes in new ways. Since transparency and fairness are widely considered important for research assessment and AI introduces new issues, this review investigates their implications. Design/methodology/approach This article reviews and briefly summarises transparency and fairness concerns in general terms and through the issues that they raise for various types of Technology Assisted Research Assessment (TARA). Findings Whilst TARA can have varying levels of problems with both transparency and bias, in most contexts it is unclear whether it worsens the transparency and bias problems that are inherent in peer review. Originality/value This is the first analysis that focuses on algorithmic bias and transparency issues for technology assisted research assessment."
Algorithmic bias: the state of the situation and policy recommendations,2023,OECD,Digital education outlook,5,W4400709265,10.1787/09e55ac4-en,https://openalex.org/W4400709265,,State (computer science); Computer science; Race (biology); Gender bias; Data science,book-chapter,False,"This chapter discusses the current state of the evidence on algorithmic bias in education. After defining algorithmic bias and its possible origins, it reviews the existing international evidence about algorithmic bias in education, which has focused on gender and race, but has also involved some other demographic categories. The chapter concludes with a few recommendations, notably to ensure that privacy requirements do not prevent researchers and developers from identifying bias, so that it can be addressed."
Automating Automaticity: How the Context of Human Choice Affects the Extent of Algorithmic Bias,2023,Amanda Agan; Diag Davenport; Jens Ludwig; Sendhil Mullainathan,SSRN Electronic Journal,11,W4321379767,10.2139/ssrn.4364729,https://openalex.org/W4321379767,https://doi.org/10.2139/ssrn.4364729,Automaticity; Context (archaeology); Computer science; Cognitive psychology; Psychology,article,True,
Algorithmic bias: Social science research integration through the 3-D Dependable AI Framework,2024,Kalinda Ukanwa,Current Opinion in Psychology,9,W4400216206,10.1016/j.copsyc.2024.101836,https://openalex.org/W4400216206,https://doi.org/10.1016/j.copsyc.2024.101836,Psychology; Psychological research; Through-the-lens metering; Data science; Cognitive science,review,True,"Algorithmic bias has emerged as a critical challenge in the age of responsible production of artificial intelligence (AI). This paper reviews recent research on algorithmic bias and proposes increased engagement of psychological and social science research to understand antecedents and consequences of algorithmic bias. Through the lens of the 3-D Dependable AI Framework, this article explores how social science disciplines, such as psychology, can contribute to identifying and mitigating bias at the Design, Develop, and Deploy stages of the AI life cycle. Finally, we propose future research directions to further address the complexities of algorithmic bias and its societal implications."
"ABCs: Differentiating Algorithmic Bias, Automation Bias, and Automation Complacency",2023,Amanda Potasznik,,6,W4381893935,10.1109/ethics57328.2023.10155094,https://openalex.org/W4381893935,,Terminology; Conflation; Automation; Documentation; Meaning (existential),article,False,"Algorithmic bias, automation bias, and automation complacency have been identified as culprits of a variety of human-computer interaction missteps, ethical offenses, and societal harms. However, these three terms are often mistaken and conflated. Students and professionals alike may have difficulty differentiating between the terms and fully understanding the impact of such psychological phenomena on their work and research. A review of relevant literature is conducted in order to establish an overview of historical documentation and analysis of the underlying themes; definitions and examples of each concept are then synthesized in order to provide a holistic understanding of the meaning behind this set of terminology."
"Evidence of What, for Whom? The Socially Contested Role of Algorithmic Bias in a Predictive Policing Tool",2024,Marta Ziosi; Dasha Pruss,,8,W4399365130,10.1145/3630106.3658991,https://openalex.org/W4399365130,https://dl.acm.org/doi/pdf/10.1145/3630106.3658991,Cognitive reframing; Criminal justice; Psychological intervention; Context (archaeology); Politics,article,True,"This paper presents a critical, qualitative study of the social role of algorithmic bias in the context of the Chicago crime prediction algorithm, a predictive policing tool that forecasts when and where in the city crime is most likely to occur. Through interviews with 18 Chicago-area community organizations, academic researchers, and public sector actors, we show that stakeholders from different groups articulate diverse problem diagnoses of the tool's algorithmic bias, strategically using it as evidence to advance criminal justice interventions that align with stakeholders' positionality and political ends. Drawing inspiration from Catherine D'Ignazio's taxonomy of ""refusing and using"" data, we find that stakeholders use evidence of algorithmic bias to reform the policies around police patrol allocation; reject algorithm-based policing interventions; reframe crime as a structural rather than interpersonal problem; reveal data on authority figures in an effort to subvert their power; repair and heal families and communities; and, in the case of more powerful actors, to reaffirm their own authority or existing power structures. We identify the implicit assumptions and scope of these varied uses of algorithmic bias as evidence, showing that they require different (and sometimes conflicting) values about policing and AI. This divergence reflects long-standing tensions in the criminal justice reform landscape between the values of liberation and healing often centered by system-impacted communities and the values of surveillance and deterrence often instantiated in data-driven reform measures. We advocate for centering the interests and experiential knowledge of communities impacted by incarceration to ensure that evidence of algorithmic bias can serve as a device to challenge the status quo."
Understanding algorithm bias in artificial intelligence-enabled ERP software customization,2023,S. Parthasarathy; S. Padmapriya,Journal of Ethics in Entrepreneurship and Technology,6,W4381376758,10.1108/jeet-04-2023-0006,https://openalex.org/W4381376758,https://www.emerald.com/insight/content/doi/10.1108/JEET-04-2023-0006/full/pdf?title=understanding-algorithm-bias-in-artificial-intelligence-enabled-erp-software-customization,Personalization; Computer science; Enterprise resource planning; Software; Artificial intelligence,article,True,"Purpose Algorithm bias refers to repetitive computer program errors that give some users more weight than others. The aim of this article is to provide a deeper insight of algorithm bias in AI-enabled ERP software customization. Although algorithmic bias in machine learning models has uneven, unfair and unjust impacts, research on it is mostly anecdotal and scattered. Design/methodology/approach As guided by the previous research (Akter et al. , 2022), this study presents the possible design bias (model, data and method) one may experience with enterprise resource planning (ERP) software customization algorithm. This study then presents the artificial intelligence (AI) version of ERP customization algorithm using k-nearest neighbours algorithm. Findings This study illustrates the possible bias when the prioritized requirements customization estimation (PRCE) algorithm available in the ERP literature is executed without any AI. Then, the authors present their newly developed AI version of the PRCE algorithm that uses ML techniques. The authors then discuss its adjoining algorithmic bias with an illustration. Further, the authors also draw a roadmap for managing algorithmic bias during ERP customization in practice. Originality/value To the best of the authors’ knowledge, no prior research has attempted to understand the algorithmic bias that occurs during the execution of the ERP customization algorithm (with or without AI)."
"Whither bias goes, I will go: An integrative, systematic review of algorithmic bias mitigation.",2024,Louis Hickman; Christopher Huynh; Jessica Gass; Brandon M. Booth; Jason Kuruzovich; Louis Tay,Journal of Applied Psychology,6,W4404355635,10.1037/apl0001255,https://openalex.org/W4404355635,https://arxiv.org/pdf/2410.19003,Selection bias; Psychology; Selection (genetic algorithm); Computer science; Management science,review,True,"Machine learning (ML) models are increasingly used for personnel assessment and selection (e.g., resume screeners, automatically scored interviews). However, concerns have been raised throughout society that ML assessments may be biased and perpetuate or exacerbate inequality. Although organizational researchers have begun investigating ML assessments from traditional psychometric and legal perspectives, there is a need to understand, clarify, and integrate fairness operationalizations and algorithmic bias mitigation methods from the computer science, data science, and organizational research literature. We present a four-stage model of developing ML assessments and applying bias mitigation methods, including (a) generating the training data, (b) training the model, (c) testing the model, and (d) deploying the model. When introducing the four-stage model, we describe potential sources of bias and unfairness at each stage. Then, we systematically review definitions and operationalizations of algorithmic bias, legal requirements governing personnel selection from the United States and Europe, and research on algorithmic bias mitigation across multiple domains and integrate these findings into our framework. Our review provides insights for both research and practice by elucidating possible mechanisms of algorithmic bias while identifying which bias mitigation methods are legal and effective. This integrative framework also reveals gaps in the knowledge of algorithmic bias mitigation that should be addressed by future collaborative research between organizational researchers, computer scientists, and data scientists. We provide recommendations for developing and deploying ML assessments, as well as recommendations for future research into algorithmic bias and fairness. (PsycInfo Database Record (c) 2025 APA, all rights reserved)."
(Some) algorithmic bias as institutional bias,2023,Camila Hernandez Flowerman,Ethics and Information Technology,3,W4328108935,10.1007/s10676-023-09698-7,https://openalex.org/W4328108935,,Computer science; Positive economics; Mathematical economics; Econometrics; Epistemology,article,False,
Artificial intelligence and algorithmic bias? Field tests on social network with teens,2024,Grazia Cecere; Clara Jean; Fabrice Le Guel; Matthieu Manant,Technological Forecasting and Social Change,9,W4391074079,10.1016/j.techfore.2023.123204,https://openalex.org/W4391074079,,Gender bias; Categorization; Visibility; Computer science; Field (mathematics),article,False,
Detection and Mitigation of Algorithmic Bias via Predictive Parity,2023,Cyrus DiCiccio; Brian Hsu; YinYin Yu; Preetam Nandy; Kinjal Basu,,6,W4379089795,10.1145/3593013.3594117,https://openalex.org/W4379089795,,Computer science; Parametric statistics; Parity (physics); Calibration; Transformation (genetics),article,False,"Predictive parity (PP), also known as sufficiency, is a core definition of algorithmic fairness essentially stating that model outputs must have the same interpretation of expected outcomes regardless of group. Testing and satisfying PP is especially important in many settings where model scores are interpreted by humans or directly provide access to opportunity, such as healthcare or banking. Solutions for PP violations have primarily been studied through the lens of model calibration. However, we find that existing calibration-based tests and mitigation methods are designed for independent data, which is often not assumable in large-scale applications such as social media or medical testing. In this work, we address this issue by developing a statistically rigorous non-parametric regression based test for PP with dependent observations. We then apply our test to illustrate that PP testing can significantly vary under the two assumptions. Lastly, we provide a mitigation solution to provide a minimally-biased post-processing transformation function to achieve PP."
AI and Discrimination: Sources of Algorithmic Biases,2024,Sara Moussawi; Xuefei Deng; K.D. Joshi,ACM SIGMIS Database the DATABASE for Advances in Information Systems,4,W4403624891,10.1145/3701613.3701615,https://openalex.org/W4403624891,,Harm; Computer science; Context (archaeology); Scholarship; Process (computing),article,False,"In this editorial, we define discrimination in the context of AI algorithms by focusing on understanding the biases arising throughout the lifecycle of building algorithms: input data for training, the process of algorithm development, and algorithm execution and usage. We draw insights from a few empirical studies to illustrate biases codified in algorithms that could result in harmful outcomes. We call on information systems scholars to prioritize scholarship in the area of algorithmic discrimination that can help generate new knowledge systems that would help safeguard against widespread and unaccountable harm."
Awareness of Racial and Ethnic Bias and Potential Solutions to Address Bias With Use of Health Care Algorithms,2023,Anjali Jain; Jasmin R. Brooks; Cleothia C. Alford; Christine Chang; Nora Mueller; Craig A. Umscheid; Arlene S. Bierman,JAMA Health Forum,56,W4379094721,10.1001/jamahealthforum.2023.1197,https://openalex.org/W4379094721,https://jamanetwork.com/journals/jama-health-forum/articlepdf/2805595/jain_2023_oi_230028_1685467552.69689.pdf,Ethnic group; Health care; Algorithm; Affect (linguistics); Health information technology,article,True,"Importance Algorithms are commonly incorporated into health care decision tools used by health systems and payers and thus affect quality of care, access, and health outcomes. Some algorithms include a patient’s race or ethnicity among their inputs and can lead clinicians and decision-makers to make choices that vary by race and potentially affect inequities. Objective To inform an evidence review on the use of race- and ethnicity-based algorithms in health care by gathering public and stakeholder perspectives about the repercussions of and efforts to address algorithm-related bias. Design, Setting, and Participants Qualitative methods were used to analyze responses. Responses were initially open coded and then consolidated to create a codebook, with themes and subthemes identified and finalized by consensus. This qualitative study was conducted from May 4, 2021, through December 7, 2022. Forty-two organization representatives (eg, clinical professional societies, universities, government agencies, payers, and health technology organizations) and individuals responded to the request for information. Main Outcomes and Measures Identification of algorithms with the potential for race- and ethnicity-based biases and qualitative themes. Results Forty-two respondents identified 18 algorithms currently in use with the potential for bias, including, for example, the Simple Calculated Osteoporosis Risk Estimation risk prediction tool and the risk calculator for vaginal birth after cesarean section. The 7 qualitative themes, with 31 subthemes, included the following: (1) algorithms are in widespread use and have significant repercussions, (2) bias can result from algorithms whether or not they explicitly include race, (3) clinicians and patients are often unaware of the use of algorithms and potential for bias, (4) race is a social construct used as a proxy for clinical variables, (5) there is a lack of standardization in how race and social determinants of health are collected and defined, (6) bias can be introduced at all stages of algorithm development, and (7) algorithms should be discussed as part of shared decision-making between the patient and clinician. Conclusions and Relevance This qualitative study found that participants perceived widespread and increasing use of algorithms in health care and lack of oversight, potentially exacerbating racial and ethnic inequities. Increasing awareness for clinicians and patients and standardized, transparent approaches for algorithm development and implementation may be needed to address racial and ethnic biases related to algorithms."
Echo Chambers and Algorithmic Bias: The Homogenization of Online Culture in a Smart Society,2024,Salsa Della Guitara Putri; Eko Priyo Purnomo; Tiara Khairunissa,SHS Web of Conferences,5,W4404386672,10.1051/shsconf/202420205001,https://openalex.org/W4404386672,https://doi.org/10.1051/shsconf/202420205001,Homogenization (climate); Echo (communications protocol); Computer science; Materials science; Art,article,True,"The rise of smart societies, characterized by extensive use of technology and data-driven algorithms, promises to improve our lives. However, this very technology presents a potential threat to the richness and diversity of online culture. This thesis explores the phenomenon of echo chambers and algorithmic bias, examining how they contribute to the homogenization of online experiences. Social media algorithms personalize content feeds, presenting users with information that reinforces their existing beliefs. This creates echo chambers, where users are isolated from diverse viewpoints. Algorithmic bias, stemming from the data used to train these algorithms, can further exacerbate this issue. The main data in this study were sourced from previous studies (secondary data) which focused on research related homogenizing on online culture. The thesis investigates the impact of echo chambers and algorithmic bias on online culture within smart societies. It explores how these factors limit exposure to a variety of ideas and perspectives, potentially leading to a homogenized online experience. By examining the interplay between echo chambers, algorithmic bias, and the homogenization of online culture in smart societies, this thesis aims to contribute to a more nuanced understanding of the impact of technology on our online experiences."
Identifying and mitigating algorithmic bias in the safety net,2025,Shaina Mackin; Vincent J. Major; Rumi Chunara; Remle Newton-Dame,npj Digital Medicine,4,W4411067641,10.1038/s41746-025-01732-w,https://openalex.org/W4411067641,https://www.nature.com/articles/s41746-025-01732-w.pdf,Computer science; Risk analysis (engineering); Business,article,True,"Algorithmic bias occurs when predictive model performance varies meaningfully across sociodemographic classes, exacerbating systemic healthcare disparities. NYC Health + Hospitals, an urban safety net system, assessed bias in two binary classification models in our electronic medical record: one predicting acute visits for asthma and one predicting unplanned readmissions. We evaluated differences in subgroup performance across race/ethnicity, sex, language, and insurance using equal opportunity difference (EOD), a metric comparing false negative rates. The most biased classes (race/ethnicity for asthma, insurance for readmission) were targeted for mitigation using threshold adjustment, which adjusts subgroup thresholds to minimize EOD, and reject option classification, which re-classifies scores near the threshold by subgroup. Successful mitigation was defined as 1) absolute subgroup EODs <5 percentage points, 2) accuracy reduction <10%, and 3) alert rate change <20%. Threshold adjustment met these criteria; reject option classification did not. We introduce a Supplementary Playbook outlining our approach for low-resource bias mitigation."
Artificial Intelligence in Criminalistics and Forensic Examination: Issues of Legal Personality and Algorithmic Bias,2023,А. В. Кокин; Yu. D. Denisov,Theory and Practice of Forensic Science,6,W4385650910,10.30764/1819-2785-2023-2-30-37,https://openalex.org/W4385650910,https://www.tipse.ru/jour/article/download/775/672,Computer science; Artificial intelligence; Personality; Human intelligence; Identification (biology),article,True,"Active development and implementation of artificial intelligence technologies (AI) in various spheres of human activity have started the processes of qualitative change in public relations. This fact necessitates the development of legal and technical standards to regulate AI technologies. In this regard, the most controversial issue is the recognition of AI personality. The analysis of various opinions on the matter shows the lack of a consolidated approach in the existing legal doctrine. Creating the legal status for AI systems would provide for several options depending on its type and purpose – from technical means to the status of an “electronic personality” and recognition as a full-fledged subject of law. Considering the specifics of criminalistics and forensic examination, it is better to position AI systems as technical means. Machine learning is considered a form of AI. It is the use of mathematical data models that enables computer training through specialized algorithms and training data. Algorithms can create or reproduce distortions and inaccuracies unintentionally embedded in the training data, which causes the manifestation of algorithmic bias. To eliminate bias of algorithms it is necessary to pay attention to the quality of training data. The author has developed special methods to prepare such data, which are presented in this article in relation to ballistic identification systems. Also, one of the elements of system technical solutions to the problem of bias of AI algorithms is the development of standards for minimizing unjustified bias in algorithmic solutions."
Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices,2023,Manish Raghavan,ACM eBooks,66,W2972445641,10.1145/3603195.3603203,https://openalex.org/W2972445641,,Citation; Computer science; Operations research; Engineering; Data science,book-chapter,False,"chapter Share on Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices Author: Manish Raghavan Massachusetts Institute of Technology, Sloan School of Management and Department of Electrical Engineering and Computer Science Massachusetts Institute of Technology, Sloan School of Management and Department of Electrical Engineering and Computer ScienceView Profile Authors Info & Claims The Societal Impacts of Algorithmic Decision-MakingSeptember 2023https://doi.org/10.1145/3603195.3603203Published:08 September 2023Publication History 0citation0DownloadsMetricsTotal Citations0Total Downloads0Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access"
Algorithmic Bias in Criminal Risk Assessment: The Consequences of Racial Differences in Arrest as a Measure of Crime,2024,Roland Neil; Michael Zanger-Tishler,Annual Review of Criminology,6,W4402250311,10.1146/annurev-criminol-022422-125019,https://openalex.org/W4402250311,https://doi.org/10.1146/annurev-criminol-022422-125019,Measure (data warehouse); Criminology; Psychology; Actuarial science; Computer science,article,True,"There is great concern about algorithmic racial bias in the risk assessment instruments (RAIs) used in the criminal legal system. When testing for algorithmic bias, most research effectively uses arrest data as an unbiased measure of criminal offending, which collides with longstanding concerns that arrest is a biased proxy of offending. Given the centrality of arrest data in RAIs, racial differences in how arrest proxies offending may be a key pathway through which RAIs become biased. In this review, we evaluate the extensive body of research on racial differences in arrest as a measure of crime. Furthermore, we detail several ways that racial bias in arrest records could create algorithmic bias, although little research has attempted to measure the degree of algorithmic bias generated by using racially biased arrest records. We provide a roadmap to assist future research in understanding the impact of biased arrest records on RAIs."
The Sociodemographic Biases in Machine Learning Algorithms: A Biomedical Informatics Perspective,2024,Gillian Franklin; Rachel Stephens; Muhammad Piracha; Shmuel Tiosano; Frank LeHouillier; Ross Koppel; Peter L. Elkin,Life,38,W4398169659,10.3390/life14060652,https://openalex.org/W4398169659,https://www.mdpi.com/2075-1729/14/6/652/pdf?version=1716284761,Socioeconomic status; Machine learning; Health care; Computer science; Selection bias,article,True,"Artificial intelligence models represented in machine learning algorithms are promising tools for risk assessment used to guide clinical and other health care decisions. Machine learning algorithms, however, may house biases that propagate stereotypes, inequities, and discrimination that contribute to socioeconomic health care disparities. The biases include those related to some sociodemographic characteristics such as race, ethnicity, gender, age, insurance, and socioeconomic status from the use of erroneous electronic health record data. Additionally, there is concern that training data and algorithmic biases in large language models pose potential drawbacks. These biases affect the lives and livelihoods of a significant percentage of the population in the United States and globally. The social and economic consequences of the associated backlash cannot be underestimated. Here, we outline some of the sociodemographic, training data, and algorithmic biases that undermine sound health care risk assessment and medical decision-making that should be addressed in the health care system. We present a perspective and overview of these biases by gender, race, ethnicity, age, historically marginalized communities, algorithmic bias, biased evaluations, implicit bias, selection/sampling bias, socioeconomic status biases, biased data distributions, cultural biases and insurance status bias, conformation bias, information bias and anchoring biases and make recommendations to improve large language model training data, including de-biasing techniques such as counterfactual role-reversed sentences during knowledge distillation, fine-tuning, prefix attachment at training time, the use of toxicity classifiers, retrieval augmented generation and algorithmic modification to mitigate the biases moving forward."
Putting algorithmic bias on top of the agenda in the discussions on autonomous weapons systems,2024,Ishmael Bhila,Digital War,5,W4399209593,10.1057/s42984-024-00094-z,https://openalex.org/W4399209593,https://link.springer.com/content/pdf/10.1057/s42984-024-00094-z.pdf,Computer science; Political science; Computer security; Epistemology; Philosophy,article,True,"Abstract Biases in artificial intelligence have been flagged in academic and policy literature for years. Autonomous weapons systems—defined as weapons that use sensors and algorithms to select, track, target, and engage targets without human intervention—have the potential to mirror systems of societal inequality which reproduce algorithmic bias. This article argues that the problem of engrained algorithmic bias poses a greater challenge to autonomous weapons systems developers than most other risks discussed in the Group of Governmental Experts on Lethal Autonomous Weapons Systems (GGE on LAWS), which should be reflected in the outcome documents of these discussions. This is mainly because it takes longer to rectify a discriminatory algorithm than it does to issue an apology for a mistake that occurs occasionally. Highly militarised states have controlled both the discussions and their outcomes, which have focused on issues that are pertinent to them while ignoring what is existential for the rest of the world. Various calls from civil society, researchers, and smaller states for a legally binding instrument to regulate the development and use of autonomous weapons systems have always included the call for recognising algorithmic bias in autonomous weapons, which has not been reflected in discussion outcomes. This paper argues that any ethical framework developed for the regulation of autonomous weapons systems should, in detail, ensure that the development and use of autonomous weapons systems do not prejudice against vulnerable sections of (global) society."
Tackling Algorithmic Bias and Promoting Transparency in Health Datasets: The STANDING Together Consensus Recommendations,2024,Joseph Alderman; Joanne Palmer; Elinor Laws; Melissa D. McCradden; Johan Ordish; Marzyeh Ghassemi; Stephen Pfohl; Negar Rostamzadeh; Heather Cole-Lewis; Ben Glocker; Melanie Calvert; Tom Pollard; Jaspret Gill; Jacqui Gath; Ade Adebajo; Jude Beng; Cheuk Wing Leung; Stephanie Kuku; L. J. Farmer; Rubeta Matin; Bilal A. Mateen; Francis McKay; Katherine Heller; Alan Karthikesalingam; Darren Treanor; Maxine Mackintosh; Lauren Oakden‐Rayner; Russell Pearson; Arjun K. Manrai; Puja Myles; Judit Kumuthini; Zoher Kapacee; Neil J. Sebire; Lama Nazer; Jarrel Seah; Ashley Akbari; Lewis E. Berman; Judy Wawira Gichoya; Lorenzo Righetto; Diana Samuel; William Wasswa; Maria Charalambides; Anmol Arora; Sameer Pujari; Charlotte Summers; Elizabeth Sapey; Stephen Wilkinson; Vishal Thakker; Alastair K. Denniston; Xiaoxuan Liu,NEJM AI,10,W4405533437,10.1056/aip2401088,https://openalex.org/W4405533437,,Transparency (behavior); Data science; Computer science; Internet privacy; Computer security,article,False,
Postcolonial Differentials in Algorithmic Bias: Challenging Digital Neo-Colonialism in Africa,2023,Sunita Menon,SCRIPTed A Journal of Law Technology & Society,5,W4386867404,10.2218/scrip.20.2.2023.8980,https://openalex.org/W4386867404,http://journals.ed.ac.uk/script-ed/article/download/8980/11926,Colonialism; Cognitive reframing; Parallels; Neocolonialism; Neutrality,article,True,"As digital technologies become the dominant driver of the global economy, Africa finds itself once again faced with the prospect of developmental stagnation. In an increasingly technological age, parallels to the colonial era can be made, particularly in reference to the detrimental impact on the African economy and the continent’s developmental trajectory. AI, which drives these technologies, is informed by algorithms. The biases inherent in these algorithms lead to digital discrimination. This discrimination has resulted in a new form of colonialism, referred to as digital neocolonialism, which denotes the exclusionary barrier that has been created by algorithms. This work challenges algorithmic bias through the application of postcolonial theory, which calls for a dismantling of colonial imposition by reimagining and reframing the concept of the ‘other’. The gaps in current AI systems, and the power imbalances created, are interrogated through an analysis of bias and its impact. Through a postcolonial lens, a call is made for more inclusive AI systems, and datasets that challenges the assumed neutrality of algorithms."
The formal rationality of artificial intelligence-based algorithms and the problem of bias,2023,Rohit Nishant; Dirk Schneckenberg; M. N. Ravishankar,Journal of Information Technology,61,W4378672952,10.1177/02683962231176842,https://openalex.org/W4378672952,https://journals.sagepub.com/doi/pdf/10.1177/02683962231176842,Rationality; Bounded rationality; Computer science; Artificial intelligence; Context (archaeology),article,True,"This paper presents a new perspective on the problem of bias in artificial intelligence (AI)-driven decision-making by examining the fundamental difference between AI and human rationality in making sense of data. Current research has focused primarily on software engineers’ bounded rationality and bias in the data fed to algorithms but has neglected the crucial role of algorithmic rationality in producing bias. Using a Weberian distinction between formal and substantive rationality, we inquire why AI-based algorithms lack the ability to display common sense in data interpretation, leading to flawed decisions. We first conduct a rigorous text analysis to uncover and exemplify contextual nuances within the sampled data. We then combine unsupervised and supervised learning, revealing that algorithmic decision-making characterizes and judges data categories mechanically as it operates through the formal rationality of mathematical optimization procedures. Next, using an AI tool, we demonstrate how formal rationality embedded in AI-based algorithms limits its capacity to perform adequately in complex contexts, thus leading to bias and poor decisions. Finally, we delineate the boundary conditions and limitations of leveraging formal rationality to automatize algorithmic decision-making. Our study provides a deeper understanding of the rationality-based causes of AI’s role in bias and poor decisions, even when data is generated in a largely bias-free context."
Sociodemographic bias in clinical machine learning models: a scoping review of algorithmic bias instances and mechanisms,2024,Michael Colacci; Yu Qing Huang; Gemma Postill; Pavel Zhelnov; Orna Fennelly; Amol A. Verma; Sharon E. Straus; Andrea C. Tricco,Journal of Clinical Epidemiology,7,W4404210001,10.1016/j.jclinepi.2024.111606,https://openalex.org/W4404210001,https://doi.org/10.1016/j.jclinepi.2024.111606,Machine learning; Artificial intelligence; Computer science; Psychology; MEDLINE,review,True,"Most ML algorithms that were evaluated for bias demonstrated bias on sociodemographic factors. Furthermore, most bias evaluations concentrated on race, sex/gender, and age, while other sociodemographic factors and their intersection were infrequently assessed. Given potential health equity implications, bias assessments should be completed for all clinical ML models."
"Algorithmic bias, generalist models, and clinical medicine",2023,Geoff Keeling,AI and Ethics,3,W4385806884,10.1007/s43681-023-00329-x,https://openalex.org/W4385806884,,Generalist and specialist species; Computer science; Data science; Artificial intelligence; Machine learning,article,False,
Hierarchical Dependencies in Classroom Settings Influence Algorithmic Bias Metrics,2024,Clara Belitz; HaeJin Lee; Nidhi Nasiar; Stephen E. Fancsali; STEVE RITTER; Husni Almoubayyed; Ryan S. Baker; Jaclyn Ocumpaugh; Nigel Bosch,,4,W4392445400,10.1145/3636555.3636869,https://openalex.org/W4392445400,https://dl.acm.org/doi/pdf/10.1145/3636555.3636869,Computer science; Multilevel model; Machine learning; Artificial intelligence; Contrast (vision),article,True,"Measuring algorithmic bias in machine learning has historically focused on statistical inequalities pertaining to specific groups. However, the most common metrics (i.e., those focused on individual- or group-conditioned error rates) are not currently well-suited to educational settings because they assume that each individual observation is independent from the others. This is not statistically appropriate when studying certain common educational outcomes, because such metrics cannot account for the relationship between students in classrooms or multiple observations per student across an academic year. In this paper, we present novel adaptations of algorithmic bias measurements for regression for both independent and nested data structures. Using hierarchical linear models, we rigorously measure algorithmic bias in a machine learning model of the relationship between student engagement in an intelligent tutoring system and year-end standardized test scores. We conclude that classroom-level influences had a small but significant effect on models. Examining significance with hierarchical linear models helps determine which inequalities in educational settings might be explained by small sample sizes rather than systematic differences."
Prevention of Bias and Discrimination in Clinical Practice Algorithms,2023,Carmel Shachar; Sara Gerke,JAMA,48,W4313545847,10.1001/jama.2022.23867,https://openalex.org/W4313545847,,Medicine; Liability; Health care; Algorithm; MEDLINE,article,False,This Viewpoint discusses a proposed DHHS rule to address discrimination in clinical algorithms and the need for additional considerations to ensure the burden of liability for biased algorithms is not disproportionately placed on health care professionals.
Social media and volunteer rescue requests prediction with random forest and algorithm bias detection: a case of Hurricane Harvey,2023,Volodymyr Mihunov; Kejin Wang; Zheye Wang; Nina Lam; Mingxuan Sun,Environmental Research Communications,10,W4380537468,10.1088/2515-7620/acde35,https://openalex.org/W4380537468,https://iopscience.iop.org/article/10.1088/2515-7620/acde35/pdf,Random forest; Flood myth; Stalking; Index (typography); Computer science,article,True,"Abstract AI fairness is tasked with evaluating and mitigating bias in algorithms that may discriminate towards protected groups. This paper examines if bias exists in AI algorithms used in disaster management and in what manner. We consider the 2017 Hurricane Harvey when flood victims in Houston resorted to social media to request for rescue. We evaluate a Random Forest regression model trained to predict Twitter rescue request rates from social-environmental data using three fairness criteria (independence, separation, and sufficiency). The Social Vulnerability Index (SVI), its four sub-indices, and four variables representing digital divide were considered sensitive attributes. The Random Forest regression model extracted seven significant predictors of rescue request rates, and from high to low importance they were percent of renter occupied housing units, percent of roads in flood zone, percent of flood zone area, percent of wetland cover, percent of herbaceous, forested and shrub cover, mean elevation, and percent of households with no computer or device. Partial Dependence plots of rescue request rates against each of the seven predictors show the non-linear nature of their relationships. Results of the fairness evaluation of the Random Forest model using the three criteria show no obvious biases for the nine sensitive attributes, except that a minor imperfect sufficiency was found with the SVI Housing and Transportation sub-index. Future AI modeling in disaster research could apply the same methodology used in this paper to evaluate fairness and help reduce unfair resource allocation and other social and geographical disparities."
Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation,2023,Hao Liang; Pietro Perona; Guha Balakrishnan,,8,W4390872687,10.1109/iccv51070.2023.00459,https://openalex.org/W4390872687,,Benchmarking; Computer science; Artificial intelligence; Facial recognition system; Face (sociological concept),article,False,"We propose an experimental method for measuring bias in face recognition systems. Existing methods to measure bias depend on benchmark datasets that are collected in the wild and annotated for protected (e.g., race, gender) and unprotected (e.g., pose, lighting) attributes. Such observational datasets only permit correlational conclusions, e.g., ""Algorithm A's accuracy is different on female and male faces in dataset X."". By contrast, experimental methods manipulate attributes individually and thus permit causal conclusions, e.g., ""Algorithm A's accuracy is affected by gender and skin color.""Our method is based on generating synthetic faces using a neural face generator, where each attribute of interest is modified independently while leaving all other attributes constant. Human observers crucially provide the ground truth on perceptual identity similarity between synthetic image pairs. We validate our method quantitatively by evaluating race and gender biases of three research-grade face recognition models. Our synthetic pipeline reveals that for these algorithms, accuracy is lower for Black and East Asian population subgroups. Our method can also quantify how perceptual changes in attributes affect face identity distances reported by these models. Our large synthetic dataset, consisting of 48,000 synthetic face image pairs (10,200 unique synthetic faces) and 555,000 human annotations (individual attributes and pairwise identity comparisons) is available to researchers in this important area."
Race Correction and Algorithmic Bias in Atrial Fibrillation Wearable Technologies,2023,Beza Merid; Vanessa V. Volpe,Health Equity,6,W4389166084,10.1089/heq.2023.0034,https://openalex.org/W4389166084,https://doi.org/10.1089/heq.2023.0034,Race (biology); Harm; Construct (python library); Race and health; Health care,article,True,"Stakeholders in biomedicine are evaluating how race corrections in clinical algorithms inequitably allocate health care resources on the basis of a misunderstanding of race-as-genetic difference. Ostensibly used to intervene on persistent disparities in health outcomes across different racial groups, these troubling corrections in risk assessments embed essentialist ideas of race as a biological reality, rather than a social and political construct that reproduces a racial hierarchy, into practice guidelines. This article explores the harms of such race corrections by considering how the technologies we use to account for disparities in health outcomes can actually innovate and amplify these harms. Focusing on the design of wearable digital health technologies that use photoplethysmographic sensors to detect atrial fibrillation, we argue that these devices, which are notoriously poor in accurately functioning on users with darker skin tones, embed a subtle form of race correction that presupposes the need for explicit adjustments in the clinical interpretation of their data outputs. We point to research on responsible innovation in health, and its commitment to being responsive in addressing inequities and harms, as a way forward for those invested in the elimination of race correction."
Algorithmic Bias in News: Can Machine Learning Be Part of the Solution?,2023,Haffaz Aladeen; Nadal Burgers; Tamir Imaad,,6,W4322724920,10.22541/au.167770658.88067473/v1,https://openalex.org/W4322724920,,Computer science; Artificial intelligence; Machine learning; Data science,preprint,False,
Algorithmic Bias of Social Media,2023,Daman Preet Singh,The Motley Undergraduate Journal,3,W4388980952,10.55016/ojs/muj.v1i2.77457,https://openalex.org/W4388980952,https://journalhosting.ucalgary.ca/index.php/muj/article/download/77457/57007,Popularity; Social media; Visibility; Internet privacy; Content (measure theory),article,True,"Social media apps like YouTube and Instagram came as platforms that allowed users to express themselves freely to their friends and families, but corporations changed social media down to its core. Due to the rising popularity of short video-based content on TikTok, platforms like Instagram introduced similar content to capitalize on the hype that TikTok created. In doing so, Instagram made changes to the content promotion algorithm to promote “Reels” over the other content options. Driven by profits the company stopped caring about their users, leading to backlash from the community. Creators on the platform started playing a visibility game (Cotter, 2019) to grow and be seen in user feeds, the “game” pushes them to make content they would not be making in the first place and following trends. In this paper I am looking at the case of a creator in the photography community affected by these changes in algorithm and analyzing the situation through a critical media theory framework. The study discusses the practices of the platform and the effects on the creator community while also looking at resistance from users. I also discuss a new potential alternative platform to Instagram for photographers, that markets itself as a platform built without an algorithm, for a community."
Addressing Algorithmic Bias in AI‐Driven HRM Systems: Implications for Strategic HRM Effectiveness,2025,Ruwan Bandara; Kumar Biswas; Shahriar Akter; Sujana Shafique; Mahfuzur Rahman,Human Resource Management Journal,5,W4410793547,10.1111/1748-8583.12609,https://openalex.org/W4410793547,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/1748-8583.12609,Business; Human resource management; Industrial organization; Knowledge management; Operations management,article,True,"ABSTRACT AI and machine learning algorithms are revolutionising the modern workplace by transforming HR functions to deliver superior outcomes for both employees and organisations. However, research shows that these algorithms often fail to deliver optimal HR solutions, primarily due to inherent biases. Developing capabilities to overcome algorithmic biases is critical for firms, as these biases present significant challenges to fairness and inclusivity in HR decision‐making, ultimately impacting the effectiveness of HR practices. To address this challenge, our study, grounded in the dynamic capability perspective, presents a model to address algorithmic biases in people management and achieve superior strategic HR outcomes. To test our theoretical model, we collected survey data using a two‐wave, time‐lagged approach from HR professionals and employees working in firms within the Australian financial and insurance industries. The key findings reveal three critical dimensions of HR algorithmic bias management capability: data bias, model bias, and deployment bias management capabilities, which significantly influence AI‐enabled high‐performance HR practices and, in turn, positively impact strategic HRM effectiveness. Our novel findings on the dimensions of HR bias management capability contribute to advancing the dynamic capability view in HRM research. They also offer a comprehensive bias management framework that allows HR professionals to address the strategic, ethical, and operational challenges emerging from the use of AI‐augmented HR practices in the dynamic workplace, helping sustain a competitive advantage."
A systematic review of socio-technical gender bias in AI algorithms,2023,Paula Hall; Debbie Ellis,Online Information Review,55,W4327611086,10.1108/oir-08-2021-0452,https://openalex.org/W4327611086,,Identification (biology); Computer science; Originality; Process (computing); Systematic review,review,False,"Purpose Gender bias in artificial intelligence (AI) should be solved as a priority before AI algorithms become ubiquitous, perpetuating and accentuating the bias. While the problem has been identified as an established research and policy agenda, a cohesive review of existing research specifically addressing gender bias from a socio-technical viewpoint is lacking. Thus, the purpose of this study is to determine the social causes and consequences of, and proposed solutions to, gender bias in AI algorithms. Design/methodology/approach A comprehensive systematic review followed established protocols to ensure accurate and verifiable identification of suitable articles. The process revealed 177 articles in the socio-technical framework, with 64 articles selected for in-depth analysis. Findings Most previous research has focused on technical rather than social causes, consequences and solutions to AI bias. From a social perspective, gender bias in AI algorithms can be attributed equally to algorithmic design and training datasets. Social consequences are wide-ranging, with amplification of existing bias the most common at 28%. Social solutions were concentrated on algorithmic design, specifically improving diversity in AI development teams (30%), increasing awareness (23%), human-in-the-loop (23%) and integrating ethics into the design process (21%). Originality/value This systematic review is the first of its kind to focus on gender bias in AI algorithms from a social perspective within a socio-technical framework. Identification of key causes and consequences of bias and the breakdown of potential solutions provides direction for future research and policy within the growing field of AI ethics. Peer review The peer review history for this article is available at https://publons.com/publon/10.1108/OIR-08-2021-0452"
Generative AI in Writing Research Papers: A New Type of Algorithmic Bias and Uncertainty in Scholarly Work,2024,Rishab Jain; Aditya Jain,Lecture notes in networks and systems,9,W4401132252,10.1007/978-3-031-66329-1_42,https://openalex.org/W4401132252,,Generative grammar; Computer science; Work (physics); Artificial intelligence; Data science,book-chapter,False,
Auditing Work: Exploring the New York City algorithmic bias audit regime,2024,Lara Groves; Jacob Metcalf; Alayna Kennedy; Briana Vecchione; Andrew Strait,arXiv (Cornell University),6,W4391833200,10.48550/arxiv.2402.08101,https://openalex.org/W4391833200,https://arxiv.org/pdf/2402.08101,Audit; Work (physics); Accounting; Business; Engineering,preprint,True,"In July 2023, New York City (NYC) initiated the first algorithm auditing system for commercial machine-learning systems. Local Law 144 (LL 144) mandates NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to undergo annual bias audits conducted by an independent auditor. This paper examines lessons from LL 144 for other national algorithm auditing attempts. Through qualitative interviews with 16 experts and practitioners within the regime, we find that LL 144 has not effectively established an auditing regime. The law fails to clearly define key aspects, such as AEDTs and independent auditors, leading auditors, AEDT vendors, and companies using AEDTs to define the law's practical implementation in ways that failed to protect job applicants. Contributing factors include the law's flawed transparency-driven theory of change, industry lobbying narrowing the definition of AEDTs, practical and cultural challenges faced by auditors in accessing data, and wide disagreement over what constitutes a legitimate auditor, resulting in four distinct 'auditor roles.' We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes, emphasizing clearer definitions, metrics, and increased accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, providing guidance for policymakers seeking to create similar regimes."
Effects of Racial Bias in Pulse Oximetry on Children and How to Address Algorithmic Bias in Clinical Medicine,2023,Keyaria D. Gray; Hamsa L. Subramaniam; Erich Huang,JAMA Pediatrics,7,W4327893915,10.1001/jamapediatrics.2023.0077,https://openalex.org/W4327893915,,Medicine; Otorhinolaryngology; Family medicine; Neurology; MEDLINE,letter,False,"Our website uses cookies to enhance your experience. By continuing to use our site, or clicking ""Continue,"" you are agreeing to our Cookie Policy | Continue JAMA Pediatrics HomeNew OnlineCurrent IssueFor Authors Podcast Journals JAMA JAMA Network Open JAMA Cardiology JAMA Dermatology JAMA Health Forum JAMA Internal Medicine JAMA Neurology JAMA Oncology JAMA Ophthalmology JAMA Otolaryngology–Head & Neck Surgery JAMA Pediatrics JAMA Psychiatry JAMA Surgery Archives of Neurology & Psychiatry (1919-1959) JN Learning / CMESubscribeJobsInstitutions / LibrariansReprints & Permissions Terms of Use | Privacy Policy | Accessibility Statement 2023 American Medical Association. All Rights Reserved Search All JAMA JAMA Network Open JAMA Cardiology JAMA Dermatology JAMA Forum Archive JAMA Health Forum JAMA Internal Medicine JAMA Neurology JAMA Oncology JAMA Ophthalmology JAMA Otolaryngology–Head & Neck Surgery JAMA Pediatrics JAMA Psychiatry JAMA Surgery Archives of Neurology & Psychiatry Input Search Term Sign In Individual Sign In Sign inCreate an Account Access through your institution Sign In Purchase Options: Buy this article Rent this article Subscribe to the JAMA Pediatrics journal"
Comprehending Algorithmic Bias and Strategies for Fostering Trust in Artificial Intelligence,2024,Sidhi Menon U; Theresa Siby; Natchimuthu Natchimuthu,Advances in web technologies and engineering book series,3,W4391644764,10.4018/979-8-3693-1762-4.ch014,https://openalex.org/W4391644764,,Prejudice (legal term); Selection bias; Computer science; Gender bias; Artificial intelligence,book-chapter,False,"Fairness is threatened by algorithm bias, systematic and unfair disparities in machine learning results. Amazon's AI-driven hiring tool favoured men. AI promised data-driven, impartial decision-making, but it has revealed sector-wide prejudice, perpetuating systematic imbalances. The algorithm's bias is data and design. Biassed historical data and feature selection and pre-processing can bias algorithms. Development is harmed by human biases. Algorithm prejudice impacts money, education, employment, and crime. Diverse and representative data collection, understanding complicated “black box” algorithms, and legal and ethical considerations are needed to address this bias. Despite these issues, algorithm bias elimination techniques are emerging. This chapter uses secondary data to study algorithm bias. Algorithm bias is defined, its origins, its prevalence in data, examples, and issues are discussed. The chapter also tackles bias reduction and elimination to make AI a more reliable and impartial decision-maker."
Pre-processing Techniques to Mitigate Against Algorithmic Bias,2023,Maliheh Heidarpour Shahrezaei; Róisín Loughran; Kevin Mc Daid,,4,W4392980986,10.1109/aics60730.2023.10470759,https://openalex.org/W4392980986,,Computer science,article,False,"A significant portion of current AI research is focused on ensuring that model decisions are fair and free of bias. Such research should consider not merely the algorithm but also the datasets, metrics and approaches used. In this paper, we work on several pre-processing techniques to achieve fair results for classification tasks by assigning weights, sampling and changing class labels. We used two well-known classifiers, Logistic Regression and Decision Tree, performing experiments on a popular data set in the fairness domain. This research aims to compare the effects of different pre-processing techniques on the resulting confusion matrix elements and the derived fairness metrics. We found that the Massaging technique with the Logistic regression classifier resulted in the Disparate Impact value that was closest to one. While, for the Decision Tree classifier, Reweighting and Uniform Sampling performed better than Massaging for all of our fairness metrics and both sensitive attributes."
Human bias in algorithm design,2023,Carey K. Morewedge; Sendhil Mullainathan; Haaya Naushan; Cass R. Sunstein; Jon Kleinberg; Manish Raghavan; J. Ludwig,Nature Human Behaviour,33,W4388834460,10.1038/s41562-023-01724-4,https://openalex.org/W4388834460,,Affect (linguistics); Computer science; Machine learning; Artificial intelligence; User modeling,article,False,
Algorithmic fairness and bias mitigation for clinical machine learning with deep reinforcement learning,2023,Jenny Yang; Andrew A. S. Soltan; David W. Eyre; David A. Clifton,Nature Machine Intelligence,83,W4385416124,10.1038/s42256-023-00697-3,https://openalex.org/W4385416124,https://www.nature.com/articles/s42256-023-00697-3.pdf,Reinforcement learning; Computer science; Reinforcement; Artificial intelligence; Computer security,article,True,"Abstract As models based on machine learning continue to be developed for healthcare applications, greater effort is needed to ensure that these technologies do not reflect or exacerbate any unwanted or discriminatory biases that may be present in the data. Here we introduce a reinforcement learning framework capable of mitigating biases that may have been acquired during data collection. In particular, we evaluated our model for the task of rapidly predicting COVID-19 for patients presenting to hospital emergency departments and aimed to mitigate any site (hospital)-specific and ethnicity-based biases present in the data. Using a specialized reward function and training procedure, we show that our method achieves clinically effective screening performances, while significantly improving outcome fairness compared with current benchmarks and state-of-the-art machine learning methods. We performed external validation across three independent hospitals, and additionally tested our method on a patient intensive care unit discharge status task, demonstrating model generalizability."
"What Goes In, Must Come Out: Generative Artificial Intelligence Does Not Present Algorithmic Bias Across Race and Gender in Medical Residency Specialties",2024,Lin Shu; Saket Pandit; Tara Tritsch; Arkene Levy; Mohammadali M. Shoja,Cureus,8,W4391930049,10.7759/cureus.54448,https://openalex.org/W4391930049,https://assets.cureus.com/uploads/original_article/pdf/215498/20240219-2672-fzq8af.pdf,Specialty; Transformative learning; Race (biology); Diversity (politics); Generative grammar,article,True,
Evaluating Algorithmic Bias in 30-Day Hospital Readmission Models: Retrospective Analysis,2024,H. Echo Wang; Jonathan P. Weiner; Suchi Saria; Hadi Kharrazi,Journal of Medical Internet Research,4,W4392314714,10.2196/47125,https://openalex.org/W4392314714,https://doi.org/10.2196/47125,Retrospective cohort study; Medicine; Computer science; Statistics; Emergency medicine,article,True,"Background The adoption of predictive algorithms in health care comes with the potential for algorithmic bias, which could exacerbate existing disparities. Fairness metrics have been proposed to measure algorithmic bias, but their application to real-world tasks is limited. Objective This study aims to evaluate the algorithmic bias associated with the application of common 30-day hospital readmission models and assess the usefulness and interpretability of selected fairness metrics. Methods We used 10.6 million adult inpatient discharges from Maryland and Florida from 2016 to 2019 in this retrospective study. Models predicting 30-day hospital readmissions were evaluated: LACE Index, modified HOSPITAL score, and modified Centers for Medicare &amp; Medicaid Services (CMS) readmission measure, which were applied as-is (using existing coefficients) and retrained (recalibrated with 50% of the data). Predictive performances and bias measures were evaluated for all, between Black and White populations, and between low- and other-income groups. Bias measures included the parity of false negative rate (FNR), false positive rate (FPR), 0-1 loss, and generalized entropy index. Racial bias represented by FNR and FPR differences was stratified to explore shifts in algorithmic bias in different populations. Results The retrained CMS model demonstrated the best predictive performance (area under the curve: 0.74 in Maryland and 0.68-0.70 in Florida), and the modified HOSPITAL score demonstrated the best calibration (Brier score: 0.16-0.19 in Maryland and 0.19-0.21 in Florida). Calibration was better in White (compared to Black) populations and other-income (compared to low-income) groups, and the area under the curve was higher or similar in the Black (compared to White) populations. The retrained CMS and modified HOSPITAL score had the lowest racial and income bias in Maryland. In Florida, both of these models overall had the lowest income bias and the modified HOSPITAL score showed the lowest racial bias. In both states, the White and higher-income populations showed a higher FNR, while the Black and low-income populations resulted in a higher FPR and a higher 0-1 loss. When stratified by hospital and population composition, these models demonstrated heterogeneous algorithmic bias in different contexts and populations. Conclusions Caution must be taken when interpreting fairness measures’ face value. A higher FNR or FPR could potentially reflect missed opportunities or wasted resources, but these measures could also reflect health care use patterns and gaps in care. Simply relying on the statistical notions of bias could obscure or underplay the causes of health disparity. The imperfect health data, analytic frameworks, and the underlying health systems must be carefully considered. Fairness measures can serve as a useful routine assessment to detect disparate model performances but are insufficient to inform mechanisms or policy changes. However, such an assessment is an important first step toward data-driven improvement to address existing health disparities."
Standards for the Control of Algorithmic Bias,2023,Natalie Heisler; Maura R. Grossman,,2,W4381735403,10.1201/b23364,https://openalex.org/W4381735403,,Computer science; Control (management); Artificial intelligence,book,False,"Governments around the world use machine learning in automated decision-making systems for a broad range of functions. However, algorithmic bias in machine learning can result in automated decisions that produce disparate impact and may compromise Charter guarantees of substantive equality. This book seeks to answer the question: what standards should be applied to machine learning to mitigate disparate impact in government use of automated decision-making? The regulatory landscape for automated decision-making, in Canada and across the world, is far from settled. Legislative and policy models are emerging, and the role of standards is evolving to support regulatory objectives. While acknowledging the contributions of leading standards development organizations, the authors argue that the rationale for standards must come from the law and that implementing such standards would help to reduce future complaints by, and would proactively enable human rights protections for, those subject to automated decision-making. The book presents a proposed standards framework for automated decision-making and provides recommendations for its implementation in the context of the government of Canada's Directive on Automated Decision-Making. As such, this book can assist public agencies around the world in developing and deploying automated decision-making systems equitably as well as being of interest to businesses that utilize automated decision-making processes."
Mitigating Racial And Ethnic Bias And Advancing Health Equity In Clinical Algorithms: A Scoping Review,2023,Michael P. Cary; Anna Zink; Sijia Wei; Andrew Olson; Mengying Yan; Rashaud Senior; Sophia Bessias; Kais Gadhoumi; Genevieve Jean-Pierre; Dingyue Wang; Leila Ledbetter; Nicoleta Economou-Zavlanos; Ziad Obermeyer; Michael Pencina,Health Affairs,62,W4387245432,10.1377/hlthaff.2023.00553,https://openalex.org/W4387245432,https://doi.org/10.1377/hlthaff.2023.00553,Health care; Notice; Health equity; Equity (law); Medicine,review,True,"In August 2022 the Department of Health and Human Services (HHS) issued a notice of proposed rulemaking prohibiting covered entities, which include health care providers and health plans, from discriminating against individuals when using clinical algorithms in decision making. However, HHS did not provide specific guidelines on how covered entities should prevent discrimination. We conducted a scoping review of literature published during the period 2011-22 to identify health care applications, frameworks, reviews and perspectives, and assessment tools that identify and mitigate bias in clinical algorithms, with a specific focus on racial and ethnic bias. Our scoping review encompassed 109 articles comprising 45 empirical health care applications that included tools tested in health care settings, 16 frameworks, and 48 reviews and perspectives. We identified a wide range of technical, operational, and systemwide bias mitigation strategies for clinical algorithms, but there was no consensus in the literature on a single best practice that covered entities could employ to meet the HHS requirements. Future research should identify optimal bias mitigation methods for various scenarios, depending on factors such as patient population, clinical setting, algorithm design, and types of bias to be addressed."
Biased random-key genetic algorithms: A review,2024,Mariana A. Londe; Luciana Fontes Pessôa; Carlos E. Andrade; Maurício G. C. Resende,European Journal of Operational Research,42,W4393204274,10.1016/j.ejor.2024.03.030,https://openalex.org/W4393204274,,Metaheuristic; Computer science; Hyperparameter; Key (lock); Genetic algorithm,review,False,
"Understanding the Ethical Challenges of AI in Retail and Addressing Data Privacy, Algorithmic Bias and Consumer Trust",2025,Srinivasa Reddy Vuyyuru,INTERNATIONAL JOURNAL OF ENGINEERING DEVELOPMENT AND RESEARCH,8,W4410045306,10.56975/ijedr.v13i2.301034,https://openalex.org/W4410045306,,Computer science; Internet privacy; Data science; Consumer privacy; Information privacy,article,False,
Pitfalls and Best Practices in Evaluation of AI Algorithmic Biases in Radiology,2025,Paul H. Yi; Preetham Bachina; Beepul Bharti; Sean P. Garin; Adway Kanhere; Pranav Kulkarni; David Li; Vishwa S. Parekh; Samantha M. Santomartino; Linda Moy; Jeremias Sulam,Radiology,3,W4410542464,10.1148/radiol.241674,https://openalex.org/W4410542464,https://doi.org/10.1148/radiol.241674,Medicine; MEDLINE; Medical physics; Data science; Artificial intelligence,review,True,"Evaluation of algorithmic biases, or artificial intelligence biases, is challenging in radiology due to incomplete reporting of demographic information in medical imaging datasets, variability in definitions of demographic categories, and inconsistent statistical definitions of bias."
Enhancing Clinical Decision Support in Nephrology: Addressing Algorithmic Bias Through Artificial Intelligence Governance,2024,Benjamin A. Goldstein; Dinushika Mohottige; Sophia Bessias; Michael P. Cary,American Journal of Kidney Diseases,5,W4399447219,10.1053/j.ajkd.2024.04.008,https://openalex.org/W4399447219,,Corporate governance; Medicine; Equity (law); PsycINFO; Set (abstract data type),review,False,
Algorithmic Bias: Causes and Effects on Marginalized Communities,2023,Katrina Baha,,2,W4377693683,10.22371/04.2023.001,https://openalex.org/W4377693683,https://digital.sandiego.edu/cgi/viewcontent.cgi?article=1109&context=honors_theses,Health care; Set (abstract data type); Order (exchange); Public relations; Computer science,dissertation,True,"Individuals from marginalized backgrounds face different healthcare outcomes due to algorithmic bias in the technological healthcare industry. Algorithmic biases, which are the biases that arise from the set of steps used to solve or analyze a problem, are evident when people from marginalized communities use healthcare technology. For example, many pulse oximeters, which are the medical devices used to measure oxygen saturation in the blood, are not able to accurately read people who have darker skin tones. Thus, people with darker skin tones are not able to receive proper health care due to their pulse oximetry data being inaccurate. This research aims to highlight the ethical implications of marginalized communities facing different healthcare outcomes and provide suggestions on how to prevent algorithmic bias from appearing in healthcare. In order to do this, this paper will first give examples of algorithmic bias, then discuss the ethical implications of those biases, and lastly provide solutions that may help prevent algorithmic bias. It is unethical that marginalized communities are being misread, misdiagnosed, and mistreated due to algorithmic biases. Additionally, the technological healthcare industry must be diversified in order to prevent algorithmic biases from arising in their medical technologies."
Ethics and discrimination in artificial intelligence-enabled recruitment practices,2023,Zhisheng Chen,Humanities and Social Sciences Communications,237,W4386714740,10.1057/s41599-023-02079-x,https://openalex.org/W4386714740,https://www.nature.com/articles/s41599-023-02079-x.pdf,Transparency (behavior); Computer science; Corporate governance; Big Five personality traits; Raw data,article,True,"Abstract This study aims to address the research gap on algorithmic discrimination caused by AI-enabled recruitment and explore technical and managerial solutions. The primary research approach used is a literature review. The findings suggest that AI-enabled recruitment has the potential to enhance recruitment quality, increase efficiency, and reduce transactional work. However, algorithmic bias results in discriminatory hiring practices based on gender, race, color, and personality traits. The study indicates that algorithmic bias stems from limited raw data sets and biased algorithm designers. To mitigate this issue, it is recommended to implement technical measures, such as unbiased dataset frameworks and improved algorithmic transparency, as well as management measures like internal corporate ethical governance and external oversight. Employing Grounded Theory, the study conducted survey analysis to collect firsthand data on respondents’ experiences and perceptions of AI-driven recruitment applications and discrimination."
Algorithmic Bias in Recommendation Systems and Its Social Impact on User Behavior,2024,Lingyuan Liu,International Theory and Practice in Humanities and Social Sciences,3,W4405554134,10.70693/itphss.v1i1.204,https://openalex.org/W4405554134,https://hal.science/hal-05171314,Recommender system; Computer science; Data science; Information retrieval,article,True,"Algorithmic bias in recommendation systems poses significant challenges, influencing user experiences and perpetuating societal inequalities. This study provides a comprehensive analysis of the origins, impacts, and mitigation strategies of algorithmic bias in recommendation systems. By categorizing bias into data bias, model bias, and feedback loops, this research highlights the multifaceted nature of algorithmic bias and its implications for user behavior, including the formation of filter bubbles, decision-making distortions, and the reinforcement of social inequalities. The study employs a mixed-methods approach, integrating both theoretical analysis and empirical case studies from popular platforms such as Netflix, YouTube, and Amazon. These case studies illustrate the real-world implications of algorithmic bias and demonstrate the effectiveness of various mitigation strategies, including diversity optimization, transparency enhancement, and fairness-aware learning. The findings underscore the importance of a balanced approach that incorporates technical, ethical, and policy-based interventions to promote socially responsible recommendation systems."
Algorithmic bias: sexualized violence against women in GPT-3 models,2025,Sarah Wyer; Sue Black,AI and Ethics,5,W4406392547,10.1007/s43681-024-00641-0,https://openalex.org/W4406392547,https://link.springer.com/content/pdf/10.1007/s43681-024-00641-0.pdf,Psychology; Criminology; Computer security; Medical emergency; Computer science,article,True,"Abstract This study explores the occurrence and implications of sexualized violence against women in text completion tasks performed by GPT-3 models. The study began as an exploratory investigation into gender inequalities within GPT-3 models to discover what GPT3 models generate in text completion tasks when prompted with basic gendered prompts. For example, ""Men can…"", ""Women can…"". Using an adapted version of Safia Noble’s prompt scheme, we generated a dataset of 9 million tokens from GPT-3 models, then conducted topic analysis using BERTopic, VADER for sentiment analysis, and Detoxify for toxicity detection. The results were unexpected. Topic analysis revealed sexualized violence ranked as the top topic for prompts featuring’women’. Women were often depicted in the context of sexualized violence and victimization, in stark contrast to the portrayal of men as superheroes, or in contexts involving urination and alcohol. For example, a randomly selected output generated by the prompt. "" Why are women so "", generated "" Why are women so arrogant about being raped?"" . Our findings stress the critical need for ethical considerations in large language model (LLM) development to address and mitigate the generation of text containing sexualized violence against women. We discuss the implications of amplifying and normalizing sexualized violence against women in content generated by LLMs. Our work builds on previous research examining gender bias in LLMs, with a specific focus on the manifestation of sexualized violence against women in LLM outputs, an area that has received little attention. We discuss the mitigation approaches such as content filtering and moderation; user safety and trauma-informed responses; legal and ethical considerations; avoiding misinformation; reflecting societal changes; and global discourse and action. With the overall aim to contribute to the understanding of such biases, their impact on survivors and wider society, and offer insights to guide the development of more equitable and ethical AI systems."
Identifying and Mitigating Algorithmic Bias in Student Emotional Analysis,2024,T. S. Ashwin; Gautam Biswas,Lecture notes in computer science,4,W4400210740,10.1007/978-3-031-64299-9_7,https://openalex.org/W4400210740,,Computer science; Artificial intelligence,book-chapter,False,
Algorithmic Bias in De-Identification Tools,2023,Paul M. Heider,,2,W4389543635,10.1109/ichi57859.2023.00129,https://openalex.org/W4389543635,,Identification (biology); Race (biology); Computer science; Racial bias; Parity (physics),article,False,"We present a series of experiments designed to quantify the algorithmic bias inherent in six off-the-shelf de-identification systems. We used false negative rate (FNR) and true positive rate (TPR) parity measures in addition to F <inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</inf> -score to evaluate the systems across two environments. In the first condition, we inferred gender and race/ethnicity labels for a pre-existing de-identification corpus and analyzed performance on disaggregated subgroups. In the second condition, we controlled the dominant race/ethnicity bias for names used as realistic surrogates for resynthesizing a different pre-existing de-identification corpus. In both conditions, we found cases of strong bias and near-zero bias."
"Digital Ageism, Algorithmic Bias, and Feminist Critical Theory",2023,Rune Nyrup; Charlene H. Chu; Elena Falco,,4,W4388932751,10.1093/oso/9780192889898.003.0018,https://openalex.org/W4388932751,https://www.repository.cam.ac.uk/handle/1810/362609,Sociology; Inequality; Intersectionality; Perspective (graphical); Social inequality,book-chapter,True,"Abstract In this chapter, Nyrup, Chu, and Falco coin the term digital ageism to highlight the interplay between social inequality and tech development, aligning with feminist work that views society and technology as co-constitutive. The essay details the results of encoded ageism, including medical technologies that offer less accurate diagnoses on older populations, the unequal division of resources, and the perspective that younger people are inevitably better at using new technologies. Drawing on the work of Sally Haslanger and Iris Marion Young, they explore how technical limitations and the insufficient representation of older people in design and development teams are shaped by self-reinforcing structural inequality. This essay therefore offers a crucial intervention in the debate by identifying and tracking ageist harms and their intersections with disability, race, gender, and class-based injustices."
Algorithmic Bias as a Core Legal Dilemma in the Age of Artificial Intelligence: Conceptual Basis and the Current State of Regulation,2025,Gergely Ferenc Lendvai; Gergely Gosztonyi,Laws,5,W4411243228,10.3390/laws14030041,https://openalex.org/W4411243228,https://www.mdpi.com/2075-471X/14/3/41/pdf?version=1749723003,Dilemma; Core (optical fiber); Current (fluid); State (computer science); Artificial intelligence,article,True,"This article examines algorithmic bias as a pressing legal challenge, situating the issue within the broader context of artificial intelligence (AI) governance. We employed comparative legal analysis and reviewed pertinent regulatory documents to examine how the fragmented U.S. approaches and the EU’s user-centric legal frameworks, such as the GDPR, DSA, and AI Act, address the systemic risks posed by biased algorithms. The findings underscore persistent enforcement gaps, particularly concerning opaque black-box algorithmic design, which hampers bias detection and remediation. The paper highlights how current regulatory efforts disproportionately affect marginalized communities and fail to provide effective protection across jurisdictions. It also identifies structural imbalances in legal instruments, particularly in relation to risk classification, transparency, and fairness standards. Notably, emerging regulations often lack the technical and ethical capacity for implementation. We argue that global cooperation is not only necessary but inevitable, as regional solutions alone are insufficient to govern transnational AI systems. Without harmonized international standards, algorithmic bias will continue to reproduce existing inequalities under the guise of objectivity. The article advocates for inclusive, cross-sectoral collaboration among governments, developers, and civil society to ensure the responsible development of AI and uphold fundamental rights."
"Whither Bias Goes, I Will Go: An Integrative, Systematic Review of Algorithmic Bias Mitigation",2024,Louis Hickman; Christopher Huynh; Jessica Gass; Brandon M. Booth; Jason Kuruzovich; Louis Tay,,3,W4403627258,10.31234/osf.io/hcxbn,https://openalex.org/W4403627258,https://osf.io/hcxbn/download,Computer science; Psychology; Data science,preprint,True,"Machine learning (ML) models are increasingly used for personnel assessment and selection (e.g., resume screeners, automatically scored interviews). However, concerns have been raised throughout society that ML assessments may be biased and perpetuate or exacerbate inequality. Although organizational researchers have begun investigating ML assessments from traditional psychometric and legal perspectives, there is a need to understand, clarify, and integrate fairness operationalizations and algorithmic bias mitigation methods from the computer science, data science, and organizational research literatures. We present a four-stage model of developing ML assessments and applying bias mitigation methods, including 1) generating the training data, 2) training the model, 3) testing the model, and 4) deploying the model. When introducing the four-stage model, we describe potential sources of bias and unfairness at each stage. Then, we systematically review definitions and operationalizations of algorithmic bias, legal requirements governing personnel selection from the United States and Europe, and research on algorithmic bias mitigation across multiple domains and integrate these findings into our framework. Our review provides insights for both research and practice by elucidating possible mechanisms of algorithmic bias while identifying which bias mitigation methods are legal and effective. This integrative framework also reveals gaps in the knowledge of algorithmic bias mitigation that should be addressed by future collaborative research between organizational researchers, computer scientists, and data scientists. We provide recommendations for developing and deploying ML assessments, as well as recommendations for future research into algorithmic bias and fairness."
Can Algorithm Knowledge Stop Women from Being Targeted by Algorithm Bias? The New Digital Divide on Weibo,2023,Yang Zhang; Huashan Chen,Journal of Broadcasting & Electronic Media,4,W4380224377,10.1080/08838151.2023.2218955,https://openalex.org/W4380224377,,Perspective (graphical); Computer science; Algorithm; Gender bias; Artificial intelligence,article,False,"Algorithm knowledge of users plays a crucial role in avoiding them from algorithm bias in recommendation systems. Gender of users has been found to correlate with algorithm bias, but also leaving behind a question of whether this relationship can be described by algorithm knowledge. By using Weibo as an example system, we clarify the aforementioned question from a digital divide theory perspective. We combine a traditional method (questionnaire) with a deep learning computational method to explain algorithm bias in two sequential studies. Our findings suggest that algorithm knowledge solely works for men while fails to protect women. Who users follow helps determine what information they are exposed to on Weibo, and this renders female users’ algorithm knowledge useless. This work provides a valuable perspective on algorithm bias: we view algorithm bias as a new digital divide and contribute to the understanding of gender differences by applying the digital divide perspective. Methodologically, we contribute by integrating traditional and computational methods to explain algorithm bias from a folk theory perspective."
Encoding normative ethics: On algorithmic bias and disability,2023,Ian Moura,First Monday,2,W4317038352,10.5210/fm.v28i1.12905,https://openalex.org/W4317038352,https://firstmonday.org/ojs/index.php/fm/article/download/12905/10789,Ableism; Harm; Normative; Context (archaeology); Psychological intervention,article,True,"Computer-based algorithms have the potential to encode and exacerbate ableism and may contribute to disparate outcomes for disabled people. The threat of algorithmic bias to people with disabilities is inseparable from the longstanding role of technology as a normalizing agent, and from questions of how society defines shared values, quantifies ethics, conceptualizes and measures risk, and strives to allocate limited resources. This article situates algorithmic bias amidst the larger context of normalization, draws on social and critical theories that can be used to better understand both ableism and algorithmic bias as they operate in the United States, and proposes concrete steps to mitigate harm to the disability community as a result of algorithmic adoption. Examination of two cases — the allocation of lifesaving medical interventions during the COVID-19 pandemic and approaches to autism diagnosis and intervention — demonstrate instances of the mismatch between disabled people’s lived experiences and the goals and understandings advanced by nondisabled people. These examples highlight the ways particular ethical norms can become part of technological systems, and the harm that can ripple outward from misalignment of formal ethics and community values."
LUCID: Exposing Algorithmic Bias through Inverse Design,2023,Carmen Mazijn; Carina Prunkl; Andres Algaba; Jan Danckaert; Vincent Ginis,Proceedings of the AAAI Conference on Artificial Intelligence,3,W4382318028,10.1609/aaai.v37i12.26683,https://openalex.org/W4382318028,https://ojs.aaai.org/index.php/AAAI/article/download/26683/26455,Toolbox; Computer science; Set (abstract data type); Focus (optics); Outcome (game theory),article,True,"AI systems can create, propagate, support, and automate bias in decision-making processes. To mitigate biased decisions, we both need to understand the origin of the bias and define what it means for an algorithm to make fair decisions. Most group fairness notions assess a model's equality of outcome by computing statistical metrics on the outputs. We argue that these output metrics encounter intrinsic obstacles and present a complementary approach that aligns with the increasing focus on equality of treatment. By Locating Unfairness through Canonical Inverse Design (LUCID), we generate a canonical set that shows the desired inputs for a model given a preferred output. The canonical set reveals the model's internal logic and exposes potential unethical biases by repeatedly interrogating the decision-making process. We evaluate LUCID on the UCI Adult and COMPAS data sets and find that some biases detected by a canonical set differ from those of output metrics. The results show that by shifting the focus towards equality of treatment and looking into the algorithm's internal workings, the canonical sets are a valuable addition to the toolbox of algorithmic fairness evaluation."
On the consequences of AI bias: when moral values supersede algorithm bias,2024,Kwadwo Asante; David Sarpong; Derrick Boakye,Journal of Managerial Psychology,3,W4403732081,10.1108/jmp-05-2024-0379,https://openalex.org/W4403732081,https://www.emerald.com/insight/content/doi/10.1108/JMP-05-2024-0379/full/pdf?title=on-the-consequences-of-ai-bias-when-moral-values-supersede-algorithm-bias,Psychology; Social psychology; Response bias; Algorithm; Mathematics,article,True,"Purpose This study responded to calls to investigate the behavioural and social antecedents that produce a highly positive response to AI bias in a constrained region, which is characterised by a high share of people with minimal buying power, growing but untapped market opportunities and a high number of related businesses operating in an unregulated market. Design/methodology/approach Drawing on empirical data from 225 human resource managers from Ghana, data were sourced from senior human resource managers across industries such as banking, insurance, media, telecommunication, oil and gas and manufacturing. Data were analysed using a fussy set qualitative comparative analysis (fsQCA). Findings The results indicated that managers who regarded their response to AI bias as a personal moral duty felt a strong sense of guilt towards the unintended consequences of AI logic and reasoning. Therefore, managers who perceived the processes that guide AI algorithms' reasoning as discriminating showed a high propensity to address this prejudicial outcome. Practical implications As awareness of consequences has to go hand in hand with an ascription of responsibility; organisational heads have to build the capacity of their HR managers to recognise the importance of taking personal responsibility for artificial intelligence algorithm bias because, by failing to nurture the appropriate attitude to reinforce personal norm among managers, no immediate action will be taken. Originality/value By integrating the social identity theory, norm activation theory and justice theory, the study improves our understanding of how a collective organisational identity, perception of justice and personal values reinforce a positive reactive response towards AI bias outcomes."
On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization,2024,Jiancong Xiao; Ziniu Li; Xingyu Xie; Emily Getzen; Cong Fang; Long Qi; Weijie Su,arXiv (Cornell University),4,W4399115638,10.48550/arxiv.2405.16455,https://openalex.org/W4399115638,https://arxiv.org/pdf/2405.16455,Regularization (linguistics); Matching (statistics); Preference; Econometrics; Computer science,preprint,True,"Accurately aligning large language models (LLMs) with human preferences is crucial for informing fair, economically sound, and statistically efficient decision-making processes. However, we argue that the predominant approach for aligning LLMs with human preferences through a reward model -- reinforcement learning from human feedback (RLHF) -- suffers from an inherent algorithmic bias due to its Kullback--Leibler-based regularization in optimization. In extreme cases, this bias could lead to a phenomenon we term preference collapse, where minority preferences are virtually disregarded. To mitigate this algorithmic bias, we introduce preference matching (PM) RLHF, a novel approach that provably aligns LLMs with the preference distribution of the reward model under the Bradley--Terry--Luce/Plackett--Luce model. Central to our approach is a PM regularizer that takes the form of the negative logarithm of the LLM's policy probability distribution over responses, which helps the LLM balance response diversification and reward maximization. Notably, we obtain this regularizer by solving an ordinary differential equation that is necessary for the PM property. For practical implementation, we introduce a conditional variant of PM RLHF that is tailored to natural language generation. Finally, we empirically validate the effectiveness of conditional PM RLHF through experiments on the OPT and Llama-family models, demonstrating a 29% to 41% improvement in alignment with human preferences, as measured by a certain metric, compared to standard RLHF."
Ethics and governance of trustworthy medical artificial intelligence,2023,Jie Zhang; Zongming Zhang,BMC Medical Informatics and Decision Making,294,W4315880904,10.1186/s12911-023-02103-9,https://openalex.org/W4315880904,https://bmcmedinformdecismak.biomedcentral.com/counter/pdf/10.1186/s12911-023-02103-9,Trustworthiness; Health informatics; Corporate governance; Computer science; Artificial intelligence,article,True,"Abstract Background The growing application of artificial intelligence (AI) in healthcare has brought technological breakthroughs to traditional diagnosis and treatment, but it is accompanied by many risks and challenges. These adverse effects are also seen as ethical issues and affect trustworthiness in medical AI and need to be managed through identification, prognosis and monitoring. Methods We adopted a multidisciplinary approach and summarized five subjects that influence the trustworthiness of medical AI: data quality, algorithmic bias, opacity, safety and security, and responsibility attribution, and discussed these factors from the perspectives of technology, law, and healthcare stakeholders and institutions. The ethical framework of ethical values-ethical principles-ethical norms is used to propose corresponding ethical governance countermeasures for trustworthy medical AI from the ethical, legal, and regulatory aspects. Results Medical data are primarily unstructured, lacking uniform and standardized annotation, and data quality will directly affect the quality of medical AI algorithm models. Algorithmic bias can affect AI clinical predictions and exacerbate health disparities. The opacity of algorithms affects patients’ and doctors’ trust in medical AI, and algorithmic errors or security vulnerabilities can pose significant risks and harm to patients. The involvement of medical AI in clinical practices may threaten doctors ‘and patients’ autonomy and dignity. When accidents occur with medical AI, the responsibility attribution is not clear. All these factors affect people’s trust in medical AI. Conclusions In order to make medical AI trustworthy, at the ethical level, the ethical value orientation of promoting human health should first and foremost be considered as the top-level design. At the legal level, current medical AI does not have moral status and humans remain the duty bearers. At the regulatory level, strengthening data quality management, improving algorithm transparency and traceability to reduce algorithm bias, and regulating and reviewing the whole process of the AI industry to control risks are proposed. It is also necessary to encourage multiple parties to discuss and assess AI risks and social impacts, and to strengthen international cooperation and communication."
Decoding Algorithmic Bias,2024,Ozgur Aksoy,Advances in human resources management and organizational development book series,1,W4391765373,10.4018/979-8-3693-1766-2.ch013,https://openalex.org/W4391765373,,Decoding methods; Computer science; Algorithm,book-chapter,False,"Predictive algorithms are increasingly used to assist decision-making for efficiency gains. However, it is essential to acknowledge that algorithms can mirror systemic biases in their predictions in a way that favors certain groups over others, even if they are immune to cognitive biases. The notion of algorithms generating unfair predictions is referred to as “algorithmic bias.” Addressing cognitive biases in humans might not always be an effective solution to mitigate algorithmic bias. Therefore, it is essential to understand when and how quantitative technical mitigation methods can address this issue. This chapter explores the fundamental concepts of algorithmic bias, its sources, and technical mitigation strategies. In a world where humans and AI are intertwined, it is our responsibility to ensure a fair digital future. Addressing algorithmic bias is critical to achieving this goal."
Making the invisible visible: Youth designs for teaching about technological and algorithmic bias,2024,Merijke Coenraad,International Journal of Child-Computer Interaction,3,W4391650902,10.1016/j.ijcci.2024.100634,https://openalex.org/W4391650902,,Covert; Experiential learning; Computer science; Gender bias; Technological change,article,False,
Emerging algorithmic bias: fairness drift as the next dimension of model maintenance and sustainability,2025,Sharon E. Davis; Chad Dorn; Daniel Park; Michael E. Matheny,Journal of the American Medical Informatics Association,4,W4408405690,10.1093/jamia/ocaf039,https://openalex.org/W4408405690,https://academic.oup.com/jamia/advance-article-pdf/doi/10.1093/jamia/ocaf039/62400687/ocaf039.pdf,Operationalization; Computer science; Metric (unit); Population; Performance metric,article,True,"Abstract Objectives While performance drift of clinical prediction models is well-documented, the potential for algorithmic biases to emerge post-deployment has had limited characterization. A better understanding of how temporal model performance may shift across subpopulations is required to incorporate fairness drift into model maintenance strategies. Materials and Methods We explore fairness drift in a national population over 11 years, with and without model maintenance aimed at sustaining population-level performance. We trained random forest models predicting 30-day post-surgical readmission, mortality, and pneumonia using 2013 data from US Department of Veterans Affairs facilities. We evaluated performance quarterly from 2014 to 2023 by self-reported race and sex. We estimated discrimination, calibration, and accuracy, and operationalized fairness using metric parity measured as the gap between disadvantaged and advantaged groups. Results Our cohort included 1 739 666 surgical cases. We observed fairness drift in both the original and temporally updated models. Model updating had a larger impact on overall performance than fairness gaps. During periods of stable fairness, updating models at the population level increased, decreased, or did not impact fairness gaps. During periods of fairness drift, updating models restored fairness in some cases and exacerbated fairness gaps in others. Discussion This exploratory study highlights that algorithmic fairness cannot be assured through one-time assessments during model development. Temporal changes in fairness may take multiple forms and interact with model updating strategies in unanticipated ways. Conclusion Equitable and sustainable clinical artificial intelligence deployments will require novel methods to monitor algorithmic fairness, detect emerging bias, and adopt model updates that promote fairness."
Are algorithmic bias claims supported?,2023,Solomon Messing,Science,2,W4387128670,10.1126/science.adk1211,https://openalex.org/W4387128670,,Computer science,letter,False,
Mitigating Nonlinear Algorithmic Bias in Binary Classification,2024,Wendy Hui; John W. Lau,,2,W4401113564,10.1109/cai59869.2024.00168,https://openalex.org/W4401113564,https://figshare.com/articles/conference_contribution/Mitigating_Nonlinear_Algorithmic_Bias_in_Binary_Classification/26112634,Binary number; Computer science; Nonlinear system; Theoretical computer science; Artificial intelligence,article,True,"&lt;p dir=""ltr""&gt;This paper proposes the use of causal modeling to detect and mitigate algorithmic bias that is nonlinear in the protected attribute. We provide a general overview of our approach. We use the German Credit data set, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on age bias and the problem of binary classification. We show that the probability of getting correctly classified as “low risk” is lowest among young people. The probability increases with age nonlinearly. To incorporate the nonlinearity into the causal model, we introduce a higher order polynomial term. Based on the fitted causal model, the de-biased probability estimates are computed, showing improved fairness with little impact on overall classification accuracy. Causal modeling is intuitive and, hence, its use can enhance explicability and promotes trust among different stakeholders of AI.&lt;/p&gt;"
"Communicating and combating algorithmic bias: effects of data diversity, labeler diversity, performance bias, and user feedback on AI trust",2024,Cheng Chen; S. Shyam Sundar,Human-Computer Interaction,7,W4403143618,10.1080/07370024.2024.2392494,https://openalex.org/W4403143618,,Diversity (politics); Computer science; Social psychology; Psychology; Political science,article,False,
Unlocking Monetization Potential in the Age of YouTube Algorithmic Bias: An Analysis of Botswana Filmmaking,2023,Gopolang Ditlhokwa,IntechOpen eBooks,3,W4389305620,10.5772/intechopen.113306,https://openalex.org/W4389305620,https://www.intechopen.com/citation-pdf-url/88528,Monetization; Audience measurement; Filmmaking; Political science; Advertising,book-chapter,True,"This chapter examines the challenges and opportunities faced by filmmakers in Botswana to monetize their film content on YouTube. The researcher uses a Critical Theory framework to explore the power dynamics of platforms toward cultural industries by dissecting the impact of YouTube’s algorithmic bias and geo-restrictions on content monetization potential. Additionally, this study extends to investigating the representation of diverse cultures and communities within the film industry and how YouTube’s policies may contribute to underrepresentation. With the help of qualitative research methods, the findings reveal that, indeed, filmmakers in Botswana face limitations in monetizing their content on YouTube due to regional IP restrictions, inability to meet subscription thresholds, and low viewership turnout. The study also highlights the potential for growth and market penetration through YouTube, as reaching a global audience by Botswana filmmakers can attract interest and investment from various funders. The study concludes that addressing YouTube’s algorithmic bias, geo-restrictions, and economic dynamics is crucial for promoting a more inclusive and equitable film industry in Botswana. It further suggests the need for pragmatic interventions that support filmmakers in navigating these challenges and maximizing their monetization opportunities on YouTube."
Algorithm Bias and Perceived Fairness: A Comprehensive Scoping Review,2024,Amirhossein Hajigholam Saryazdi,,2,W4399141338,10.1145/3632634.3655848,https://openalex.org/W4399141338,https://dl.acm.org/doi/pdf/10.1145/3632634.3655848,Fairness measure; Computer science; Telecommunications; Wireless; Throughput,article,True,"Artificial intelligence (AI)-based algorithms are playing an increasingly prominent role in shaping daily life. However, these algorithms can exhibit biases that exacerbate societal injustices. Such biases have a substantial impact on people's perceptions of algorithmic fairness, yet the precise mechanisms and scope of this phenomenon remain relatively understudied. To address this research gap, a comprehensive scoping literature review is conducted, providing an overview of current research in the field. Subsequently, a novel theoretical model is developed that synthesizes key themes, including algorithm bias, algorithm fairness, perceived fairness, individual characteristics, social characteristics, task characteristics, and technology characteristics. The paper contributes proposing a set of propositions that underscore the critical gaps in the existing literature, contribute to a deeper comprehension of the relationships among the identified themes and their constituent elements, and offer a roadmap for future research in the domain."
Algorithmic fairness in artificial intelligence for medicine and healthcare,2023,Richard J. Chen; Judy J. Wang; Drew F. K. Williamson; Tiffany Chen; Jana Lipková; Ming Y. Lu; Sharifa Sahai; Faisal Mahmood,Nature Biomedical Engineering,383,W4382394524,10.1038/s41551-023-01056-8,https://openalex.org/W4382394524,https://pmc.ncbi.nlm.nih.gov/articles/PMC10632090/pdf/nihms-1940941.pdf,Health care; Computer science; Artificial intelligence; Medicine; Intensive care medicine,review,True,
Female perspectives on algorithmic bias: implications for AI researchers and practitioners,2025,Belen Fraile-Rojas; Carmen De‐Pablos‐Heredero; Mariano Méndez-Suárez,Management Decision,3,W4406230024,10.1108/md-04-2024-0884,https://openalex.org/W4406230024,,Empowerment; Injustice; Inequality; Constructive; Empirical research,article,False,"Purpose This article explores the use of natural language processing (NLP) techniques and machine learning (ML) models to discover underlying concepts of gender inequality applied to artificial intelligence (AI) technologies in female social media conversations. The first purpose is to characterize female users who use this platform to share content around this area. The second is to identify the most prominent themes among female users’ digital production of gender inequality concepts, applied to AI technologies. Design/methodology/approach Social opinion mining has been applied to historical Twitter data. Data were gathered using a combination of analytical methods such as word clouds, sentiment analyses and clustering. It examines 172,041 tweets worldwide over a limited period of 359 days. Findings Empirical data gathered from interactions of female users in digital dialogues highlight that the most prominent topics of interest are the future of AI technologies and the active role of women to guarantee gender balanced systems. Algorithmic bias impacts female user behaviours in response to injustice and inequality in algorithmic outcomes. They share topics of interest and lead constructive conversations with profiles affiliated with gender or race empowerment associations. Women challenged by stereotypes and prejudices are likely to fund entrepreneurial solutions to create opportunities for change. Research limitations/implications This study does have its limitations, however. First, different keywords are likely to result in a different pool of related research. Moreover, due to the nature of our sample, the largest proportion of posts are from native English speakers, predominantly (88%) from the US, UK, Australia and Canada. This demographic concentration reflects specific social structures and practices that influence gender equity priorities within the sample. These cultural contexts, which often emphasize inclusivity and equity, play a significant role in shaping the discourse around gender issues. These cultural norms, preferences and practices are critical in understanding the individual behaviours, perspectives and priorities expressed in the posts; in other words, it is vital to consider cultural context and economic determinants in an analysis of gender equity discussions. The US, UK, Australia and Canada share a cultural and legal heritage, a common language, values, democracy and the rule of law. Bennett (2007) emphasizes the potential for enhanced cooperation in areas like technology, trade and security, suggesting that the anglosphere’s cultural and institutional commonalities create a natural foundation for a cohesive, influential global network. These shared characteristics further influence the common approaches and perspectives on gender equity in public discourse. Yet findings from Western nations should not be assumed to apply easily to the contexts of other countries. Practical implications From a practical perspective, the results help us understand the role of female influencers and scrutinize public conversations. From a theoretical one, this research upholds the argument that feminist critical thought is indispensable in the development of balanced AI systems. Social implications The results also help us understand the role of female influencers: ordinary individuals often challenged by gender and race discrimination. They request an intersectional, collaborative and pluralistic understanding of gender and race in AI. They act alone and endure the consequences of stigmatized products and services. AI curators should strongly consider advocating for responsible, impartial technologies, recognizing the indispensable role of women. This must consider all stakeholders, including representatives from industry, small and medium-sized enterprises (SMEs), civil society and academia. Originality/value This study aims to fill critical research gaps by addressing the lack of a socio-technical perspective on AI-based decision-making systems, the shortage of empirical studies in the field and the need for a critical analysis using feminist theories. The study offers valuable insights that can guide managerial decision-making for AI researchers and practitioners, providing a comprehensive understanding of the topic through a critical lens."
Engineering a Fiduciary: Expanding the Regulatory Scope of Algorithmic Bias,2023,Bao Kham Chau,SSRN Electronic Journal,2,W4387886268,10.2139/ssrn.4372258,https://openalex.org/W4387886268,https://doi.org/10.2139/ssrn.4372258,Promulgation; Scope (computer science); Computer science; Fiduciary; Corporate governance,article,True,
Unpacking Algorithmic Bias in YouTube Shorts by Analyzing Thumbnails,2025,Mert Can Çakmak; Nitin Agarwal,Proceedings of the ... Annual Hawaii International Conference on System Sciences/Proceedings of the Annual Hawaii International Conference on System Sciences,3,W4407208106,10.24251/hicss.2025.304,https://openalex.org/W4407208106,,Unpacking; Thumbnail; Computer science; Artificial intelligence; Linguistics,article,False,
Algorithmic Bias and Data Injustice: Dark Side or Dark Matter?,2023,Aleksi Aaltonen; Francesco Gualdi; Mayur Joshi; Silvia Masiero; Monideepa Tarafdar; Marta Stelmaszak; Kari Koskinen,Academy of Management Proceedings,2,W4385224588,10.5465/amproc.2023.16682symposium,https://openalex.org/W4385224588,,Harm; Injustice; Great Rift; Framing (construction); Big data,article,False,"This panel brings together five contributions that, impinging on the notions of algorithmic bias and data injustice, explore both the dynamics producing data-induced harm and the manifestations of such harm on people. Ranging from data-based treatment of LGBTQ+ communities, to algorithmic bias in e-government and exclusion of recipients from datafied food security systems, the panel engages the debate on whether the notion of a ‘dark side’, widely applied to the adverse side effects of information systems, is appropriate to discuss data-induced unfairness. As an alternative framing, the panel introduces the notion of a ‘dark matter’ of datafied systems, where bias and injustice are designed into the technology. The panel aims at generating debate on unfairness with the view of imagining fairer data-based technologies, and thus contributing to building a future where a ‘force for good’ can effectively stem from datafication."
Raising Algorithm Bias Awareness Among Computer Science Students Through Library and Computer Science Instruction,2024,Shalini Ramachandran; Steve Cutchin; Sheree Fu,2021 ASEE Virtual Annual Conference Content Access Proceedings,3,W3191615150,10.18260/1-2--37634,https://openalex.org/W3191615150,https://peer.asee.org/37634.pdf,Raising (metalworking); Computer science; Mathematics education; Psychology; Engineering,article,True,"Abstract We are a computer science professor and two librarians who work closely with computer science students. In this paper, we outline the development of an introductory algorithm bias instruction session. As part of our lesson development, we analyzed the results of a survey we conducted of computer science students at three universities on their perceptions about search-engine and big-data algorithms. We examined whether an information literacy component focused on algorithmic bias was beneficial to offer to students in the computational sciences and designed an instructional prototype. We studied qualitative data, including feedback from students and colleagues on our initial instruction module to create the next two modules. We found that students' reception to the subject of algorithm bias can range from defensive and unaccepting to open and accepting of the existence of such bias. Since the topic ultimately deals with issues of racial, gender-based, and other discrimination, a multidisciplinary approach is needed when teaching about algorithm bias. Our assertion is that librarians have a role in partnering with computer science instructors to ensure that students who major in computer science, who will be the primary creators of algorithms as they enter the workforce, can develop an early awareness and understanding of bias in information systems. Further, when the students receive such training, the automated systems they generate will produce more fair outcomes. Our pedagogy incorporates insights from computer science, library science, medical ethics, and critical theory. The aim of our algorithm bias instruction is to help computer science students recognize and mitigate the systematic marginalization of groups within the current technological environment."
A Genealogical Approach to Algorithmic Bias,2024,Marta Ziosi; David Watson; Luciano Floridi,SSRN Electronic Journal,1,W4393032343,10.2139/ssrn.4734082,https://openalex.org/W4393032343,https://doi.org/10.2139/ssrn.4734082,Computer science; Econometrics; Mathematics,article,True,
A Genealogical Approach to Algorithmic Bias,2024,Marta Ziosi; David Watson; Luciano Floridi,Minds and Machines,1,W4396581355,10.1007/s11023-024-09672-2,https://openalex.org/W4396581355,https://doi.org/10.1007/s11023-024-09672-2,Philosophy of science; Theory of computation; Philosophy of mind; Computer science; Philosophy of language,article,True,"Abstract The Fairness, Accountability, and Transparency (FAccT) literature tends to focus on bias as a problem that requires ex post solutions (e.g. fairness metrics), rather than addressing the underlying social and technical conditions that (re)produce it. In this article, we propose a complementary strategy that uses genealogy as a constructive, epistemic critique to explain algorithmic bias in terms of the conditions that enable it. We focus on XAI feature attributions (Shapley values) and counterfactual approaches as potential tools to gauge these conditions and offer two main contributions. One is constructive: we develop a theoretical framework to classify these approaches according to their relevance for bias as evidence of social disparities. We draw on Pearl’s ladder of causation (Causality: models, reasoning, and inference. Cambridge University Press, Cambridge, 2000, Causality, 2nd edn. Cambridge University Press, Cambridge, 2009. https://doi.org/10.1017/CBO9780511803161 ) to order these XAI approaches concerning their ability to answer fairness-relevant questions and identify fairness-relevant solutions. The other contribution is critical: we evaluate these approaches in terms of their assumptions about the role of protected characteristics in discriminatory outcomes. We achieve this by building on Kohler-Hausmann’s (Northwest Univ Law Rev 113(5):1163–1227, 2019) constructivist theory of discrimination. We derive three recommendations for XAI practitioners to develop and AI policymakers to regulate tools that address algorithmic bias in its conditions and hence mitigate its future occurrence."
On the Implicit Bias in Deep-Learning Algorithms,2023,Gal Vardi,Communications of the ACM,35,W4377941395,10.1145/3571070,https://openalex.org/W4377941395,https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3571070&file=p86-vardi-supp.pdf,Computer science; Artificial neural network; Artificial intelligence; Algorithm; Deep learning,article,True,Examining the implicit bias in training neural networks using gradient-based methods.
The role of Artificial Intelligence in shaping the future of Media production and the application of Algorithm bias theory in Storytelling,2024,Mary Y. Habib; Maha ElTarabishi,Journal of Media and Interdisciplinary Studies,5,W4400923167,10.21608/jmis.2024.299137.1033,https://openalex.org/W4400923167,,Storytelling; Production (economics); Computer science; Artificial intelligence; Algorithm,article,False,
Addressing Algorithmic Bias in India: Ethical Implications and Pitfalls,2023,Yoshita Sood,SSRN Electronic Journal,2,W4379399777,10.2139/ssrn.4466681,https://openalex.org/W4379399777,https://doi.org/10.2139/ssrn.4466681,Political science; Computer science,article,True,
Michael is better than Mehmet: exploring the perils of algorithmic biases and selective adherence to advice from automated decision support systems in hiring,2024,Astrid M. Rosenthal‐von der Pütten; Alexandra Sach,Frontiers in Psychology,5,W4402418959,10.3389/fpsyg.2024.1416504,https://openalex.org/W4402418959,https://doi.org/10.3389/fpsyg.2024.1416504,Advice (programming); Psychology; Social psychology; Applied psychology; Computer science,article,True,"Introduction Artificial intelligence algorithms are increasingly adopted as decisional aides in many contexts such as human resources, often with the promise of being fast, efficient, and even capable of overcoming biases of human decision-makers. Simultaneously, this promise of objectivity and the increasing supervisory role of humans may make it more likely for existing biases in algorithms to be overlooked, as humans are prone to over-rely on such automated systems. This study therefore aims to investigate such reliance on biased algorithmic advice in a hiring context. Method Simulating the algorithmic pre-selection of applicants we confronted participants with biased or non-biased recommendations in a 1 × 2 between-subjects online experiment ( n = 260). Results The findings suggest that the algorithmic bias went unnoticed for about 60% of the participants in the bias condition when explicitly asking for this. However, overall individuals relied less on biased algorithms making more changes to the algorithmic scores. Reduced reliance on the algorithms led to the increased noticing of the bias. The biased recommendations did not lower general attitudes toward algorithms but only evaluations for this specific hiring algorithm, while explicitly noticing the bias affected both. Individuals with a more negative attitude toward decision subjects were more likely to not notice the bias. Discussion This study extends the literature by examining the interplay of (biased) human operators and biased algorithmic decision support systems to highlight the potential negative impacts of such automation for vulnerable and disadvantaged individuals."
Ethical Considerations of Using ChatGPT in Health Care,2023,Changyu Wang; Siru Liu; Hao Yang; Guo Jiu-lin; Yuxuan Wu; Jialin Liu,Journal of Medical Internet Research,431,W4385242971,10.2196/48009,https://openalex.org/W4385242971,https://www.jmir.org/2023/1/e48009/PDF,Transparency (behavior); Harm; Health care; Compassion; Liability,article,True,"ChatGPT has promising applications in health care, but potential ethical issues need to be addressed proactively to prevent harm. ChatGPT presents potential ethical challenges from legal, humanistic, algorithmic, and informational perspectives. Legal ethics concerns arise from the unclear allocation of responsibility when patient harm occurs and from potential breaches of patient privacy due to data collection. Clear rules and legal boundaries are needed to properly allocate liability and protect users. Humanistic ethics concerns arise from the potential disruption of the physician-patient relationship, humanistic care, and issues of integrity. Overreliance on artificial intelligence (AI) can undermine compassion and erode trust. Transparency and disclosure of AI-generated content are critical to maintaining integrity. Algorithmic ethics raise concerns about algorithmic bias, responsibility, transparency and explainability, as well as validation and evaluation. Information ethics include data bias, validity, and effectiveness. Biased training data can lead to biased output, and overreliance on ChatGPT can reduce patient adherence and encourage self-diagnosis. Ensuring the accuracy, reliability, and validity of ChatGPT-generated content requires rigorous validation and ongoing updates based on clinical practice. To navigate the evolving ethical landscape of AI, AI in health care must adhere to the strictest ethical standards. Through comprehensive ethical guidelines, health care professionals can ensure the responsible use of ChatGPT, promote accurate and reliable information exchange, protect patient privacy, and empower patients to make informed decisions about their health care."
Algorithmic Bias: When stigmatization becomes a perception,2023,Olalekan J. Akintande,,2,W4386246861,10.1145/3600211.3604723,https://openalex.org/W4386246861,,Perception; Racism; Social psychology; Perspective (graphical); Subject (documents),article,False,"In this study, the author examines how perceived stigmatization endangered the stigmatized groups within a society or community. Thus, he goes back in history to dig deep into the sources of perceived stigmatization associated with the black race and how perceived stigmatization has emigrated into AI tools and machine outputs - subjecting vulnerable communities to hypervisibility by exposing them to systems of racial surveillance. To justify the study goal, he conducted a summarized text analysis on racial stigmatization using Twitter hashtags ∈ { black people, blackness, Africa, African-Americans}, all coined out of the Twitter Users' perception of the subject and hypothesized to find high negative sentiment correlation of stigmatization perspective in association with black race and Africa. He finds that Black people are associated with Africa and have a strong negative sentiment correlation with - poorness, crime, death, abuses (stupid), among others, and a subject of racist scum and racism. Similarly, there is a weak negative sentiment correlation with being - bad, abused (such bitch), hate, violence, and protest. He also finds similar strong and weak negative sentiment correlations with other hashtags. He discusses the danger of racial stigmatization and proposes a cycle of ethical algorithmic development & deployment and recommendations."
Fairness and Bias in Algorithmic Hiring: A Multidisciplinary Survey,2024,Alessandro Fabris; Nina Baranowska; Matthew Dennis; David Graus; Philipp Hacker; Jorge Saldivar; Frederik Zuiderveen Borgesius; Asia J. Biega,ACM Transactions on Intelligent Systems and Technology,25,W4402747641,10.1145/3696457,https://openalex.org/W4402747641,https://doi.org/10.1145/3696457,Computer science; Multidisciplinary approach; Data science; Artificial intelligence; Law,article,True,"Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of , algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders."
Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling,2024,Wendy Hui; John W. Lau,,2,W4402593210,10.1109/cccis63483.2024.00016,https://openalex.org/W4402593210,https://figshare.com/articles/conference_contribution/Detecting_and_Mitigating_Algorithmic_Bias_in_Binary_Classification_using_Causal_Modeling/24679857,Computer science; Binary number; Binary classification; Artificial intelligence; Data mining,article,True,"&lt;p dir=""ltr""&gt;This paper proposes the use of causal modeling to detect and mitigate algorithmic bias. We provide a brief description of causal modeling and a general overview of our approach. We then use the Adult dataset, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on gender bias and the problem of binary classification. We show that gender bias in the prediction model is statistically significant at the 0.05 level. We demonstrate the effectiveness of the causal model in mitigating gender bias by cross-validation. Furthermore, we show that the overall classification accuracy is improved slightly. Our novel approach is intuitive, easy-to-use, and can be implemented using existing statistical software tools such as lavaan in R. Hence, it enhances explainability and promotes trust.&lt;/p&gt;"
Algorithmic Bias and Discrimination: Legal and Policy Considerations,2023,Kartik Pendharkar,SSRN Electronic Journal,1,W4389254538,10.2139/ssrn.4640433,https://openalex.org/W4389254538,https://doi.org/10.2139/ssrn.4640433,Political science; Law and economics; Computer science; Econometrics; Psychology,article,True,
"Algorithmic Bias, Generalist Models,and Clinical Medicine",2023,Geoff Keeling,arXiv (Cornell University),1,W4375957568,10.48550/arxiv.2305.04008,https://openalex.org/W4375957568,https://arxiv.org/pdf/2305.04008,Generalist and specialist species; Computer science; Machine learning; Artificial intelligence; Data science,preprint,True,"The technical landscape of clinical machine learning is shifting in ways that destabilize pervasive assumptions about the nature and causes of algorithmic bias. On one hand, the dominant paradigm in clinical machine learning is narrow in the sense that models are trained on biomedical datasets for particular clinical tasks such as diagnosis and treatment recommendation. On the other hand, the emerging paradigm is generalist in the sense that general-purpose language models such as Google's BERT and PaLM are increasingly being adapted for clinical use cases via prompting or fine-tuning on biomedical datasets. Many of these next-generation models provide substantial performance gains over prior clinical models, but at the same time introduce novel kinds of algorithmic bias and complicate the explanatory relationship between algorithmic biases and biases in training data. This paper articulates how and in what respects biases in generalist models differ from biases in prior clinical models, and draws out practical recommendations for algorithmic bias mitigation."
Generative AI in Writing Research Papers: A New Type of Algorithmic Bias and Uncertainty in Scholarly Work,2023,Rishab Jain; Aditya Jain,arXiv (Cornell University),3,W4389982241,10.48550/arxiv.2312.10057,https://openalex.org/W4389982241,https://arxiv.org/pdf/2312.10057,Generative grammar; Interpretability; Computer science; Context (archaeology); Artificial intelligence,preprint,True,"The use of artificial intelligence (AI) in research across all disciplines is becoming ubiquitous. However, this ubiquity is largely driven by hyperspecific AI models developed during scientific studies for accomplishing a well-defined, data-dense task. These AI models introduce apparent, human-recognizable biases because they are trained with finite, specific data sets and parameters. However, the efficacy of using large language models (LLMs) -- and LLM-powered generative AI tools, such as ChatGPT -- to assist the research process is currently indeterminate. These generative AI tools, trained on general and imperceptibly large datasets along with human feedback, present challenges in identifying and addressing biases. Furthermore, these models are susceptible to goal misgeneralization, hallucinations, and adversarial attacks such as red teaming prompts -- which can be unintentionally performed by human researchers, resulting in harmful outputs. These outputs are reinforced in research -- where an increasing number of individuals have begun to use generative AI to compose manuscripts. Efforts into AI interpretability lag behind development, and the implicit variations that occur when prompting and providing context to a chatbot introduce uncertainty and irreproducibility. We thereby find that incorporating generative AI in the process of writing research manuscripts introduces a new type of context-induced algorithmic bias and has unintended side effects that are largely detrimental to academia, knowledge production, and communicating research."
"Data and science engineering: The ethical dilemma of our time-exploring privacy breaches, algorithmic biases, and the need for transparency",2023,Shubham Shubham; Saloni Saloni; Sidra-Tul-Muntaha,World Journal of Advanced Research and Reviews,3,W4366773799,10.30574/wjarr.2023.18.1.0677,https://openalex.org/W4366773799,https://wjarr.com/sites/default/files/WJARR-2023-0677.pdf,Transparency (behavior); Autonomy; Accountability; Engineering ethics; Ethical issues,article,True,"This paper explores the ethical dilemmas associated with data and science engineering, with a focus on privacy breaches, algorithmic biases, and the need for transparency. With the increasing reliance on data-driven decision making and machine learning algorithms, the ethical implications of these technologies have become a pressing issue in various sectors. The study aimed to identify the most significant ethical concerns, analyze their impact on society, and provide solutions to address these issues. The research utilized a systematic review of 18 studies to identify the key ethical issues in data and science engineering. The findings revealed that privacy breaches, algorithmic biases, and lack of transparency were the most prevalent ethical concerns. These issues can have significant implications for individuals and groups, including discrimination, loss of autonomy, and reputational harm. The study also identified vulnerable groups, such as marginalized communities, who may be disproportionately affected by these issues. To address these ethical concerns, the study proposed several solutions, including the development of ethical guidelines, increased transparency and accountability, and the use of diverse and representative datasets. The solutions were informed by the literature review, case studies, and analysis of real-world examples. The study also assessed the feasibility of implementing these solutions and highlighted potential barriers to implementation."
The Alignment of Values: Embedding Human Dignity in Algorithmic Bias Governance for the AGI Era,2025,Yilin Zhao; Z. M. Ren,International Journal of Digital Law and Governance,2,W4409824542,10.1515/ijdlg-2025-0006,https://openalex.org/W4409824542,https://www.degruyterbrill.com/document/doi/10.1515/ijdlg-2025-0006/pdf,Dignity; Embedding; Corporate governance; Computer science; Political science,article,True,"Abstract Decision-makers across both technological and political fields increasingly recognize the need for AI regulation. In the context of AI governance, alignment refers to the requirement that AI systems operate in accordance with human values and interests. This article argues that misalignment is a key driver of algorithmic bias, which not only perpetuates rights infringements but also undermines AI safety, posing risks to its societal integration. This alignment imperative is rooted in the enduring principle of human dignity, a juridical concept that has evolved from its origins in Roman jurisprudence to its establishment as a cornerstone of modern constitutional democracies. Today, human dignity serves as a foundational value underpinning the rule of law. Through comparative legal analysis, this article examines how human dignity informs algorithmic governance across major jurisdictions, analyzing regulatory texts, directives, and case law addressing AI-related challenges. Despite varying implementation approaches, this paper demonstrates that human dignity can serve as a universal foundation for AI governance across cultural contexts. While the European Union prioritizes human dignity in regulating algorithmic bias, emphasizing individual rights, public interests, and human oversight, this principle extends beyond European law, offering a normative anchor for global AI governance. The article concludes with governance recommendations for the AGI era, advocating for the integration of human dignity into AI alignment. This requires both embedding dignity-preserving constraints at the technical level and developing robust assessment frameworks capable of evaluating increasingly advanced AI systems. As AI surpasses human intelligence, governance mechanisms must ensure these systems align with ethical principles, remain under meaningful human control, and operate within legally and socially acceptable boundaries."
Algorithmic Bias and Fairness in Biomedical and Health Research,2025,Rebet Keith Jones,Advances in computational intelligence and robotics book series,1,W4410355209,10.4018/979-8-3373-4252-8.ch008,https://openalex.org/W4410355209,,Psychology; Computer science; Sociology; Political science; Internet privacy,book-chapter,False,"The rapid integration of artificial intelligence (AI) and machine learning (ML) into biomedical and health research has the potential to transform patient care, diagnosis, and treatment outcomes. However, as these technologies evolve, concerns surrounding algorithmic bias and fairness have emerged. In the context of healthcare, biased algorithms can exacerbate disparities in health outcomes, leading to inequality in care and undermining trust in AI-driven systems. This chapter explores the ethical implications of algorithmic bias in biomedical research, focusing on the factors contributing to bias in datasets, model design, and decision-making processes. Additionally, it examines various strategies and frameworks aimed at promoting fairness and equity in AI applications. Through a multidisciplinary lens, the chapter presents a critical analysis of how algorithmic fairness can be achieved, with particular emphasis on practical solutions and regulatory considerations to safeguard both the integrity of research and the well-being of diverse patient populations"
"Large Language Models in Medical Education: Opportunities, Challenges, and Future Directions",2023,Alaa Abd‐Alrazaq; Rawan AlSaad; Dari Alhuwail; Arfan Ahmed; M Healy; Syed Latifi; Sarah Aziz; Rafat Damseh; Sadam Alabed Alrazak; Javaid I. Sheikh,JMIR Medical Education,489,W4376866715,10.2196/48291,https://openalex.org/W4376866715,https://mededu.jmir.org/2023/1/e48291/PDF,Engineering ethics; Curriculum; Misinformation; Competence (human resources); Paradigm shift,article,True,"The integration of large language models (LLMs), such as those in the Generative Pre-trained Transformers (GPT) series, into medical education has the potential to transform learning experiences for students and elevate their knowledge, skills, and competence. Drawing on a wealth of professional and academic experience, we propose that LLMs hold promise for revolutionizing medical curriculum development, teaching methodologies, personalized study plans and learning materials, student assessments, and more. However, we also critically examine the challenges that such integration might pose by addressing issues of algorithmic bias, overreliance, plagiarism, misinformation, inequity, privacy, and copyright concerns in medical education. As we navigate the shift from an information-driven educational paradigm to an artificial intelligence (AI)–driven educational paradigm, we argue that it is paramount to understand both the potential and the pitfalls of LLMs in medical education. This paper thus offers our perspective on the opportunities and challenges of using LLMs in this context. We believe that the insights gleaned from this analysis will serve as a foundation for future recommendations and best practices in the field, fostering the responsible and effective use of AI technologies in medical education."
"Out of One, Many: Using Language Models to Simulate Human Samples",2023,Lisa P. Argyle; Ethan C. Busby; Nancy Fulda; Joshua R. Gubler; Christopher Rytting; David Wingate,Political Analysis,396,W4321455981,10.1017/pan.2023.2,https://openalex.org/W4321455981,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/035D7C8A55B237942FB6DBAD7CAA4E49/S1047198723000025a.pdf/div-class-title-out-of-one-many-using-language-models-to-simulate-human-samples-div.pdf,Variety (cybernetics); Fidelity; Computer science; Context (archaeology); Sociocultural evolution,article,True,"Abstract We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the “algorithmic bias” within one such tool—the GPT-3 language model—is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines."
Algorithmic bias in anthropomorphic artificial intelligence: Critical perspectives through the practice of women media artists and designers,2023,Caterina Antonopoulou,Technoetic Arts,3,W4391184189,10.1386/tear_00109_1,https://openalex.org/W4391184189,,Computer science; Psychology; Artificial intelligence; Multimedia; Human–computer interaction,article,False,"Current research in artificial intelligence (AI) sheds light on algorithmic bias embedded in AI systems. The underrepresentation of women in the AI design sector of the tech industry, as well as in training datasets, results in technological products that encode gender bias, reinforce stereotypes and reproduce normative notions of gender and femininity. Biased behaviour is notably reflected in anthropomorphic AI systems, such as personal intelligent assistants (PIAs) and chatbots, that are usually feminized through various design parameters, such as names, voices and traits. Gendering of AI entities, however, is often reduced to the encoding of stereotypical behavioural patterns that perpetuate normative assumptions about the role of women in society. The impact of this behaviour on social life increases, as human-to-(anthropomorphic)machine interactions are mirrored in human-to-human social interactions. This article presents current critical research on AI bias, focusing on anthropomorphic systems. Moreover, it discusses the significance of women’s engagement in AI design and programming, by presenting selected case studies of contemporary female media artists and designers. Finally, it suggests that women, through their creative practice, provide feminist and critical approaches to AI design which are essential for imagining alternative, inclusive, ethic and de-biased futures for anthropomorphic AIs."
Mitigating Algorithmic Bias: Strategies for Addressing Discrimination in Data,2024,Sonia Gipson Rankin,SSRN Electronic Journal,2,W4400918480,10.2139/ssrn.4902043,https://openalex.org/W4400918480,https://doi.org/10.2139/ssrn.4902043,Computer science; Data science,article,True,
Exposing the chimp optimization algorithm: A misleading metaheuristic technique with structural bias,2024,Lingyun Deng; Sanyang Liu,Applied Soft Computing,32,W4394566650,10.1016/j.asoc.2024.111574,https://openalex.org/W4394566650,,Metaheuristic; Computer science; Optimization algorithm; Algorithm; Mathematical optimization,article,False,
Investigating What Factors Influence Users’ Rating of Harmful Algorithmic Bias and Discrimination,2024,Sara Kingsley; Jiayin Zhi; Wesley Hanwen Deng; Jaimie Lee; S.Q. Zhang; Motahhare Eslami; Kenneth Holstein; Jason Hong; Tianshi Li; Mouquan Shen,Proceedings of the AAAI Conference on Human Computation and Crowdsourcing,2,W4403434407,10.1609/hcomp.v12i1.31602,https://openalex.org/W4403434407,https://ojs.aaai.org/index.php/HCOMP/article/download/31602/33768,Psychology; Computer science; Applied psychology; Cognitive psychology,article,True,"There has been growing recognition of the crucial role users, especially those from marginalized groups, play in uncovering harmful algorithmic biases. However, it remains unclear how users’ identities and experiences might impact their rating of harmful biases. We present an online experiment (N=2,197) examining these factors: demographics, discrimination experiences, and social and technical knowledge. Participants were shown examples of image search results, including ones that previous literature has identified as biased against marginalized racial, gender, or sexual orientation groups. We found participants from marginalized gender or sexual orientation groups were more likely to rate the examples as more severely harmful. Belonging to marginalized races did not have a similar pattern. Additional factors affecting users’ ratings included discrimination experiences, and having friends or family belonging to marginalized demographics. A qualitative analysis offers insights into users' bias recognition, and why they see biases the way they do. We provide guidance for designing future methods to support effective user-driven auditing."
"Editorial Note for Special Issue on Al and Fake News, Mis(dis)information, and Algorithmic Bias",2023,Donghee Shin; Kerk F. Kee,Journal of Broadcasting & Electronic Media,4,W4380878039,10.1080/08838151.2023.2225665,https://openalex.org/W4380878039,,Misinformation; Journalism; Sociology; Digital media; Digital era,editorial,False,"Click to increase image sizeClick to decrease image size Disclosure statementNo potential conflict of interest was reported by the author(s).Additional informationNotes on contributorsDonghee ShinDonghee Shin (Ph.D. & M.A., Syracuse University) is a professor and chair at the College of Media & Communication at Texas Tech University. His research interests include digital journalism, human-computer interaction, and algorithmic media. His recent research in the algorithm as media addresses misinformation, inoculation theory, and algorithmic biases.Kerk F. KeeKerk F. Kee (Ph.D., the University of Texas at Austin) is an associate professor at the College of Media & Communication at Texas Tech University. His research primarily investigates the development, adoption, implementation, and ultimate diffusion of big data technologies in scientific organizations. He also studies the dissemination of health information in cultural communities and the spread of pro-environmental attitudes in modern societies."
Mitigating algorithmic bias in opioid risk-score modeling to ensure equitable access to pain relief,2023,Atharva M. Bhagwat; Kadija Ferryman; Jason B. Gibbons,Nature Medicine,4,W4360612372,10.1038/s41591-023-02256-0,https://openalex.org/W4360612372,,Opioid; Medicine; Opioid epidemic; Pain relief; Business,letter,False,
Questioning AI: How Racial Identity Shapes the Perceptions of Algorithmic Bias,2023,Soojong Kim; Joomi Lee; Poong Oh,,2,W4381513294,10.31234/osf.io/afcdn,https://openalex.org/W4381513294,https://psyarxiv.com/afcdn/download,Injustice; Social psychology; Perception; Social identity theory; Psychology,preprint,True,"[Published in International Journal of Communication] Growing concerns indicate that automated decision-making (ADM) may discriminate against certain social groups, but little is known about how social identities of people influence their perceptions of biased automated decisions. Focusing on the context of racial disparity, this study examined if individuals’ social identities (white vs. People of Color) and social contexts that entail discrimination (discrimination target: the self vs. the other) affect the perceptions of algorithm outcomes. A randomized controlled experiment (N = 604) demonstrated that a participant’s social identity significantly moderated the effects of the discrimination target on the perceptions. Among POC participants, algorithms that discriminate against the subject decreased their perceived fairness and trust, whereas among white participants opposite patterns were observed. The findings imply that social disparity and inequality, and different social groups’ lived experiences of the existing discrimination and injustice should be at the center of understanding how people make sense of biased algorithms."
A Scoping Review On The Impact Of Algorithm Bias On The Perceived Fairness,2023,Amirhossein Hajigholam Saryazdi; Mahdi Mirhosseini,Research Square (Research Square),1,W4387574695,10.21203/rs.3.rs-3428999/v1,https://openalex.org/W4387574695,https://www.researchsquare.com/article/rs-3428999/latest.pdf,Scope (computer science); Injustice; Computer science; Task (project management); Mechanism (biology),review,True,"Abstract Artificial intelligence (AI) based algorithms increasingly shape our daily lives. These algorithms can be biased in a way that compounds injustice in society. This affects how people perceive the fairness of the algorithms, but little is known about the mechanism and scope. To address this research gap, this study conducts a scoping literature review of published papers representing the current state of the research. Then develops a novel theoretical model about the impact of algorithm bias on perceived fairness by synthesizing common themes namely: algorithm bias, algorithm fairness, perceived fairness, individual characteristics, social characteristics, task characteristics, and technology characteristics. The paper concludes by formulating propositions that highlight the significant gap in the literature, contribute to a better understanding of the relationships between identified themes and their components, and provide a roadmap for future research."
"Antimicrobial resistance: Impacts, challenges, and future prospects",2024,Sirwan Khalid Ahmed; Safin Hussein; Karzan Qurbani; Radhwan Hussein Ibrahim; Abdulmalik Fareeq; Kochr Ali Mahmood; Mona Gamal Mohamed,Journal of Medicine Surgery and Public Health,582,W4392349824,10.1016/j.glmedi.2024.100081,https://openalex.org/W4392349824,https://doi.org/10.1016/j.glmedi.2024.100081,Antibiotic resistance; Antimicrobial stewardship; Business; Agriculture; Pandemic,article,True,"Antimicrobial resistance (AMR) is a critical global health issue driven by antibiotic misuse and overuse in various sectors, leading to the emergence of resistant microorganisms. The history of AMR dates back to the discovery of penicillin, with the rise of multidrug-resistant pathogens posing significant challenges to healthcare systems worldwide. The misuse of antibiotics in human and animal health, as well as in agriculture, contributes to the spread of resistance genes, creating a ""Silent Pandemic"" that could surpass other causes of mortality by 2050. AMR affects both humans and animals, with resistant pathogens posing challenges in treating infections. Various mechanisms, such as enzymatic modification and biofilm formation, enable microbes to withstand the effects of antibiotics. The lack of effective antibiotics threatens routine medical procedures and could lead to millions of deaths annually if left unchecked. The economic impact of AMR is substantial, with projected losses in the trillions of dollars and significant financial burdens on healthcare systems and agriculture. Artificial intelligence is being explored as a tool to combat AMR by improving diagnostics and treatment strategies, although challenges such as data quality and algorithmic biases exist. To address AMR effectively, a One Health approach that considers human, animal, and environmental factors is crucial. This includes enhancing surveillance systems, promoting stewardship programs, and investing in research and development for new antimicrobial options. Public awareness, education, and international collaboration are essential for combating AMR and preserving the efficacy of antibiotics for future generations."
In This Special Section: Algorithmic Bias—Australia’s Robodebt and Its Human Rights Aftermath,2024,Katina Michael,IEEE Transactions on Technology and Society,3,W4402704508,10.1109/tts.2024.3444248,https://openalex.org/W4402704508,,Section (typography); Human rights; Special section; Political science; Computer science,article,False,
Algorithmic Bias and Physician Liability,2024,Shujie Luan; Shubhranshu Singh; Tinglong Dai,SSRN Electronic Journal,1,W4405236536,10.2139/ssrn.5046254,https://openalex.org/W4405236536,https://doi.org/10.2139/ssrn.5046254,Liability; Business; Actuarial science; Law and economics; Accounting,preprint,True,
Algorithmic Bias as Ultimately Fixable,2024,Elisabeth Kelan,,1,W4401973221,10.4324/9781003427100-4,https://openalex.org/W4401973221,https://doi.org/10.4324/9781003427100-4,Computer science,book-chapter,True,
"Focusing on Decisions, Outcomes, and Value Judgments to Confront Algorithmic Bias",2023,Apurvakumar Pandya; Jinyi Zhu,JAMA Network Open,2,W4380730826,10.1001/jamanetworkopen.2023.18501,https://openalex.org/W4380730826,https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2806104/pandya_2023_ic_230093_1686167895.4643.pdf,Value (mathematics); Psychology; Sociology; Positive economics; Statistics,article,True,"Sara Khor, MASc; Eric C. Haupt, ScM; Erin E. Hahn, PhD, MPH; Lindsay Joe L. Lyons, LVN; Veena Shankaran, MD, MS; Aasthaa Bansal, PhD"
Algorithmic bias in public health AI: a silent threat to equity in low-resource settings,2025,Jeena Joseph,Frontiers in Public Health,3,W4412583896,10.3389/fpubh.2025.1643180,https://openalex.org/W4412583896,https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2025.1643180/pdf,Health equity; Equity (law); Public health; Resource (disambiguation); Computer science,article,True,
Opening the ‘black box’ of HRM algorithmic biases – How hiring practices induce discrimination on freelancing platforms,2025,Yannik Trautwein; Felix Zechiel; Kristof Coussement; Matthijs Meire; Marion Büttgen,Journal of Business Research,4,W4408310373,10.1016/j.jbusres.2025.115298,https://openalex.org/W4408310373,,Black box; Business; Computer science; Labour economics; Data science,article,False,
Algorithmic bias: Looking beyond data bias to ensure algorithmic accountability and equity,2023,S. J. Thais; Hannah Shumway; Austin Saragih,,1,W4386301315,10.38105/spr.5lwvw66ssy,https://openalex.org/W4386301315,https://sciencepolicyreview.org/wp-content/uploads/securepdfs/2023/08/MITSPR-v4-191618004007.pdf,Accountability; Transparency (behavior); Computer science; Equity (law); Audit,article,True,"As algorithms increasingly aid public sector decision making in the United States, it becomes important to understand how to effectively tackle algorithmic bias in systems that local, state, and federal government entities use and procure, including what kinds of policies are currently in place or proposed. There is a prevalent belief that algorithmic bias arises primarily from statistical biases present in the data used to train or develop the algorithm, but bias may also arise during data collection, problem specification, how and where algorithms are deployed, and within the broader societal contextualization of algorithms. So far, enacted policies in in the US center on temporary bans of particular types of algorithms, transparency, and post-hoc bias audits, as well as more wide-ranging (but non-binding) policy frameworks; all largely focus on quantitative notions of fairness when they assess bias, leaving room for more comprehensive legislation to meaningfully address this issue going forward."
Algorithmic Bias in BERT for Response Accuracy Prediction: A Case Study for Investigating Population Validity,2024,Guher Gorgun; Seyma N. Yildirim‐Erbasli,Journal of Educational Measurement,2,W4403847593,10.1111/jedm.12420,https://openalex.org/W4403847593,https://doi.org/10.1111/jedm.12420,Item response theory; Population; Psychology; Test validity; Predictive validity,article,True,"Abstract Pretrained large language models (LLMs) have gained popularity in recent years due to their high performance in various educational tasks such as learner modeling, automated scoring, automatic item generation, and prediction. Nevertheless, LLMs are black box approaches where models are less interpretable, and they may carry human biases and prejudices because historical human data have been used for pretraining these large‐scale models. For these reasons, the prediction tasks based on LLMs require scrutiny to ensure that the prediction models are fair and unbiased. In this study, we used BERT—a pretrained encoder‐only LLM for predicting response accuracy using action sequences extracted from the 2012 PIAAC assessment. We selected three countries (i.e., Finland, Slovakia, and the United States) representing different performance levels in the overall PIAAC assessment. We found promising results for predicting response accuracy using the fine‐tuned BERT model. Additionally, we examined algorithmic bias in the prediction models trained with different countries. We found differences in model performance, suggesting that some trained models are not free from bias, and thus the models are less generalizable across countries. Our results highlighted the importance of investigating algorithmic fairness in prediction models utilizing algorithmic systems to ensure models are bias‐free."
"Missingness and algorithmic bias: an example from the United States National Outbreak Reporting System, 2009–2019",2024,Emily Diemer; Elena N. Naumova,Journal of Public Health Policy,2,W4396602160,10.1057/s41271-024-00477-2,https://openalex.org/W4396602160,,Public health law; Medical sociology; Social policy; Public health; Health care reform,article,False,
Laissez-Faire Harms: Algorithmic Biases in Generative Language Models,2024,Evan Shieh; Faye-Marie Vassel; Cassidy R. Sugimoto; Thema Monroe‐White,arXiv (Cornell University),2,W4394780976,10.48550/arxiv.2404.07475,https://openalex.org/W4394780976,https://arxiv.org/pdf/2404.07475,Psychology; Identity (music); Social psychology; Generative grammar; Sexual orientation,preprint,True,"The rapid deployment of generative language models (LMs) has raised concerns about social biases affecting the well-being of diverse consumers. The extant literature on generative LMs has primarily examined bias via explicit identity prompting. However, prior research on bias in earlier language-based technology platforms, including search engines, has shown that discrimination can occur even when identity terms are not specified explicitly. Studies of bias in LM responses to open-ended prompts (where identity classifications are left unspecified) are lacking and have not yet been grounded in end-consumer harms. Here, we advance studies of generative LM bias by considering a broader set of natural use cases via open-ended prompting. In this ""laissez-faire"" setting, we find that synthetically generated texts from five of the most pervasive LMs (ChatGPT3.5, ChatGPT4, Claude2.0, Llama2, and PaLM2) perpetuate harms of omission, subordination, and stereotyping for minoritized individuals with intersectional race, gender, and/or sexual orientation identities (AI/AN, Asian, Black, Latine, MENA, NH/PI, Female, Non-binary, Queer). We find widespread evidence of bias to an extent that such individuals are hundreds to thousands of times more likely to encounter LM-generated outputs that portray their identities in a subordinated manner compared to representative or empowering portrayals. We also document a prevalence of stereotypes (e.g. perpetual foreigner) in LM-generated outputs that are known to trigger psychological harms that disproportionately affect minoritized individuals. These include stereotype threat, which leads to impaired cognitive performance and increased negative self-perception. Our findings highlight the urgent need to protect consumers from discriminatory harms caused by language models and invest in critical AI education programs tailored towards empowering diverse consumers."
Mitigating Algorithmic Bias Through Probability Calibration: A Case Study on Lead Generation Data,2025,M. Nikolić; Danilo Nikolić; Miroslav Stefanović; Sara Koprivica; Darko Stefanović,Mathematics,2,W4412039864,10.3390/math13132183,https://openalex.org/W4412039864,https://doi.org/10.3390/math13132183,Calibration; Lead (geology); Computer science; Statistics; Econometrics,article,True,"Probability calibration is commonly utilized to enhance the reliability and interpretability of probabilistic classifiers, yet its potential for reducing algorithmic bias remains under-explored. In this study, the role of probability calibration techniques in mitigating bias associated with sensitive attributes, specifically country of origin, within binary classification models is investigated. Using a real-world lead-generation 2853 × 8 matrix dataset characterized by substantial class imbalance, with the positive class representing 1.4% of observations, several binary classification models were evaluated and the best-performing model was selected as the baseline for further analysis. The evaluated models included Binary Logistic Regression with polynomial degrees of 1, 2, 3, and 4, Random Forest, and XGBoost classification algorithms. Three widely used calibration methods, Platt scaling, isotonic regression, and temperature scaling, were then used to assess their impact on both probabilistic accuracy and fairness metrics of the best-performing model. The findings suggest that post hoc calibration can effectively reduce the influence of sensitive features on predictions by improving fairness without compromising overall classification performance. This study demonstrates the practical value of incorporating calibration as a straightforward and effective fairness intervention within machine learning workflows."
"Balancing Privacy and Progress: A Review of Privacy Challenges, Systemic Oversight, and Patient Perceptions in AI-Driven Healthcare",2024,S. Williamson; Victor R. Prybutok,Applied Sciences,323,W4390829176,10.3390/app14020675,https://openalex.org/W4390829176,https://www.mdpi.com/2076-3417/14/2/675/pdf?version=1705396255,Health care; Autonomy; Confidentiality; Transformative learning; Information privacy,review,True,"Integrating Artificial Intelligence (AI) in healthcare represents a transformative shift with substantial potential for enhancing patient care. This paper critically examines this integration, confronting significant ethical, legal, and technological challenges, particularly in patient privacy, decision-making autonomy, and data integrity. A structured exploration of these issues focuses on Differential Privacy as a critical method for preserving patient confidentiality in AI-driven healthcare systems. We analyze the balance between privacy preservation and the practical utility of healthcare data, emphasizing the effectiveness of encryption, Differential Privacy, and mixed-model approaches. The paper navigates the complex ethical and legal frameworks essential for AI integration in healthcare. We comprehensively examine patient rights and the nuances of informed consent, along with the challenges of harmonizing advanced technologies like blockchain with the General Data Protection Regulation (GDPR). The issue of algorithmic bias in healthcare is also explored, underscoring the urgent need for effective bias detection and mitigation strategies to build patient trust. The evolving roles of decentralized data sharing, regulatory frameworks, and patient agency are discussed in depth. Advocating for an interdisciplinary, multi-stakeholder approach and responsive governance, the paper aims to align healthcare AI with ethical principles, prioritize patient-centered outcomes, and steer AI towards responsible and equitable enhancements in patient care."
People see more of their biases in algorithms,2024,Begum Celiktutan; Romain Cadario; Carey K. Morewedge,Proceedings of the National Academy of Sciences,12,W4394675493,10.1073/pnas.2317602121,https://openalex.org/W4394675493,https://www.pnas.org/doi/pdf/10.1073/pnas.2317602121,Debiasing; Cognitive bias; Algorithm; Confirmation bias; Computer science,article,True,"Algorithmic bias occurs when algorithms incorporate biases in the human decisions on which they are trained. We find that people see more of their biases (e.g., age, gender, race) in the decisions of algorithms than in their own decisions. Research participants saw more bias in the decisions of algorithms trained on their decisions than in their own decisions, even when those decisions were the same and participants were incentivized to reveal their true beliefs. By contrast, participants saw as much bias in the decisions of algorithms trained on their decisions as in the decisions of other participants and algorithms trained on the decisions of other participants. Cognitive psychological processes and motivated reasoning help explain why people see more of their biases in algorithms. Research participants most susceptible to bias blind spot were most likely to see more bias in algorithms than self. Participants were also more likely to perceive algorithms than themselves to have been influenced by irrelevant biasing attributes (e.g., race) but not by relevant attributes (e.g., user reviews). Because participants saw more of their biases in algorithms than themselves, they were more likely to make debiasing corrections to decisions attributed to an algorithm than to themselves. Our findings show that bias is more readily perceived in algorithms than in self and suggest how to use algorithms to reveal and correct biased human decisions."
Understanding how users may work around algorithmic bias,2025,Hannah Overbye-Thompson; Ronald E. Rice,AI & Society,1,W4412676577,10.1007/s00146-025-02498-1,https://openalex.org/W4412676577,https://link.springer.com/content/pdf/10.1007/s00146-025-02498-1.pdf,Work (physics); Performing arts; Computer science; Data science; Human–computer interaction,article,True,"Abstract Algorithms increasingly mediate critical aspects of daily life across healthcare, hiring, and social media, shaping user experiences through automated decision-making processes. Yet, algorithmic bias, the systematic disadvantaging of certain groups through automated systems, has been widely documented across a variety of algorithms. Thus this study addresses the gap in understanding how users may respond to four epistemic categories of algorithm bias, depending on whether it exists or not, and is perceived or not. We apply the information systems concept of workarounds to characterize potential user responses to these categories of algorithmic bias. Then, we apply the Human–AI Interaction Theory of Interactive Media Effects to understand how users may detect bias through cue routes and develop workaround strategies through action routes. Our theoretical framework proposes how users' detection and workarounds may vary based on the four categories of bias. Understanding these adaptive strategies provides crucial insights for developing inclusive technologies and fostering algorithmic literacy, ultimately enhancing the ongoing negotiation between human agency and technological constraint in digital societies."
Mitigating Algorithmic Bias on Facial Expression Recognition,2023,Glauco Amigo; Pablo Rivas; Robert J. Marks,arXiv (Cornell University),1,W4390306227,10.48550/arxiv.2312.15307,https://openalex.org/W4390306227,https://arxiv.org/pdf/2312.15307,Debiasing; Autoencoder; Computer science; Facial expression recognition; Facial expression,preprint,True,"Biased datasets are ubiquitous and present a challenge for machine learning. For a number of categories on a dataset that are equally important but some are sparse and others are common, the learning algorithms will favor the ones with more presence. The problem of biased datasets is especially sensitive when dealing with minority people groups. How can we, from biased data, generate algorithms that treat every person equally? This work explores one way to mitigate bias using a debiasing variational autoencoder with experiments on facial expression recognition."
"Structural Racism in Tech: Social Media Platforms, Algorithmic Bias, and Racist Tech",2024,Safiya Noble; Sarah Roberts; Matthew Bui; André Brock; Olivia Snow,,2,W4405059960,10.1007/978-3-031-69362-5_37,https://openalex.org/W4405059960,https://doi.org/10.1007/978-3-031-69362-5_37,Racism; Key (lock); Social media; Racial bias; High tech,book-chapter,True,"Abstract Covers key studies published over the last decade documenting the harmful effects of racist technologies, which include how algorithms are racially biased and produce harmful effects."
IEEE Standard for Algorithmic Bias Considerations,2024,,,1,W4406755120,10.1109/ieeestd.2025.10851955,https://openalex.org/W4406755120,,Computer science,standard,False,
Are algorithmic bias claims supported?—Response,2023,Sandra González‐Bailón; David Lazer,Science,1,W4387128734,10.1126/science.adk4899,https://openalex.org/W4387128734,,Computer science; Data science,article,False,
Avoidable and Unavoidable AI Algorithmic Bias,2024,Tshilidzi Marwala,,1,W4404283849,10.1007/978-981-97-9251-1_8,https://openalex.org/W4404283849,,Computer science,book-chapter,False,
Algorithms propagate gender bias in the marketplace—with consumers’ cooperation,2023,Shelly Rathee; Sachin Banker; Arul Mishra; Himanshu Mishra,Journal of Consumer Psychology,20,W4365449190,10.1002/jcpy.1351,https://openalex.org/W4365449190,,Psychographic; Gender bias; Product (mathematics); Field (mathematics); Digital advertising,article,False,"Abstract Recent research shows that algorithms learn societal biases from large text corpora. We examine the marketplace‐relevant consequences of such bias for consumers. Based on billions of documents from online text corpora, we first demonstrate that from gender biases embedded in language, algorithms learn to associate women with more negative consumer psychographic attributes than men (e.g., associating women more closely with impulsive vs. planned investors). Second, in a series of field experiments, we show that such learning results in the delivery of gender‐biased digital advertisements and product recommendations. Specifically, across multiple platforms, products, and attributes, we find that digital advertisements containing negative psychographic attributes (e.g., impulsive) are more likely to be delivered to women compared to men, and that search engine product recommendations are similarly biased, which influences consumer's consideration sets and choice. Finally, we empirically examine consumer's role in co‐producing algorithmic gender bias in the marketplace and observe that consumers reinforce these biases by accepting gender stereotypes (i.e., clicking on biased ads). We conclude by discussing theoretical and practical implications."
A COMPREHENSIVE REVIEW OF BIAS IN AI ALGORITHMS,2024,Abdul Wajid Fazil; Musawer Hakimi; Amir Kror Shahidzay,Nusantara Hasana Journal,16,W4391069191,10.59003/nhj.v3i8.1052,https://openalex.org/W4391069191,https://nusantarahasanajournal.com/index.php/nhj/article/download/1052/853,Scopus; Computer science; Software deployment; Artificial intelligence; Inclusion (mineral),review,True,"This comprehensive review aims to analyze and synthesize the existing literature on bias in AI algorithms, providing a thorough understanding of the challenges, methodologies, and implications associated with biased artificial intelligence systems.Employing a narrative synthesis and systematic literature review approach, this study systematically explores a wide array of sources from prominent databases such as PubMed, Google Scholar, Scopus, Web of Science, and ScienceDirect. The inclusion criteria focused on studies that distinctly defined artificial intelligence in the education sector, were published in English, and underwent peer-review. Five independent reviewers meticulously evaluated search results, extracted pertinent data, and assessed the quality of included studies, ensuring a rigorous and comprehensive analysis. The synthesis of findings reveals pervasive patterns of bias in AI algorithms across various domains, shedding light on the nuanced aspects of discriminatory practices. The systematic review highlights the need for continued research, emphasizing the intricate interplay between bias, technological advancements, and societal impacts. The comprehensive analysis underscores the complexity of bias in AI algorithms, emphasizing the critical importance of addressing these issues in future developments. Recognizing the limitations and potential consequences, the study calls for a concerted effort from researchers, developers, and policymakers to mitigate bias and foster the responsible deployment of AI technologies. Based on the findings, recommendations include implementing robust bias detection mechanisms, enhancing diversity in AI development teams, and establishing transparent frameworks for algorithmic decision-making. The implications of this study extend beyond academia, informing industry practices and policy formulations to create a more equitable and ethically grounded AI landscape."
Inside the Black Box: Detecting and Mitigating Algorithmic Bias across Racialized Groups in College Student-Success Prediction,2023,Denisa Gándara; Hadis Anahideh; Matthew P. Ison; Anuja Tayal,arXiv (Cornell University),2,W4315705885,10.48550/arxiv.2301.03784,https://openalex.org/W4315705885,https://arxiv.org/pdf/2301.03784,Bachelor; Psychological intervention; Racism; Racial bias; Implicit bias,preprint,True,"Colleges and universities are increasingly turning to algorithms that predict college-student success to inform various decisions, including those related to admissions, budgeting, and student-success interventions. Because predictive algorithms rely on historical data, they capture societal injustices, including racism. In this study, we examine how the accuracy of college student success predictions differs between racialized groups, signaling algorithmic bias. We also evaluate the utility of leading bias-mitigating techniques in addressing this bias. Using nationally representative data from the Education Longitudinal Study of 2002 and various machine learning modeling approaches, we demonstrate how models incorporating commonly used features to predict college-student success are less accurate when predicting success for racially minoritized students. Common approaches to mitigating algorithmic bias are generally ineffective at eliminating disparities in prediction outcomes and accuracy between racialized groups."
Algorithmic Bias in Image-Generating Artificial Intelligence: Prevalence and User Perceptions,2024,Tanja Messingschlager; Markus Appel,,1,W4393864832,10.31234/osf.io/wjeps,https://openalex.org/W4393864832,https://osf.io/wjeps/download,Perception; Image (mathematics); Artificial intelligence; Computer science; Psychology,preprint,True,"Image-generating AI is among the most popular generative AI applications, likely changing the visual mediated environments humans are exposed to on a mass scale. Prior work found that AI can be biased against women and minorities (algorithmic bias), whereas humans attribute rather high objectivity to AI. We focused on image-generating AI, analyzing the extent of algorithmic bias in AI-generated pictures, as well as human responses to bias in image-generating AI. Study 1 showed that AI-generated portraits of people in STEM professions were almost exclusively depicting male, white (and older) individuals. Study 2 (experimental, N = 495) showed that the responses to AI-generated pictures vary, depending on the portrayed group. Participants perceived pictures to be less biased if they were introduced as AI-generated, but only if pictures showed college students (vs. older people). If images showed older people, participants reported higher moral outrage if pictures were supposedly generated by AI (vs. human creators)."
Detecting algorithmic bias in medical-AI models using trees,2023,Jeffrey C. Smith; Andre L. Holder; Rishikesan Kamaleswaran; Yao Xie,arXiv (Cornell University),1,W4389422181,10.48550/arxiv.2312.02959,https://openalex.org/W4389422181,https://arxiv.org/pdf/2312.02959,Computer science; Artificial intelligence; Machine learning; Clinical decision support system; Context (archaeology),preprint,True,"With the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. This paper presents an innovative framework for detecting areas of algorithmic bias in medical-AI decision support systems. Our approach efficiently identifies potential biases in medical-AI models, specifically in the context of sepsis prediction, by employing the Classification and Regression Trees (CART) algorithm with conformity scores. We verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. The effectiveness of the concept is further validated by experiments using electronic medical records from Grady Memorial Hospital in Atlanta, Georgia. These tests demonstrate the practical implementation of our strategy in a clinical environment, where it can function as a vital instrument for guaranteeing fairness and equity in AI-based medical decisions."
Investigating AI systems: examining data and algorithmic bias through hermeneutic reverse engineering,2025,Nitin Shukla,Frontiers in Communication,2,W4407002433,10.3389/fcomm.2025.1380252,https://openalex.org/W4407002433,https://doi.org/10.3389/fcomm.2025.1380252,Reverse engineering; Data science; Computer science; Epistemology; Philosophy,article,True,"Considering Artificial Intelligence systems as boundary objects, which are interdisciplinary objects sustained differently by diverse fields while providing shared discourses between them, this essay summarizes the approaches of examining bias in AI systems. It argues that examining each part related to the building and working of AI systems is essential for unpacking the political play and potential insert points of biases in them. It concentrates on the critical analysis of data and algorithms as two core parts of AI systems by operationalizing hermeneutic reverse engineering. Hermeneutic reverse engineering is a framework to unpack and understand different elements of a technocultural object and/or system that contribute to the construction of its meaning and contexts. It employs a speculative imagination of what other realities can be designed and includes cultural analysis to identify existing meanings and assumptions behind the technocultural object, identifying key elements of signification, and speculating possibilities of reassembling different meanings for the object. The main results obtained by this method on AI systems is using cultural consideration and technological imagination to unpack existing meanings created by AI and design innovative approaches for AI to exert alternate/ inclusive meanings. The research perspectives presented in this article include critical examination of biases and politics within different elements of AI systems, and the impact of these biases on different social groups. The paper proposes using the method of hermeneutic reverse engineering to investigate AI systems and speculate possible alternate and more accountable futures for AI systems."
Do not treat Bill Gates for prostate cancer! Algorithmic bias and causality in medical prediction,2023,Andrew J. Vickers,British Journal of Urology,2,W4318670249,10.1111/bju.15951,https://openalex.org/W4318670249,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/bju.15951,Life expectancy; Prostate cancer; Prostatectomy; Medicine; Wife,letter,True,"Chase et al. [1] recently published in the BJUI a life-expectancy prediction model for patients with prostate cancer. They expressly recommended this be used to guide treatment decision-making for patients, and even provide an easy-to-use app for doing so. It is the authors’ explicit intention that urologists use their prediction model to decide who should and should not receive curative treatment depending on whether life expectancy is >10 years. The underlying principle is completely sound. Life expectancy is indeed a critical determinant of prostate cancer treatment and many current approaches, such as using social security tables, or informal clinical assessment, are known to be invalid [2]. The problem with the Chase et al. [1] model is that it includes education and marital status. If you take two men with identical tumours, and who are also the same age and have the same comorbidities, you might recommend one but not the other to undergo curative treatment based on his higher level of education, or because he is married. Indeed, one might imagine a situation where a man is scheduled for a radical prostatectomy, but then his wife leaves him and the urologist, after using the app to make a quick calculation, tells the patient that he is no longer eligible for surgery. Perhaps even more problematic, use of the prediction model would systematically discriminate against Black Americans. Centuries of racism has left Blacks less likely to be well-educated or married. So randomly select a Black man and a White man of the same age, cancer and comorbidity profile, there is a good chance that the White man will be referred for surgery or radiotherapy whereas the Black man is denied curative treatment. This can only worsen the severe and chronic racial disparities in prostate cancer outcome that continue to blight American healthcare. To their credit, the authors are not unaware of the problem of algorithmic bias, even citing the seminal work of Kent and Paulus [3]. My own view, however, is that their arguments are weak. Take, for instance, their assertion that ‘<5% of patients … would have had their prediction appreciably changed by a modification to their marital status or educational attainment’. This is far from reassuring. Let us be 100% clear: no-one should be denied treatment because they are unmarried or never got a degree. As it turns out, Bill Gates, an unmarried college drop-out billionaire, might be one of the men who would be given a different prediction because the Chase et al. [1] model includes education and marital status. Indeed, the Bill Gates problem tells us exactly why we should avoid correlates of social determinants of health in prediction models. Causality is not something we normally worry about when making predictions: if it predicts, it predicts. For instance, grip strength in older patients predicts life expectancy and so might be used as a simple, in-office test to help guide shared decision-making [4]. No one thinks that poor grip causes an early death, but that is irrelevant. However, it is rather different for predictors that have social consequences. We must think about causality if our model might exacerbate disparities. Educational and marital status are predictive of life expectancy in patients with prostate cancer because they are associated with social support and access to care and so, as in the case of Bill Gates, we are better off examining a patient's level of social support and access to care than to simply ask about their education and marital status. The numerous medical prediction models that directly include race as a predictor [5] should similarly explore causality. In many prostate-cancer models, Black race is a correlate for genetic differences that are yet to be fully elucidated, and it seems appropriate to include race until better markers of genetic risk are available. In other models, it seems that race is used simply because it is predictive and that will often be because of racism. This has included life-expectancy models in prostate cancer [6], where Black men are given lower life expectancy, and less chance of treatment, simply because they are Black. I am pretty sure that none of us would agree with ‘discriminate against black people because, as a result of previous discrimination, they have poorer outcomes’. Yet, that is often exactly what is implied by including race, or social correlates of race, in prediction models. Laudably, Chase et al. [1] provide a version of their model that avoids education and marital status as inputs. I would encourage anyone interested in using the model to use this version. Moving forward, we need to extremely careful about including race or social determinants of health in prediction models or clinical algorithms, and carefully evaluate how such models or algorithms might impact disparities. None relevant. This work was supported in part by the National Institutes of Health/National Cancer Institute (NIH/NCI) with a Cancer Center Support Grant to Memorial Sloan Kettering Cancer Center [P30-CA008748]."
"Algorithmic bias, fairness, and inclusivity: a multilevel framework for justice-oriented AI",2025,Paola Panarese; Marco Grasso; Claudia Solinas,AI & Society,1,W4412420621,10.1007/s00146-025-02451-2,https://openalex.org/W4412420621,,Economic Justice; Sociology; Fairness measure; Computer science; Psychology,article,False,
Algorithmic Bias and Data Justice: ethical challenges in Artificial Intelligence Systems,2025,Javier González‐Argote; Emilia Urrutia Maldonado; Karina Maldonado,EthAIca,1,W4411475682,10.56294/ai2025159,https://openalex.org/W4411475682,https://doi.org/10.56294/ai2025159,Equity (law); Economic Justice; Corporate governance; Social justice; Inequality,article,True,"This article examines the critical ethical challenges posed by algorithmic bias in artificial intelligence (AI) systems, focusing on its implications for social justice and data equity. Through a systematic review of case studies and theoretical frameworks, we analyze how biased datasets and algorithmic designs perpetuate structural inequalities, particularly affecting marginalized communities. The study highlights key examples, such as gender and racial biases in facial recognition and hiring algorithms, while exploring mitigation strategies rooted in data justice principles. Additionally, we evaluate regulatory responses, including the European Union's AI Act, which proposes a risk-based governance framework. The findings underscore the urgent need for interdisciplinary approaches to develop fairer AI systems that align with ethical standards and human rights."
Algorithmic Bias and Racial Inequality: A Critical Review,2024,Maximilian Kasy,SSRN Electronic Journal,1,W4395684717,10.2139/ssrn.4805547,https://openalex.org/W4395684717,http://doi.org/10.2139/ssrn.4805547,Inequality; Racial bias; Racism; Critical race theory; Sociology,review,True,
Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling,2023,Wendy Hui; John W. Lau,arXiv (Cornell University),1,W4387838730,10.48550/arxiv.2310.12421,https://openalex.org/W4387838730,https://arxiv.org/pdf/2310.12421,Computer science; Black box; Gender bias; Binary number; Focus (optics),preprint,True,"This paper proposes the use of causal modeling to detect and mitigate algorithmic bias. We provide a brief description of causal modeling and a general overview of our approach. We then use the Adult dataset, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on gender bias and the problem of binary classification. We show that gender bias in the prediction model is statistically significant at the 0.05 level. We demonstrate the effectiveness of the causal model in mitigating gender bias by cross-validation. Furthermore, we show that the overall classification accuracy is improved slightly. Our novel approach is intuitive, easy-to-use, and can be implemented using existing statistical software tools such as ""lavaan"" in R. Hence, it enhances explainability and promotes trust."
Algorithmic Bias and Farmers' Autonomy in AI-Driven Agricultural Marketing and Supply Chains,2024,Farai Alice Gwelo,"Advances in marketing, customer relationship management, and e-services book series",1,W4403537131,10.4018/979-8-3693-6760-5.ch013,https://openalex.org/W4403537131,,Autonomy; Supply chain; Agriculture; Marketing; Business,book-chapter,False,"This study examines the ethical and social implications of artificial intelligence (AI) adoption in smallholder agricultural marketing and supply chains, focusing on algorithmic bias and its impact on the autonomy of smallholder farmers in agricultural marketing and supply chains. The findings reveal that data bias is the primary contributor to algorithmic bias. The output of AI systems reflects the data on which the algorithms were trained and when AI-driven agricultural marketing systems are trained on biased data machine learning algorithms and users' choices are informed by the biased data, perpetuating a cycle of bias, limiting smallholder farmers' choices. The study highlights the importance of inclusive data practices to reduce biases and ensure equitable outcomes and support AI-driven agricultural systems that uphold smallholder farmers' interests. It calls attention the need for ethical AI frameworks, comprehensive policy analysis, and strategies to mitigate socioeconomic risks faced by smallholders owing to biased AI training data and algorithms are crucial."
Mitigating Algorithmic Bias in Multiclass CNN Classifications Using Causal Modeling,2025,Min Kwang Byun; Wendy Hui; John W. Lau,arXiv (Cornell University),1,W4406449464,10.48550/arxiv.2501.07885,https://openalex.org/W4406449464,https://arxiv.org/pdf/2501.07885,Computer science; Artificial intelligence; Machine learning,preprint,True,"This study describes a procedure for applying causal modeling to detect and mitigate algorithmic bias in a multiclass classification problem. The dataset was derived from the FairFace dataset, supplemented with emotional labels generated by the DeepFace pre-trained model. A custom Convolutional Neural Network (CNN) was developed, consisting of four convolutional blocks, followed by fully connected layers and dropout layers to mitigate overfitting. Gender bias was identified in the CNN model's classifications: Females were more likely to be classified as ""happy"" or ""sad,"" while males were more likely to be classified as ""neutral."" To address this, the one-vs-all (OvA) technique was applied. A causal model was constructed for each emotion class to adjust the CNN model's predicted class probabilities. The adjusted probabilities for the various classes were then aggregated by selecting the class with the highest probability. The resulting debiased classifications demonstrated enhanced gender fairness across all classes, with negligible impact--or even a slight improvement--on overall accuracy. This study highlights that algorithmic fairness and accuracy are not necessarily trade-offs. All data and code for this study are publicly available for download."
Impact of Explainable AI on Reduction of Algorithm Bias in Facial Recognition Technologies,2024,B Ankita; H ChienChen; Paula Lauren; O Lydia,,1,W4398174331,10.1109/sieds61124.2024.10534745,https://openalex.org/W4398174331,,Computer science; Facial recognition system; Artificial intelligence; Reputation; Machine learning,article,False,"Artificial Intelligence (AI) has grown dramatically over the past few decades and has much greater influence today on human performance in every walk of life. AI is great for accelerating our work, but there are also downsides to it. One such downside is the application of Facial Recognition Technologies (FRT) available today, which has adverse consequences like racial discrimination or wrongful judgement by commercial firms and even law enforcement organizations. The landmark 'Gender Shades' project [1] in 2018 showed a highly skewed error bar between facial recognition accuracies of subjects who are black females compared to white males using an intersectional comparison approach between facial recognition software by IBM, Microsoft and Face++. This supplemented previous studies which highlighted issues with gender classification or on the whole misidentification issues posed by facial recognition technologies (FRT) [2]. This algorithm bias towards misgendering or misclassification based on skin color could be attributed to a highly unbalanced training datasets that lack diversity as well as a lack of understanding of the black box machine learning (ML) models that are used to build the FRTs. From a Code of Ethics perspective, such algorithms do not conform to the fundamental canons [3] such as prioritizing the safety, and welfare of the public. Furthermore, they cannot be used honorably, responsibly and ethically to enhance the reputation, and usefulness of the companies promoting these technologies because they are unable to provide truthful and objective information regarding facial recognition. A possible remedy could be to curate an exhaustively diverse dataset w.r.t both color and gender. However, the task of building such a dataset would require access to a diverse population, which is not always possible in a multi-racial and multi-ethnic society. We thus propose an exhaustive review-based study of existing work on the introduction of explainable AI [4],[5] (XAI) techniques in such ML models to understand how the model itself learns. This study would explore what constraints to put on the learning method of the model, which can enforce reduction in misclassification errors from gender and skin color thus enforcing various ethical aspects like taking care of algorithm bias, introducing transparency and accountability. It would also explore and underline the overall social impact improved FRTs might have from a code of ethics perspective."
Computer Vision: Anthropology of Algorithmic Bias in Facial Analysis Tool,2023,Mayane Batista Lima,IntechOpen eBooks,1,W4379619635,10.5772/intechopen.110330,https://openalex.org/W4379619635,https://www.intechopen.com/citation-pdf-url/87206,Computer science; Artificial intelligence; Machine learning; Ethnic group; Race (biology),book-chapter,True,"The usage of Computer Vision (CV) has led to debates about the bias within the technology. Despite machines being labeled as autonomous, human bias is embedded in data labeling for effective machine learning. Proper training of neural network machines requires massive amounts of “relevant data,” however, not all data is collected. This contributes to a one-sided view and feeds a “standard of data that is not collected.” The machine develops algorithmic decision-making based on the data it is presented, which can create machinic biases such as differences in gender, race/ethnicity, and class. This raises questions about which bodies are recognized by machines and how they are taught to “see” beyond binary “male or female” limitations. The study aims to understand how Amazon’s Rekognition, a facial recognition and analysis tool, analyzes and classifies people of dissident genders who do not conform to “conventional” gender norms. Understanding the mechanisms behind the technology’s decision-making processes can lead to more equitable and inclusive outcomes."
"TrustScapes: A Visualisation Tool to Capture Stakeholders’ Concerns and Recommendations About Data Protection, Algorithmic Bias, and Online Safety",2023,Sachiyo Ito‐Jaeger; Giles Lane; Liz Dowthwaite; Helena Webb; Menisha Patel; Mat Rawsthorne; Virginia Portillo; Marina Jirotka; Elvira Pérez Vallejos,International Journal of Qualitative Methods,2,W4383112433,10.1177/16094069231186965,https://openalex.org/W4383112433,https://journals.sagepub.com/doi/pdf/10.1177/16094069231186965,Worksheet; Flexibility (engineering); Computer science; Focus group; Data collection,article,True,"This paper presents a new methodological approach, TrustScapes, an open access tool designed to identify and visualise stakeholders’ concerns and policy recommendations on data protection, algorithmic bias, and online safety for a fairer and more trustworthy online world. We first describe how the tool was co-created with young people and other stakeholders through a series of workshops. We then present two sets of TrustScapes focus groups to illustrate how the tool can be used, and the data analysed. The paper then provides the methodological insights, including the strengths of the TrustScapes and the lessons for future research using TrustScapes. A key strength of this method is that it allows people to visualise their ideas and thoughts on the worksheet, using the keywords and sketches provided. The flexibility in the mode of delivery is another strength of the TrustScapes method. The TrustScapes focus groups can be conducted in a relatively short time (1.5–2 hours), either in person or online depending on the participants’ needs, geological locations, and practicality. Our experience with the TrustScapes offers some lessons (related to the data collection and analysis) for researchers who wish to use this method in the future. Finally, we describe how the outcomes from the TrustScapes focus groups should help to inform future policy decisions."
"Robust Bias-Compensated LMS Algorithm: Design, Performance Analysis and Applications",2023,Fuyi Huang; Fan Song; Sheng Zhang; Hing Cheung So; Jun Yang,IEEE Transactions on Vehicular Technology,20,W4376851446,10.1109/tvt.2023.3276573,https://openalex.org/W4376851446,,Least mean squares filter; Estimator; Algorithm; Estimation theory; Adaptive filter,article,False,"This paper considers the problem of system parameter estimation using adaptive filter. Conventional adaptive algorithms will result in degraded performance in the presence of impulsive noise and biased estimation when the input signal is noisy. To address these issues, this paper proposes a robust bias-compensated least mean squares (R-BC-LMS) algorithm. It is derived by performing the maximum- <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">a</b> - <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">posteriori</b> estimation subject to a constraint on the squared norm of the weight vector difference, and then introducing an unbiasedness criterion to insert a bias compensation term in the update. Under common statistical assumptions, the mean and mean square behaviors of weight deviation are derived for the R-BC-LMS algorithm. In addition, we develop the estimator for the input and output noise variances. Simulations in channel estimation, vehicle handsfree echo cancellation, and direction-of-arrival estimation demonstrate that our method outperforms the competing algorithms."
On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization,2025,Jiancong Xiao; Ziniu Li; Xingyu Xie; Emily Getzen; Cong Fang; Long Qi; Weijie Su,Journal of the American Statistical Association,2,W4414274825,10.1080/01621459.2025.2555067,https://openalex.org/W4414274825,,,article,False,
Measuring algorithmic bias to analyze the reliability of AI tools that predict depression risk using smartphone sensed-behavioral data,2024,Daniel A. Adler; Caitlin A. Stamatis; Jonah Meyerhoff; David C. Mohr; Fei Wang; Gabriel J. Aranovich; Srijan Sen; Tanzeem Choudhury,Research Square (Research Square),2,W4395007622,10.21203/rs.3.rs-3044613/v1,https://openalex.org/W4395007622,https://www.researchsquare.com/article/rs-3044613/latest.pdf,Reliability (semiconductor); Computer science; Depression (economics); Artificial intelligence; Psychology,preprint,True,"<title>Abstract</title> AI tools intend to transform mental healthcare by providing remote estimates of depression risk using behavioral data collected by sensors embedded in smartphones. While these tools accurately predict elevated symptoms in small, homogenous populations, recent studies show that these tools are less accurate in larger, more diverse populations. In this work, we show that accuracy is reduced because sensed-behaviors are unreliable predictors of depression across individuals; specifically the sensed-behaviors that predict depression risk are inconsistent across demographic and socioeconomic subgroups. We first identified subgroups where a developed AI tool underperformed by measuring algorithmic bias, where subgroups with depression were incorrectly predicted to be at lower risk than healthier subgroups. We then found inconsistencies between sensed-behaviors predictive of depression across these subgroups. Our findings suggest that researchers developing AI tools predicting mental health from behavior should think critically about the generalizability of these tools, and consider tailored solutions for targeted populations."
AI’s Diversity Problem in Radiology: Addressing Algorithm Bias,2024,Kerri Reeves,Applied Radiology,1,W4391547243,10.37549/ar2952,https://openalex.org/W4391547243,https://doi.org/10.37549/ar2952,Diversity (politics); Computer science; Psychology; Algorithm; Artificial intelligence,article,True,
Algorithmic Bias and Historical Injustice: Race and Digital Profiling,2024,A. Matthew; Amalia R. Miller; Catherine E. Tucker,SSRN Electronic Journal,1,W4399110376,10.2139/ssrn.4843044,https://openalex.org/W4399110376,https://doi.org/10.2139/ssrn.4843044,Injustice; Download; Profiling (computer programming); Racial profiling; Ethnic group,article,True,
"The emergent role of artificial intelligence, natural learning processing, and large language models in higher education and research",2023,Tariq Alqahtani; Hisham A. Badreldin; Mohammed Alrashed; Abdulrahman Alshaya; Sahar S. Alghamdi; Khalid Bin Saleh; Shuroug A. Alowais; Omar A. Alshaya; Ishrat Rahman; Majed S. Al Yami; Abdulkareem Albekairy,Research in Social and Administrative Pharmacy,364,W4379284826,10.1016/j.sapharm.2023.05.016,https://openalex.org/W4379284826,,Computer science; Curriculum; Artificial intelligence; Engineering ethics; Field (mathematics),review,False,
Algorithmic Bias and Historical Injustice: Race and Digital Profiling,2024,A. Matthew; Amalia R. Miller; Catherine E. Tucker,SSRN Electronic Journal,1,W4396540664,10.2139/ssrn.4812943,https://openalex.org/W4396540664,http://doi.org/10.2139/ssrn.4812943,Injustice; Profiling (computer programming); Racial profiling; Race (biology); Computer science,article,True,
Mitigating Algorithmic Bias in Healthcare AI for Equitable Care,2024,,,1,W4404410152,10.1002/9781394263752.ch6,https://openalex.org/W4404410152,,Health care; Computer science; Data science; Political science; Law,other,False,
The bias algorithm: how AI in healthcare exacerbates ethnic and racial disparities – a scoping review,2024,Syed Ali Hussain; Mary Bresnahan; Jie Zhang,Ethnicity and Health,15,W4404016455,10.1080/13557858.2024.2422848,https://openalex.org/W4404016455,,Health equity; Ethnic group; Equity (law); Health care; Transparency (behavior),review,False,"This scoping review examined racial and ethnic bias in artificial intelligence health algorithms (AIHA), the role of stakeholders in oversight, and the consequences of AIHA for health equity. Using the PRISMA-ScR guidelines, databases were searched between 2020 and 2024 using the terms racial and ethnic bias in health algorithms resulting in a final sample of 23 sources. Suggestions for how to mitigate algorithmic bias were compiled and evaluated, roles played by stakeholders were identified, and governance and stewardship plans for AIHA were examined. While AIHA represent a significant breakthrough in predictive analytics and treatment optimization, regularly outperforming humans in diagnostic precision and accuracy, they also present serious challenges to patient privacy, data security, institutional transparency, and health equity. Evidence from extant sources including those in this review showed that AIHA carry the potential to perpetuate health inequities. While the current study considered AIHA in the US, the use of AIHA carries implications for global health equity."
Reinvention mediates impacts of skin tone bias in algorithms: implications for technology diffusion,2024,Hannah Overbye-Thompson; Kristy A. Hamilton; Dana Mastro,Journal of Computer-Mediated Communication,11,W4402741719,10.1093/jcmc/zmae016,https://openalex.org/W4402741719,https://doi.org/10.1093/jcmc/zmae016,Tone (literature); Diffusion; Algorithm; Computer science; Physics,article,True,"Abstract Two studies examine how skin tone bias in image recognition algorithms impacts users’ adoption and usage of image recognition technology. We employed a diffusion of innovations framework to explore perceptions of compatibility, complexity, observability, relative advantage, and reinvention to determine their influence on participants' utilization of image recognition algorithms. Despite being more likely to encounter algorithm bias, individuals with darker skin tones perceived image recognition algorithms as having greater levels of compatibility and relative advantage, being more observable, and less complex and thus used them more extensively compared to those with lighter skin tones. Individuals with darker skin tones also displayed higher levels of reinvention behaviors, suggesting a potential adaptive response to counteract algorithm biases."
Visual Representations in AI: A Study on the Most Discriminatory Algorithmic Biases in Image Generation,2025,Yazmina Vargas-Veleda; María del Mar Rodríguez-González; Íñigo Marauri Castillo,Journalism and Media,1,W4413063565,10.3390/journalmedia6030110,https://openalex.org/W4413063565,https://doi.org/10.3390/journalmedia6030110,Normative; Narrative; Diversity (politics); Beauty; Psychology,article,True,"This study analyses algorithmic biases in AI-generated images, focusing on aesthetic violence, gender stereotypes, and weight discrimination. By examining images produced by the DALL-E Nature and Flux 1 systems, it becomes evident how these tools reproduce and amplify hegemonic beauty standards, excluding bodily diversity. Likewise, gender representations reinforce traditional roles, sexualising women and limiting the presence of non-normative bodies in positive contexts. The results show that training data and the algorithms used significantly influence these trends, perpetuating exclusionary visual narratives. The research highlights the need to develop more inclusive and ethical AI models, with diverse data that reflect the plurality of bodies and social realities. The study concludes that artificial intelligence (AI), far from being neutral, actively contributes to the reproduction of power structures and inequality, posing an urgent challenge for the development and regulation of these technologies."
Human Freedom from Algorithmic Bias: What is the role of Accountability in addressing Health Disparities?,2024,Sajda Qureshi; Blessing Oladokun,Medical Research Archives,1,W4402143614,10.18103/mra.v12i8.5635,https://openalex.org/W4402143614,http://doi.org/10.18103/mra.v12i8.5635,Accountability; Human health; Degrees of freedom (physics and chemistry); Computer science; Political science,article,True,"While there are many causes of health disparities, the application of Artificial Intelligence tools in healthcare may have mixed results. The purpose of this paper is to investigate the role of human freedoms and accountability to achieving digital inclusion. It discovers the role of algorithmic bias in mediating the relationship between human freedom and mobile health. The following research questions are investigated: 1) How do human freedoms effect digital inclusion and mobile health? 2) Do human freedoms effect mobile health? And 3) Does AI accountability mediate the relationship between human freedoms and mobile health? The findings suggest that human freedoms are central to digital inclusion and mobile health. Accountability does affect the extent to which digital inclusion can be achieved through human freedoms. AI accountability significantly mediates the relationship between human freedoms and the mobile index. This offers an important contribution in uncovering the role of algorithmic bias in human freedom and mobile health, and of accountability between human freedom and digital inclusion."
Uncovering the Challenges From Algorithmic Bias Affecting the Marginalized Patient Groups in Healthcare,2024,Sarthak Bhatia; Shobhit; Anuj Kumar; Stuti Tandon,SSRN Electronic Journal,1,W4399153522,10.2139/ssrn.4848690,https://openalex.org/W4399153522,https://doi.org/10.2139/ssrn.4848690,Health care; Psychology; Political science; Public relations; Data science,article,True,
The dark side of generative artificial intelligence: A critical analysis of controversies and risks of ChatGPT,2023,Krzysztof Wach; Cong Doanh Duong; Joanna Ejdys; Rūta Kazlauskaitė; Paweł Korzyński; Grzegorz Mazurek; Joanna Paliszkiewicz; Ewa Ziemba,Entrepreneurial Business and Economics Review,413,W4383959108,10.15678/eber.2023.110201,https://openalex.org/W4383959108,https://eber.uek.krakow.pl/index.php/eber/article/view/2113/852,Great Rift; Generative grammar; Psychology; Artificial intelligence; Computer science,article,True,"Objective: The objective of the article is to provide a comprehensive identification and understanding of the challenges and opportunities associated with the use of generative artificial intelligence (GAI) in business. This study sought to develop a conceptual framework that gathers the negative aspects of GAI development in management and economics, with a focus on ChatGPT. Research Design & Methods: The study employed a narrative and critical literature review and developed a conceptual framework based on prior literature. We used a line of deductive reasoning in formulating our theoretical framework to make the study’s overall structure rational and productive. Therefore, this article should be viewed as a conceptual article that highlights the controversies and threats of GAI in management and economics, with ChatGPT as a case study. Findings: Based on the conducted deep and extensive query of academic literature on the subject as well as professional press and Internet portals, we identified various controversies, threats, defects, and disadvantages of GAI, in particular ChatGPT. Next, we grouped the identified threats into clusters to summarize the seven main threats we see. In our opinion they are as follows: (i) no regulation of the AI market and urgent need for regulation, (ii) poor quality, lack of quality control, disinformation, deepfake content, algorithmic bias, (iii) automation-spurred job losses, (iv) personal data violation, social surveillance, and privacy violation, (v) social manipulation, weakening ethics and goodwill, (vi) widening socio-economic inequalities, and (vii) AI technostress. Implications & Recommendations: It is important to regulate the AI/GAI market. Advocating for the regulation of the AI market is crucial to ensure a level playing field, promote fair competition, protect intellectual property rights and privacy, and prevent potential geopolitical risks. The changing job market requires workers to continuously acquire new (digital) skills through education and retraining. As the training of AI systems becomes a prominent job category, it is important to adapt and take advantage of new opportunities. To mitigate the risks related to personal data violation, social surveillance, and privacy violation, GAI developers must prioritize ethical considerations and work to develop systems that prioritize user privacy and security. To avoid social manipulation and weaken ethics and goodwill, it is important to implement responsible AI practices and ethical guidelines: transparency in data usage, bias mitigation techniques, and monitoring of generated content for harmful or misleading information. Contribution & Value Added: This article may aid in bringing attention to the significance of resolving the ethical and legal considerations that arise from the use of GAI and ChatGPT by drawing attention to the controversies and hazards associated with these technologies."
Fourth International Workshop on Algorithmic Bias in Search and Recommendation (Bias 2023),2023,Ludovico Boratto; Stefano Faralli; Mirko Marras; Giovanni Stilo,Lecture notes in computer science,1,W4327499177,10.1007/978-3-031-28241-6_39,https://openalex.org/W4327499177,,Debiasing; Computer science; Trustworthiness; Audit; Field (mathematics),book-chapter,False,
Artificial intelligence algorithm bias in information retrieval systems and its implication for library and information science professionals: A scoping review,2025,Magnus Osahon Igbinovia; Monica Mensah,Technical Services Quarterly,3,W4410906817,10.1080/07317131.2025.2512282,https://openalex.org/W4410906817,https://doi.org/10.1080/07317131.2025.2512282,Computer science; Information retrieval; Data science; Information science; Artificial intelligence,review,True,
Group Fairness for Content Creators: the Role of Human and Algorithmic Biases under Popularity-based Recommendations,2023,Stefania Ionescu; Anikó Hannák; Nicolò Pagan,,2,W4386728800,10.1145/3604915.3608841,https://openalex.org/W4386728800,https://dl.acm.org/doi/pdf/10.1145/3604915.3608841,Popularity; Computer science; Content (measure theory); Group (periodic table); Internet privacy,article,True,"The Creator Economy faces concerning levels of unfairness. Content creators (CCs) publicly accuse platforms of purposefully reducing the visibility of their content based on protected attributes, while platforms place the blame on viewer biases. Meanwhile, prior work warns about the “rich-get-richer” effect perpetuated by existing popularity biases in recommender systems: Any initial advantage in visibility will likely be exacerbated over time. What remains unclear is how the biases based on protected attributes from platforms and viewers interact and contribute to the observed inequality in the context of popularity-biased recommender systems. The difficulty of the question lies in the complexity and opacity of the system. To overcome this challenge, we design a simple agent-based model (ABM) that unifies the platform systems which allocate the visibility of CCs (e.g., recommender systems, moderation) into a single popularity-based function, which we call the visibility allocation system (VAS). Through simulations, we find that although viewer homophilic biases do alone create inequalities, small levels of additional biases in VAS are more harmful. From the perspective of interventions, our results suggest that (a) attempts to reduce attribute-biases in moderation and recommendations should precede those reducing viewers’ homophilic tendencies, (b) decreasing the popularity-biases in VAS decreases but not eliminates inequalities, (c) boosting the visibility of protected CCs to overcome viewers’ homophily with respect to one fairness metric is unlikely to produce fair outcomes with respect to all metrics, and (d) the process is also unfair for viewers and this unfairness could be overcome through the same interventions. More generally, this work demonstrates the potential of using ABMs to better understand the causes and effects of biases and interventions within complex sociotechnical systems."
The Layered Injection Model of Algorithmic Bias as a Conceptual Framework to Understand Biases Impacting the Output of Text-To-Image Models,2025,Dirk Spennemann,SSRN Electronic Journal,2,W4413477093,10.2139/ssrn.5393987,https://openalex.org/W4413477093,https://doi.org/10.2139/ssrn.5393987,Image (mathematics); Computer science; Econometrics; Conceptual model; Conceptual framework,article,True,
Addressing Bias in Machine Learning Algorithms: Promoting Fairness and Ethical Design,2024,Dharmesh Dhabliya; Sukhvinder Singh Dari; Anishkumar Dhablia; N. Akhila; Renu Kachhoria; Vinit Khetani,E3S Web of Conferences,16,W4391998543,10.1051/e3sconf/202449102040,https://openalex.org/W4391998543,https://www.e3s-conferences.org/articles/e3sconf/pdf/2024/21/e3sconf_icecs2024_02040.pdf,Computer science; Artificial intelligence; Machine learning; Psychology,article,True,"Machine learning algorithms have quickly risen to the top of several fields' decision-making processes in recent years. However, it is simple for these algorithms to confirm already present prejudices in data, leading to biassed and unfair choices. In this work, we examine bias in machine learning in great detail and offer strategies for promoting fair and moral algorithm design. The paper then emphasises the value of fairnessaware machine learning algorithms, which aim to lessen bias by including fairness constraints into the training and evaluation procedures. Reweighting, adversarial training, and resampling are a few strategies that could be used to overcome prejudice. Machine learning systems that better serve society and respect ethical ideals can be developed by promoting justice, transparency, and inclusivity. This paper lays the groundwork for researchers, practitioners, and policymakers to forward the cause of ethical and fair machine learning through concerted effort."
Gender Bias in Hiring: An Analysis of the Impact of Amazon's Recruiting Algorithm,2023,Xinyu Chang,Advances in Economics Management and Political Sciences,8,W4386641279,10.54254/2754-1169/23/20230367,https://openalex.org/W4386641279,https://doi.org/10.54254/2754-1169/23/20230367,Gender bias; Amazon rainforest; Subject (documents); Implicit bias; Diversity (politics),article,True,"Algorithmic bias in artificial intelligence (AI) is a growing concern, especially in the employment sector, where it can have devastating effects on both individuals and society. Gender discrimination is one of the most prevalent forms of algorithmic bias seen in numerous industries, including technology. The underrepresentation of women in the field of information technology is a well-known issue, and several organizations have made tackling this issue a top priority. Amazon, one of the world's top technology businesses, has been at the forefront of initiatives to increase inclusiveness and diversity in the sector. Concerns exist, however, that algorithmic bias in their recruitment process may perpetuate discrimination based on gender. This study intends to investigate these issues by employing an interpretive epistemology and utilizing interviews and focus groups to acquire a more nuanced knowledge of the subject,with keyfactors contributing to algorithmic gender bias in Amazon's recruitmentprocess andrecommend strategies for improving women's employment in information technology."
"Existing challenges in ethical AI: Addressing algorithmic bias, transparency, accountability and regulatory compliance",2025,Manikanta Rajendra Kumar Kakarala; Sateesh Kumar Rongali,World Journal of Advanced Research and Reviews,1,W4408339137,10.30574/wjarr.2025.25.3.0554,https://openalex.org/W4408339137,,Transparency (behavior); Accountability; Compliance (psychology); Ethical issues; Business,article,False,"Artificial Intelligence has transformed industries in terms of efficiency, decision-making, and personalization across healthcare, finance, and education. This rapid integration of AI into daily life has also brought forth significant ethical challenges regarding algorithmic bias, transparency, accountability, and regulatory compliance. These come with risks to the equitable application of AI, leading to outcomes that can perpetuate discrimination and systemic injustices. Examples include biased algorithms leading to disparate hiring practices, healthcare access inequity, and credit distribution differences. Most instances of ethical gaps in the use of AI go unmonitored due to a need for well-defined mechanisms for responsibility. Besides that, regulation at a pace equal to AI innovation is a great challenge that creates gaps in oversight and increases risks to privacy, fairness, and other elements of well-being in society. The paper explores these challenges, discussing the causality of the challenges and suggesting practical ways of mitigating them. It converses technical developments in fairness-aware algorithms, explainable AI, and the legal framework of GDPR to make a case for a multi-stakeholder comprehensive approach towards ethical AI. It would call for collaboration among policymakers, technologists, and industry leaders to build public confidence, ensure fairness and align AI progress with societal values. In the final analysis, the findings have underlined the urgent need for ethical foresight to tap into the potential of AI responsibly and equitably."
Unbias me! Mitigating Algorithmic Bias for Less-studied Demographic Groups in the Context of Language Learning Technology,2023,Nathalie Rzepka; Linda Fernsel; Hans‐Georg Müller; Katharina Simbeck; N. Pinkwart,,1,W4380854637,10.35542/osf.io/qa9vz,https://openalex.org/W4380854637,https://doi.org/10.35542/osf.io/qa9vz,Computer science; Context (archaeology); Odds; Machine learning; Artificial intelligence,preprint,True,"Rzepka, N., Fernsel, L., Müller, H., Simbeck, K., &amp;amp; Pinkwart, N., (2023) Unbias me! Mitigating Algorithmic Bias for Less-studied Demographic Groups in the Context of Language Learning Technology. Computer-Based Learning in Context, 6 (1), 1-23. 10.5281/zenodo.7996194 Algorithms and machine learning models are being used more frequently in educational settings, but there are concerns that they may discriminate against certain groups. While there is some research on algorithmic fairness, there are two main issues with the current research. Firstly, it often focuses on gender and race and ignores other groups. Secondly, studies often find algorithmic bias in educational models but don't explore ways to reduce it. This study evaluates three drop-out prediction models used in an online learning platform to teach German spelling skills. The aim is to assess the fairness of the models for (in part) less-studied demographic groups, including first spoken language, home literacy environment, parental education background, and gender. To evaluate the models, four fairness metrics are used: predictive parity, equalized odds, predictive equality, and ABROCA. The study also examines ways to reduce algorithmic bias by analyzing the models at each stage of the machine learning process. The results show that all three models had biases that affected the fairness of all four demographic groups to varying degrees. However, the study found that most biases could be mitigated during the process. The methods used to mitigate bias differed by demographic group, and some methods improved fairness for one group but worsened it for others. Therefore, the study concludes that reducing algorithmic bias for less-studied demographic groups is possible, but finding the right method for each algorithm and demographic group is crucial."
Unbias me! Mitigating Algorithmic Bias for Less-studied Demographic Groups in the Context of Language Learning Technology,2023,Nathalie Rzepka; Linda Fernsel; Hans‐Georg Müller; Katharina Simbeck; Niels Pinkwart,Zenodo (CERN European Organization for Nuclear Research),1,W4379092151,10.5281/zenodo.7996194,https://openalex.org/W4379092151,https://zenodo.org/record/7996194,Context (archaeology); Computer science; Psychology; Natural language processing; Cognitive psychology,article,True,"&lt;p&gt;Rzepka, N., Fernsel, L., M&uuml;ller, H., Simbeck, K., &amp; Pinkwart, N., (2023) Unbias me! Mitigating Algorithmic Bias for Less-studied Demographic Groups in the Context of Language Learning Technology. Computer-Based Learning in Context, 6 (1), 1-23.&lt;/p&gt; &lt;p&gt;Abstract. Algorithms and machine learning models are being used more frequently in educational settings, but there are concerns that they may discriminate against certain groups. While there is some research on algorithmic fairness, there are two main issues with the current research. Firstly, it often focuses on gender and race and ignores other groups. Secondly, studies often find algorithmic bias in educational models but don&#39;t explore ways to reduce it. This study evaluates three drop-out prediction models used in an online learning platform to teach German spelling skills. The aim is to assess the fairness of the models for (in part) less-studied demographic groups, including first spoken language, home literacy environment, parental education background, and gender. To evaluate the models, four fairness metrics are used: predictive parity, equalized odds, predictive equality, and ABROCA. The study also examines ways to reduce algorithmic bias by analyzing the models at each stage of the machine learning process. The results show that all three models had biases that affected the fairness of all four demographic groups to varying degrees. However, the study found that most biases could be mitigated during the process. The methods used to mitigate bias differed by demographic group, and some methods improved fairness for one group but worsened it for others. Therefore, the study concludes that reducing algorithmic bias for lessstudied demographic groups is possible, but finding the right method for each algorithm and demographic group is crucial.&lt;/p&gt; &lt;p&gt;&nbsp;&lt;/p&gt;"
The value of standards for health datasets in artificial intelligence-based applications,2023,Anmol Arora; Joseph Alderman; Joanne Palmer; Shaswath Ganapathi; Elinor Laws; Melissa D. McCradden; Lauren Oakden‐Rayner; Stephen Pfohl; Marzyeh Ghassemi; Francis McKay; Darren Treanor; Negar Rostamzadeh; Bilal A. Mateen; Jacqui Gath; Adewole O. Adebajo; Stephanie Kuku; Rubeta Matin; Katherine Heller; Elizabeth Sapey; Neil J. Sebire; Heather Cole-Lewis; Melanie Calvert; Alastair K. Denniston; Xiaoxuan Liu,Nature Medicine,188,W4387966489,10.1038/s41591-023-02608-w,https://openalex.org/W4387966489,https://www.nature.com/articles/s41591-023-02608-w.pdf,Best practice; Data science; Diversity (politics); Health care; Computer science,article,True,
The Tangled Web of Social Bias: Unraveling Human and Algorithmic Biases on Digital Platforms,2024,Runshan Fu,SSRN Electronic Journal,1,W4405596493,10.2139/ssrn.5008463,https://openalex.org/W4405596493,https://doi.org/10.2139/ssrn.5008463,Computer science; Data science; World Wide Web; Internet privacy,preprint,True,
Biased stochastic conjugate gradient algorithm with adaptive step size for nonconvex problems,2023,Ruping Huang; Yan Qin; Kejun Liu; Gonglin Yuan,Expert Systems with Applications,19,W4386969692,10.1016/j.eswa.2023.121556,https://openalex.org/W4386969692,,Conjugate gradient method; Stochastic gradient descent; Convergence (economics); Algorithm; Gradient method,article,False,
Analysing Coping Strategies of Teenage Girls Towards Instagram’s Algorithmic Bias,2024,Intisãr Constant; Pitso Tsibolane; Adheesh Budree; Grant Oosterwyk,Lecture notes in computer science,1,W4399267544,10.1007/978-3-031-61281-7_10,https://openalex.org/W4399267544,,Computer science; Coping (psychology); Psychology; Psychotherapist,book-chapter,False,
Assessing racial bias in type 2 diabetes risk prediction algorithms,2023,Héléne T. Cronjé; Alexandros Katsiferis; Leonie K. Elsenburg; Thea Otte Andersen; Naja Hulvej Rod; Tri-Long Nguyên; Tibor V. Varga,PLOS Global Public Health,25,W4376959293,10.1371/journal.pgph.0001556,https://openalex.org/W4376959293,https://doi.org/10.1371/journal.pgph.0001556,Medicine; Framingham Risk Score; Type 2 diabetes; National Health and Nutrition Examination Survey; Demography,article,True,"Risk prediction models for type 2 diabetes can be useful for the early detection of individuals at high risk. However, models may also bias clinical decision-making processes, for instance by differential risk miscalibration across racial groups. We investigated whether the Prediabetes Risk Test (PRT) issued by the National Diabetes Prevention Program, and two prognostic models, the Framingham Offspring Risk Score, and the ARIC Model, demonstrate racial bias between non-Hispanic Whites and non-Hispanic Blacks. We used National Health and Nutrition Examination Survey (NHANES) data, sampled in six independent two-year batches between 1999 and 2010. A total of 9,987 adults without a prior diagnosis of diabetes and with fasting blood samples available were included. We calculated race- and year-specific average predicted risks of type 2 diabetes according to the risk models. We compared the predicted risks with observed ones extracted from the US Diabetes Surveillance System across racial groups (summary calibration). All investigated models were found to be miscalibrated with regard to race, consistently across the survey years. The Framingham Offspring Risk Score overestimated type 2 diabetes risk for non-Hispanic Whites and underestimated risk for non-Hispanic Blacks. The PRT and the ARIC models overestimated risk for both races, but more so for non-Hispanic Whites. These landmark models overestimated the risk of type 2 diabetes for non-Hispanic Whites more severely than for non-Hispanic Blacks. This may result in a larger proportion of non-Hispanic Whites being prioritized for preventive interventions, but it also increases the risk of overdiagnosis and overtreatment in this group. On the other hand, a larger proportion of non-Hispanic Blacks may be potentially underprioritized and undertreated."
Framework for Algorithmic Bias Quantification and its Application to Automated Sleep Scoring,2024,Michal Bechný; Giuliana Monachino; Luigi Fiorillo; Julia van der Meer; Markus H. Schmidt; Claudio L. Bassetti; Athina Tzovara; Francesca Dalia Faraci,,1,W4402594212,10.1109/sds60720.2024.00045,https://openalex.org/W4402594212,,Computer science; Sleep (system call); Artificial intelligence; Machine learning; Programming language,article,False,
"ALGORITHMIC BIAS IN MEDIA CONTENT DISTRIBUTION AND ITS INFLUENCE ON MEDIA CONSUMPTION: IMPLICATIONS FOR DIVERSITY, EQUITY, AND INCLUSION (DEI)",2024,CHIZOROM EBOSIE OKORONKWO,International Journal of Social Sciences and Management Review,1,W4403433610,10.37602/ijssmr.2024.7523,https://openalex.org/W4403433610,https://doi.org/10.37602/ijssmr.2024.7523,Inclusion (mineral); Diversity (politics); Content distribution; Consumption (sociology); Equity (law),article,True,"In today’s digital age, algorithms play a pivotal role in shaping media content distribution, which may possibly influence what content individuals are exposed to. Consequently, this may have implications for diversity, equity, and inclusion (DEI). Hence, this review analyzes algorithmic bias in media material distribution and its impact on media consumption and the implications for diversity, equity, and inclusion (DEI). The study concludes that algorithm bias limits the visibility of underprivileged groups and perpetuates current social injustices, posing serious problems for media distribution. Moreover, there are risks and opportunities associated with the development of artificial intelligence (AI) and machine learning in tackling algorithmic inequities. Furthermore, there is a need for collaborative efforts among different stakeholders (engineers, policymakers, and media platforms) in creating a more inclusive and equitable algorithms in order to ensure that media distribution systems promote fairness and diversity."
Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias,2024,Sanguk Lee; Tai‐Quan Peng; Matthew H. Goldberg; Seth A. Rosenthal; John Kotcher; Edward Maibach; Anthony Leiserowitz,PLOS Climate,15,W4401389158,10.1371/journal.pclm.0000429,https://openalex.org/W4401389158,https://journals.plos.org/climate/article/file?id=10.1371/journal.pclm.0000429&type=printable,Global warming; Fidelity; Covariate; Public opinion; Psychology,article,True,"Large language models (LLMs) can be used to estimate human attitudes and behavior, including measures of public opinion, a concept referred to as algorithmic fidelity. This study assesses the algorithmic fidelity and bias of LLMs in estimating public opinion about global warming. LLMs were conditioned on demographics and/or psychological covariates to simulate survey responses. Findings indicate that LLMs can effectively reproduce presidential voting behaviors but not global warming opinions unless the issue relevant covariates are included. When conditioned on both demographic and covariates, GPT-4 demonstrates improved accuracy, ranging from 53% to 91%, in predicting beliefs and attitudes about global warming. Additionally, we find an algorithmic bias that underestimates the global warming opinions of Black Americans. While highlighting the potential of LLMs to aid social science research, these results underscore the importance of conditioning, model selection, survey question format, and bias assessment when employing LLMs for survey simulation."
Towards early detection of algorithmic bias from dataset’s bias symptoms: An empirical study,2025,Giordano d’Aloisio; Claudio Di Sipio; Antinisca Di Marco; Davide Di Ruscio,Information and Software Technology,1,W4414675203,10.1016/j.infsof.2025.107905,https://openalex.org/W4414675203,https://doi.org/10.1016/j.infsof.2025.107905,,article,True,
Uncovering structural bias in population-based optimization algorithms: A theoretical and simulation-based analysis of the Generalized Signature Test,2023,Kanchan Rajwar; Kusum Deep,Expert Systems with Applications,20,W4388089286,10.1016/j.eswa.2023.122332,https://openalex.org/W4388089286,,Population; Metaheuristic; Algorithm; Computer science; Differential evolution,article,False,
Deep Isolation Forest for Anomaly Detection,2023,Hongzuo Xu; Guansong Pang; Yijie Wang; Yongjun Wang,IEEE Transactions on Knowledge and Data Engineering,315,W4367016885,10.1109/tkde.2023.3270293,https://openalex.org/W4367016885,,Computer science; Partition (number theory); Anomaly detection; Scalability; Linear subspace,article,False,"Isolation forest (iForest) has been emerging as arguably the most popular anomaly detector in recent years due to its general effectiveness across different benchmarks and strong scalability. Nevertheless, its linear axis-parallel isolation method often leads to (i) failure in detecting hard anomalies that are difficult to isolate in high-dimensional/non-linear-separable data space, and (ii) notorious algorithmic bias that assigns unexpectedly lower anomaly scores to artefact regions. These issues contribute to high false negative errors. Several iForest extensions are introduced, but they essentially still employ shallow, linear data partition, restricting their power in isolating true anomalies. Therefore, this paper proposes deep isolation forest. We introduce a new representation scheme that utilises casually initialised neural networks to map original data into random representation ensembles, where random axis-parallel cuts are subsequently applied to perform the data partition. This representation scheme facilitates high freedom of the partition in the original data space (equivalent to non-linear partition on subspaces of varying sizes), encouraging a unique synergy between random representations and random partition-based isolation. Extensive experiments show that our model achieves significant improvement over state-of-the-art isolation-based methods and deep detectors on tabular, graph and time series datasets; our model also inherits desired scalability from iForest."
"Data and science engineering: The ethical dilemma of our time-exploring privacy breaches, algorithmic biases, and the need for transparency",2023,Shubham Shubham; Saloni Saloni; Sidra-Tul-Muntaha,Zenodo (CERN European Organization for Nuclear Research),1,W4384921908,10.5281/zenodo.8172333,https://openalex.org/W4384921908,https://zenodo.org/record/8172333,Transparency (behavior); Dilemma; Internet privacy; Ethical dilemma; Information privacy,article,True,"&lt;p&gt;This paper explores the ethical dilemmas associated with data and science engineering, with a focus on privacy breaches, algorithmic biases, and the need for transparency. With the increasing reliance on data-driven decision making and machine learning algorithms, the ethical implications of these technologies have become a pressing issue in various sectors. The study aimed to identify the most significant ethical concerns, analyze their impact on society, and provide solutions to address these issues.&lt;/p&gt; &lt;p&gt;The research utilized a systematic review of 18 studies to identify the key ethical issues in data and science engineering. The findings revealed that privacy breaches, algorithmic biases, and lack of transparency were the most prevalent ethical concerns. These issues can have significant implications for individuals and groups, including discrimination, loss of autonomy, and reputational harm. The study also identified vulnerable groups, such as marginalized communities, who may be disproportionately affected by these issues.&lt;/p&gt; &lt;p&gt;To address these ethical concerns, the study proposed several solutions, including the development of ethical guidelines, increased transparency and accountability, and the use of diverse and representative datasets. The solutions were informed by the literature review, case studies, and analysis of real-world examples. The study also assessed the feasibility of implementing these solutions and highlighted potential barriers to implementation.&lt;/p&gt;"
Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation,2023,Hao Liang; Pietro Perona; Guha Balakrishnan,arXiv (Cornell University),1,W4385774866,10.48550/arxiv.2308.05441,https://openalex.org/W4385774866,https://arxiv.org/pdf/2308.05441,Artificial intelligence; Computer science; Benchmark (surveying); Synthetic data; Pattern recognition (psychology),preprint,True,"We propose an experimental method for measuring bias in face recognition systems. Existing methods to measure bias depend on benchmark datasets that are collected in the wild and annotated for protected (e.g., race, gender) and non-protected (e.g., pose, lighting) attributes. Such observational datasets only permit correlational conclusions, e.g., ""Algorithm A's accuracy is different on female and male faces in dataset X."". By contrast, experimental methods manipulate attributes individually and thus permit causal conclusions, e.g., ""Algorithm A's accuracy is affected by gender and skin color."" Our method is based on generating synthetic faces using a neural face generator, where each attribute of interest is modified independently while leaving all other attributes constant. Human observers crucially provide the ground truth on perceptual identity similarity between synthetic image pairs. We validate our method quantitatively by evaluating race and gender biases of three research-grade face recognition models. Our synthetic pipeline reveals that for these algorithms, accuracy is lower for Black and East Asian population subgroups. Our method can also quantify how perceptual changes in attributes affect face identity distances reported by these models. Our large synthetic dataset, consisting of 48,000 synthetic face image pairs (10,200 unique synthetic faces) and 555,000 human annotations (individual attributes and pairwise identity comparisons) is available to researchers in this important area."
Engaging an advisory board in discussions about the ethical relevance of algorithmic bias and fairness,2025,Laura Brandt; Larry Au; Clinton Castro; Gabriel Odom,npj Digital Medicine,1,W4410469255,10.1038/s41746-025-01711-1,https://openalex.org/W4410469255,https://www.nature.com/articles/s41746-025-01711-1.pdf,Relevance (law); Advisory committee; Ethical issues; Psychology; Political science,review,True,"We are an interdisciplinary team of researchers that are working to advance algorithmic fairness in the research of opioid use disorders. We discuss challenges that our research team faced when engaging with our Advisory Board, as well as several strategies that we came up with to help us find a common language to ensure semantic transparency and to ensure the thick alignment with values of affected parties."
A novel approach for bias mitigation of gender classification algorithms using consistency regularization,2023,Anoop Krishnan; Ajita Rattani,Image and Vision Computing,15,W4385597536,10.1016/j.imavis.2023.104793,https://openalex.org/W4385597536,,Generalizability theory; Computer science; Classifier (UML); Artificial intelligence; Machine learning,article,False,
Laissez-Faire Harms: Algorithmic Biases in Generative Language Models (Extended Abstract),2025,Evan Shieh; Faye Marie Vassel; Cassidy R. Sugimoto; Thema Monroe‐White,Proceedings of the AAAI/ACM Conference on AI Ethics and Society,1,W4415230509,10.1609/aies.v8i3.36722,https://openalex.org/W4415230509,https://ojs.aaai.org/index.php/AIES/article/download/36722/38860,,article,True,"The widespread deployment of generative language models (LMs) is raising concerns about societal harms. Despite this, studies of bias in generative LMs, including attempted self-audits by LM developers, have thus far been conducted in limited contexts. To address this gap, this study examines representational harms in synthetic texts produced by leading language models in response to open-ended creative writing prompts based in the United States. We conduct our investigation on 500,000 synthetic texts generated by five publicly available generative language models: ChatGPT 3.5 and ChatGPT 4 (developed by OpenAI), Llama 2 (Meta), PaLM 2 (Google), and Claude 2.0 (Anthropic). We base our selection of models on both the sizable amount of funding wielded by these companies and their investors (on the order of tens of billions in USD), as well as the prominent policy roles that each company has played on the federal level. At the time of data collection (from August 16th to November 7th, 2023), the selected models were considered state-of-the-art for each company. Creative writing prompts reflect three domains of life set in the United States: classroom interactions (“Learning”), the workplace (“Labor”), and interpersonal relationships (“Love”). Informed by intersectionality theory, we considered the role of power embedded in language by creating one power-neutral scenario and one power-laden scenario for each prompt. For example, power-neutral Learning prompts consist of a single student excelling in an academic subject, whereas the power-laden prompts consist of one star student helping a struggling student in an academic subject. We then analyze the resulting model responses for textual cues shown to exacerbate socio-psychological harms for minoritized individuals by race, gender, and sexual orientation. To do this at scale, we fine-tuned a coreference resolution model (gpt3.5-turbo) to perform automated extraction of characters’ gender references and names at high precision. To evaluate our model, we hand-label the inferred gender (based on gender references) and name on an evaluation set of 4,600 uniformly down-sampled story generations from all five models (0.0063, 95% CI). Fine-tuning our model on a non-overlapping set of 150 training examples yields precision above 98% for both gender references and names. Recall rates reach 97% for gender references and exceed 99% for names. Following previous studies, we infer racial signals from first names using fractionalized counting over the Florida Voter Registration Dataset (which consists of 27 million named individuals and self-identified racial identities). We find that when LMs are used for story writing, they generate texts that reinforce discrimination against minoritized groups by race, gender, and sexual orientation. Using mixed-methods analyses, we identify three specific harms: omission, subordination, and stereotyping. Stories produced by language models simultaneously underrepresent minoritized individuals as main characters while overrepresenting them as subordinated characters. Diverse consumers, if they are to be represented at all, disproportionately see themselves portrayed by language models as “struggling students” (as opposed to “star students”), “patients” or “defendants” (as opposed to “doctors” or “lawyers”), and a friend or romantic partner who is more likely to borrow money or do the chores for someone else. The magnitude of bias far exceeds the level of ""real-world"" inequities. Underrepresentation of non-dominant identities in power-neutral stories exceeds national demographics in the US by up to two orders of magnitude. Meanwhile, non-dominant character identities are up to thousands of times more likely to appear as subordinated than empowered. For example, Claude casts the name ”Juan” as a struggling student 1,380 times, yet only once as a star student. We find that these harms impact every non-dominant group we studied (in the US context). These include individuals with intersectional Asian, Black, Indigenous, Latine, NH/PI, MENA, Female, Non-binary, and Queer identities. Language models propagate a plethora of stereotypes that are known to inflict psychological harm and negative self-perception, including the ” glass/bamboo ceiling”, ” perpetual foreigner”, ”noble savage”, ”white savior”, and others."
Strategic Integration of AIGC in Asian Elderly Fashion: Human-Centric Design Enhancement and Algorithmic Bias Neutralization,2024,Hongcai Chen; Vongphantuset Jirawat; Yan Wang,AHFE international,1,W4399479261,10.54941/ahfe1004658,https://openalex.org/W4399479261,,Neutralization; Computer science; Human–computer interaction; Medicine; Immunology,article,False,"The advent of Artificial Intelligence Generated Content (AIGC) has catalyzed transformative shifts in the domain of fashion design, providing novel opportunities for customization and innovation. This research delineates the strategic integration of AIGC within Asian elderly fashion design, critically examining its role in augmenting human-centric design principles while addressing the prevalent algorithmic biases. The objective is to empirically assess the efficacy of AIGC in creating designs that resonate with the functional and aesthetic preferences of the elderly Asian demographic. Employing a mixed-methods approach, the study first delineates the current limitations and potential enhancements AIGC offers to the fashion design process. Through iterative design experiments, AIGC applications are evaluated for their capacity to accommodate the nuanced needs of the target population. Concurrently, a fuzzy evaluation method systematically quantifies the feedback from design practitioners, revealing the salient factors and their relative influence on the AIGC-driven design process. Findings from the study highlight the dichotomy between AIGC's potential for personalized design and the inherent risks of reinforcing biases. The analysis provides a granular understanding of the interplay between AIGC capabilities and user-centered design requirements, emphasizing the necessity for a calibrated approach that prioritizes ethical considerations. The study culminates in a set of actionable guidelines that advocate for the integration of comprehensive educational modules on AIGC technologies in design curricula, aiming to bridge the interdisciplinary gap and enhance designer preparedness. The conclusion underscores the imperative for ongoing scrutiny of AIGC outputs, advocating for the development of robust frameworks that ensure equitable and inclusive design practices. Through this research, a path is charted toward the responsible utilization of AIGC, fostering a fashion industry that is adaptive, empathetic, and attuned to the diverse spectrum of aging consumers in Asia."
"Research on Ethical Issues, Data Privacy Protection, Algorithmic Bias, and Regulatory Policy of Artificial Intelligence Technology in Digital Transformation",2025,R Zou,Applied economics and policy studies,2,W4409136616,10.1007/978-981-96-3236-7_21,https://openalex.org/W4409136616,,Digital transformation; Data Protection Act 1998; Transformation (genetics); Privacy protection; Privacy policy,book-chapter,False,
"Fairness and Bias in Artificial Intelligence: A Brief Survey of Sources, Impacts, and Mitigation Strategies",2023,Emilio Ferrara,Sci,497,W4390229867,10.3390/sci6010003,https://openalex.org/W4390229867,https://www.mdpi.com/2413-4155/6/1/3/pdf?version=1703577406,Generative grammar; Artificial intelligence; Generative model; Computer science; Selection bias,article,True,"The significant advancements in applying artificial intelligence (AI) to healthcare decision-making, medical diagnosis, and other domains have simultaneously raised concerns about the fairness and bias of AI systems. This is particularly critical in areas like healthcare, employment, criminal justice, credit scoring, and increasingly, in generative AI models (GenAI) that produce synthetic media. Such systems can lead to unfair outcomes and perpetuate existing inequalities, including generative biases that affect the representation of individuals in synthetic data. This survey study offers a succinct, comprehensive overview of fairness and bias in AI, addressing their sources, impacts, and mitigation strategies. We review sources of bias, such as data, algorithm, and human decision biases—highlighting the emergent issue of generative AI bias, where models may reproduce and amplify societal stereotypes. We assess the societal impact of biased AI systems, focusing on perpetuating inequalities and reinforcing harmful stereotypes, especially as generative AI becomes more prevalent in creating content that influences public perception. We explore various proposed mitigation strategies, discuss the ethical considerations of their implementation, and emphasize the need for interdisciplinary collaboration to ensure effectiveness. Through a systematic literature review spanning multiple academic disciplines, we present definitions of AI bias and its different types, including a detailed look at generative AI bias. We discuss the negative impacts of AI bias on individuals and society and provide an overview of current approaches to mitigate AI bias, including data pre-processing, model selection, and post-processing. We emphasize the unique challenges presented by generative AI models and the importance of strategies specifically tailored to address these. Addressing bias in AI requires a holistic approach involving diverse and representative datasets, enhanced transparency and accountability in AI systems, and the exploration of alternative AI paradigms that prioritize fairness and ethical considerations. This survey contributes to the ongoing discussion on developing fair and unbiased AI systems by providing an overview of the sources, impacts, and mitigation strategies related to AI bias, with a particular focus on the emerging field of generative AI."
AI in education: A review of personalized learning and educational technology,2024,Oyebola Olusola Ayeni; Nancy Mohd Al Hamad; Onyebuchi Nneamaka Chisom; Blessing Osawaru; Ololade Elizabeth Adewusi,GSC Advanced Research and Reviews,235,W4392772072,10.30574/gscarr.2024.18.2.0062,https://openalex.org/W4392772072,https://gsconlinepress.com/journals/gscarr/sites/default/files/GSCARR-2024-0062.pdf,Personalized learning; Mathematics education; Educational technology; Computer science; Psychology,review,True,"The integration of Artificial Intelligence (AI) in education has ushered in a transformative era, redefining traditional teaching and learning methods. This review explores the multifaceted role of AI in education, with a particular focus on personalized learning and educational technology. The synergy between AI and education promises to address individualized needs, enhance student engagement, and optimize learning outcomes. Personalized learning, enabled by AI algorithms, tailors educational experiences to the unique needs, preferences, and pace of each student. This approach goes beyond a one-size-fits-all model, fostering a more inclusive and effective learning environment. The review delves into the diverse applications of AI-driven personalized learning, ranging from adaptive content delivery and real-time feedback to intelligent tutoring systems. It analyzes the impact of these technologies on student performance, highlighting the potential to narrow educational gaps and cater to diverse learning styles. Educational technology, powered by AI, extends beyond the classroom, encompassing online platforms, virtual reality, and interactive tools. The review explores the integration of AI in curriculum development, content creation, and assessment methods, offering insights into how these technologies augment the teaching and learning experience. Furthermore, the review examines the role of AI in automating administrative tasks, allowing educators to redirect their focus towards personalized instruction. Challenges and ethical considerations associated with the adoption of AI in education are also scrutinized. Privacy concerns, algorithmic biases, and the digital divide are discussed, emphasizing the importance of responsible AI implementation. The review underscores the need for collaborative efforts among educators, policymakers, and technologists to establish ethical guidelines and ensure the equitable distribution of AI-enhanced educational resources. This review provides a comprehensive examination of the evolving landscape of AI in education, with a spotlight on personalized learning and educational technology. As the symbiosis between AI and education continues to evolve, this synthesis of current research and trends aims to guide future developments, fostering an informed and progressive approach to the integration of AI in the educational sphere."
Ethical and Bias Considerations in Artificial Intelligence/Machine Learning,2024,Matthew G. Hanna; Liron Pantanowitz; Brian Jackson; Octavia M. Peck Palmer; Shyam Visweswaran; Joshua Pantanowitz; Mustafa Deebajah; Hooman H. Rashidi,Modern Pathology,181,W4405439405,10.1016/j.modpat.2024.100686,https://openalex.org/W4405439405,,Artificial intelligence; Psychology; Computer science,review,False,
"Regulatory and Ethical Challenges in AI-Driven and Machine learning Credit Risk Assessment for Buy Now, Pay Later (BNPL) in U.S. E-Commerce: Compliance, Fair Lending, and Algorithmic Bias",2025,Aditya Mishra; Sanjida Nowshin Mou; Jannat Ara; M. Sarkar,Journal of Business and Management Studies,2,W4408289860,10.32996/jbms.2025.7.2.3,https://openalex.org/W4408289860,https://al-kindipublisher.com/index.php/jbms/article/download/8945/7623,Compliance (psychology); Business; Actuarial science; Credit risk; Accounting,article,True,"The integration of artificial intelligence (AI) and machine learning (ML) in credit risk assessment for Buy Now, Pay Later (BNPL) services has transformed the U.S. e-commerce landscape. However, these advancements present significant regulatory and ethical challenges, particularly regarding compliance, fair lending practices, and algorithmic bias. This study examines the legal framework governing BNPL credit assessments, including adherence to the Equal Credit Opportunity Act (ECOA), Fair Credit Reporting Act (FCRA), and other consumer protection regulations (Federal Trade Commission [FTC], 2022; U.S. Consumer Financial Protection Bureau [CFPB], 2023). Additionally, the paper explores the implications of algorithmic bias in AI-driven credit decisions, highlighting the potential for disparate impacts on marginalized communities (Bartlett et al., 2022; Bragg, 2021; Zarsky, 2016). The ethical concerns surrounding transparency, explain ability, and consumer rights are also discussed (Kroll et al., 2017; Pasquale, 2020). A comparative analysis of current regulatory approaches and proposed reforms is conducted, with a focus on mitigating bias and ensuring equitable access to credit. This research concludes with recommendations for policymakers, regulators, and financial technology firms to foster responsible AI deployment in BNPL services while safeguarding consumer protection and financial inclusion."
"Exploring the Dark Side of AI: The Challenges of Explain Ability, Algorithmic Bias, and Ensuring Fairness",2024,Aanya Shridhar,,1,W4408401515,10.1109/ictbig64922.2024.10911065,https://openalex.org/W4408401515,,Great Rift; Computer science; Far side of the Moon; Data science; Physics,article,False,
Mind evolutionary algorithm optimization in the prediction of satellite clock bias using the back propagation neural network,2023,Hongwei Bai; Qianqian Cao; Subang An,Scientific Reports,17,W4319298381,10.1038/s41598-023-28855-y,https://openalex.org/W4319298381,https://www.nature.com/articles/s41598-023-28855-y.pdf,Artificial neural network; Computer science; Algorithm; Satellite; Mean squared error,article,True,"Abstract Satellite clock bias is the key factor affecting the accuracy of the single point positioning of a global navigation satellite system. The traditional model back propagation (BP) neural network is prone to local optimum problems. This paper presents a prediction model and algorithm for the clock bias of the BP neural network based on the optimization of the mind evolutionary algorithm (MEA), which is used to optimize the initial weights and thresholds of the BP neural network. The accuracy of the comparison between clock bias data is verified with and without one-time difference processing. Compared with grey model (GM (1,1)) and BP neural network, this paper discusses the advantages and general applicability of this method from different constellation satellites, different atomic clock type satellites, and the amount of modeling data. The accuracy of the grey model (GM(1,1)), BP, and MEA-BP models for satellite clock bias prediction is analyzed and the root mean square error, range difference error, and the mean of the clock bias data compared. The results demonstrate that the prediction accuracy of the three satellites significantly increased after one-time difference processing and that they have good stability. The prediction accuracy of four sessions of 2 h, 3 h, 6 h, and 12 h obtained using the MEA-BP model was better than 0.74, 0.80, 1.12, and 0.87 ns, respectively. The MEA-BP model has a specific degree of improvement in the prediction accuracy of the different sessions. Additionally, the prediction accuracy of different models has a specific relationship with the length of the original modeling sequence, of which BP model is the most affected, and MEABP is relatively less affected by the length of the modeling sequence, indicating that the MEA-BP model has strong anti-interference ability."
"Ethical Considerations of AI in Financial Services: Privacy, Bias, and Algorithmic Transparency",2024,Naila Iqbal Qureshi; Saurabh Suman Choudhuri; Yaramala Nagamani; Raj Varma; Rutul Shah,,19,W4401387946,10.1109/ickecs61492.2024.10616483,https://openalex.org/W4401387946,,Transparency (behavior); Information privacy; Computer science; Internet privacy; Accounting,article,False,
Using artificial intelligence in craft education: crafting with text-to-image generative models,2023,Henriikka Vartiainen; Matti Tedre,Digital Creativity,183,W4321596574,10.1080/14626268.2023.2174557,https://openalex.org/W4321596574,https://doi.org/10.1080/14626268.2023.2174557,Craft; Computer science; Generative grammar; Multimedia; Artificial intelligence,article,True,"Artificial intelligence (AI) and the automation of creative work have received little attention in craft education. This study aimed to address this gap by exploring Finnish pre-service craft teachers' and teacher educators' (N = 15) insights into the potential benefits and challenges of AI, particularly text-to-image generative AI. This study implemented a hands-on workshop on creative making with text-to-image generative AI in order to stimulate discourses and capture imaginaries concerning generative AI. The results revealed that making with AI inspired teachers to consider the unique nature of crafts as well as the tensions and tradeoffs of adopting generative AI in craft practices. The teachers identified concerns in data-driven design, including algorithmic bias, copyright violations and black-boxing creativity, as well as in power relationships, hybrid influencing and behaviour engineering. The article concludes with a discussion of the complicated relationships the results uncovered between creative making and generative AI."
Stability analysis of the bias compensated LMS algorithm,2024,Rodrigo Pimenta; Mariane R. Petraglia; Diego B. Haddad,Digital Signal Processing,8,W4391147781,10.1016/j.dsp.2024.104395,https://openalex.org/W4391147781,,Independence (probability theory); Stability (learning theory); Algorithm; Heuristic; Noise (video),article,False,
U.S. Antidiscrimination Law in Healthcare &amp; Algorithmic Bias : Its Lessons for South Korean Legislation,2024,Won Bok Lee,Public Law Journal,1,W4399904163,10.31779/plj.25.2.202405.007,https://openalex.org/W4399904163,https://doi.org/10.31779/plj.25.2.202405.007,Legislation; Law; Health care; Political science; Law and economics,article,True,"미국 의료 분야의 차별금지법은 1964년 제정된 민권법에서 시작하여 지금은 Patient Protection and Affordable Care Act의 제1557조로 통합되었다. 이는 의료 인공지능이야기하는 편향이나 차별에 적용되는 법적 프레임워크를 형성한다. 이와 같이 미국은 성문법과 판례가 축적한 차별금지법리가 오랜 세월을 통하여 섬세하게 구축되어 있음에도 불구하고 이를 알고리즘에 적용하는 것에는 어려움이 적지 않다. 특히 의료 분야 차별금지법은 결과적 차별에 대한 개인의 소권을 인정하는데 대하여 소극적인 미국 항소법원들의 입장 때문에 알고리즘의 차별에 적극적으로 대처하기에 미흡하다는 지적도 있다.BR 우리나라의 경우에는 국회에 상정되었던 인공지능 규제 법률안들이나 의료 인공지능에 적용되는 윤리지침 등에서 공히 차별 또는 편향의 예방을 중요한 원칙으로 규정하지만, 의료 분야 차별금지법의 부재 및 외국에 비하여 높은 의료 형평성의 현실로 인하여 과연 어떤 사유를 이유로 하는 차별이나 편향을 예방하여야 할 것인지 아무런 아이디어를 얻을 수 없는 상황이다. 이를 해소하기 위하여는 (실제 입법까지 이르는 것을 상정하지 않더라도) 우리나라에서 의료 분야 차별금지법을 제정할 경우 어떤 모습이 되어야 하는지를 가정적으로 논의하고, 그 과정에서 우리나라 현실에 부합하는 차별금지사유를 추출하는 작업이 먼저 이루어져야 할 것이다."
Using Cognitive Models to Understand and Counteract the Effect of Self-Induced Bias on Recommendation Algorithms,2023,Justyna Pawłowska; Klara Rydzewska; Adam Wierzbicki,Journal of Artificial Intelligence and Soft Computing Research,12,W4323928465,10.2478/jaiscr-2023-0008,https://openalex.org/W4323928465,https://sciendo.com/pdf/10.2478/jaiscr-2023-0008,Collaborative filtering; Computer science; Cognition; Recommender system; Set (abstract data type),article,True,"Abstract Recommendation algorithms trained on a training set containing sub-optimal decisions may increase the likelihood of making more bad decisions in the future. We call this harmful effect self-induced bias, to emphasize that the bias is driven directly by the user’s past choices. In order to better understand the nature of self-induced bias of recommendation algorithms that are used by older adults with cognitive limitations, we have used agent-based simulation. Based on state-of-the-art results in psychology of aging and cognitive science, as well as our own empirical results, we have developed a cognitive model of an e-commerce client that incorporates cognitive decision-making abilities. We have evaluated the magnitude of self-induced bias by comparing results achieved by simulated agents with and without cognitive limitations due to age. We have also proposed new recommendation algorithms designed to counteract self-induced bias. The algorithms take into account user preferences and cognitive abilities relevant to decision making. To evaluate the algorithms, we have introduced 3 benchmarks: a simple product filtering method and two types of widely used recommendation algorithms: Content-Based and Collaborative filtering. Results indicate that the new algorithms outperform benchmarks both in terms of increasing the utility of simulated agents (both old and young), and in reducing self-induced bias."
Algorithmic gender bias: investigating perceptions of discrimination in automated decision-making,2024,Soojong Kim; Poong Oh; Joomi Lee,Behaviour and Information Technology,7,W4391384911,10.1080/0144929x.2024.2306484,https://openalex.org/W4391384911,,Perception; Psychology; Situational ethics; Injustice; Social psychology,article,False,"With the widespread use of artificial intelligence and automated decision-making (ADM), concerns are increasing about automated decisions biased against certain social groups, such as women and racial minorities. The public's skepticism and the danger of algorithmic discrimination are widely acknowledged, yet the role of key factors constituting the context of discriminatory situations is underexplored. This study examined people's perceptions of gender bias in ADM, focusing on three factors influencing the responses to discriminatory automated decisions: the target of discrimination (subject vs. other), the gender identity of the subject, and situational contexts that engender biases. Based on a randomised experiment (N = 602), we found stronger negative reactions to automated decisions that discriminate against the gender group of the subject than those discriminating against other gender groups, evidenced by lower perceived fairness and trust in ADM, and greater negative emotion and tendency to question the outcome. The negative reactions were more pronounced among participants in underserved gender groups than men. Also, participants were more sensitive to biases in economic and occupational contexts than in other situations. These findings suggest that perceptions of algorithmic biases should be understood in relation to the public's lived experience of inequality and injustice in society."
Organizational Dynamics and Bias in Artificial Intelligence (AI) Recruitment Algorithms,2023,Marwan Omar; Darrell Norman Burrell,Advances in business information systems and analytics book series,10,W4391603409,10.4018/979-8-3693-1970-3.ch015,https://openalex.org/W4391603409,,Artificial intelligence; Computer science; Machine learning; Algorithm,book-chapter,False,"The integration of artificial intelligence (AI) into recruitment processes has promised to revolutionize and optimize the hiring landscape. However, recent legal proceedings have shed light on the alarming implications of AI algorithms in the employment sector. This chapter delves into a significant case study where African American, Latina American, Arab American, and other marginalized job applicants and employees filed a 100-million-dollar class action lawsuit against a prominent organization, Context Systems. The suit alleges that AI screening tools, entrusted with the crucial task of selecting candidates, have been marred by programming bias, leading to discriminatory outcomes. This case study critically examines the multifaceted problems arising from bias in AI algorithms, revealing their detrimental effects on marginalized communities in the employment sector. By scrutinizing this pivotal case, the authors aim to provide insights into the urgent need for transparency, accountability, and ethical considerations in the development and deployment of AI-driven recruitment tools."
Equal accuracy for Andrew and Abubakar—detecting and mitigating bias in name-ethnicity classification algorithms,2023,Lena Hafner; Theodor Peter Peifer; Franziska Sofia Hafner,AI & Society,12,W4319661985,10.1007/s00146-022-01619-4,https://openalex.org/W4319661985,https://link.springer.com/content/pdf/10.1007/s00146-022-01619-4.pdf,Ethnic group; Computer science; Audit; Artificial intelligence; Contrast (vision),article,True,
"Algorithmic Bias in AI-Based Diabetes Care: Systematic Review of Model Performance, Equity Reporting, and Physiological Label Bias",2025,Mohammad Amin Alipour; Abolfazl Alipour,InfoScience Trends,1,W4410482192,10.61186/ist.202502.05.04,https://openalex.org/W4410482192,https://www.isjtrend.com/article_220108_6fcbd86f7230972b20baf7c19025c097.pdf,Equity (law); Computer science; Psychology; Actuarial science; Econometrics,article,True,
Algorithmic Fairness and Bias in Machine Learning Systems,2023,Rushil Chandra; Karun Sanjaya; AR Aravind; Ahmed Abbas; Ruzieva Gulrukh; T. S. Senthil kumar,E3S Web of Conferences,4,W4384572189,10.1051/e3sconf/202339904036,https://openalex.org/W4384572189,https://www.e3s-conferences.org/articles/e3sconf/pdf/2023/36/e3sconf_iconnect2023_04036.pdf,Interpretability; Transparency (behavior); Machine learning; Computer science; Artificial intelligence,article,True,"In recent years, research into and concern over algorithmic fairness and bias in machine learning systems has grown significantly. It is vital to make sure that these systems are fair, impartial, and do not support discrimination or social injustices since machine learning algorithms are becoming more and more prevalent in decision-making processes across a variety of disciplines. This abstract gives a general explanation of the idea of algorithmic fairness, the difficulties posed by bias in machine learning systems, and different solutions to these problems. Algorithmic bias and fairness in machine learning systems are crucial issues in this regard that demand the attention of academics, practitioners, and policymakers. Building fair and unbiased machine learning systems that uphold equality and prevent discrimination requires addressing biases in training data, creating fairness-aware algorithms, encouraging transparency and interpretability, and encouraging diversity and inclusivity."
Making decisions: Bias in artificial intelligence and data‑driven diagnostic tools,2023,Yves Saint James Aquino,Australian Journal of General Practice,43,W4383710950,10.31128/ajgp-12-22-6630,https://openalex.org/W4383710950,https://www1.racgp.org.au/getattachment/ae297919-ed4c-4add-9dc0-244c3acc08cc/Making-decisions.aspx,Compromise; Replicate; Computer science; Artificial intelligence; Disadvantaged,article,True,"AI relies on data generated, collected, recorded and labelled by humans. If AI systems remain unchecked, whatever biases that exist in the real world that are embedded in data will be incorporated into the AI algorithms. Algorithmic bias can be considered as an extension, if not a new manifestation, of existing social biases, understood as negative attitudes towards or the discriminatory treatment of some groups. In medicine, algorithmic bias can compromise patient safety and risks perpetuating disparities in care and outcome. Thus, clinicians should consider the risk of bias when deploying AI-enabled tools in their practice."
Solution of SAT problems with the adaptive-bias quantum approximate optimization algorithm,2023,Yunlong Yu; Chenfeng Cao; Xiang‐Bin Wang; Nic Shannon; Robert Joynt,Physical Review Research,12,W4379231024,10.1103/physrevresearch.5.023147,https://openalex.org/W4379231024,http://link.aps.org/pdf/10.1103/PhysRevResearch.5.023147,Computer science; Optimization problem; Quantum entanglement; Quantum; Mathematical optimization,article,True,"The quantum approximate optimization algorithm (QAOA) is a promising method for solving certain classical combinatorial optimization problems on near-term quantum devices. When employing the QAOA to 3-SAT and Max-3-SAT problems, the quantum cost exhibits an easy-hard-easy or easy-hard pattern, respectively, as the clause density is changed. The quantum resources needed in the hard-region problems are out of reach for current noisy intermediate-scale quantum (NISQ) devices. We show by numerical simulations with up to 14 variables and analytical arguments that the adaptive-bias QAOA (ab-QAOA) greatly improves performance in the hard region of the 3-SAT problems and hard region of the Max-3-SAT problems. For similar accuracy, on average, ab-QAOA needs 3 levels for 10-variable 3-SAT problems as compared to 22 for QAOA. For 10-variable Max-3-SAT problems, the numbers are 7 levels and 62 levels. The improvement comes from a more targeted and more limited generation of entanglement during the evolution. We demonstrate that classical optimization is not strictly necessary in the ab-QAOA since local fields are used to guide the evolution. This leads us to propose an optimization-free ab-QAOA that can solve the hard-region 3-SAT and Max-3-SAT problems effectively with significantly fewer quantum gates as compared to the original ab-QAOA. Our work paves the way for realizing quantum advantages for optimization problems on NISQ devices. © 2023 authors. Published by the American Physical Society. Published by the American Physical Society under the terms of the Creative Commons Attribution 4.0 International license. Further distribution of this work must maintain attribution to the author(s) and the published article's title, journal citation, and DOI."
E-commerce and consumer behavior: A review of AI-powered personalization and market trends,2024,Mustafa Ayobami Raji; Hameedat Bukola Olodo; Timothy Tolulope Oke; Wilhelmina Afua Addy; Onyeka Chrisanctus Ofodile; Adedoyin Tolulope Oyewole,GSC Advanced Research and Reviews,156,W4392621828,10.30574/gscarr.2024.18.3.0090,https://openalex.org/W4392621828,https://gsconlinepress.com/journals/gscarr/sites/default/files/GSCARR-2024-0090.pdf,Personalization; E-commerce; Business; Advertising; Commerce,review,True,"In the dynamic landscape of electronic commerce (e-commerce), understanding and adapting to evolving consumer behavior is critical for the sustained success of online businesses. This review delves into the intersection of e-commerce and consumer behavior, focusing on the transformative role of Artificial Intelligence (AI)-powered personalization and its impact on market trends. The advent of AI has revolutionized the way e-commerce platforms engage with and cater to individual consumer preferences. AI-powered personalization techniques leverage advanced algorithms to analyze vast datasets, enabling the delivery of highly tailored and relevant content, product recommendations, and user experiences. This review explores the intricate mechanisms of AI-driven personalization, examining how it enhances customer engagement, satisfaction, and loyalty. Furthermore, the study investigates the prominent market trends shaped by AI in e-commerce. From chatbots and virtual assistants facilitating seamless customer interactions to predictive analytics optimizing inventory management, AI is driving innovation across various facets of the online retail landscape. The analysis delves into the integration of machine learning algorithms in predicting consumer preferences, streamlining the purchasing process, and fostering a more personalized shopping journey. As e-commerce continues to evolve, the review also explores the challenges and ethical considerations associated with AI-powered personalization. Issues such as data privacy, algorithmic bias, and the delicate balance between customization and intrusiveness are examined to provide a comprehensive understanding of the broader implications of AI in shaping consumer behavior. Ultimately, this review offers valuable insights into the symbiotic relationship between e-commerce and consumer behavior, shedding light on the transformative power of AI-powered personalization and its influence on emerging market trends. As businesses navigate the digital landscape, understanding and harnessing the potential of AI-driven strategies become imperative for staying competitive and meeting the evolving expectations of tech-savvy consumers."
Call for algorithmic fairness to mitigate amplification of racial biases in artificial intelligence models used in orthodontics and craniofacial health,2023,Veerasathpurush Allareddy; Maysaa Oubaidin; Sankeerth Rampa; Shankar Rengasamy Venugopalan; Mohammed H. Elnagar; Sumit Yadav; Min Kyeong Lee,Orthodontics and Craniofacial Research,14,W4387692646,10.1111/ocr.12721,https://openalex.org/W4387692646,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/ocr.12721,Orthodontics; Craniofacial; Computer science; Psychology; Artificial intelligence,review,True,"Abstract Machine Learning (ML), a subfield of Artificial Intelligence (AI), is being increasingly used in Orthodontics and craniofacial health for predicting clinical outcomes. Current ML/AI models are prone to accentuate racial disparities. The objective of this narrative review is to provide an overview of how AI/ML models perpetuate racial biases and how we can mitigate this situation. A narrative review of articles published in the medical literature on racial biases and the use of AI/ML models was undertaken. Current AI/ML models are built on homogenous clinical datasets that have a gross underrepresentation of historically disadvantages demographic groups, especially the ethno‐racial minorities. The consequence of such AI/ML models is that they perform poorly when deployed on ethno‐racial minorities thus further amplifying racial biases. Healthcare providers, policymakers, AI developers and all stakeholders should pay close attention to various steps in the pipeline of building AI/ML models and every effort must be made to establish algorithmic fairness to redress inequities."
A biased random-key genetic algorithm for the chordal completion problem,2023,Samuel Eduardo da Silva; Celso C. Ribeiro; Uéverton S. Souza,RAIRO. Operations research,9,W4379387944,10.1051/ro/2023081,https://openalex.org/W4379387944,https://doi.org/10.1051/ro/2023081,Chordal graph; Heuristics; Chord (peer-to-peer); Algorithm; Implementation,article,True,"A graph is chordal if all its cycles of length greater than or equal to four contain a chord, i.e. , an edge connecting two nonconsecutive vertices of the cycle. Given a graph G = ( V , E ), the chordal completion problem consists in finding the minimum set of edges to be added to G to obtain a chordal graph. It has applications in sparse linear systems, database management and computer vision programming. In this article, we developed a biased random-key genetic algorithm (BRKGA) for solving the chordal completion problem, based on the strategy of manipulating permutations that represent perfect elimination orderings of triangulations. Computational results show that the proposed heuristic improve the results of the constructive heuristics fill-in and min-degree. We also developed a strategy for injecting externally constructed feasible solutions coded as random keys into the initial population of the BRKGA that significantly improves the solutions obtained and may benefit other implementations of biased random-key genetic algorithms."
THE ROLE OF AI IN MARKETING PERSONALIZATION: A THEORETICAL EXPLORATION OF CONSUMER ENGAGEMENT STRATEGIES,2024,Sodiq Odetunde Babatunde; Opeyemi Abayomi Odejide; Tolulope Esther Edunjobi; Damilola Oluwaseun Ogundipe,International Journal of Management & Entrepreneurship Research,189,W4393278147,10.51594/ijmer.v6i3.964,https://openalex.org/W4393278147,https://fepbl.com/index.php/ijmer/article/download/964/1179,Personalization; Customer engagement; Marketing; Business; Psychology,article,True,"This paper explores the transformative potential of Artificial Intelligence (AI) in personalizing marketing strategies. It delves into the theoretical underpinnings of consumer engagement sand investigates how AI can be leveraged to develop targeted and relevant marketing experiences. AI can personalize messages based on consumer behavior and demographics, influencing the processing route and maximizing engagement. This theory explores the use of game mechanics to motivate and engage users. AI can personalize gamified marketing experiences, tailoring rewards and challenges to individual consumer preferences, driving deeper engagement. Algorithms can analyze vast amounts of customer data to predict individual preferences and behaviors. This allows for targeted advertising, product recommendations, and content that resonates with specific consumer segments. Natural Language Processing (NLP), AI-powered NLP tools analyze customer reviews, social media conversations, and other forms of unstructured data. This allows brands to understand customer sentiment and personalize communication styles for optimal engagement AI-powered chatbots and virtual assistants can provide personalized customer support and product recommendations in real-time, fostering a more interactive and engaging brand experience. Potential Benefits and Considerations Personalized marketing messages and experiences cater to individual needs and preferences, leading to higher satisfaction and loyalty. By tailoring content and offerings to specific consumer segments, brands can establish a more relevant and relatable image. Improved Conversion Rates, Personalized marketing campaigns can be highly targeted and effective, leading to increased conversions and sales. Balancing personalization with data privacy concerns is crucial. Transparency and user control over data collection practices are essential. AI algorithms can perpetuate biases present in training data. Ensuring fairness and inclusivity in AI-powered marketing is paramount. AI is revolutionizing marketing personalization. By leveraging AI's analytical capabilities and understanding the theoretical aspects of consumer engagement, brands can develop targeted and relevant marketing strategies that foster deeper customer connections and drive business growth. Keywords: AI Personalization, Consumer Engagement, Marketing Strategy, Theoretical Exploration, Data Privacy, Algorithmic Bias."
Racial Bias in Clinical and Population Health Algorithms: A Critical Review of Current Debates,2024,Madison Coots; Kristin A. Linn; Sharad Goel; Amol S. Navathe; Ravi B. Parikh,Annual Review of Public Health,10,W4404945662,10.1146/annurev-publhealth-071823-112058,https://openalex.org/W4404945662,https://doi.org/10.1146/annurev-publhealth-071823-112058,Health care; Health equity; Ethnic group; Population; Algorithm,review,True,"Among health care researchers, there is increasing debate over how best to assess and ensure the fairness of algorithms used for clinical decision support and population health, particularly concerning potential racial bias. Here we first distill concerns over the fairness of health care algorithms into four broad categories: ( a ) the explicit inclusion (or, conversely, the exclusion) of race and ethnicity in algorithms, ( b ) unequal algorithm decision rates across groups, ( c ) unequal error rates across groups, and ( d ) potential bias in the target variable used in prediction. With this taxonomy, we critically examine seven prominent and controversial health care algorithms. We show that popular approaches that aim to improve the fairness of health care algorithms can in fact worsen outcomes for individuals across all racial and ethnic groups. We conclude by offering an alternative, consequentialist framework for algorithm design that mitigates these harms by instead foregrounding outcomes and clarifying trade-offs in the pursuit of equitable decision-making."
Towards a holistic view of bias in machine learning: bridging algorithmic fairness and imbalanced learning,2024,Damien Dablain; Bartosz Krawczyk; Nitesh V. Chawla,Discover Data,10,W4393952022,10.1007/s44248-024-00007-1,https://openalex.org/W4393952022,https://link.springer.com/content/pdf/10.1007/s44248-024-00007-1.pdf,Bridging (networking); Computer science; Artificial intelligence; Psychology; Machine learning,article,True,"Abstract Machine learning (ML) is playing an increasingly important role in rendering decisions that affect a broad range of groups in society. This posits the requirement of algorithmic fairness , which holds that automated decisions should be equitable with respect to protected features (e.g., gender, race). Training datasets can contain both class imbalance and protected feature bias. We postulate that, to be effective, both class and protected feature bias should be reduced—which allows for an increase in model accuracy and fairness. Our method, Fair OverSampling (FOS), uses SMOTE (Chawla in J Artif Intell Res 16:321–357, 2002) to reduce class imbalance and feature blurring to enhance group fairness. Because we view bias in imbalanced learning and algorithmic fairness differently, we do not attempt to balance classes and features; instead, we seek to de-bias features and balance the number of class instances. FOS restores numerical class balance through the creation of synthetic minority class instances and causes a classifier to pay less attention to protected features. Therefore, it reduces bias for both classes and protected features . Additionally, we take a step toward bridging the gap between fairness and imbalanced learning with a new metric, Fair Utility , that measures model effectiveness with respect to accuracy and fairness. Our source code and data are publicly available at https://github.com/dd1github/Fair-Over-Sampling."
The bias beneath: analyzing drift in YouTube’s algorithmic recommendations,2024,Mert Can Çakmak; Nitin Agarwal; Remi Oni,Social Network Analysis and Mining,8,W4401843528,10.1007/s13278-024-01343-5,https://openalex.org/W4401843528,https://doi.org/10.1007/s13278-024-01343-5,Computer science; Limiting; Transparency (behavior); Variety (cybernetics); Space (punctuation),article,True,"Abstract In today’s digital world, understanding how YouTube’s recommendation systems guide what we watch is crucial. This study dives into these systems, revealing how they influence the content we see over time. We found that YouTube’s algorithms tend to push content in certain directions, affecting the variety and type of videos recommended to viewers. To uncover these patterns, we used a mixed methods approach to analyze videos recommended by YouTube. We looked at the emotions conveyed in videos, the moral messages they might carry, and whether they contained harmful content. Our research also involved statistical analysis to detect biases in how these videos are recommended and network analysis to see how certain videos become more influential than others. Our findings show that YouTube’s algorithms can lead to a narrowing of the content landscape, limiting the diversity of what gets recommended. This has important implications for how information is spread and consumed online, suggesting a need for more transparency and fairness in how these algorithms work. In summary, this paper highlights the need for a more inclusive approach to how digital platforms recommend content. By better understanding the impact of YouTube’s algorithms, we can work towards creating a digital space that offers a wider range of perspectives and voices, affording fairness, and enriching everyone’s online experience."
Analyzing Bias in Recommender Systems: A Comprehensive Evaluation of YouTube's Recommendation Algorithm,2023,Mert Can Çakmak; Obianuju Okeke; Ugochukwu Onyepunuka; Billy Spann; Nitin Agarwal,,9,W4392845346,10.1145/3625007.3627300,https://openalex.org/W4392845346,https://dl.acm.org/doi/pdf/10.1145/3625007.3627300,Recommender system; Computer science; Collaborative filtering; Filter (signal processing); Information retrieval,article,True,"Recommender systems play a crucial role in suggesting relevant content to users based on their past activities. They employ a process known as ""collaborative filtering"" to efficiently navigate extensive content repositories. However, concerns have been raised regarding the potential bias and homogeneity in recommendations, resulting in filter-bubbles and echo-chambers. Detecting and mitigating these biases is crucial for ensuring fair and diverse automated decision-making systems. This study investigates the impact of YouTube's recommendation algorithm on three distinct narratives across multiple dimensions. Our objective is to identify potential biases and gain insights into its decision-making behavior. We applied a multi-method approach to evaluate emotional content, moral foundations, lexical similarity, and social network analysis across 5 depths of YouTube recommendations. The results of our analysis showed diversity in emotions, significant drift in topics, and a push toward non-related, but highly influential videos across multiple recommendation depths. The findings from this study contribute to the understanding of bias in recommender systems. These insights inform the development of strategies to mitigate biases and improve the user experience. Policymakers and platform developers can utilize this knowledge to establish effective guidelines and policies for their recommender systems, enhancing decision-making processes."
Fairness and Bias in Algorithmic Hiring: a Multidisciplinary Survey,2023,Alessandro Fabris; Nina Baranowska; Matthew Dennis; Philipp Hacker; Jorge Saldivar; Frederik Zuiderveen Borgesius; Asia J. Biega,arXiv (Cornell University),6,W4387075971,10.48550/arxiv.2309.13933,https://openalex.org/W4387075971,https://arxiv.org/pdf/2309.13933,Work (physics); Pipeline (software); Corporate governance; Domain (mathematical analysis); Computer science,preprint,True,"Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of, algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders."
Fairness of artificial intelligence in healthcare: review and recommendations,2023,Daiju Ueda; Taichi Kakinuma; Shohei Fujita; Koji Kamagata; Yasutaka Fushimi; Rintaro Ito; Yusuke Matsui; Taiki Nozaki; Takeshi Nakaura; Noriyuki Fujima; Fuminari Tatsugami; Masahiro Yanagawa; Kenji Hirata; Akira Yamada; Takahiro Tsuboyama; Mariko Kawamura; Tomoyuki Fujioka; Shinji Naganawa,Japanese Journal of Radiology,363,W4385564466,10.1007/s11604-023-01474-3,https://openalex.org/W4385564466,https://link.springer.com/content/pdf/10.1007/s11604-023-01474-3.pdf,Accountability; Health care; Transparency (behavior); Computer science; Software deployment,review,True,"Abstract In this review, we address the issue of fairness in the clinical integration of artificial intelligence (AI) in the medical field. As the clinical adoption of deep learning algorithms, a subfield of AI, progresses, concerns have arisen regarding the impact of AI biases and discrimination on patient health. This review aims to provide a comprehensive overview of concerns associated with AI fairness; discuss strategies to mitigate AI biases; and emphasize the need for cooperation among physicians, AI researchers, AI developers, policymakers, and patients to ensure equitable AI integration. First, we define and introduce the concept of fairness in AI applications in healthcare and radiology, emphasizing the benefits and challenges of incorporating AI into clinical practice. Next, we delve into concerns regarding fairness in healthcare, addressing the various causes of biases in AI and potential concerns such as misdiagnosis, unequal access to treatment, and ethical considerations. We then outline strategies for addressing fairness, such as the importance of diverse and representative data and algorithm audits. Additionally, we discuss ethical and legal considerations such as data privacy, responsibility, accountability, transparency, and explainability in AI. Finally, we present the Fairness of Artificial Intelligence Recommendations in healthcare (FAIR) statement to offer best practices. Through these efforts, we aim to provide a foundation for discussing the responsible and equitable implementation and deployment of AI in healthcare."
Analysis of Marine Predators Algorithm using BIAS toolbox and Generalized Signature Test,2024,Manish Kumar; Kanchan Rajwar; Kusum Deep,Alexandria Engineering Journal,9,W4393305890,10.1016/j.aej.2024.03.060,https://openalex.org/W4393305890,https://doi.org/10.1016/j.aej.2024.03.060,Foraging; Algorithm; Toolbox; Convergence (economics); Computer science,article,True,"The Marine Predators Algorithm (MPA) is a prominent Nature-Inspired Optimization Algorithm (NIOA) that has garnered significant research interest due to its effectiveness. It draws inspiration from the foraging behaviors of marine predators, predominantly using the Lévy or Brownian approach for its foraging strategy. Despite its acclaim, the structural bias within MPA has not been thoroughly investigated, marking a significant gap in the current research. This absence of targeted research forms the core rationale behind initiating this study. Structural bias has recently been identified in NIOAs, causing the population to revisit specific regions of the search space without gaining new information. As a result, it may lead to increased computational costs and slow down the rate of convergence. Therefore, identifying structural bias is essential to better understand the search mechanism of MPA. To ascertain the presence of any structural bias, two recently introduced models are employed: the BIAS toolbox and the Generalized Signature Test. These examinations reveal a notable structural bias in MPA, predominantly towards the center of the search space. Also, possible future research directions for MPA are discussed. Our findings provide valuable insights into the search dynamics of the algorithm, fostering the development of new, unbiased, and efficient algorithms."
A Graph-Based Algorithm for Robust Sequential Localization Exploiting Multipath for Obstructed-LOS-Bias Mitigation,2023,Alexander Venus; Erik Leitinger; Stefan Tertinek; Klaus Witrisal,IEEE Transactions on Wireless Communications,18,W4380534835,10.1109/twc.2023.3285530,https://openalex.org/W4380534835,https://ieeexplore.ieee.org/ielx7/7693/4656680/10151807.pdf,Computer science; Multipath propagation; Algorithm; Graph; Theoretical computer science,article,True,"This paper presents a factor graph formulation and particle-based sum-product algorithm (SPA) for robust sequential localization in multipath-prone environments. The proposed algorithm jointly performs data association, sequential estimation of a mobile agent position, and adapts all relevant model parameters. We derive a novel non-uniform false alarm (FA) model that captures the delay and amplitude statistics of the multipath radio channel. This model enables the algorithm to indirectly exploit position-related information contained in the multipath components (MPCs) for the estimation of the agent position without using any prior information such as floorplan information or training data. Using simulated and real measurements in different channel conditions, we demonstrate that the algorithm can provide high-accuracy position estimates even in fully obstructed line-of-sight (OLOS) situations and show that the performance of our algorithm constantly attains the posterior Cramér-Rao lower bound (P-CRLB), facilitating the additional information contained in the presented FA model. The algorithm is shown to provide robust estimates in both, dense multipath channels as well as channels showing specular, resolved MPCs, significantly outperforming state-of-the-art radio-based localization methods."
Addressing algorithmic bias in AI-based pain management: a comparative analysis of inference models and pretrained large language models,2025,Xiang Liu; Wei Gao; Pengfei Li; Xiaohua Fan; Chao Wang,Pain,1,W4413394921,10.1097/j.pain.0000000000003694,https://openalex.org/W4413394921,,Inference; Computer science; Natural language processing; Pain management; Artificial intelligence,article,False,
Ethical implications of AI and robotics in healthcare: A review,2023,Chukwuka Elendu; Dependable C. Amaechi; Tochi C. Elendu; Klein A. Jingwa; Osinachi K. Okoye; Minichimso John Okah; John A. Ladele; Abdirahman H. Farah; Hameed A. Alimi,Medicine,221,W4389775637,10.1097/md.0000000000036671,https://openalex.org/W4389775637,https://doi.org/10.1097/md.0000000000036671,Accountability; Health care; Transparency (behavior); Robotics; Artificial intelligence,review,True,"Integrating Artificial Intelligence (AI) and robotics in healthcare heralds a new era of medical innovation, promising enhanced diagnostics, streamlined processes, and improved patient care. However, this technological revolution is accompanied by intricate ethical implications that demand meticulous consideration. This article navigates the complex ethical terrain surrounding AI and robotics in healthcare, delving into specific dimensions and providing strategies and best practices for ethical navigation. Privacy and data security are paramount concerns, necessitating robust encryption and anonymization techniques to safeguard patient data. Responsible data handling practices, including decentralized data sharing, are critical to preserve patient privacy. Algorithmic bias poses a significant challenge, demanding diverse datasets and ongoing monitoring to ensure fairness. Transparency and explainability in AI decision-making processes enhance trust and accountability. Clear responsibility frameworks are essential to address the accountability of manufacturers, healthcare institutions, and professionals. Ethical guidelines, regularly updated and accessible to all stakeholders, guide decision-making in this dynamic landscape. Moreover, the societal implications of AI and robotics extend to accessibility, equity, and societal trust. Strategies to bridge the digital divide and ensure equitable access must be prioritized. Global collaboration is pivotal in developing adaptable regulations and addressing legal challenges like liability and intellectual property. Ethics must remain at the forefront in the ever-evolving realm of healthcare technology. By embracing these strategies and best practices, healthcare systems and professionals can harness the potential of AI and robotics, ensuring responsible and ethical integration that benefits patients while upholding the highest ethical standards."
Social norm bias: residual harms of fairness-aware algorithms,2023,Myra Cheng; Maria De‐Arteaga; Lester Mackey; Adam Tauman Kalai,Data Mining and Knowledge Discovery,7,W4317734474,10.1007/s10618-022-00910-8,https://openalex.org/W4317734474,,Conformity; Norm (philosophy); Computer science; Artificial intelligence; Machine learning,article,False,
Small Target Detection in Sea Clutter by Weighted Biased Soft-Margin SVM Algorithm in Feature Spaces,2024,Peng‐Lang Shui; L.R. Zhang; Xiaohui Bai,IEEE Sensors Journal,13,W4392182816,10.1109/jsen.2024.3350571,https://openalex.org/W4392182816,,Clutter; Margin (machine learning); Support vector machine; Feature (linguistics); Pattern recognition (psychology),article,False,"Sea-surface small target detection in high-resolution sea clutter is always an intractable problem. Feature-based detection in multidimensional feature spaces is recognized to be an effective way, and therein, learning algorithms with controllable false alarm rate play an important role. In this article, a weighted biased soft-margin support vector machine (WBSM-SVM) algorithm is proposed to design two-class classifiers with controllable false alarm rate in feature spaces and the induced feature-based detectors can accurately control false alarm rate and have excellent detection ability of small targets in sea clutter. The WBSM-SVM algorithm contains three innovations. First, special two-class SVM classifiers are used to lower the loss from the one-class <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$\nu $ </tex-math></inline-formula> -SVM classifiers by additional use of the training samples of the feature vector from simulated returns of typical targets plus measured sea clutter. Second, extremely unbalanced misclassification weight of penalty factors of the two classes and the Mahalanobis distance of the training sample vectors are introduced in the SVM to meet the demand of extremely unbalanced false alarm rate versus target missing probability. Third, a biased classification boundary is used to tune the false alarm rate to the expected one. The experimental results on the two recognized databases for sea-surface small target detection show that the WBSM-SVM-based detectors attain better detection performance than existing feature-based detectors."
Predictive algorithms and racial bias: a qualitative descriptive study on the perceptions of algorithm accuracy in higher education,2023,Stacey Lynn von Winckelmann,Information and Learning Sciences,8,W4387024961,10.1108/ils-05-2023-0045,https://openalex.org/W4387024961,,Data collection; Algorithm; Originality; Stakeholder; Perception,article,False,"Purpose This study aims to explore the perception of algorithm accuracy among data professionals in higher education. Design/methodology/approach Social justice theory guided the qualitative descriptive study and emphasized four principles: access, participation, equity and human rights. Data collection included eight online open-ended questionnaires and six semi-structured interviews. Participants included higher education professionals who have worked with predictive algorithm (PA) recommendations programmed with student data. Findings Participants are aware of systemic and racial bias in their PA inputs and outputs and acknowledge their responsibility to ethically use PA recommendations with students in historically underrepresented groups (HUGs). For some participants, examining these topics through the lens of social justice was a new experience, which caused them to look at PAs in new ways. Research limitations/implications Small sample size is a limitation of the study. Implications for practice include increased stakeholder training, creating an ethical data strategy that protects students, incorporating adverse childhood experiences data with algorithm recommendations, and applying a modified critical race theory framework to algorithm outputs. Originality/value The study explored the perception of algorithm accuracy among data professionals in higher education. Examining this topic through a social justice lens contributes to limited research in the field. It also presents implications for addressing racial bias when using PAs with students in HUGs."
Large language models to identify social determinants of health in electronic health records,2024,Marco Guevara-Vega; Shan Chen; Spencer A. Thomas; Tafadzwa L. Chaunzwa; Idalid Franco; Benjamin H. Kann; Shalini Moningi; Jack M. Qian; Madeleine Goldstein; Susan Harper; Hugo J.W.L. Aerts; Paul J. Catalano; Guergana Savova; Raymond H. Mak; Danielle S. Bitterman,npj Digital Medicine,191,W4390745503,10.1038/s41746-023-00970-0,https://openalex.org/W4390745503,https://www.nature.com/articles/s41746-023-00970-0.pdf,Social determinants of health; Health care; Medicine; Political science; Law,article,True,
"Cultural and ethical dimensions of learning management system adoption in rural universities: Exploring data privacy, algorithmic bias, and contextual realities",2025,Oluwatoyin Ayodele Ajani; Bongani Thulani Gamede; Samantha Govender,Multidisciplinary Science Journal,1,W4414818342,10.31893/multiscience.2026142,https://openalex.org/W4414818342,https://malque.pub/ojs/index.php/msj/article/download/10045/4680,,article,True,"Digital transformation in higher education has positioned Learning Management Systems (LMS) as vital platforms for student learning and academic engagement. However, in rural university contexts, LMS adoption by students is shaped not only by access to technology, but also by deeper cultural and ethical dynamics. This study explores how cultural factors and ethical concerns, particularly data privacy and algorithmic biasaffect students’ adoption and utilization of LMS in rural South African universities. Guided by the Technological Pedagogical Content Knowledge (TPACK) and Unified Theory of Acceptance and Use of Technology (UTAUT) frameworks, the research draws on mixed-methods data to investigate how students interact with LMS under conditions of infrastructural limitation, sociocultural complexity, and digital vulnerability. The findings reveal that students’ acceptance and sustained use of LMS platforms are closely tied to performance expectancy, facilitating conditions, and perceived ease of use, in line with UTAUT. However, cultural relevance, language barriers, and distrust in how personal data are used significantly moderate these perceptions. From a TPACK perspective, students are more engaged when content is designed in a way that aligns with their lived experiences and is delivered through culturally responsive digital pedagogy. Ethical concerns, particularly regarding algorithmic profiling and lack of transparency in LMS analytics, further influence students’ sense of safety and inclusion. The study argues that effective LMS integration for rural students requires more than just functional accessit demands ethical sensitivity, cultural alignment, and pedagogical intentionality. By situating student experiences within TPACK and UTAUT frameworks, the research offers a holistic understanding of LMS adoption, calling for institutional strategies that honour student agency, protect digital rights, and support equitable participation in the digital learning ecosystem."
"Artifical Intelligence and Bias: Challenges, Implications, and Remedies",2023,A. Min,Journal Of Social Research,34,W4387369817,10.55324/josr.v2i11.1477,https://openalex.org/W4387369817,https://ijsr.internationaljournallabs.com/index.php/ijsr/article/download/1477/976,Multidisciplinary approach; Dignity; Engineering ethics; Construct (python library); Human rights,article,True,"This paper investigates the multifaceted issue of algorithmic bias in artificial intelligence (AI) systems and explores its ethical and human rights implications. The study encompasses a comprehensive analysis of AI bias, its causes, and potential remedies, with a particular focus on its impact on individuals and marginalized communities. The primary objectives of this research are to examine the concept of algorithmic bias, assess its ethical and human rights implications, identify its causes and mechanisms, evaluate its societal impact, explore mitigation strategies, and examine regulatory and community-driven approaches to address this critical issue. The research employs a multidisciplinary approach, drawing from literature reviews, case studies, and ethical analyses. It synthesizes insights from academic papers, governmental reports, and industry guidelines to construct a comprehensive overview of algorithmic bias and its ramifications. This research paper underscores the urgency of addressing algorithmic bias, as it raises profound ethical and human rights concerns. It advocates for comprehensive approaches, spanning technical, ethical, regulatory, and community-driven dimensions, to ensure that AI technologies respect the rights and dignity of individuals and communities in our increasingly AI-driven world."
"Use of Evolutionary Optimization Algorithms for the Design and Analysis of Low Bias, Low Phase Noise Photodetectors",2023,Ishraq Md Anjum; Ergün Şimşek; Seyed Ehsan Jamali Mahabadi; Thomas F. Carruthers; Curtis R. Menyuk; Joe C. Campbell; D.A. Tulchinsky; Keith J. Williams,Journal of Lightwave Technology,11,W4388283811,10.1109/jlt.2023.3330099,https://openalex.org/W4388283811,http://hdl.handle.net/11603/30864,Particle swarm optimization; Computer science; Phase noise; Noise (video); Photonics,article,True,"With the rapid advance of machine learning techniques and the increased availability of high-speed computing resources, it has become possible to exploit machine-learning technologies to aid in the design of photonic devices. In this work we use evolutionary optimization algorithms, machine learning techniques, and the drift-diffusion equations to optimize a modified uni- traveling-carrier (MUTC) photodetector for low phase noise at a relatively low bias of 5 V. We compare the particle swarm optimization (PSO), genetic, and surrogate optimization algorithms. We find that PSO yields the solution with the lowest phase noise, with an improvement over a current design of 4.4 dBc/Hz. We then analyze the machine-optimized design to understand the physics behind the phase noise reduction and show that the optimized design removes electrical bottlenecks in the current design."
Bias in AI-based models for medical applications: challenges and mitigation strategies,2023,Mirja Mittermaier; Marium Raza; Joseph C. Kvedar,npj Digital Medicine,256,W4380538374,10.1038/s41746-023-00858-z,https://openalex.org/W4380538374,https://www.nature.com/articles/s41746-023-00858-z.pdf,Disadvantaged; Socioeconomic status; Health care; Sexual orientation; Prejudice (legal term),editorial,True,
30.3 A Bias-Flip Rectifier with a Duty-Cycle-Based MPPT Algorithm for Piezoelectric Energy Harvesting with 98% Peak MPPT Efficiency and 738% Energy-Extraction Enhancement,2023,Xinling Yue; Sundeep Javvaji; Zhong Tang; Kofi A. A. Makinwa; Sijun Du,,18,W4360605772,10.1109/isscc42615.2023.10067284,https://openalex.org/W4360605772,https://repository.tudelft.nl/file/File_6572afc9-c8e5-4bf0-bbda-620b056e5b96,Maximum power point tracking; Rectifier (neural networks); Energy harvesting; Maximum power principle; Topology (electrical circuits),article,True,"&lt;p&gt;Synchronized bias-flip rectifiers, such as synchronized switch harvesting on inductor (SSHI) rectifiers, are widely used for piezoelectric energy harvesting (PEH) [1], which can replace the use of batteries in many loT applications, thus reducing both system volume and maintenance cost. However, the output power extracted by such rectifiers strongly depends on the impedance matching between the piezoelectric transducer (PT) and the circuit. To maximize this, two maximum power point tracking (MPPT) algorithms are often used. As shown in Fig. 30.3.1 (left), the Perturb &amp;amp; Observe (P&amp;amp;O) (a.k.a. hill-climbing) algorithm adjusts the rectified output power in a stepwise manner towards the maximum power point (MPP), thus establishing robust and continuous MPPT. However, accurately sensing the rectified output power often requires complex and power-hungry hardware [1], [2]. Another simpler algorithm is based on the fractional open-circuit voltage (FOCV) and involves periodically measuring the PT's open-circuit voltage amplitude (VOC) and regulating the rectified voltage (VREC) to a level (VMPP), which corresponds to the MPP [3-6]. However, the PT must be periodically disconnected from the rectifier to measure VOC, resulting in wasted energy, while the inherent delay in sensing VOC variations reduces the overall tracking efficiency. Furthermore, a calibration step is usually necessary to determine VMPP, since this depends on the actual PT voltage flip efficiency (etaF) of the bias-flip rectifier.&lt;/p&gt;"
A biased random-key genetic algorithm for the two-level hub location routing problem with directed tours,2023,Caio César De Freitas; Dário José Aloise; Fábio Francisco da Costa Fontes; Andréa Cynthia Santos; Matheus da Silva Menezes,OR Spectrum,13,W4366420580,10.1007/s00291-023-00718-y,https://openalex.org/W4366420580,,Metaheuristic; Computer science; Key (lock); Genetic algorithm; Routing (electronic design automation),article,False,
<scp>SeroTracker‐RoB</scp> : A decision rule‐based algorithm for reproducible risk of bias assessment of seroprevalence studies,2023,Niklas Bobrovitz; Kim C. Noël; Zihan Li; Christian Cao; Gabriel Deveaux; Anabel Selemon; David A. Clifton; Mercedes Yanes‐Lane; Tingting Yan; Rahul K. Arora,Research Synthesis Methods,11,W4315754436,10.1002/jrsm.1620,https://openalex.org/W4315754436,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/jrsm.1620,Reliability (semiconductor); Algorithm; Computer science; Checklist; Intraclass correlation,article,True,"Abstract Risk of bias (RoB) assessments are a core element of evidence synthesis but can be time consuming and subjective. We aimed to develop a decision rule‐based algorithm for RoB assessment of seroprevalence studies. We developed the SeroTracker‐RoB algorithm. The algorithm derives seven objective and two subjective critical appraisal items from the Joanna Briggs Institute Critical Appraisal Checklist for Prevalence studies and implements decision rules that determine study risk of bias based on the items. Decision rules were validated using the SeroTracker seroprevalence study database, which included non‐algorithmic RoB judgments from two reviewers. We quantified efficiency as the mean difference in time for the algorithmic and non‐algorithmic assessments of 80 randomly selected articles, coverage as the proportion of studies where the decision rules yielded an assessment, and reliability using intraclass correlations comparing algorithmic and non‐algorithmic assessments for 2070 articles. A set of decision rules with 61 branches was developed using responses to the nine critical appraisal items. The algorithmic approach was faster than non‐algorithmic assessment (mean reduction 2.32 min [SD 1.09] per article), classified 100% ( n = 2070) of studies, and had good reliability compared to non‐algorithmic assessment (ICC 0.77, 95% CI 0.74–0.80). We built the SeroTracker‐RoB Excel Tool, which embeds this algorithm for use by other researchers. The SeroTracker‐RoB decision‐rule based algorithm was faster than non‐algorithmic assessment with complete coverage and good reliability. This algorithm enabled rapid, transparent, and reproducible RoB evaluations of seroprevalence studies and may support evidence synthesis efforts during future disease outbreaks. This decision rule‐based approach could be applied to other types of prevalence studies."
"Simplicity bias, algorithmic probability, and the random logistic map",2024,Boumediene Hamzi; Kamaludin Dingle,Physica D Nonlinear Phenomena,6,W4394760427,10.1016/j.physd.2024.134160,https://openalex.org/W4394760427,https://doi.org/10.1016/j.physd.2024.134160,Simplicity; Noise (video); Mathematics; Logistic map; Algorithm,article,True,"Simplicity bias is an intriguing phenomenon prevalent in various input–output maps, characterized by a preference for simpler, more regular, or symmetric outputs. Notably, these maps typically feature high-probability outputs with simple patterns, whereas complex patterns are exponentially less probable. This bias has been extensively examined and attributed to principles derived from algorithmic information theory and algorithmic probability. In a significant advancement, it has been demonstrated that the renowned logistic map xk+1=μxk(1−xk), a staple in dynamical systems theory, and other one-dimensional maps exhibit simplicity bias when conceptualized as input–output systems. Building upon this work, our research delves into the manifestations of simplicity bias within the random logistic map, specifically focusing on scenarios involving additive noise. This investigation is driven by the overarching goal of formulating a comprehensive theory for the prediction and analysis of time series. Our primary contributions are multifaceted. We discover that simplicity bias is observable in the random logistic map for specific ranges of μ and noise magnitudes. Additionally, we find that this bias persists even with the introduction of small measurement noise, though it diminishes as noise levels increase. Our studies also revisit the phenomenon of noise-induced chaos, particularly when μ=3.83, revealing its characteristics through complexity-probability plots. Intriguingly, we employ the logistic map to illustrate a paradoxical aspect of data analysis: more data adhering to a consistent trend can occasionally lead to reduced confidence in extrapolation predictions, challenging conventional wisdom. We propose that adopting a probability-complexity perspective in analyzing dynamical systems could significantly enrich statistical learning theories related to series prediction and analysis. This approach not only facilitates a deeper understanding of simplicity bias and its implications but also paves the way for novel methodologies in forecasting complex systems behavior, especially in scenarios dominated by uncertainty and stochasticity."
REVOLUTIONIZING EDUCATION THROUGH AI: A COMPREHENSIVE REVIEW OF ENHANCING LEARNING EXPERIENCES,2024,Oseremi Onesi-Ozigagun; Yinka James Ololade; Nsisong Louis Eyo-Udo; Damilola Oluwaseun Ogundipe,International Journal of Applied Research in Social Sciences,184,W4394688113,10.51594/ijarss.v6i4.1011,https://openalex.org/W4394688113,https://fepbl.com/index.php/ijarss/article/download/1011/1233,Engineering ethics; Psychology; Engineering,review,True,"Artificial Intelligence (AI) is transforming the landscape of education, offering innovative solutions to enhance learning experiences. This review provides a comprehensive overview of how AI is revolutionizing education, focusing on its impact on learning outcomes, teaching methodologies, and the overall educational ecosystem. The adoption of AI in education has led to personalized learning experiences tailored to individual student needs. AI-powered adaptive learning systems analyze student performance data to create customized learning paths, ensuring that students receive content at their pace and level of understanding. This personalized approach improves student engagement and academic performance. AI is also reshaping teaching methodologies, providing educators with tools to streamline administrative tasks and enhance instructional strategies. AI-powered tools can automate grading, create interactive lessons, and provide real-time feedback to students. This allows teachers to focus more on facilitating learning and developing critical thinking skills in students. Furthermore, AI is revolutionizing the assessment process, moving beyond traditional exams to more dynamic and insightful evaluation methods. AI-powered assessment tools can analyze student responses in real-time, providing immediate feedback and insights into student comprehension and learning progress. The integration of AI in education also extends to administrative functions, such as student enrollment, scheduling, and resource allocation. AI-powered systems can optimize these processes, leading to more efficient and effective management of educational institutions. Despite the numerous benefits of AI in education, challenges remain, including concerns about data privacy, algorithmic bias, and the need for teacher training. Addressing these challenges will be crucial to maximizing the potential of AI in education and ensuring equitable access to quality education for all. In conclusion, AI is revolutionizing education by enhancing learning experiences, transforming teaching methodologies, and optimizing administrative processes. As AI continues to evolve, its impact on education is expected to grow, offering new opportunities to improve learning outcomes and prepare students for success in the digital age.&#x0D; Keywords: Revolutionizing, AI, Enhancing, Learning, Experiences."
Bias correction of <scp>CMIP6</scp> simulations of precipitation over Indian monsoon core region using deep learning algorithms,2023,T. Kesavavarthini; A. Naga Rajesh; C. V. Srinivas; T. V. Lakshmi Kumar,International Journal of Climatology,13,W4322721694,10.1002/joc.8056,https://openalex.org/W4322721694,,Climatology; Indian Ocean Dipole; Coupled model intercomparison project; Monsoon; Precipitation,article,False,"Abstract General Circulation Models or Global Climate Models (GCMs) output consists of inevitable bias due to insufficient knowledge about parameterization schemes and other mathematical computations that involve thermodynamical and physical laws while designing climate models. Indian summer monsoon (southwest monsoon) accounts for 75%–90% of the annual rainfall over most climatic zones of India during the months, June, July, August, and September, which has a direct impact on the agricultural economy of India. The aim of this study is to bias correct the Coupled Model Intercomparison Project Phase – 6 (CMIP6) GCMs' precipitation data for the historical period from 1985 to 2014 and two Shared Socioeconomic Pathways (SSP) SSP1‐2.6 and SSP5‐8.5, from the period 2015 to 2100, with reference to the India Meteorological Department (IMD) observed rainfall gridded dataset. The datasets used are for the rain‐bearing Indian southwest monsoon season from the months, June to September. Monsoon Core Region is selected to carry out the bias correction using a couple of deep learning algorithms, namely one‐dimensional Convolutional Neural Network (CNN1D) and Long Short‐Term Memory Encoder‐Decoder (LSTM‐ED) Neural Network. The performance of both algorithms is evaluated with metrics. The LSTM‐ED algorithm yielded better results with least error output. The bias‐corrected data obtained using the LSTM‐ED algorithm is then compared with IMD observed rainfall data for the climatic events such as ENSO (El Niño and La Niña) and Positive and Negative IOD (Indian Ocean Dipole)."
Combining Transition Path Sampling with Data-Driven Collective Variables through a Reactivity-Biased Shooting Algorithm,2024,Jintu Zhang; Odin Zhang; Luigi Bonati; Tingjun Hou,Journal of Chemical Theory and Computation,12,W4399038084,10.1021/acs.jctc.4c00423,https://openalex.org/W4399038084,,Computer science; Path (computing); Algorithm; Sampling (signal processing); Transition (genetics),article,False,"Rare event sampling is a central problem in modern computational chemistry research. Among the existing methods, transition path sampling (TPS) can generate unbiased representations of reaction processes. However, its efficiency depends on the ability to generate reactive trial paths, which in turn depends on the quality of the shooting algorithm used. We propose a new algorithm based on the shooting success rate, i.e., reactivity, measured as a function of a reduced set of collective variables (CVs). These variables are extracted with a machine learning approach directly from TPS simulations, using a multitask objective function. Iteratively, this workflow significantly improves the shooting efficiency without any prior knowledge of the process. In addition, the optimized CVs can be used with biased enhanced sampling methodologies to accurately reconstruct the free energy profiles. We tested the method on three different systems: a two-dimensional toy model, conformational transitions of alanine dipeptide, and hydrolysis of acetyl chloride in bulk water. In the latter, we integrated our workflow with an active learning scheme to learn a reactive machine learning-based potential, which allowed us to study the mechanism and free energy profile with an ab initio-like accuracy."
FedUB: Federated Learning Algorithm Based on Update Bias,2024,Hesheng Zhang; Ping Zhang; M. Hu; Muhua Liu; Jiechang Wang,Mathematics,6,W4398145625,10.3390/math12101601,https://openalex.org/W4398145625,https://www.mdpi.com/2227-7390/12/10/1601/pdf?version=1716209561,Computer science; Artificial intelligence; Machine learning; Algorithm,article,True,"Federated learning, as a distributed machine learning framework, aims to protect data privacy while addressing the issue of data silos by collaboratively training models across multiple clients. However, a significant challenge to federated learning arises from the non-independent and identically distributed (non-iid) nature of data across different clients. non-iid data can lead to inconsistencies between the minimal loss experienced by individual clients and the global loss observed after the central server aggregates the local models, affecting the model’s convergence speed and generalization capability. To address this challenge, we propose a novel federated learning algorithm based on update bias (FedUB). Unlike traditional federated learning approaches such as FedAvg and FedProx, which independently update model parameters on each client before direct aggregation to form a global model, the FedUB algorithm incorporates an update bias in the loss function of local models—specifically, the difference between each round’s local model updates and the global model updates. This design aims to reduce discrepancies between local and global updates, thus aligning the parameters of locally updated models more closely with those of the globally aggregated model, thereby mitigating the fundamental conflict between local and global optima. Additionally, during the aggregation phase at the server side, we introduce a metric called the bias metric, which assesses the similarity between each client’s local model and the global model. This metric adaptively sets the weight of each client during aggregation after each training round to achieve a better global model. Extensive experiments conducted on multiple datasets have confirmed the effectiveness of the FedUB algorithm. The results indicate that FedUB generally outperforms methods such as FedDC, FedDyn, and Scaffold, especially in scenarios involving partial client participation and non-iid data distributions. It demonstrates superior performance and faster convergence in tasks such as image classification."
The ethical implications of using generative chatbots in higher education,2024,Ryan Williams,Frontiers in Education,97,W4390665705,10.3389/feduc.2023.1331607,https://openalex.org/W4390665705,https://www.frontiersin.org/articles/10.3389/feduc.2023.1331607/pdf?isPublishedV2=False,Autonomy; Engineering ethics; Generative grammar; Ethical issues; Psychology,article,True,"Incorporating artificial intelligence (AI) into education, specifically through generative chatbots, can transform teaching and learning for education professionals in both administrative and pedagogical ways. However, the ethical implications of using generative chatbots in education must be carefully considered. Ethical concerns about advanced chatbots have yet to be explored in the education sector. This short article introduces the ethical concerns associated with introducing platforms such as ChatGPT in education. The article outlines how handling sensitive student data by chatbots presents significant privacy challenges, thus requiring adherence to data protection regulations, which may not always be possible. It highlights the risk of algorithmic bias in chatbots, which could perpetuate societal biases, which can be problematic. The article also examines the balance between fostering student autonomy in learning and the potential impact on academic self-efficacy, noting the risk of over-reliance on AI for educational purposes. Plagiarism continues to emerge as a critical ethical concern, with AI-generated content threatening academic integrity. The article advocates for comprehensive measures to address these ethical issues, including clear policies, advanced plagiarism detection techniques, and innovative assessment methods. By addressing these ethical challenges, the article argues that educators, AI developers, policymakers, and students can fully harness the potential of chatbots in education, creating a more inclusive, empowering, and ethically sound educational future."
BC-PINN: an adaptive physics informed neural network based on biased multiobjective coevolutionary algorithm,2023,Zhicheng Zhu; Hao Jia; Jin Huang; Baoling Huang,Neural Computing and Applications,10,W4385495295,10.1007/s00521-023-08876-4,https://openalex.org/W4385495295,,Penalty method; Mathematical optimization; Artificial neural network; Computer science; Benchmark (surveying),article,False,
Ethical Considerations in the Use of Artificial Intelligence and Machine Learning in Health Care: A Comprehensive Review,2024,Mitul Harishbhai Tilala; Pradeep Kumar Chenchala; Ashok Choppadandi; Jagbir Kaur; Savitha Naguri; Rahul Saoji; Bhanu Devaguptapu,Cureus,125,W4399698937,10.7759/cureus.62443,https://openalex.org/W4399698937,https://assets.cureus.com/uploads/review_article/pdf/259236/20240615-15665-hz42ob.pdf,Artificial intelligence; Health care; Computer science; Engineering ethics; Psychology,review,True,"Artificial intelligence (AI) and machine learning (ML) technologies are revolutionizing health care by offering unprecedented opportunities to enhance patient care, optimize clinical workflows, and advance medical research. However, the integration of AI and ML into healthcare systems raises significant ethical considerations that must be carefully addressed to ensure responsible and equitable deployment. This comprehensive review explored the multifaceted ethical considerations surrounding the use of AI and ML in health care, including privacy and data security, algorithmic bias, transparency, clinical validation, and professional responsibility. By critically examining these ethical dimensions, stakeholders can navigate the ethical complexities of AI and ML integration in health care, while safeguarding patient welfare and upholding ethical principles. By embracing ethical best practices and fostering collaboration across interdisciplinary teams, the healthcare community can harness the full potential of AI and ML technologies to usher in a new era of personalized data-driven health care that prioritizes patient well-being and equity."
Exploring Gender Bias and Algorithm Transparency: Ethical Considerations of AI in HRM,2024,Jiaxing Du,Journal of Theory and Practice of Management Science,6,W4393522655,10.53469/jtpms.2024.04(03).06,https://openalex.org/W4393522655,https://centuryscipub.com/index.php/JTPMS/article/download/539/461,Transparency (behavior); Computer science; Psychology; Algorithm; Computer security,article,True,"Opportunities and challenges are introduced by the integration of Artificial Intelligence (AI) into Human Resource Management (HRM). The paragraph discusses the ethical implications of AI applications in HRM, focusing on gender bias and algorithm transparency. It explores how AI-driven decision-making in HRM perpetuates gender bias, the importance of transparent algorithms for trust and accountability, and the role of regulatory frameworks in safeguarding ethical standards. The paper aims to provide a comprehensive analysis of the ethical landscape of AI in HRM and offers policy recommendations to mitigate bias and enhance transparency."
Biased Random-Key Genetic Algorithm with Local Search Applied to the Maximum Diversity Problem,2023,Geiza Silva; André Ferreira Leite; Raydonal Ospina; Víctor Leiva; Jorge Figueroa-Zúñiga; Cecília Castro,Mathematics,8,W4384202382,10.3390/math11143072,https://openalex.org/W4384202382,https://www.mdpi.com/2227-7390/11/14/3072/pdf?version=1689151046,Key (lock); Computer science; Set (abstract data type); Genetic algorithm; Mathematical optimization,article,True,"The maximum diversity problem (MDP) aims to select a subset with a predetermined number of elements from a given set, maximizing the diversity among them. This NP-hard problem requires efficient algorithms that can generate high-quality solutions within reasonable computational time. In this study, we propose a novel approach that combines the biased random-key genetic algorithm (BRKGA) with local search to tackle the MDP. Our computational study utilizes a comprehensive set of MDPLib instances, and demonstrates the superior average performance of our proposed algorithm compared to existing literature results. The MDP has a wide range of practical applications, including biology, ecology, and management. We provide future research directions for improving the algorithm’s performance and exploring its applicability in real-world scenarios."
Trust and reliance on AI — An experimental study on the extent and costs of overreliance on AI,2024,Artur Klingbeil; Cassandra Grützner; Philipp Schreck,Computers in Human Behavior,116,W4399992361,10.1016/j.chb.2024.108352,https://openalex.org/W4399992361,https://doi.org/10.1016/j.chb.2024.108352,Psychology; Social psychology; Epistemology; Positive economics; Sociology,article,True,"Decision-making is undergoing rapid changes due to the introduction of artificial intelligence (AI), as AI recommender systems can help mitigate human flaws and increase decision accuracy and efficiency. However, AI can also commit errors or suffer from algorithmic bias. Hence, blind trust in technologies carries risks, as users may follow detrimental advice resulting in undesired consequences. Building upon research on algorithm appreciation and trust in AI, the current study investigates whether users who receive AI advice in an uncertain situation overrely on this advice — to their own detriment and that of other parties. In a domain-independent, incentivized, and interactive behavioral experiment, we find that the mere knowledge of advice being generated by an AI causes people to overrely on it, that is, to follow AI advice even when it contradicts available contextual information as well as their own assessment. Frequently, this overreliance leads not only to inefficient outcomes for the advisee, but also to undesired effects regarding third parties. The results call into question how AI is being used in assisted decision making, emphasizing the importance of AI literacy and effective trust calibration for productive deployment of such systems."
"Opaque algorithms, transparent biases: Automated content moderation during the Sheikh Jarrah Crisis",2024,Norah Abokhodair; Yarden Skop; Sarah Rüller; Konstantin Aal; Houda Elmimouni,First Monday,13,W4394806508,10.5210/fm.v29i4.13620,https://openalex.org/W4394806508,https://firstmonday.org/ojs/index.php/fm/article/download/13620/11609,Content (measure theory); Moderation; Opacity; Computer science; Algorithm,article,True,"Social media platforms, while influential tools for human rights activism, free speech, and mobilization, also bear the influence of corporate ownership and commercial interests. This dual character can lead to clashing interests in the operations of these platforms. This study centers on the May 2021 Sheikh Jarrah events in East Jerusalem, a focal point in the Israeli-Palestinian conflict that garnered global attention. During this period, Palestinian activists and their allies observed and encountered a notable increase in automated content moderation actions, like shadow banning and content removal. We surveyed 201 users who faced content moderation and conducted 12 interviews with political influencers to assess the impact of these practices on activism. Our analysis centers on automated content moderation and transparency, investigating how users and activists perceive the content moderation systems employed by social media platforms, and their opacity. Findings reveal perceived censorship by pro-Palestinian activists due to opaque and obfuscated technological mechanisms of content demotion, complicating harm substantiation and lack of redress mechanisms. We view this difficulty as part of algorithmic harms, in the realm of automated content moderation. This dynamic has far-reaching implications for activism’s future and it raises questions about power centralization in digital spaces."
Human intelligence and artificial intelligence and the challenges of biases in ai algorithms,2024,Erika Ribeiro Fernandes; Marcelo Augusto Vieira Graglia,Journal on Innovation and Sustainability RISUS,3,W4398186076,10.23925/2179-3565.2023v15i1p133-142,https://openalex.org/W4398186076,https://revistas.pucsp.br/index.php/risus/article/download/66296/44877,Human intelligence; Artificial intelligence; Computer science; Cognitive science; Machine learning,article,True,"This article acknowledges the profound transformations that Artificial Intelligence imposes on society. A descriptive-exploratory study aims to discuss algorithmic biases and understand their impacts on society. The article starts from the understanding of human intelligence and learning from a pluralistic perspective, based on the analysis of literary works and scientific articles. This approach provides a context in which AI and machine learning can be conceived from an innovation perspective for the common good. The critical analysis emphasizes the need for ethical approaches in the development of these systems. The topics discussed highlight the importance of a multidimensional approach in mitigating algorithmic biases. From data selection to audits and accountability, diversity of perspectives, both in datasets and development teams, is crucial. The implementation of continuous training and human supervision reflects a continuous commitment to transparency and fairness in artificial intelligence. These integrated strategies are essential for the ethical, transparent, and equitable development of AI. This holistic approach, involving diverse skills and people, continuous training, and vigilant oversight, is vital to ensure the ethical use of AI for the collective well-being."
A Conceptual Model for Inclusive Technology: Advancing Disability Inclusion through Artificial Intelligence,2024,Maram Fahaad Almufareh; Sumaira Kausar; Mamoona Humayun; Samabia Tehsin,Journal of Disability Research,94,W4390584313,10.57197/jdr-2023-0060,https://openalex.org/W4390584313,https://www.scienceopen.com/document_file/10696e8f-ab40-4c4f-be70-09fda6533ae8/ScienceOpen/jdr20230060.pdf,Inclusion (mineral); Safeguarding; Transformative learning; Realm; Computer science,article,True,"Artificial intelligence (AI) has ushered in transformative changes, championing inclusion and accessibility for individuals with disabilities. This article delves into the remarkable AI-driven solutions that have revolutionized their lives across various domains. From assistive technologies such as voice recognition and AI-powered smart glasses catering to diverse needs, to healthcare benefiting from early disease detection algorithms and wearable devices that monitor vital signs and alert caregivers in emergencies, AI has steered in significant enhancements. Moreover, AI-driven prosthetics and exoskeletons have substantially improved mobility for those with limb impairments. The realm of education has not been left untouched, with AI tools creating inclusive learning environments that adapt to individual learning styles, paving the way for academic success among students with disabilities. However, the boundless potential of AI also presents ethical concerns and challenges. Issues like safeguarding data privacy, mitigating algorithmic bias, and bridging the digital divide must be thoughtfully addressed to fully harness AI’s potential in empowering individuals with disabilities. To complement these achievements, a robust conceptual model for AI disability inclusion serves as the theoretical framework, guiding the development of tailored AI solutions. By striking a harmonious balance between innovation and ethics, AI has the power to significantly enhance the overall quality of life for individuals with disabilities across a spectrum of vital areas."
AdaGC: A Novel Adaptive Optimization Algorithm with Gradient Bias Correction,2024,Qi Wang; Feng Su; Shipeng Dai; Xiaojun Lu; Yang Liu,Expert Systems with Applications,6,W4401261565,10.1016/j.eswa.2024.124956,https://openalex.org/W4401261565,,Convergence (economics); Moment (physics); Algorithm; Rate of convergence; Mathematical optimization,article,False,
"Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods",2023,Tiago Palma Pagano; Rafael B. Loureiro; Fernanda V. N. Lisboa; Rodrigo M. Peixoto; Guilherme A. de Sousa Guimarães; Gustavo O. R. Cruz; Maira M. Araujo; Lucas Lisboa dos Santos; Marco A S Cruz; Ewerton L. S. Oliveira; Ingrid Winkler; Erick Giovani Sperandio Nascimento,Big Data and Cognitive Computing,212,W4316038168,10.3390/bdcc7010015,https://openalex.org/W4316038168,https://www.mdpi.com/2504-2289/7/1/15/pdf?version=1674136071,Computer science; Machine learning; Identification (biology); Systematic review; Scopus,review,True,"One of the difficulties of artificial intelligence is to ensure that model decisions are fair and free of bias. In research, datasets, metrics, techniques, and tools are applied to detect and mitigate algorithmic unfairness and bias. This study examines the current knowledge on bias and unfairness in machine learning models. The systematic review followed the PRISMA guidelines and is registered on OSF plataform. The search was carried out between 2021 and early 2022 in the Scopus, IEEE Xplore, Web of Science, and Google Scholar knowledge bases and found 128 articles published between 2017 and 2022, of which 45 were chosen based on search string optimization and inclusion and exclusion criteria. We discovered that the majority of retrieved works focus on bias and unfairness identification and mitigation techniques, offering tools, statistical approaches, important metrics, and datasets typically used for bias experiments. In terms of the primary forms of bias, data, algorithm, and user interaction were addressed in connection to the preprocessing, in-processing, and postprocessing mitigation methods. The use of Equalized Odds, Opportunity Equality, and Demographic Parity as primary fairness metrics emphasizes the crucial role of sensitive attributes in mitigating bias. The 25 datasets chosen span a wide range of areas, including criminal justice image enhancement, finance, education, product pricing, and health, with the majority including sensitive attributes. In terms of tools, Aequitas is the most often referenced, yet many of the tools were not employed in empirical experiments. A limitation of current research is the lack of multiclass and multimetric studies, which are found in just a few works and constrain the investigation to binary-focused method. Furthermore, the results indicate that different fairness metrics do not present uniform results for a given use case, and that more research with varied model architectures is necessary to standardize which ones are more appropriate for a given context. We also observed that all research addressed the transparency of the algorithm, or its capacity to explain how decisions are taken."
A biased random-key genetic algorithm for the minimum quasi-clique partitioning problem,2023,Rafael A. Melo; Celso C. Ribeiro; José A. Riveaux,Annals of Operations Research,8,W4387140368,10.1007/s10479-023-05609-7,https://openalex.org/W4387140368,,Theory of computation; Mathematics; Clique problem; Algorithm; Random graph,article,False,
Bias-compensated based diffusion affine projection like maximum correntropy algorithm,2024,Chengjin Li; Haiquan Zhao; Xiang Wang,Digital Signal Processing,6,W4401043211,10.1016/j.dsp.2024.104702,https://openalex.org/W4401043211,,Algorithm; Affine transformation; Computer science; Diffusion; Projection (relational algebra),article,False,
A causal perspective on dataset bias in machine learning for medical imaging,2024,Charles Jones; Daniel C. Castro; Fabio De Sousa Ribeiro; Ozan Oktay; Melissa D. McCradden; Ben Glocker,Nature Machine Intelligence,37,W4391843481,10.1038/s42256-024-00797-8,https://openalex.org/W4391843481,,Perspective (graphical); Computer science; Artificial intelligence; Machine learning; Data science,article,False,
"Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare",2023,Eran Tal,,8,W4385644241,10.1145/3600211.3604678,https://openalex.org/W4385644241,https://arxiv.org/pdf/2308.02081,Counterfactual thinking; Computer science; Health care; Fairness measure; Econometrics,preprint,True,"Bias in applications of machine learning (ML) to healthcare is usually\nattributed to unrepresentative or incomplete data, or to underlying health\ndisparities. This article identifies a more pervasive source of bias that\naffects the clinical utility of ML-enabled prediction tools: target\nspecification bias. Target specification bias arises when the\noperationalization of the target variable does not match its definition by\ndecision makers. The mismatch is often subtle, and stems from the fact that\ndecision makers are typically interested in predicting the outcomes of\ncounterfactual, rather than actual, healthcare scenarios. Target specification\nbias persists independently of data limitations and health disparities. When\nleft uncorrected, it gives rise to an overestimation of predictive accuracy, to\ninefficient utilization of medical resources, and to suboptimal decisions that\ncan harm patients. Recent work in metrology - the science of measurement -\nsuggests ways of counteracting target specification bias and avoiding its\nharmful consequences.\n"
Bias in Machine Learning: A Literature Review,2024,Konstantinos Mavrogiorgos; Athanasios Kiourtis; Argyro Mavrogiorgou; Andreas Menychtas; Dimosthenis Kyriazis,Applied Sciences,37,W4403074418,10.3390/app14198860,https://openalex.org/W4403074418,https://doi.org/10.3390/app14198860,Computer science; Psychology,review,True,"Bias could be defined as the tendency to be in favor or against a person or a group, thus promoting unfairness. In computer science, bias is called algorithmic or artificial intelligence (i.e., AI) and can be described as the tendency to showcase recurrent errors in a computer system, which result in “unfair” outcomes. Bias in the “outside world” and algorithmic bias are interconnected since many types of algorithmic bias originate from external factors. The enormous variety of different types of AI biases that have been identified in diverse domains highlights the need for classifying the said types of AI bias and providing a detailed overview of ways to identify and mitigate them. The different types of algorithmic bias that exist could be divided into categories based on the origin of the bias, since bias can occur during the different stages of the Machine Learning (i.e., ML) lifecycle. This manuscript is a literature study that provides a detailed survey regarding the different categories of bias and the corresponding approaches that have been proposed to identify and mitigate them. This study not only provides ready-to-use algorithms for identifying and mitigating bias, but also enhances the empirical knowledge of ML engineers to identify bias based on the similarity that their use cases have to other approaches that are presented in this manuscript. Based on the findings of this study, it is observed that some types of AI bias are better covered in the literature, both in terms of identification and mitigation, whilst others need to be studied more. The overall contribution of this research work is to provide a useful guideline for the identification and mitigation of bias that can be utilized by ML engineers and everyone who is interested in developing, evaluating and/or utilizing ML models."
An Improved Goal-bias RRT algorithm for Unmanned Aerial Vehicle Path Planning,2024,Hongyu Zhang; Xiaomei Xie; Mingzhu Wei; Xinhua Wang; Dongjing Song; Jin Luo,,10,W4401687143,10.1109/icma61710.2024.10633102,https://openalex.org/W4401687143,,Motion planning; Computer science; Path (computing); Artificial intelligence; Algorithm,article,False,
Heuristics and biases in human–algorithm interaction and hotel revenue management override decision-making,2024,Ibrahim Mohammed; Basak Denizci Guillet,International Journal of Contemporary Hospitality Management,7,W4401924202,10.1108/ijchm-02-2024-0288,https://openalex.org/W4401924202,https://hdl.handle.net/10072/433040,Heuristics; Revenue management; Revenue; Computer science; Yield management,article,True,"Purpose This study aims to provide insights into human–algorithm interaction in revenue management (RM) decision-making and to uncover the underlying heuristics and biases of overriding systems’ recommendations. Design/methodology/approach Following constructivist traditions, 20 in-depth interviews were conducted with revenue optimisers, analysts, managers and directors with vast experience in over 25 markets and working with different RM systems (RMSs) at the property and corporate levels. The hermeneutics approach was used to interpret and make meaning of the participants’ lived experiences and interactions with RMSs. Findings The findings explain the nature of the interaction between RM professionals and RMSs, the cognitive mechanism by which the system users judgementally adjust or override its recommendations and the heuristics and biases behind override decisions. Additionally, the findings reveal the individual decision-maker characteristics and organisational factors influencing human–algorithm interactions. Research limitations/implications Although the study focused on human–system interaction in hotel RM, it has larger implications for integrating human judgement into computerised systems for optimal decision-making. Practical implications The study findings expose human biases in working with RMSs and highlight the influencing factors that can be addressed to achieve effective human–algorithm interactions. Originality/value The study offers a holistic framework underpinned by the organisational role and expectation confirmation theories to explain the cognitive mechanisms of human–system interaction in managerial decision-making."
IARC-NCI workshop on an epidemiological toolkit to assess biases in human cancer studies for hazard identification: beyond the algorithm,2023,Mary K. Schubauer‐Berigan; David B. Richardson; Matthew P. Fox; Lin Fritschi; Irina Guseva Canu; Neil Pearce; Leslie Stayner; Amy Berrington de González,Occupational and Environmental Medicine,14,W4318499601,10.1136/oemed-2022-108724,https://openalex.org/W4318499601,https://oem.bmj.com/content/oemed/80/3/119.full.pdf,Identification (biology); Epidemiology; Hazard; Cancer; Hazard analysis,editorial,True,
Quad-Rotor Unmanned Aerial Vehicle Path Planning Based on the Target Bias Extension and Dynamic Step Size RRT* Algorithm,2024,Haitao Gao; Xiaozhu Hou; Jiangpeng Xu; Banggui Guan,World Electric Vehicle Journal,9,W4390912814,10.3390/wevj15010029,https://openalex.org/W4390912814,https://www.mdpi.com/2032-6653/15/1/29/pdf?version=1705393723,Motion planning; Random tree; Algorithm; Computer science; Mathematical optimization,article,True,"For the path planning of quad-rotor UAVs, the traditional RRT* algorithm has weak exploration ability, low planning efficiency, and a poor planning effect. A TD-RRT* algorithm based on target bias expansion and dynamic step size is proposed herein. First, random-tree expansion is combined with the target bias strategy to remove the blindness of the random tree, and we assign different weights to the sampling point and the target point so that the target point can be quickly approached and the search speed can be improved. Then, the dynamic step size is introduced to speed up the search speed, effectively solving the problem of invalid expansion in the process of trajectory generation. We then adjust the step length required for the expansion tree and obstacles in real time, solve the opposition between smoothness and real time in path planning, and improve the algorithm’s search efficiency. Finally, the cubic B-spline interpolation method is used to modify the local inflection point of the path of the improved RRT* algorithm to smooth the path. The simulation results show that compared with the traditional RRT* algorithm, the number of iterations of path planning of the TD-RRT* algorithm is reduced, the travel distance from the starting position to the end position is shortened, the time consumption is reduced, the path route is smoother, and the path optimization effect is better. The TD-RRT* algorithm based on target bias expansion and dynamic step size significantly improves the planning efficiency and planning effect of quad-rotor UAVs in a three-dimensional-space environment."
De-Selection Bias Recommendation Algorithm Based on Propensity Score Estimation,2023,Teng Ma; Yu Su,Applied Sciences,5,W4383877140,10.3390/app13148038,https://openalex.org/W4383877140,https://www.mdpi.com/2076-3417/13/14/8038/pdf?version=1688970249,Propensity score matching; MovieLens; Mean squared error; Selection (genetic algorithm); Computer science,article,True,"There are various biases present in recommendation systems, and recommendation results that do not consider these biases are unfair to users, items, and platforms. To address the problem of selection bias in recommendation systems, in this study, the propensity score was utilized to mitigate this bias. A selection bias propensity score estimation method (SPE) was developed, which takes into account both user and item information. This method accurately estimates the user’s choice tendency by calculating the degree of difference between the user’s selection rate and the selected preference of the item. Subsequently, the SPE method was combined with the traditional matrix decomposition-based recommendation algorithms, such as the latent semantic model (LFM) and the bias singular value model (BiasSVD). The propensity score was then inversely weighted into the loss function, creating a recommendation model that effectively eliminated selection bias. The experiments were carried out on the public dataset MovieLens, and root mean square error (RMSE) and mean absolute error (MAE) were selected as evaluation indicators and compared with two baseline models and three models with other propensity score estimation methods. Overall, the experimental results demonstrate that the model combined with SPE achieves a minimum increase of 2.00% in RMSE and 2.97% in MAE compared to its baseline model. Moreover, in comparison to other propensity score estimation methods, the SPE method effectively eliminates selection bias in the scoring data, thereby enhancing the performance of the recommendation model."
Empowering Education through Generative AI: Innovative Instructional Strategies for Tomorrow's Learners,2023,Kadaruddin Kadaruddin,International Journal of Business Law and Education,118,W4385497035,10.56442/ijble.v4i2.215,https://openalex.org/W4385497035,https://ijble.com/index.php/journal/article/download/215/225,Generative grammar; Generative model; Computer science; Instructional design; Artificial intelligence,article,True,"As the educational landscape endures continuous change, artificial intelligence (AI) has presented unprecedented opportunities to revolutionize instructional methods. Among these cutting-edge AI technologies, Generative AI has emerged as a promising instrument with the potential to empower educators and students through innovative instructional strategies. This article aims to investigate the various applications of Generative AI in education and cast light on its role in shaping the future of education. The objectives of this study are twofold: first, to investigate the various instructional strategies that can be enhanced by employing Generative AI, and second, to assess the potential impact of these strategies on student learning outcomes. To accomplish these goals, a comprehensive literature review was conducted analyzing existing studies and applications of Generative AI in educational settings. The results and discussions emphasize the numerous educational benefits of Generative AI. Educators can personalize learning experiences, create interactive content, and facilitate adaptive assessments by leveraging the capabilities of Generative AI. This individualized strategy has the potential to boost learner engagement and knowledge retention. However, despite the numerous advantages, ethical concerns and difficulties arise. The responsible incorporation of Generative AI in education requires addressing issues such as data privacy, algorithmic bias, and the educator's role in directing AI-driven learning experiences. The research concludes by emphasizing that Generative AI holds enormous promise for empowering education and transforming instructional practices. The findings highlight the importance of ongoing collaboration between educators, policymakers, and AI developers to ensure the ethical and equitable integration of Generative AI into educational environments. By embracing the potential of Generative AI while remaining vigilant regarding its challenges, the field of education can unlock novel opportunities to nurture an inclusive, adaptive, and learner-centric pedagogical landscape for tomorrow's learners.&#x0D;"
Three Satellites Dynamic Switching Range Integrated Navigation and Positioning Algorithm with Clock Bias Cancellation and Altimeter Assistance,2023,Lvyang Ye; Ning Gao; Yikang Yang; Lingyu Deng; Hengnian Li,Aerospace,13,W4367310835,10.3390/aerospace10050411,https://openalex.org/W4367310835,https://www.mdpi.com/2226-4310/10/5/411/pdf?version=1683196602,Computer science; Altimeter; Global Positioning System; Constellation; Elevation (ballistics),article,True,"Challenging environments such as cities, canyons, and forests have become key factors affecting navigation stability. When users pass through intricate overpasses and winding road sections, due to the fluctuation of the geoid, there will be a large fluctuation problem in the elevation measurement error of the user’s receiver. In addition, even if the low Earth orbit (LEO) constellation has thousands of satellites, there will be no technical problems in regard to destroying LEO satellites with existing technology in extreme situations such as warfare and in challenging environments such as dense forests, canyons, and ravines, where three or fewer visible satellites is a foreseeable scenario. To solve the problem of providing location services in such challenging environments, first, we analyze the relationship between temperature and atmospheric pressure and altitude; and then, based on this, we propose an initialization correction method for elevation measurements. Next, based on the broadband LEO constellation, we give an integrated navigation and positioning scheme with the assistance of both a clock bias elimination system and an altimeter. Finally, the proposed scheme is simulated and verified. The experimental results show that the dynamic switching of LEO satellites, combined with the assistance of the altimeter, can effectively improve the stability and positioning accuracy of navigation and positioning and can suppress the large navigation errors caused by the long switching time without the assistance of the altimeter. This allows the switching time to be extended; thus, it can be used as a technical reference solution for integrated communication and navigation (ICN) in the future."
Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models,2024,Feng Chen; Liqin Wang; Julie Hong; Jiaqi Jiang; Li Zhou,Journal of the American Medical Informatics Association,91,W4393119757,10.1093/jamia/ocae060,https://openalex.org/W4393119757,https://www.ncbi.nlm.nih.gov/pmc/articles/11031231,Computer science; Systematic review; Data science; Artificial intelligence; Health records,review,True,"Abstract Objectives Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. Materials and Methods We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. Results Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. Discussion This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models’ practical impact and fostering ethical AI that ensures fairness and equity in healthcare."
Mitigating Representation Bias in Action Recognition: Algorithms and Benchmarks,2023,Haodong Duan; Yue Zhao; Kai Chen; Yuanjun Xiong; Dahua Lin,Lecture notes in computer science,4,W4320502918,10.1007/978-3-031-25069-9_36,https://openalex.org/W4320502918,,Debiasing; Computer science; Leverage (statistics); Machine learning; Artificial intelligence,book-chapter,False,
A Bias-Compensated NMMCC Algorithm Against Noisy Input and Non-Gaussian Interference,2023,Xiaoqiang Long; Haiquan Zhao; Xinyan Hou,IEEE Transactions on Circuits & Systems II Express Briefs,5,W4367663187,10.1109/tcsii.2023.3271634,https://openalex.org/W4367663187,,Robustness (evolution); Algorithm; Computer science; Gaussian; Gaussian function,article,False,"This brief proposes a bias-compensated normalized adaptive filtering algorithm (BC-NMMCC) under the mixture maximum correntropy (MMCC), which can increase the flexibility of the correntropy by changing the weight of the mixture parameter, as well as change the mixture kernel function to a single kernel function when the mixture parameter takes endpoint value. In order to deal with the case of noisy input and non-Gaussian noise interference, a bias-compensated term is obtained by utilizing an unbiased criterion and some assumptions to increase the robustness of the NMMCC algorithm. To further improve the performance of the proposed algorithm, a modified parameter is added to the bias-compensated term. Then, the computational complexity of the proposed algorithm is analyzed. Finally, the numerical simulation results also show that the BC-NMMCC algorithm performs better in terms of convergence speed and steady-state performance than several other existing algorithms with better robustness under different noise disturbances."
Innovation and challenges of artificial intelligence technology in personalized healthcare,2024,Yu-Hao Li; Yulin Li; Mu-Yang Wei; Guangyu Li,Scientific Reports,141,W4401642802,10.1038/s41598-024-70073-7,https://openalex.org/W4401642802,https://doi.org/10.1038/s41598-024-70073-7,Health care; Data science; Computer science; Personalized medicine; Knowledge management,review,True,"As the burgeoning field of Artificial Intelligence (AI) continues to permeate the fabric of healthcare, particularly in the realms of patient surveillance and telemedicine, a transformative era beckons. This manuscript endeavors to unravel the intricacies of recent AI advancements and their profound implications for reconceptualizing the delivery of medical care. Through the introduction of innovative instruments such as virtual assistant chatbots, wearable monitoring devices, predictive analytic models, personalized treatment regimens, and automated appointment systems, AI is not only amplifying the quality of care but also empowering patients and fostering a more interactive dynamic between the patient and the healthcare provider. Yet, this progressive infiltration of AI into the healthcare sphere grapples with a plethora of challenges hitherto unseen. The exigent issues of data security and privacy, the specter of algorithmic bias, the requisite adaptability of regulatory frameworks, and the matter of patient acceptance and trust in AI solutions demand immediate and thoughtful resolution .The importance of establishing stringent and far-reaching policies, ensuring technological impartiality, and cultivating patient confidence is paramount to ensure that AI-driven enhancements in healthcare service provision remain both ethically sound and efficient. In conclusion, we advocate for an expansion of research efforts aimed at navigating the ethical complexities inherent to a technology-evolving landscape, catalyzing policy innovation, and devising AI applications that are not only clinically effective but also earn the trust of the patient populace. By melding expertise across disciplines, we stand at the threshold of an era wherein AI's role in healthcare is both ethically unimpeachable and conducive to elevating the global health quotient."
"Artificial intelligence, ChatGPT, and other large language models for social determinants of health: Current state and future directions",2024,Jasmine Chiat Ling Ong; Jun Jie Benjamin Seng; Jeren Zheng Feng Law; Lian Leng Low; Andrea Lay‐Hoon Kwa; Kathleen M. Giacomini; Daniel Shu Wei Ting,Cell Reports Medicine,74,W4390921272,10.1016/j.xcrm.2023.101356,https://openalex.org/W4390921272,http://www.cell.com/article/S2666379123005736/pdf,Disinformation; Transformative learning; Social determinants of health; Public relations; Health care,review,True,
Unveiling the Relationship Between News Recommendation Algorithms and Media Bias: A Simulation-Based Analysis of the Evolution of Bias Prevalence,2023,Qin Ruan; Brian Mac Namee; Ruihai Dong,Lecture notes in computer science,4,W4388468543,10.1007/978-3-031-47994-6_17,https://openalex.org/W4388468543,https://doi.org/10.1007/978-3-031-47994-6_17,Computer science; Recommender system; Media bias; Algorithm; News media,book-chapter,True,"Media bias has significant negative effects, such as influencing elections and shaping people's perceptions. However, the relationship between media bias and personalised news recommendation algorithms (widely adopted by many news platforms) remains unclear. In this study, we describe a novel framework that simulates user interactions with recommendation algorithms, allowing us to explore how the degree of bias in the news articles presented to users by personalized recommendation systems changes over time. Our experiments show that leading personalized news recommendation algorithms are sensitive to media bias, causing shifts in the proportion of biased news articles they recommend over time. These findings emphasize the importance of recognizing the influence of media bias on personalized news recommendation algorithms and the need to raise user awareness about media bias to encourage more diverse and balanced news consumption. The source code is available at https://github.com/ruanqin0706/UserRecSimulation.git ."
A Systematic Review of Synthetic Data Generation Techniques Using Generative AI,2024,Mandeep Goyal; Qusay H. Mahmoud,Electronics,105,W4402230126,10.3390/electronics13173509,https://openalex.org/W4402230126,https://doi.org/10.3390/electronics13173509,Generative grammar; Computer science; Synthetic data; Artificial intelligence; Data science,review,True,"Synthetic data are increasingly being recognized for their potential to address serious real-world challenges in various domains. They provide innovative solutions to combat the data scarcity, privacy concerns, and algorithmic biases commonly used in machine learning applications. Synthetic data preserve all underlying patterns and behaviors of the original dataset while altering the actual content. The methods proposed in the literature to generate synthetic data vary from large language models (LLMs), which are pre-trained on gigantic datasets, to generative adversarial networks (GANs) and variational autoencoders (VAEs). This study provides a systematic review of the various techniques proposed in the literature that can be used to generate synthetic data to identify their limitations and suggest potential future research areas. The findings indicate that while these technologies generate synthetic data of specific data types, they still have some drawbacks, such as computational requirements, training stability, and privacy-preserving measures which limit their real-world usability. Addressing these issues will facilitate the broader adoption of synthetic data generation techniques across various disciplines, thereby advancing machine learning and data-driven solutions."
Your robot therapist is not your therapist: understanding the role of AI-powered mental health chatbots,2023,Zoha Khawaja; Jean‐Christophe Bélisle‐Pipon,Frontiers in Digital Health,106,W4388524014,10.3389/fdgth.2023.1278186,https://openalex.org/W4388524014,https://doi.org/10.3389/fdgth.2023.1278186,Chatbot; Ignorance; Mental health; Internet privacy; Psychology,article,True,"Artificial intelligence (AI)-powered chatbots have the potential to substantially increase access to affordable and effective mental health services by supplementing the work of clinicians. Their 24/7 availability and accessibility through a mobile phone allow individuals to obtain help whenever and wherever needed, overcoming financial and logistical barriers. Although psychological AI chatbots have the ability to make significant improvements in providing mental health care services, they do not come without ethical and technical challenges. Some major concerns include providing inadequate or harmful support, exploiting vulnerable populations, and potentially producing discriminatory advice due to algorithmic bias. However, it is not always obvious for users to fully understand the nature of the relationship they have with chatbots. There can be significant misunderstandings about the exact purpose of the chatbot, particularly in terms of care expectations, ability to adapt to the particularities of users and responsiveness in terms of the needs and resources/treatments that can be offered. Hence, it is imperative that users are aware of the limited therapeutic relationship they can enjoy when interacting with mental health chatbots. Ignorance or misunderstanding of such limitations or of the role of psychological AI chatbots may lead to a therapeutic misconception (TM) where the user would underestimate the restrictions of such technologies and overestimate their ability to provide actual therapeutic support and guidance. TM raises major ethical concerns that can exacerbate one's mental health contributing to the global mental health crisis. This paper will explore the various ways in which TM can occur particularly through inaccurate marketing of these chatbots, forming a digital therapeutic alliance with them, receiving harmful advice due to bias in the design and algorithm, and the chatbots inability to foster autonomy with patients."
"Digital Trace Data Collection for Social Media Effects Research: APIs, Data Donation, and (Screen) Tracking",2023,Jakob Ohme; Theo Araujo; Laura Boeschoten; Deen Freelon; Nilàm Ram; Byron Reeves; Thomas N. Robinson,Communication Methods and Measures,109,W4322500410,10.1080/19312458.2023.2181319,https://openalex.org/W4322500410,https://doi.org/10.1080/19312458.2023.2181319,Social media; Computer science; TRACE (psycholinguistics); Misinformation; Data collection,article,True,"In social media effects research, the role of specific social media content is understudied, in part attributable to the fact that communication science previously lacked methods to access social media content directly. Digital trace data (DTD) can shed light on textual and audio-visual content of social media use and enable the analysis of content usage on a granular individual level that has been previously unavailable. However, because digital trace data are not specifically designed for research purposes, collection and analysis present several uncertainties. This article is a collaborative effort by scholars to provide an overview of how three methods of digital trace data collection - APIs, data donations, and tracking - can be used in studying the effects of social media content in three important topic areas of communication research: misinformation, algorithmic bias, and well-being. We address the question of how to collect raw social media content data and arrive at meaningful measures with multiple state-of-the-art data collection techniques that can be used to study the effects of social media use on different levels of detail. We conclude with a discussion of best practices for the implementation of each technique, and a comparison of their advantages and disadvantages."
Calibration of the SMAP Soil Moisture Retrieval Algorithm to Reduce Bias Over the Amazon Rainforest,2024,Kyeungwoo Cho; Robinson Negrón‐Juárez; Andreas Colliander; Eric G. Cosio; Norma Salinas; A LETCIA PONTES DE ARAUJO; Jeffrey Q. Chambers; Jingfeng Wang,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,9,W4394828130,10.1109/jstars.2024.3388914,https://openalex.org/W4394828130,https://ieeexplore.ieee.org/ielx7/4609443/4609444/10499827.pdf,Amazon rainforest; Rainforest; Vegetation (pathology); Environmental science; Remote sensing,article,True,"Soil moisture (<italic>SM</italic>) is crucial for the Earth&#x0027;s ecosystem, impacting climate and vegetation health. Obtaining in situ observations of <italic>SM</italic> is labor-intensive and complex, particularly in remote and densely vegetated regions like the Amazon rainforest. NASA&#x0027;s soil moisture active and passive (SMAP) mission, utilizing an L-band radiometer, aims to monitor global <italic>SM</italic>. While it has been validated in areas with low vegetation water content (<italic>VWC</italic>) (&lt; 5 <inline-formula><tex-math notation=""LaTeX"">${\text{kgm}}^{ - 2}$</tex-math></inline-formula>), its efficiency in the Amazon, with dense canopies and high <italic>VWC</italic> (&gt; 10 <inline-formula><tex-math notation=""LaTeX"">${\text{kgm}}^{ - 2}$</tex-math></inline-formula>), is limitedly investigated due to scarce in situ measurements. This study assessed and analyzed the SMAP <italic>SM</italic> retrievals in the Amazon, employing the single-channel algorithm and adjusting vegetation optical depth (&#x03C4;) and single scattering albedo (&#x03C9;), two key vegetation parameters. It incorporated in situ <italic>SM</italic> observations from three old-growth rainforest locations: Tambopata (Southwest Amazon), Manaus (Central Amazon), and Caxiuana (Eastern Amazon). The SMAP <italic>SM</italic> deviated substantially from the in situ <italic>SM</italic>. However, calibrating &#x03C4; and &#x03C9; values, characterized by a lower &#x03C4;, resulted in better agreement with the in situ measurements. This study emphasizes the pressing need for innovative methodologies to accurately retrieve <italic>SM</italic> in high-<italic>VWC</italic> regions like the Amazon rainforest using SMAP data."
Artificial Intelligence in Point-of-Care Biosensing: Challenges and Opportunities,2024,Connor D. Flynn; Dingran Chang,Diagnostics,84,W4399054302,10.3390/diagnostics14111100,https://openalex.org/W4399054302,https://www.mdpi.com/2075-4418/14/11/1100/pdf?version=1716636804,Computer science; Biosensor; Field (mathematics); Health care; Transformative learning,article,True,"The integration of artificial intelligence (AI) into point-of-care (POC) biosensing has the potential to revolutionize diagnostic methodologies by offering rapid, accurate, and accessible health assessment directly at the patient level. This review paper explores the transformative impact of AI technologies on POC biosensing, emphasizing recent computational advancements, ongoing challenges, and future prospects in the field. We provide an overview of core biosensing technologies and their use at the POC, highlighting ongoing issues and challenges that may be solved with AI. We follow with an overview of AI methodologies that can be applied to biosensing, including machine learning algorithms, neural networks, and data processing frameworks that facilitate real-time analytical decision-making. We explore the applications of AI at each stage of the biosensor development process, highlighting the diverse opportunities beyond simple data analysis procedures. We include a thorough analysis of outstanding challenges in the field of AI-assisted biosensing, focusing on the technical and ethical challenges regarding the widespread adoption of these technologies, such as data security, algorithmic bias, and regulatory compliance. Through this review, we aim to emphasize the role of AI in advancing POC biosensing and inform researchers, clinicians, and policymakers about the potential of these technologies in reshaping global healthcare landscapes."
Double-Layer RRT* Objective Bias Anytime Motion Planning Algorithm,2024,Hamada Esmaiel; Zhao Guolin; Zeyad A. H. Qasem; Jie Qi; Haixin Sun,Robotics,4,W4392358192,10.3390/robotics13030041,https://openalex.org/W4392358192,https://www.mdpi.com/2218-6581/13/3/41/pdf?version=1709261572,Motion planning; Computer science; Motion (physics); Algorithm; Artificial intelligence,article,True,"This paper proposes a double-layer structure RRT* algorithm based on objective bias called DOB-RRT*. The algorithm adopts an initial path with an online optimization structure for motion planning. The first layer of RRT* introduces a feedback-based objective bias strategy with segment forward pruning processing to quickly obtain a smooth initial path. The second layer of RRT* uses the heuristics of the initial tree structure to optimize the path by using reverse maintenance strategies. Compared with conventional RRT and RRT* algorithms, the proposed algorithm can obtain the initial path with high quality, and it can quickly converge to the progressive optimal path during the optimization process. The performance of the proposed algorithm is effectively evaluated and tested in real experiments on an actual wheeled robotic vehicle running ROS Kinetic in a real environment."
Digitalization and inclusiveness of HRM practices: The example of neurodiversity initiatives,2023,Emmanuelle Walkowiak,Human Resource Management Journal,52,W4321377417,10.1111/1748-8583.12499,https://openalex.org/W4321377417,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/1748-8583.12499,Facilitation; Digital transformation; Multidisciplinary approach; Perspective (graphical); Knowledge management,article,True,"Abstract The transformation of the intelligence ecosystem associated with the digital transformation represents a critical juncture for diversity and inclusion (D&amp;I). We present a multidisciplinary perspective on digital transformation and D&amp;I that demonstrates that, in the context of automated decision making, where algorithmic biases and the standardisation of thought represent new risks, neurodiversity initiatives become a cornerstone for advancing D&amp;I. Based on interviews with neurodiversity experts, we identify innovative ways to efficiently configure an inclusive organisational design targeting neurodiversity by leveraging technologies. We identify several properties of technologies that support D&amp;I in neurodiversity initiatives: the neutralisation of biases during interviews, the development of digital support for physical and mental well‐being and the facilitation of different cognition modes. Finally, we critically discuss the risks and opportunities offered by various technologies in terms of performance evaluation, new forms of dominance, and design of a digital ecosystem for mental well‐being."
Using Explainable Artificial Intelligence (XAI) to reduce opacity and address bias in algorithmic models,2024,Otávio Morato de Andrade; Marco Antônio Sousa Alves,Revista Thesis Juris,5,W4400111128,10.5585/13.2024.26510,https://openalex.org/W4400111128,https://doi.org/10.5585/13.2024.26510,Transparency (behavior); Computer science; Black box; Artificial intelligence; Identification (biology),article,True,"Artificial intelligence (AI) has been extensively employed across various domains, with increasing social, ethical, and privacy implications. As their potential and applications expand, concerns arise about the reliability of AI systems, particularly those that use deep learning techniques that can make them true “black boxes”. Explainable artificial intelligence (XAI) aims to offer information that helps explain the predictive process of a given algorithmic model. This article examines the potential of XAI in elucidating algorithmic decisions and mitigating bias in AI systems. In the first stage of the work, the issue of AI fallibility and bias is discussed, emphasizing how opacity exacerbates these issues. The second part explores how XAI can enhance transparency, helping to combat algorithmic errors and biases. The article concludes that XAI can contribute to the identification of biases in algorithmic models, then it is suggested that the ability to “explain” should be a requirement for adopting AI systems in sensitive areas such as court decisions."
Reviewing the role of AI in environmental monitoring and conservation: A data-driven revolution for our planet,2024,Onyebuchi Nneamaka Chisom; Preye Winston Biu; Aniekan Akpan Umoh; Bartholomew Obehioye Obaedo; Abimbola Oluwatoyin Adegbite; Ayodeji Abatan,World Journal of Advanced Research and Reviews,79,W4390564739,10.30574/wjarr.2024.21.1.2720,https://openalex.org/W4390564739,https://wjarr.com/sites/default/files/WJARR-2023-2720.pdf,Environmental resource management; Biodiversity; Citizen science; Wildlife; Deforestation (computer science),article,True,"The rapid increase in human activities is causing significant damage to our planet's ecosystems, necessitating innovative solutions to preserve biodiversity and counteract ecological threats. Artificial Intelligence (AI) has emerged as a transformative force, providing unparalleled capabilities for environmental monitoring and conservation. This research paper explores the applications of AI in ecosystem management, including wildlife tracking, habitat assessment, biodiversity analysis, and natural disaster prediction. AI's role in environmental monitoring and conservation includes wildlife tracking, habitat assessment, resource conservation, biodiversity analysis, and species identification. AI algorithms analyze camera trap footage, drone imagery, and GPS data to identify and estimate population sizes, leading to improved anti-poaching efforts and enhanced protection of diverse species. Habitat assessment and resource conservation involve AI-powered image analysis, which aids in assessing forest health, detecting deforestation, and identifying areas in need of restoration. Biodiversity analysis and species identification are achieved through AI algorithms that analyze acoustic recordings, environmental DNA (eDNA), and camera trap footage. These innovations identify different species, assess biodiversity levels, and even discover new or endangered species. AI-powered flood prediction systems provide early warnings, empowering communities with better preparedness and evacuation efforts. Challenges, such as data quality and availability, algorithmic bias, and infrastructure limitations, are acknowledged as opportunities for growth and improvement. In policy and regulation, the paper advocates for clear frameworks prioritizing data privacy and security, algorithmic transparency, and equitable access. Responsible development and ethical use of AI are emphasized as foundational pillars, ensuring that the integration of AI into environmental conservation aligns with principles of fairness, transparency, and societal benefit."
A scoping review of reporting gaps in FDA-approved AI medical devices,2024,Vijaytha Muralidharan; Boluwatife Adeleye Adewale; Caroline J. Huang; Mfon Thelma Nta; Peter Oluwaduyilemi Ademiju; Pirunthan Pathmarajah; Man Kien Hang; Oluwafolajimi Adesanya; Ridwanullah Olamide Abdullateef; Abdulhammed Opeyemi Babatunde; Abdulquddus Ajibade; Sonia Onyeka; Zhou Ran Cai; Roxana Daneshjou; Tobi Olatunji,npj Digital Medicine,69,W4403107901,10.1038/s41746-024-01270-x,https://openalex.org/W4403107901,https://doi.org/10.1038/s41746-024-01270-x,Medicine; Medical physics,review,True,
Addressing bias in artificial intelligence for public health surveillance,2023,Lidia Flores; SeungJun Kim; Sean D. Young,Journal of Medical Ethics,36,W4367675464,10.1136/jme-2022-108875,https://openalex.org/W4367675464,,Computer science; Artificial intelligence; Social media; Data science; Health care,article,False,"Components of artificial intelligence (AI) for analysing social big data, such as natural language processing (NLP) algorithms, have improved the timeliness and robustness of health data. NLP techniques have been implemented to analyse large volumes of text from social media platforms to gain insights on disease symptoms, understand barriers to care and predict disease outbreaks. However, AI-based decisions may contain biases that could misrepresent populations, skew results or lead to errors. Bias, within the scope of this paper, is described as the difference between the predictive values and true values within the modelling of an algorithm. Bias within algorithms may lead to inaccurate healthcare outcomes and exacerbate health disparities when results derived from these biased algorithms are applied to health interventions. Researchers who implement these algorithms must consider when and how bias may arise. This paper explores algorithmic biases as a result of data collection, labelling and modelling of NLP algorithms. Researchers have a role in ensuring that efforts towards combating bias are enforced, especially when drawing health conclusions derived from social media posts that are linguistically diverse. Through the implementation of open collaboration, auditing processes and the development of guidelines, researchers may be able to reduce bias and improve NLP algorithms that improve health surveillance."
Algorithmic Fairness in Recruitment: Designing AI-Powered Hiring Tools to Identify and Reduce Biases in Candidate Selection,2025,Chinyere Linda Agbasiere; Goodness Rex Nze-Igwe,Path of Science,4,W4410201757,10.22178/pos.116-10,https://openalex.org/W4410201757,https://pathofscience.org/index.php/ps/article/download/3471/1690,Selection (genetic algorithm); Psychology; Computer science; Personnel selection; Applied psychology,article,True,"The study looks into how artificial intelligence (AI) affects hiring procedures, focusing on the fairness of the algorithms that drive these tools. AI has improved the efficiency of the hiring process, yet its use results in institutionalised discrimination. The AI systems used for recruitment, which base evaluations on past performance data, have the potential to discriminate against minority candidates as well as women through unintentional actions. The ability of AI systems to decrease human biases during recruitment encounters major challenges, as Amazon's discriminatory resume screening demonstrates the issues in systemic bias maintenance. This paper discusses the origins of algorithmic bias, including biased training records, defining labels, and choosing features, and suggests debiasing methods. Methods such as reweighting, adversarial debiasing, and fairness-aware algorithms are assessed for suitability in developing unbiased AI hiring systems. A quantitative approach is used in the research, web scraping data from extensive secondary sources to assess these biases and their mitigation measures. A Fair Machine Learning (FML) theoretical framework is utilised, which introduces fairness constraints into machine learning models so that hiring models do not perpetuate present discrimination. The ethical, legal, and organisational ramifications of using AI for recruitment are further examined under GDPR and Equal Employment Opportunity law provisions. By investigating HR practitioners' experiences and AI-based recruitment data, the study aims to develop guidelines for designing open, accountable, and equitable AI-based hiring processes. The findings emphasise the value of human oversight and the necessity of regular audits to guarantee equity in AI hiring software and, consequently, encourage diversity and equal opportunity during employment."
Eliminating Algorithmic Racial Bias in Clinical Decision Support Algorithms: Use Cases from the Veterans Health Administration,2023,Justin M. List; Paul M. Palevsky; Suzanne Tamang; Susan T. Crowley; David H. Au; William C. Yarbrough; Amol S. Navathe; Craig Kreisler; Ravi B. Parikh; Jessica Wang‐Rodriguez; J. Stacey Klutts; Paul R. Conlin; Leonard Pogach; Esther L. Meerwijk; Ernest Moy,Health Equity,5,W4389162187,10.1089/heq.2023.0037,https://openalex.org/W4389162187,https://doi.org/10.1089/heq.2023.0037,Equity (law); Clinical decision support system; Health care; Algorithm; Clinical decision making,article,True,"The Veterans Health Administration uses equity- and evidence-based principles to examine, correct, and eliminate use of potentially biased clinical equations and predictive models. We discuss the processes, successes, challenges, and next steps in four examples. We detail elimination of the race modifier for estimated kidney function and discuss steps to achieve more equitable pulmonary function testing measurement. We detail the use of equity lenses in two predictive clinical modeling tools: Stratification Tool for Opioid Risk Mitigation (STORM) and Care Assessment Need (CAN) predictive models. We conclude with consideration of ways to advance racial health equity in clinical decision support algorithms."
Explainable AI in Algorithmic Trading: Mitigating Bias and Improving Regulatory Compliance in Finance,2025,,International Journal of Computer Applications Technology and Research,8,W4408777236,10.7753/ijcatr1404.1006,https://openalex.org/W4408777236,https://doi.org/10.7753/ijcatr1404.1006,Computer science; Compliance (psychology); Social psychology; Psychology,article,True,
A diffusion bias-compensated LMS algorithm for distributed estimation with ARMAX models,2023,Jiale Zeng; Wen Mi; Wei Xing Zheng,Digital Signal Processing,4,W4386227081,10.1016/j.dsp.2023.104202,https://openalex.org/W4386227081,,Colors of noise; Algorithm; Least mean squares filter; Noise (video); Stability (learning theory),article,False,
"Structural bias in metaheuristic algorithms: Insights, open problems, and future prospects",2024,Kanchan Rajwar; Kusum Deep,Swarm and Evolutionary Computation,7,W4405303904,10.1016/j.swevo.2024.101812,https://openalex.org/W4405303904,,Computer science; Metaheuristic; Algorithm,article,False,
Challenges and strategies for wide-scale artificial intelligence (AI) deployment in healthcare practices: A perspective for healthcare organizations,2024,Pouyan Esmaeilzadeh,Artificial Intelligence in Medicine,218,W4393336357,10.1016/j.artmed.2024.102861,https://openalex.org/W4393336357,,Health care; Workflow; Knowledge management; Computer science; Transparency (behavior),article,False,
Shapley value: from cooperative game to explainable artificial intelligence,2024,Meng Li; Hengyang Sun; Yanjun Huang; Hong Chen,Autonomous Intelligent Systems,73,W4391685200,10.1007/s43684-023-00060-8,https://openalex.org/W4391685200,https://link.springer.com/content/pdf/10.1007/s43684-023-00060-8.pdf,Shapley value; Mathematical economics; Value (mathematics); Computer science; Artificial intelligence,article,True,"Abstract With the tremendous success of machine learning (ML), concerns about their black-box nature have grown. The issue of interpretability affects trust in ML systems and raises ethical concerns such as algorithmic bias. In recent years, the feature attribution explanation method based on Shapley value has become the mainstream explainable artificial intelligence approach for explaining ML models. This paper provides a comprehensive overview of Shapley value-based attribution methods. We begin by outlining the foundational theory of Shapley value rooted in cooperative game theory and discussing its desirable properties. To enhance comprehension and aid in identifying relevant algorithms, we propose a comprehensive classification framework for existing Shapley value-based feature attribution methods from three dimensions: Shapley value type, feature replacement method, and approximation method. Furthermore, we emphasize the practical application of the Shapley value at different stages of ML model development, encompassing pre-modeling, modeling, and post-modeling phases. Finally, this work summarizes the limitations associated with the Shapley value and discusses potential directions for future research."
Ethical Considerations in Artificial Intelligence: Addressing Bias and Fairness in Algorithmic Decision-Making,2024,Yogesh Morchhale,INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT,4,W4395669504,10.55041/ijsrem31693,https://openalex.org/W4395669504,,Prejudice (legal term); Transparency (behavior); Ethical decision; Economic Justice; Equity (law),article,False,"The expanding use of artificial intelligence (AI) in decision-making across a range of industries has given rise to serious ethical questions about prejudice and justice. This study looks at the moral ramifications of using AI algorithms in decision-making and looks at methods to combat prejudice and advance justice. The study investigates the underlying causes of prejudice in AI systems, the effects of biased algorithms on people and society, and the moral obligations of stakeholders in reducing bias, drawing on prior research and real-world examples. The study also addresses new frameworks and strategies for advancing justice in algorithmic decision-making, emphasizing the value of openness, responsibility, and diversity in dataset gathering and algorithm development. The study concludes with suggestions for further investigation and legislative actions to guarantee that AI systems respect moral standards and advance justice and equity in the processes of making decisions. Keywords Ethical considerations, Artificial intelligence, Bias, Fairness, Algorithmic decision-making, Ethical implications, Ethical responsibilities, Stakeholders, Bias in AI systems, Impact of biased algorithms, Strategies for addressing bias, Promoting fairness, Algorithmic transparency."
Investigating Biases in COVID-19 Diagnostic Systems Processed with Automated Speech Anonymization Algorithms,2023,Yi Zhu; Mohamed Imoussaïne-Aïkous; Carolyn Côté‐Lussier; Tiago H. Falk,,5,W4386712553,10.21437/spsc.2023-8,https://openalex.org/W4386712553,https://www.isca-archive.org/spsc_2023/zhu23_spsc.pdf,Metadata; Computer science; Socioeconomic status; Coronavirus disease 2019 (COVID-19); Speech recognition,article,True,"Automated voice anonymization algorithms are used to obfuscate speaker identity while leaving other vocal attributes untouched; they have been used for e.g., speech recognition, speech emotion detection, and most recently, remote speechbased health diagnostics.However, speech data is commonly collected in an uncontrolled manner in various environments, potentially compromising its quality, and frequently omits key metadata that could improve model performance.In this study, we employed the Cambridge COVID-19 sound database and used COVID-19 detection as a case study.We first present descriptive statistics on sample composition (i.e., COVID-19 status, age, gender).We also present a measure of signal-tonoise ratio (SNR), a feature of speech that can denote individuals' socioeconomic status.Next, we assess how age and SNR, the two most unbalanced features of the dataset, are associated with model performance and the impact of automated anonymization algorithms performance.Our findings suggest the existence of diagnostic biases related to age and SNR of the recording, which become more prominent after anonymization.To tackle these biases, we explore the usefulness of two data augmentation methods.We show that although data augmentation helps to recover some loss in overall performance, it can lead to a larger discrepancy in performance for over-represented and under-represented groups.We conclude with a discussion of the limitations associated with using SNR as an indicator of socioeconomic status, and of the potential effects of diagnostic biases associated with socioeconomic status."
Should ChatGPT be biased? Challenges and risks of bias in large language models,2023,Emilio Ferrara,First Monday,176,W4388488349,10.5210/fm.v28i11.13346,https://openalex.org/W4388488349,https://firstmonday.org/ojs/index.php/fm/article/download/13346/11369,Unintended consequences; Transparency (behavior); Computer science; Generative grammar; Language model,article,True,"As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI."
Artificial intelligence bias in medical system designs: a systematic review,2023,Ashish Kumar; Vivekanand Aelgani; Rubeena Vohra; Suneet Kumar Gupta; Mrinalini Bhagawati; Sudip Paul; Luca Saba; Neha Suri; Narendra N. Khanna; John R. Laird; Amer M. Johri; Manudeep Kalra; Mostafa M. Fouda; Mostafa Fatemi; Subbaram Naidu; Jasjit S. Suri,Multimedia Tools and Applications,45,W4385074164,10.1007/s11042-023-16029-x,https://openalex.org/W4385074164,https://link.springer.com/content/pdf/10.1007/s11042-023-16029-x.pdf,Computer science; Artificial intelligence; Machine learning; Selection bias; Ranking (information retrieval),review,True,"Inherent bias in the artificial intelligence (AI)-model brings inaccuracies and variabilities during clinical deployment of the model. It is challenging to recognize the source of bias in AI-model due to variations in datasets and black box nature of system design. Additionally, there is no distinct process to identify the potential source of bias in the AI-model. To the best of our knowledge, this is the first review of its kind that addresses the bias in AI-model by categorizing 48 studies into three classes, namely, point-based, image-based, and hybrid-based AI-models. Selection strategy using PRISMA is adopted to select the 72 crucial AI studies for identifying bias in AI models. Using the three classes, bias is identified in these studies based on 44 critical AI attributes. Bias in the AI-models is computed by analytical, butterfly, and ranking-based bias models. These bias models were evaluated using two experts and compared using variability analysis. AI-studies that lacked sufficient AI-attributes are more prone to risk-of-bias (RoB) in all three classes. Studies with high RoB loses fins in the butterfly model. It has been analyzed that the majority of the studies in healthcare suffer from data bias and algorithmic bias due to incomplete specifications mentioned in the design protocol and weak AI design exploited for prediction."
Stress-Testing Bias Mitigation Algorithms to Understand Fairness Vulnerabilities,2023,Karan Bhanot; Ioana Baldini; Dennis Wei; Jiaming Zeng; Kristin P. Bennett,,3,W4386242375,10.1145/3600211.3604713,https://openalex.org/W4386242375,https://dl.acm.org/doi/pdf/10.1145/3600211.3604713,Computer science; Metric (unit); Audit; Algorithm; Fairness measure,article,True,"To address the growing concern of unfairness in Artificial Intelligence (AI), several bias mitigation algorithms have been introduced in prior research. Their capabilities are often evaluated on certain overly-used datasets without rigorously stress-testing them under simultaneous train and test distribution shifts. To address this, we investigate the fairness vulnerabilities of these algorithms across several distribution shift scenarios using synthetic data, to highlight scenarios where these algorithms do and don't work to encourage their trustworthy use. The paper makes three important contributions. Firstly, we propose a flexible pipeline called the Fairness Auditor to systematically stress-test bias mitigation algorithms using multiple synthetic datasets with shifts. Secondly, we introduce the Deviation Metric for measuring the fairness and utility performance of these algorithms under such shifts. Thirdly, we propose an interactive reporting tool for comparing algorithmic performance across various synthetic datasets, mitigation algorithms and metrics called the Fairness Report."
Artificial Intelligence and Machine Learning in Pharmacological Research: Bridging the Gap Between Data and Drug Discovery,2023,Shruti Singh; Rajesh Kumar; Shuvasree Payra; Sunil Kumar Singh,Cureus,119,W4386291745,10.7759/cureus.44359,https://openalex.org/W4386291745,https://assets.cureus.com/uploads/review_article/pdf/172577/20230830-19258-1lnt6y5.pdf,Artificial intelligence; Repurposing; Deep learning; Medicine; Identification (biology),review,True,
Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias,2023,Sanguk Lee; Tai‐Quan Peng; Matthew H. Goldberg; Seth A. Rosenthal; John Kotcher; Edward Maibach; Anthony Leiserowitz,arXiv (Cornell University),7,W4388275389,10.48550/arxiv.2311.00217,https://openalex.org/W4388275389,https://arxiv.org/pdf/2311.00217,Fidelity; Global warming; Psychology; Covariate; Climate change,preprint,True,"Large language models (LLMs) have demonstrated their potential in social science research by emulating human perceptions and behaviors, a concept referred to as algorithmic fidelity. This study assesses the algorithmic fidelity and bias of LLMs by utilizing two nationally representative climate change surveys. The LLMs were conditioned on demographics and/or psychological covariates to simulate survey responses. The findings indicate that LLMs can effectively capture presidential voting behaviors but encounter challenges in accurately representing global warming perspectives when relevant covariates are not included. GPT-4 exhibits improved performance when conditioned on both demographics and covariates. However, disparities emerge in LLM estimations of the views of certain groups, with LLMs tending to underestimate worry about global warming among Black Americans. While highlighting the potential of LLMs to aid social science research, these results underscore the importance of meticulous conditioning, model selection, survey question format, and bias assessment when employing LLMs for survey simulation. Further investigation into prompt engineering and algorithm auditing is essential to harness the power of LLMs while addressing their inherent limitations."
Algorithmic Gender Bias: Investigating Perceptions of Discrimination in Automated Decision-Making,2024,Soojong Kim; Poong Oh; Joomi Lee,,2,W4390840428,10.31234/osf.io/zpqrt,https://openalex.org/W4390840428,https://osf.io/zpqrt/download,Injustice; Situational ethics; Social psychology; Perception; Psychology,preprint,True,"With the widespread use of artificial intelligence and automated decision-making (ADM), concerns are increasing about automated decisions biased against certain social groups, such as women and racial minorities. The public's skepticism and the danger of algorithmic discrimination are widely acknowledged, yet the role of key factors constituting the context of discriminatory situations is underexplored. This study examined people’s perceptions of gender bias in ADM, focusing on three factors influencing the responses to discriminatory automated decisions: the target of discrimination (subject vs. other), the gender identity of the subject, and situational contexts that engender biases. Based on a randomized experiment (N = 602), we found stronger negative reactions to automated decisions that discriminate against the gender group of the subject than those discriminating against other gender groups, evidenced by lower perceived fairness and trust in ADM, and greater negative emotion and tendency to question the outcome. The negative reactions were more pronounced among participants in underserved gender groups than men. Also, participants were more sensitive to biases in economic and occupational contexts than in other situations. These findings suggest that perceptions of algorithmic biases should be understood in relation to the public's lived experience of inequality and injustice in society."
The Power of Artificial Intelligence in Recruitment: An Analytical Review of Current AI-Based Recruitment Strategies,2023,Wael Abdulrahman Albassam,International Journal of Professional Business Review,73,W4381684347,10.26668/businessreview/2023.v8i6.2089,https://openalex.org/W4381684347,https://openaccessojs.com/JBReview/article/download/2089/969,Originality; Value (mathematics); Quality (philosophy); Ethical issues; Applications of artificial intelligence,article,True,"Purpose: The aim of this study is to contribute to the understanding of the power of artificial intelligence (AI) in recruitment and to highlight the opportunities and challenges associated with its use. Theoretical framework: This paper provides a comprehensive analytical review of current AI-based recruitment strategies, drawing on both academic research and industry reports. Design/methodology/approach: The paper critically evaluates the potential benefits and drawbacks of using AI in recruitment and assesses the effectiveness of various AI-based recruitment strategies. Findings: The results indicate that AI-based recruitment strategies such as resume screening, candidate matching, video interviewing, chatbots, predictive analytics, gamification, virtual reality assessments, and social media screening offer significant potential benefits for organizations, including improved efficiency, cost savings, and better-quality hires. However, the use of AI in recruitment also raises ethical and legal concerns, including the potential for algorithmic bias and discrimination. Research, Practical &amp; Social implications: The study concludes by emphasizing the need for further research and development to ensure that AI-based recruitment strategies are effective, unbiased, and aligned with ethical and legal standards. Originality/value: The value of the study lies in its comprehensive exploration of AI in recruitment, synthesizing insights from academic and industry perspectives, and assessing the balance of potential benefits against ethical and legal concerns."
A CRITICAL REVIEW OF AI-DRIVEN STRATEGIES FOR ENTREPRENEURIAL SUCCESS,2024,Favour Oluwadamilare Usman; Nsisong Louis Eyo-Udo; Emmanuel Augustine Etukudoh; Beryl Odonkor; Chidera Victoria Ibeh; Ayodeji Adegbola,International Journal of Management & Entrepreneurship Research,86,W4391579939,10.51594/ijmer.v6i1.748,https://openalex.org/W4391579939,https://fepbl.com/index.php/ijmer/article/download/748/939,Critical success factor; Psychology; Computer science; Knowledge management,review,True,"In the rapidly evolving landscape of entrepreneurship, the integration of Artificial Intelligence (AI) has emerged as a transformative force, reshaping traditional business paradigms and offering unprecedented opportunities for success. This paper provides a comprehensive and critical review of AI-driven strategies employed by entrepreneurs to enhance their ventures. The review encompasses a thorough analysis of key AI applications, their impact on various aspects of entrepreneurship, and the potential benefits and challenges associated with their implementation. The first section explores the role of AI in market analysis, highlighting how advanced data analytics and predictive modelling contribute to informed decision-making and market forecasting. The discussion then extends to AI-driven innovations in product development, emphasizing the acceleration of ideation, prototyping, and customization through machine learning algorithms. Next, the paper scrutinizes the influence of AI on customer engagement and relationship management. It delves into the personalized customer experiences facilitated by chatbots, recommendation systems, and sentiment analysis, while also addressing ethical considerations surrounding data privacy and algorithmic biases. Entrepreneurial operations and efficiency gains are examined in the subsequent section, emphasizing AI's impact on supply chain management, logistics, and resource optimization. The review underscores the potential for increased productivity and cost-effectiveness through the implementation of AI-powered automation and smart systems. Despite the myriad advantages, the paper critically examines challenges such as ethical concerns, job displacement, and the digital divide. It emphasizes the need for a balanced approach that addresses the societal impact of AI adoption while fostering inclusive entrepreneurial ecosystems. In conclusion, this critical review not only provides a comprehensive overview of the current landscape of AI-driven strategies in entrepreneurship but also offers insights into the potential future developments and challenges. Entrepreneurs, policymakers, and researchers can leverage this analysis to navigate the evolving intersection of AI and entrepreneurship, fostering a sustainable and ethically sound environment for entrepreneurial success in the digital era.&#x0D; Keywords: Artificial Intelligence (AI), Entrepreneurship, Strategic Implementation, Innovation, Market Analysis, Predictive Modelling."
Bias as Boundary Object: Unpacking The Politics Of An Austerity Algorithm Using Bias Frameworks,2023,Gabriel Grill; Fabian Fischer; Florian Cech,,4,W4380320344,10.1145/3593013.3594120,https://openalex.org/W4380320344,,Unpacking; Austerity; Politics; Boundary (topology); Computer science,article,False,
Based on the Heuristic Bias Method of Efficient Algorithm of RRT - Connect,2023,Fan Yang; Jinyang Fan,,3,W4387698303,10.1109/aicit59054.2023.10278084,https://openalex.org/W4387698303,,Sampling (signal processing); Convergence (economics); Heuristic; Computer science; Algorithm,article,False,"Aiming at the problems of Rapidly Exploring Random Tree (RRT) algorithm with a large number of redundant points and slow convergence in the sampling stage, An efficient RRT-Connect algorithm based on Heuristic Bias RRT-Connect (HB-RRT-Connect) is proposed. The algorithm divides the sampling process into global random sampling and biased target sampling, and determines the next sampling method by judging the environment of the previous sampling point, so as to realize the heuristic biased sampling. At the same time, the concept of double optimized dynamic step is also introduced to improve the expansion efficiency. The simulation results show that the proposed algorithm has less convergence time and less number of nodes than other algorithms, and can effectively improve the convergence speed."
Funds of Knowledge used by Adolescents of Color in Scaffolded Sensemaking around Algorithmic Fairness,2023,Jean Salac; Alannah Oleson; Lena Armstrong; Audrey Le Meur; Amy J. Ko,,17,W4386588485,10.1145/3568813.3600110,https://openalex.org/W4386588485,https://dl.acm.org/doi/pdf/10.1145/3568813.3600110,Sensemaking; Computer science; Knowledge management,article,True,"With the ubiquity of computing technologies, adolescents are increasingly affected by algorithmic biases. While previous work provides insight into adolescents' perceptions of algorithmic bias, few provide guidance on how to engage adolescents in discourse on algorithmic bias that prioritizes both their agency and safety. To address this, we developed and conducted group discussions and design activities based on three scenarios of algorithmic bias with 15 adolescents of color (ages 15-17) in a summer academic program in the United States targeted at students from families with low-income backgrounds or who would be the first in their family to pursue post-secondary education. When sensemaking, all participants considered factors beyond the scenarios, using their situated knowledge to contextualize perceptions of unfairness. They also considered sources of bias and impacts of unfairness at different levels of individuals, communities, and society. However, when designing solutions, they tended to design for hypothetical ""average users"" instead of considering nuances of user populations. We offer insights for algorithmic fairness learning experiences that support situated reasoning in adolescents."
Misclassifications of Contact Lens Iris PAD Algorithms: Is it Gender Bias or Environmental Conditions?,2023,Akshay Agarwal; Nalini Ratha; Afzel Noore; Richa Singh; Mayank Vatsa,2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),4,W4319299996,10.1109/wacv56688.2023.00102,https://openalex.org/W4319299996,,Computer science; Biometrics; Adversary; Iris recognition; Classifier (UML),article,False,"One of the critical steps in biometrics pipeline is detection of presentation attacks, a physical adversary. Several presentation (adversary) attack detection (PAD) algorithms, including iris PAD, have been proposed and have shown superlative performance. However, a recent study, on a small-scale database, has highlighted that iris PAD may have gender biases. In this research, we present a rigorous study on gender bias in iris presentation attack detection algorithms using a large-scale and gender-balanced database. The paper provides several interesting observations which can help in building future presentation attack detection algorithms with aim of fair treatment of each demography. In addition, we also present a robust iris presentation attack detection algorithm by combining gender-covariate based classifiers. The proposed robust classifier not only reduces the difference in accuracy between different genders but also improves the overall performance of the PAD system."
Contribution of Artificial Intelligence in Improving Accessibility for Individuals with Disabilities,2023,Jeff Shuford,Journal of Knowledge Learning and Science Technology ISSN 2959-6386 (online),33,W4394586626,10.60087/jklst.vol2.n2.p433,https://openalex.org/W4394586626,https://jklst.org/index.php/home/article/download/178/151,Psychology; Computer science; Artificial intelligence,article,True,"Artificial intelligence (AI) stands as a revolutionary force with profound implications for society, offering considerable advantages for individuals with disabilities. Yet, alongside its promise, AI brings inherent risks, including ethical dilemmas that could heighten discrimination against marginalized communities. This article conducts a thorough analysis of the benefits and drawbacks of AI for people with disabilities, with a specific focus on algorithmic biases. These biases, capable of molding societal frameworks and influencing decision-making, hold the potential to perpetuate unfair treatment and bias. Given these challenges, the article delves into potential remedies to mitigate these concerns and ensure that AI effectively caters to the needs of all individuals, irrespective of disability status."
Enhancing Consumer Behavior and Experience Through AI-Driven Insights Optimization,2024,N. Naveeenkumar; Sreekanth Rallapalli; K. Sasikala; P. Vidhya Priya; Jakeer Husain; Sampath Boopathi,"Advances in marketing, customer relationship management, and e-services book series",53,W4392368823,10.4018/979-8-3693-1918-5.ch001,https://openalex.org/W4392368823,,Computer science; Cognitive science; Psychology,book-chapter,False,"This chapter delves into the role of AI-driven behavioral analytics in understanding, predicting, and enhancing consumer experiences. It highlights the integration of behavioral analytics, consumer experiences, and predictive modeling in reshaping market dynamics. The chapter explains the fundamental components of behavioral analytics, emphasizing its significance in understanding consumer preferences and decision-making processes. It also discusses the impact of AI-powered predictive analytics on consumer experiences, anticipating behaviors and fostering proactive strategies. It addresses ethical concerns like data privacy and algorithmic biases. The chapter provides a guide for practitioners, researchers, and businesses to harness AI's transformative potential in contemporary markets."
A bias-compensated NLMS algorithm based on arctangent framework for system identification,2024,Rosalin Rosalin; Ansuman Patnaik; Sarita Nanda; Deepak Kumar Rout,Signal Image and Video Processing,6,W4391896268,10.1007/s11760-024-03024-4,https://openalex.org/W4391896268,,Identification (biology); Inverse trigonometric functions; Algorithm; Computer science; System identification,article,False,
CYBERSECURITY CHALLENGES IN THE AGE OF AI: THEORETICAL APPROACHES AND PRACTICAL SOLUTIONS,2024,Babajide Tolulope Familoni,Computer Science & IT Research Journal,99,W4393077032,10.51594/csitrj.v5i3.930,https://openalex.org/W4393077032,https://fepbl.com/index.php/csitrj/article/download/930/1144,Computer science; Computer security; Data science,article,True,"In the ever-evolving landscape of cybersecurity, the proliferation of artificial intelligence (AI) technologies introduces both promising advancements and daunting challenges. This paper explores the theoretical underpinnings and practical implications of addressing cybersecurity challenges in the age of AI. With the integration of AI into various facets of digital infrastructure, including threat detection, authentication, and response mechanisms, cyber threats have become increasingly sophisticated and difficult to mitigate. Theoretical approaches delve into understanding the intricate interplay between AI algorithms, human behavior, and adversarial tactics, elucidating the underlying mechanisms of cyber attacks and defense strategies. However, this complexity also engenders novel vulnerabilities, as AI-driven attacks leverage machine learning algorithms to evade traditional security measures, posing formidable challenges to organizations across sectors. As such, practical solutions necessitate a multifaceted approach, encompassing robust threat intelligence, adaptive defense mechanisms, and ethical considerations to safeguard against AI-driven cyber threats effectively. Leveraging AI for cybersecurity defense holds promise in enhancing detection capabilities, automating response actions, and augmenting human analysts' capabilities. Yet, inherent limitations, such as algorithmic biases, data privacy concerns, and the potential for AI-enabled attacks, underscore the need for a comprehensive risk management framework. Regulatory frameworks and industry standards play a crucial role in shaping the development and deployment of AI-powered cybersecurity solutions, ensuring accountability, transparency, and compliance with ethical principles. Moreover, fostering interdisciplinary collaboration and investing in cybersecurity education and training are vital for cultivating a skilled workforce equipped to navigate the evolving threat landscape. By integrating theoretical insights with practical strategies, this paper elucidates key challenges and opportunities in securing AI-driven systems, offering insights for policymakers, researchers, and practitioners alike.&#x0D; Keywords: Cybersecurity; Artificial Intelligence; Threat Detection; Defense Strategies; Ethical Considerations; Regulatory Frameworks."
Early years of biased random-key genetic algorithms: a systematic review,2024,Mariana A. Londe; Luciana Fontes Pessôa; Carlos E. Andrade; Maurício G. C. Resende,Journal of Global Optimization,6,W4404062777,10.1007/s10898-024-01446-5,https://openalex.org/W4404062777,,Mathematics; Key (lock); Algorithm; Computer science; Computer security,review,False,
The smart future for sustainable development: Artificial intelligence solutions for sustainable urbanization,2024,Marwan Al‐Raeei,Sustainable Development,99,W4400957436,10.1002/sd.3131,https://openalex.org/W4400957436,,Urbanization; Sustainable development; Sustainable city; Smart city; Big data,article,False,"Abstract Future tools for supporting collaborations between technology and sustainable development include artificial intelligence (AI) applications in sustainable Urbanization roles. This article highlights the various applications of AI in advancing sustainable urbanization. From urban planning to disaster management, AI technology is revolutionizing the way cities are designed and managed. By leveraging data analytics, machine learning, and predictive modeling, AI is helping city officials make informed decisions, optimize resource usage, and improve quality of life for urban residents. Despite the immense potential of AI in sustainable urban development, there are still challenges and limitations to overcome. We show some of the most significant problems related to these issues. These include issues related to data privacy, algorithm bias, and ethical considerations. Continued research and innovation are needed to address these challenges and ensure that AI technology is used responsibly and effectively in shaping sustainable cities. As a result, AI has the power to transform urban environments and create more sustainable, resilient communities. By harnessing the capabilities of AI, cities can become more efficient, environmentally‐friendly, and prepared for the challenges of the future. It is essential for policymakers, urban planners, and technology developers to work together to harness the full potential of AI in sustainable urbanization and create a better future for all. Proactively addressing these challenges can unlock the full potential of AI in combating sustainable cities and building a sustainable future for all."
TRANSFORMING FINTECH FRAUD DETECTION WITH ADVANCED ARTIFICIAL INTELLIGENCE ALGORITHMS,2024,Philip Olaseni Shoetan; Babajide Tolulope Familoni,Finance & Accounting Research Journal,70,W4394884079,10.51594/farj.v6i4.1036,https://openalex.org/W4394884079,https://fepbl.com/index.php/farj/article/download/1036/1259,Computer science; Artificial intelligence; Machine learning; Scalability; Transformative learning,article,True,"The rapid evolution of financial technology (fintech) platforms has exponentially increased the volume and sophistication of financial transactions, concurrently elevating the risk and complexity of fraudulent activities. This necessitates a paradigm shift in fraud detection methodologies towards more agile, accurate, and predictive solutions. This paper presents a comprehensive study on the transformative potential of advanced Artificial Intelligence (AI) algorithms in enhancing fintech fraud detection mechanisms. By leveraging cutting-edge AI techniques including deep learning, machine learning, and natural language processing, this research aims to develop a robust fraud detection framework capable of identifying, analyzing, and preventing fraudulent transactions in real-time.&#x0D; Our methodology encompasses the deployment of several AI algorithms on extensive datasets comprising genuine and fraudulent financial transactions. Through a comparative analysis, we identify the most effective algorithms in terms of accuracy, efficiency, and scalability. Key findings reveal that deep learning models, particularly those employing neural networks, outperform traditional machine learning models in detecting complex and nuanced fraudulent activities. Furthermore, the integration of natural language processing enables the extraction and analysis of unstructured data, significantly enhancing the detection capabilities.&#x0D; Conclusively, this paper underscores the critical role of advanced AI algorithms in revolutionizing fintech fraud detection. It highlights the superior performance of AI-based models over conventional methods, offering fintech platforms a more dynamic and predictive approach to fraud prevention. This research not only contributes to the academic discourse on financial security but also provides practical insights for fintech companies striving to safeguard their operations against fraud.&#x0D; Keywords: Artificial Intelligence, Fintech, Fraud Detection, Ethical Ai, Regulatory Compliance, Data Privacy, Algorithmic Bias, Predictive Analytics, Blockchain Technology, Quantum Computing, Interdisciplinary Collaboration, Innovation, Transparency, Accountability, Continuous Learning, Ethical Principles, Real-Time Processing, Financial Sector."
Application of deep learning algorithms to correct bias in <scp>CMIP6</scp> simulations of surface air temperature over the Indian monsoon core region,2023,A. Sabarinath; A. Naga Rajesh; Sachin S. Gunthe; T. V. Lakshmi Kumar,International Journal of Climatology,7,W4387617059,10.1002/joc.8276,https://openalex.org/W4387617059,,Climatology; Mean squared error; Climate model; GCM transcription factors; Environmental science,article,False,"Abstract Indian subcontinent witnessed a rise in surface air temperature (SAT) in recent decades, during the summer months of March, April and May. The monsoon core region (MCR) of India experiences a hot and humid climate, with temperatures typically highest in May and June before the onset of the monsoon. Global climate model (GCM) simulations of SAT are very much essential to understand the future climate of Indian MCR. Biases in GCMs simulations are due to insufficient knowledge of parameterizations and various assumptions that are made to simulate the complex interactions between land, ocean and atmosphere. The objective of this study is to correct the bias in the Coupled Model Intercomparison Project Phase 6 (CMIP6)–GCM simulations of SAT during March, April and May months over MCR for the historical period 1985–2014 and shared socio‐economic pathways (SSPs) SSP2‐4.5 and SSP5‐8.5 for the period 2015–2100. SAT dataset of fifth‐generation reanalysis (ERA5) of the European Centre for Medium‐Range Weather Forecasts (ECMWF) is used as reference dataset to perform bias correction for the historical period. Preliminary investigation of both SAT datasets has shown that there exists considerable warm bias (1.47°C) over the MCR. Bias correction is performed using a one‐dimensional convolutional neural network (CNN‐1D) and a convolutional long short‐term memory network (CNN‐LSTM) deep learning algorithm. The performance of these algorithms is evaluated with the statistical metrics such as root‐mean‐square error (RMSE), normalized root‐mean‐square error, Nash–Sutcliffe efficiency, mean absolute error, percent bias, correlation coefficient and dynamic time warping. RMSE and percent bias were decreased to 0.35°C and 0.8% with CNN‐LSTM algorithm. The CNN‐LSTM algorithm also preserves the year‐to‐year variability of SAT. Hence, CNN‐LSTM algorithm is found to be suitable for the bias correction of GCM simulations of SAT with encouraging results."
"A Comprehensive Review of Bias in Deep Learning Models: Methods, Impacts, and Future Directions",2024,Milind Shah; Nitesh Sureja,Archives of Computational Methods in Engineering,47,W4396733184,10.1007/s11831-024-10134-2,https://openalex.org/W4396733184,,Computer science; Artificial intelligence; Risk analysis (engineering); Data science; Business,review,False,
Fairness and Biases in AI Algorithms and Interfaces,2024,Seul Lee,Proceedings of the ALISE Annual Conference,2,W4403529711,10.21900/j.alise.2024.1701,https://openalex.org/W4403529711,https://iopn.library.illinois.edu/journals/aliseacp/article/download/1701/1442,Computer science; Algorithm,article,True,"This workshop will explore various biases embedded in AI algorithms and interfaces across online platforms, including digital archives and libraries, social media, search engines, and AI-powered services like ChatGPT. Participants will critically engage with issues surrounding information credibility, transparency in content selection and appraisal, the accuracy and integrity in representation, and the intricate dynamics of information authority, format, and editorial oversight. The workshop aims to equip participants with practical strategies to identify, assess, and address biases inherent in various media, empowering them to navigate and evaluate online information across platforms with informed judgment. The workshop will begin with a 10-minute introduction, welcoming participants and outlining the goals of the session. The workshop will kick off with an interactive introductory lecture focused on various potential biases in AI algorithms. The first part, which will last about 10 to 20 minutes, will address recognizing biases in social media. Participants will analyze these biases by identifying fake news in provided articles and reflecting on their own everyday information practices on social media. After this informative segment, participants will engage in hands-on workshops aimed at applying their knowledge to recognize the types of biases they may encounter during their online information-seeking activities. They will participate in hands-on activities lasting 20 to 30 minutes, using different online tools to help identify such biases. Following the interactive hands-on workshops, participants will have the opportunity to share their insights and findings regarding the biases they discovered. The second section, which will explore biases in digital archives, digital libraries, and search engines through case studies, will last approximately one hour. It will include a 40-minute lecture that covers different types of biases in digital archives, libraries, and search engines, illustrated through case studies. After the lecture, participants will reflect on the biases presented for 10 minutes in a group discussion, sharing their thoughts and insights. Following participants' reflections on these biases, the last part of the session will focus on potential biases in ChatGPT. It will begin with a 30-minute lecture on how ChatGPT generates responses, emphasizing potential biases that could arise at each layer or step of the process. By examining a step-by-step analysis of how ChatGPT generates its answers, participants will deepen their understanding of the biases and errors inherent in the information generated by ChatGPT and its complex operational dynamics. Following this lecture, participants will engage in discussion for 20 minutes, working in small groups to identify biases and discussing necessary educational changes for themselves and their students. The workshop will conclude with a final reflection, where participants will share insights gained throughout the session and discuss how to apply their learning moving forward. The workshop will be wrapped up in a brief 10-minute summary of key points, encouraging participants to implement the strategies discussed in their own information evaluation practices. This workshop is designed for beginner-level students and researchers, as well as those interested in incorporating critical digital literacy principles into their teaching. It is open to people with diverse backgrounds and levels of expertise. No prior knowledge is required, allowing participants to embark on their learning journey with a fresh perspective and receptive attitude. The goal of this workshop is to enhance critical thinking skills related to the credibility of information while unpacking the complexities of information curation, representation, and editorial control in today's intricate digital landscape. Participants will gain a more nuanced understanding of how information is selected, represented, and potentially biased across various platforms, including social media, digital archives and libraries, search engines, and AI-powered services like ChatGPT. Through this workshop, participants will learn to effectively navigate the complexities of their information practices and develop the ability to critically assess the credibility of the online information they consume. Through engaging lectures and hands-on activities, participants will learn to identify and evaluate the inherent biases present in various media channels. They will explore practical strategies for assessing the credibility of information, recognizing potential mis/disinformation, and critically analyzing the content they encounter in their everyday information-seeking activities. By the end of the workshop, participants will be equipped with the necessary tools to navigate the digital information landscape safely and effectively, empowering them to make informed decisions not only for themselves but also in guiding others, such as children, students, or peers, in developing their own critical digital media literacy skills. This comprehensive approach aims to enhance participants’ ability to engage thoughtfully with information, ultimately fostering a more informed and critically aware community."
Stock Market Directional Bias Prediction Using ML Algorithms,2023,Ryan Chipwanya,arXiv (Cornell University),3,W4387994863,10.48550/arxiv.2310.16855,https://openalex.org/W4387994863,https://arxiv.org/pdf/2310.16855,Stock market; Machine learning; Computer science; Random forest; Artificial intelligence,preprint,True,"The stock market has been established since the 13th century, but in the current epoch of time, it is substantially more practicable to anticipate the stock market than it was at any other point in time due to the tools and data that are available for both traditional and algorithmic trading. There are many different machine learning models that can do time-series forecasting in the context of machine learning. These models can be used to anticipate the future prices of assets and/or the directional bias of assets. In this study, we examine and contrast the effectiveness of three different machine learning algorithms, namely, logistic regression, decision tree, and random forest to forecast the movement of the assets traded on the Japanese stock market. In addition, the models are compared to a feed forward deep neural network, and it is found that all of the models consistently reach above 50% in directional bias forecasting for the stock market. The results of our study contribute to a better understanding of the complexity involved in stock market forecasting and give insight on the possible role that machine learning could play in this context."
Investigating anatomical bias in clinical machine learning algorithms,2023,Jannik Skyttegaard Pedersen; Martin Sundahl Laursen; Pernille Just Vinholt; Anne Alnor; Thiusius Rajeeth Savarimuthu,,2,W4386566592,10.18653/v1/2023.findings-eacl.103,https://openalex.org/W4386566592,https://aclanthology.org/2023.findings-eacl.103.pdf,Computer science; Artificial intelligence; Machine learning; Algorithm,article,True,"Clinical machine learning algorithms have shown promising results and could potentially be implemented in clinical practice to provide diagnosis support and improve patient treatment. Barriers for realisation of the algorithms’ full potential include bias which is systematic and unfair discrimination against certain individuals in favor of others. The objective of this work is to measure anatomical bias in clinical text algorithms. We define anatomical bias as unfair algorithmic outcomes against patients with medical conditions in specific anatomical locations. We measure the degree of anatomical bias across two machine learning models and two Danish clinical text classification tasks, and find that clinical text algorithms are highly prone to anatomical bias. We argue that datasets for creating clinical text algorithms should be curated carefully to isolate the effect of anatomical location in order to avoid bias against patient subgroups."
Uses of Generative AI in the Newsroom: Mapping Journalists’ Perceptions of Perils and Possibilities,2024,Hannes Cools; Nicholas Diakopoulos,Journalism Practice,61,W4401926094,10.1080/17512786.2024.2394558,https://openalex.org/W4401926094,https://doi.org/10.1080/17512786.2024.2394558,Generative grammar; Journalism; Perception; Political science; Sociology,article,True,"This study delves into journalists' perspectives on the perils and possibilities of using generative AI-tools like ChatGPT, Bard, and DALL-E in the newsroom. Semi-structured interviews with journalists from The Netherlands, and Denmark, who self-identify as early adopters of generative AI-tools, were conducted. Results reveal 16 different specific uses of generative-AI tools across the news reporting process, mostly situated in the news production and distribution phase. The rationale for specific uses (or non-uses) of generative AI were grounded in journalistic intuitions and gut feeling. While journalists appreciate the advantages of these tools, such as improved efficiency and data handling capabilities, respondents also voice concerns about the potential for harm to journalism's accuracy and credibility, as well as ethical considerations like algorithmic bias. The study further emphasizes the necessity of providing journalists with sufficient education and algorithmic literacy in using generative AI tools, as well as the significance of ongoing monitoring and assessment to guarantee their ethical and responsible usage in journalism."
Investigating Structural Bias in Real-Coded Genetic Algorithms,2024,Kanchan Rajwar; Yogesh Kumar; Kusum Deep,Proceedings of the Genetic and Evolutionary Computation Conference Companion,4,W4401212855,10.1145/3638530.3654312,https://openalex.org/W4401212855,,Computer science; Algorithm,article,False,
On the Potential of Algorithm Fusion for Demographic Bias Mitigation in Face Recognition,2024,Jascha Kolberg; Yannik Schäfer; Christian Rathgeb; Christoph Busch,IET Biometrics,4,W4392122172,10.1049/2024/1808587,https://openalex.org/W4392122172,https://downloads.hindawi.com/journals/ietbm/2024/1808587.pdf,Biometrics; Computer science; Facial recognition system; Demographics; Face (sociological concept),article,True,"With the rise of deep neural networks, the performance of biometric systems has increased tremendously. Biometric systems for face recognition are now used in everyday life, e.g., border control, crime prevention, or personal device access control. Although the accuracy of face recognition systems is generally high, they are not without flaws. Many biometric systems have been found to exhibit demographic bias, resulting in different demographic groups being not recognized with the same accuracy. This is especially true for facial recognition due to demographic factors, e.g., gender and skin color. While many previous works already reported demographic bias, this work aims to reduce demographic bias for biometric face recognition applications. In this regard, 12 face recognition systems are benchmarked regarding biometric recognition performance as well as demographic differentials, i.e., fairness. Subsequently, multiple fusion techniques are applied with the goal to improve the fairness in contrast to single systems. The experimental results show that it is possible to improve the fairness regarding single demographics, e.g., skin color or gender, while improving fairness for demographic subgroups turns out to be more challenging."
Addressing Bias in AI Algorithms for Health Applications,2025,Kansiime Agnes,IAA Journal of Biological Sciences,1,W4412867869,10.59298/iaajb/2025/1313743,https://openalex.org/W4412867869,https://www.iaajournals.org/wp-content/uploads/2025/07/IAA-JBS-P6.pdf,Computer science; Algorithm; Artificial intelligence; Data science,article,True,"Artificial Intelligence (AI) has transformed healthcare by enhancing diagnostic accuracy, treatment personalization, and health service efficiency. However, mounting evidence reveals that AI systems can perpetuate or even amplify existing disparities related to race, gender, socioeconomic status, and geographic location. Biases often originate from imbalanced training datasets, flawed algorithm design, and unequal data collection practices. These biases have led to misdiagnoses, unequal resource allocation, and inadequate treatment recommendations, disproportionately affecting marginalized communities. This review explores the roots of algorithmic bias in healthcare AI, analyzing real-world examples such as COVID-19 triage systems and diagnostic tools that underperform in minority populations. It also examines mitigation strategies, including bias-aware data collection, algorithm design techniques, regulatory frameworks, and stakeholder engagement. Successful case studies and future research directions are presented, emphasizing fairness, transparency, and trust in computational medicine. Establishing robust, bias-resilient AI frameworks is critical to achieving equitable health outcomes and reinforcing the ethical foundations of digital health. Keywords: AI bias, health equity, algorithmic fairness, medical AI, healthcare disparities, machine learning, ethical AI, computational medicine."
"Examining the research taxonomy of artificial intelligence, deep learning &amp; machine learning in the financial sphere—a bibliometric analysis",2023,Ajithakumari Vijayappan Nair Biju; Ann Susan Thomas; J Thasneem,Quality & Quantity,56,W4367671427,10.1007/s11135-023-01673-0,https://openalex.org/W4367671427,https://link.springer.com/content/pdf/10.1007/s11135-023-01673-0.pdf,Archetype; Artificial intelligence; Extant taxon; Empirical research; Social sphere,article,True,
Modulator Bias Control Based on Bi-PID Algorithm for Optical Time and Frequency Transmission,2023,Zhuoze Zhao; Zhaohui Wang; Hao Gao; Jiameng Dong; Jiahui Cheng; Jie Zhang; Ziyang Chen; Song Yu; Bin Luo; Hong Guo,IEEE Photonics Technology Letters,4,W4388854561,10.1109/lpt.2023.3335376,https://openalex.org/W4388854561,,Algorithm; Stability (learning theory); PID controller; Dither; Computer science,article,False,"To resist the bias drift of the Mach–Zehnder modulator (MZM), we proposed and demonstrated a dither-free bias control method based on the bidirectional proportion integration differentiation (Bi-PID) algorithm. We theoretically analyzed the characteristics of time and frequency signals and made the bias point stable at the null point and quadrature point through optical power feedback and harmonic component feedback, respectively. 726.6 km transmission experiments were conducted to measure the effectiveness of the proposed modulator bias control (MBC) method. The results demonstrated that the stability of the time-sync system with the proposed MBC was better than that of the traditional method in the long-term measurement, whose time deviation (TDEV) was 2.8×10 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-12</sup> @10 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">4</sup> s. The frequency stability characterized by Allan deviation (ADEV), reaching 3.1×10 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-17</sup> @10 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">4</sup> s, outperformed traditional methods as well."
How Much Does Racial Bias Affect Mortgage Lending? Evidence from Human and Algorithmic Credit Decisions,2025,Neil Bhutta; Aurel Hizmo; Daniel Ringo,The Journal of Finance,9,W4409344392,10.1111/jofi.13444,https://openalex.org/W4409344392,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/jofi.13444,Affect (linguistics); Economics; Business; Monetary economics; Actuarial science,article,True,"ABSTRACT We assess racial discrimination in mortgage approvals using confidential data on mortgage applications. Minority applicants tend to have lower credit scores and higher leverage, and are less likely to receive algorithmic approval from race‐blind automated underwriting systems (AUS). Observable applicant‐risk factors explain most of the racial disparities in lender denials. Further, exploiting the AUS data, we show there are risk factors we do not observe, and these factors at least partially explain the residual 1 to 2 percentage point denial gaps. We conclude that differential treatment plays a more limited role in generating denial disparities than previous research suggests."
"Exploring Nexus of Social Media Algorithms, Content Creators, and Gender Bias: A Systematic Literature Review",2024,,Asian Journal of Research in Education and Social Sciences,3,W4393417718,10.55057/ajress.2024.6.1.39,https://openalex.org/W4393417718,https://myjms.mohe.gov.my/index.php/ajress/article/download/25798/14431,Nexus (standard); Social media; Content (measure theory); Computer science; Data science,article,True,"Drawing on the PRISMA framework, this study systematically investigates the dynamics between social media algorithms, content creators, and gender bias. An analysis of 18 quantitative and mixed-method studies from the Web of Science and Scopus databases, spanning 2019 to 2023, uncovers three main research trajectories: algorithms' influence on gender bias, their role in shaping content, and the interactions between algorithms, gender bias, and content creators. The review synthesizes diverse theoretical approaches and models, offering comprehensive insights into the complex nexus of algorithms, gender bias, and content creators. The application of varied research methodologies, including experiments, surveys, and content analyses, facilitates a thorough examination of algorithmic impacts. The chosen studies, focusing on different social media platforms and algorithmic features, reflect the varied interests of researchers. The findings reveal that algorithms perpetuate gender stereotypes by processing and learning content imbued with gender biases and further marginalizing gender minorities, reinforcing binary gender norms. The algorithmic curation of popular content also introduces inequities among content creators. Highlighting the need for equitable and inclusive digital environments, this review advocates for ethical content creation and algorithmic practices to mitigate gender bias and foster equality on social media platforms."
Exploring Bias and Fairness in Machine Learning Algorithms,2025,T. Venkat Narayana Rao; Mary Stephen; E. Manoj; Bhavana Sangers,Advances in computational intelligence and robotics book series,1,W4406694166,10.4018/979-8-3693-5231-1.ch014,https://openalex.org/W4406694166,,Computer science; Machine learning; Artificial intelligence; Algorithm,book-chapter,False,"When developing and implementing machine learning algorithms, bias and fairness are essential factors to consider. Systematic mistakes or inconsistencies in the data or the algorithmic decision-making process are the root cause of bias in machine learning algorithms. In algorithmic systems, ensuring fairness is crucial to preventing harm and advancing equity and justice. In order to assess the fairness of machine learning algorithms, several measures and criteria have been put forth, such as differential impact, equal opportunity, and demographic parity. This study reviews how these algorithms work and their applications in real-world scenarios such as healthcare, hiring and recruitment, financial services, and recommender systems. In conclusion, in order to guarantee fair results and minimize potential harm, machine learning algorithms must address prejudice and promote fairness. Through the chapter the authors hope the readers will be able to review the fairness on existing algorithms and become responsible AI practitioners by addressing bias and fairness."
Biased Target-tree <sup>*</sup> Algorithm with RRT <sup>*</sup> for Reducing Parking Path Planning Time,2023,Joonwoo Ahn; Minsoo Kim; Jaeheung Park,,5,W4385312479,10.1109/iv55152.2023.10186712,https://openalex.org/W4385312479,,Tree (set theory); Path (computing); Algorithm; Computer science; Combinatorics,article,False,"The target-tree <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> algorithm, which is a variant of the optimal rapidly-exploring random tree (RRT <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> ) has been proposed to reduce the parking path planning time. This algorithm pre-generates a set of backward paths (target-tree) around a parking spot and extends an RRT <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> from the initial pose until it is connected to a random sample of the target-tree. However, it is difficult to obtain the shortest (optimal) parking path within a short planning time because connected samples between the tree and the target-tree are randomly searched. To deal with this problem, this paper proposes a biased target-tree <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> algorithm with RRT <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> that searches connected random samples in a biased range near the target-tree. This range has a Gaussian distribution centered on the optimal connected sample where the shortest parking path can be obtained quickly and is obtained through supervised learning. In actual parking situations, the biased target-tree <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> algorithm obtained a shorter path with less length deviation than the original target-tree <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> algorithm within a shorter planning time."
How can we manage biases in artificial intelligence systems – A systematic literature review,2023,P. S. Varsha,International Journal of Information Management Data Insights,197,W4323041949,10.1016/j.jjimei.2023.100165,https://openalex.org/W4323041949,https://doi.org/10.1016/j.jjimei.2023.100165,Variety (cybernetics); Ambiguity; Process (computing); Automation; Order (exchange),article,True,"Artificial intelligence is similar to human intelligence, and robots in organisations always perform human tasks. However, AI encounters a variety of biases during its operational process in the online economy. The coded algorithms helps in decision-making in firms with a variety of biases and ambiguity. The study is qualitative in nature and asserts that AI biases and vulnerabilities experienced by people across industries lead to gender biases and racial discrimination. Furthermore, the study describes the different types of biases and emphasises the importance of responsible AI in firms in order to reduce the risk from AI. The implications discuss how policymakers, managers, and employees must understand biases to improve corporate fairness and societal well-being. Future research can be carryout on consumer bias, bias in job automation and bias in societal data."
Advancing Community Engaged Approaches to Identifying Structural Drivers of Racial Bias in Health Diagnostic Algorithms,2023,Jill Kuhlberg; Irene Headen; Ellis Ballard; Donald Martin,arXiv (Cornell University),4,W4378100319,10.48550/arxiv.2305.13485,https://openalex.org/W4378100319,https://arxiv.org/pdf/2305.13485,Health care; Context (archaeology); Process (computing); Health equity; Computer science,preprint,True,"Much attention and concern has been raised recently about bias and the use of machine learning algorithms in healthcare, especially as it relates to perpetuating racial discrimination and health disparities. Following an initial system dynamics workshop at the Data for Black Lives II conference hosted at MIT in January of 2019, a group of conference participants interested in building capabilities to use system dynamics to understand complex societal issues convened monthly to explore issues related to racial bias in AI and implications for health disparities through qualitative and simulation modeling. In this paper we present results and insights from the modeling process and highlight the importance of centering the discussion of data and healthcare on people and their experiences with healthcare and science, and recognizing the societal context where the algorithm is operating. Collective memory of community trauma, through deaths attributed to poor healthcare, and negative experiences with healthcare are endogenous drivers of seeking treatment and experiencing effective care, which impact the availability and quality of data for algorithms. These drivers have drastically disparate initial conditions for different racial groups and point to limited impact of focusing solely on improving diagnostic algorithms for achieving better health outcomes for some groups."
Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT,2023,Akshaj Kumar Veldanda; Fabian Grob; Shailja Thakur; Hammond Pearce; Benjamin Tan; Ramesh Karri; Siddharth Garg,arXiv (Cornell University),5,W4387559711,10.48550/arxiv.2310.05135,https://openalex.org/W4387559711,https://arxiv.org/pdf/2310.05135,Race (biology); Matching (statistics); Set (abstract data type); Politics; Psychology,preprint,True,"Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibit applicability across numerous tasks. One domain of interest is their use in algorithmic hiring, specifically in matching resumes with job categories. Yet, this introduces issues of bias on protected attributes like gender, race and maternity status. The seminal work of Bertrand &amp; Mullainathan (2003) set the gold-standard for identifying hiring bias via field experiments where the response rate for identical resumes that differ only in protected attributes, e.g., racially suggestive names such as Emily or Lakisha, is compared. We replicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude and Llama) to evaluate bias (or lack thereof) on gender, race, maternity status, pregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1) matching resumes to job categories; and (2) summarizing resumes with employment relevant information. Overall, LLMs are robust across race and gender. They differ in their performance on pregnancy status and political affiliation. We use contrastive input decoding on open-source LLMs to uncover potential sources of bias."
The Rise of Artificial Intelligence in Educational Measurement: Opportunities and Ethical Challenges,2024,Okan Bulut; Maggie Beiting-Parrish,Chinese/English Journal of Educational Measurement and Evaluation,48,W4405276448,10.59863/miql7785,https://openalex.org/W4405276448,https://www.ce-jeme.org/cgi/viewcontent.cgi?article=1090&context=journal,Transparency (behavior); Equity (law); Educational assessment; Engineering ethics; Computer science,article,True,"The integration of artificial intelligence (AI) in educational measurement has transformed assessment methods, allowing for automated scoring, swift content analysis, and personalized feedback through machine learning and natural language processing. These advancements provide valuable insights into student performance while also enhancing the overall assessment experience. However, the implementation of AI in education also raises significant ethical concerns regarding validity, reliability, transparency, fairness, and equity. Issues such as algorithmic bias and the opacity of AI decision-making processes risk perpetuating inequalities and affecting assessment outcomes. In response, various stakeholders, including educators, policymakers, and testing organizations, have developed guidelines to ensure the ethical use of AI in education. The National Council of Measurement in Education’s Special Interest Group on AI in Measurement and Education (AIME) is dedicated to establishing ethical standards and advancing research in this area. In this paper, a diverse group of AIME members examines the ethical implications of AI-powered tools in educational measurement, explores significant challenges such as automation bias and environmental impact, and proposes solutions to ensure AI’s responsible and effective use in education."
Intelligent classification and personalized recommendation of E-commerce products based on machine learning,2024,Kangming Xu; Huiming Zhou; Haotian Zheng; Mingwei Zhu; Xin Qi,Applied and Computational Engineering,45,W4396903457,10.54254/2755-2721/64/20241365,https://openalex.org/W4396903457,https://www.ewadirect.com/proceedings/ace/article/view/12391/pdf,Recommender system; Computer science; Scalability; Operability; Information overload,article,True,"With the rapid evolution of the Internet and the exponential proliferation of information, users encounter information overload and the conundrum of choice. Personalized recommendation systems play a pivotal role in alleviating this burden by aiding users in filtering and selecting information tailored to their preferences and requirements. This paper undertakes a comparative analysis between the operational mechanisms of traditional e-commerce commodity classification systems and personalized recommendation systems. It delineates the significance and application of personalized recommendation systems across e-commerce, content information, and media domains. Furthermore, it delves into the challenges confronting personalized recommendation systems in e-commerce, including data privacy, algorithmic bias, scalability, and the cold start problem. Strategies to address these challenges are elucidated. Subsequently, the paper outlines a personalized recommendation system leveraging the BERT model and nearest neighbor algorithm, specifically tailored to address the exigencies of the eBay e-commerce platform. The efficacy of this recommendation system is substantiated through manual evaluation, and a practical application operational guide and structured output recommendation results are furnished to ensure the system's operability and scalability."
A biased random-key genetic algorithm for the knapsack problem with forfeit sets,2024,Raffaele Cerulli; Ciriaco D’Ambrosio; Andrea Raiconi,Soft Computing,3,W4401166730,10.1007/s00500-024-09948-w,https://openalex.org/W4401166730,https://doi.org/10.1007/s00500-024-09948-w,Knapsack problem; Continuous knapsack problem; Key (lock); Genetic algorithm; Computer science,article,True,"Abstract This work addresses the Knapsack Problem with Forfeit Sets, a recently introduced variant of the 0/1 Knapsack Problem considering subsets of items associated with contrasting choices. Some penalty costs need to be paid whenever the number of items in the solution belonging to a forfeit set exceeds a predefined allowance threshold. We propose an effective metaheuristic to solve the problem, based on the Biased Random-Key Genetic Algorithm paradigm. An appropriately designed decoder function assigns a feasible solution to each chromosome, and improves it using some additional heuristic procedures. We show experimentally that the algorithm outperforms significantly a previously introduced metaheuristic for the problem."
Unleashing the Power of Algorithms in Antitrust Enforcement: Navigating the Boundaries of Bias and Opportunity,2023,Holli Sargeant; Teodora Groza,HAL (Le Centre pour la Communication Scientifique Directe),1,W4407602903,10.17863/cam.99555,https://openalex.org/W4407602903,https://doi.org/10.17863/cam.99555,Enforcement; Power (physics); Computer science; Algorithm; Business,article,True,"In the digital age, the intersection of data, technology, and antitrust enforcement has brought algorithms into focus as potential tools for uncovering anticompetitive practices and improving decision-making. However, concerns about algorithmic bias have raised questions about their use in this critical field. This article examines the balance between the benefits of algorithms in antitrust enforcement and the genuine concerns surrounding bias. It argues that while algorithmic bias should not be ignored, algorithms can be valuable tools when carefully designed, and the overemphasis on bias concerns stems from a lack of technical understanding. The article explores the use of algorithms in law enforcement, highlights the risks of bias, and presents how algorithmic design can mitigate these concerns. It then delves into the specific context of antitrust enforcement, explaining why the problem of algorithmic bias is less relevant compared to other regulatory areas. By offering a nuanced perspective on the potential and threats of algorithmic tools, the article contributes to the ongoing discourse on the responsible and effective utilization of algorithms in antitrust enforcement."
Unveiling the shadows: Beyond the hype of AI in education,2024,Abdulrahman M. Al-Zahrani,Heliyon,78,W4396622564,10.1016/j.heliyon.2024.e30696,https://openalex.org/W4396622564,http://www.cell.com/article/S2405844024067276/pdf,Transparency (behavior); Transformative learning; Safeguarding; Creativity; Engineering ethics,article,True,
"Artificial intelligence−powered electrochemical sensor: Recent advances, challenges, and prospects",2024,Siti Nur Ashakirin Binti Mohd Nashruddin; Faridah Hani Mohamed Salleh; Rozan Mohamad Yunus; Halimah Badioze Zaman,Heliyon,66,W4402536410,10.1016/j.heliyon.2024.e37964,https://openalex.org/W4402536410,https://doi.org/10.1016/j.heliyon.2024.e37964,Nanotechnology; Engineering; Data science; Computer science; Systems engineering,review,True,
Assessment of the Support Vector Regression and Random Forest Algorithms in the Bias Correction Process on Temperatures,2024,Brina Miftahurrohmah; Heri Kuswanto; Doni Setio Pambudi; Fatkhurokhman Fauzi; Felix Atmaja,Procedia Computer Science,6,W4396220438,10.1016/j.procs.2024.03.049,https://openalex.org/W4396220438,https://doi.org/10.1016/j.procs.2024.03.049,Computer science; Random forest; Support vector machine; Process (computing); Algorithm,article,True,"Climate information can be obtained from General circulation models (GCMs). However, this model has poor resolution, so it is necessary to do bias correction to overcome this problem. This study carried out a bias correction process using the Support Vector Regression (SVR) and Random Forest (RF) approaches. Bias correction is carried out for temperature in Indonesia using the BNU-ESM and MERRA-2 climate models, which act as observational data. The results show that the RF method (RMSE: 0.334; Correlation: 0.694; Standard Deviation: 0.582) is better than SVR (RMSE: 0.341; Correlation: 0.675; Standard Deviation: 0.588) in performing bias correction."
An overview of the effects of algorithm use on judgmental biases affecting forecasting,2024,Alvaro Chacon; Esther Kaufmann,International Journal of Forecasting,4,W4404592879,10.1016/j.ijforecast.2024.09.007,https://openalex.org/W4404592879,https://doi.org/10.1016/j.ijforecast.2024.09.007,Econometrics; Computer science; Algorithm; Economics,article,True,
Surveying Racial Bias in Facial Recognition: Balancing Datasets and Algorithmic Enhancements,2024,Andrew Sumsion; Shad Torrie; Dah-Jye Lee; Zheng Sun,Electronics,3,W4399623639,10.3390/electronics13122317,https://openalex.org/W4399623639,https://www.mdpi.com/2079-9292/13/12/2317/pdf?version=1718282302,Computer science; Machine learning; Relevance (law); Artificial intelligence; Range (aeronautics),article,True,"Facial recognition systems frequently exhibit high accuracies when evaluated on standard test datasets. However, their performance tends to degrade significantly when confronted with more challenging tests, particularly involving specific racial categories. To measure this inconsistency, many have created racially aware datasets to evaluate facial recognition algorithms. This paper analyzes facial recognition datasets, categorizing them as racially balanced or unbalanced while limiting racially balanced datasets to have each race be represented within five percentage points of all other represented races. We investigate methods to address concerns about racial bias due to uneven datasets by using generative adversarial networks and latent diffusion models to balance the data, and we also assess the impact of these techniques. In an effort to mitigate accuracy discrepancies across different racial groups, we investigate a range of network enhancements in facial recognition performance across human races. These improvements encompass architectural improvements, loss functions, training methods, data modifications, and incorporating additional data. Additionally, we discuss the interrelation of racial and gender bias. Lastly, we outline avenues for future research in this domain."
REVIEWING THE ETHICAL IMPLICATIONS OF AI IN DECISION MAKING PROCESSES,2024,Femi Osasona; Olukunle Oladipupo Amoo; Akoh Atadoga; Temitayo Oluwaseun Abrahams; Oluwatoyin Ajoke Farayola; Benjamin Samson Ayinla,International Journal of Management & Entrepreneurship Research,125,W4391898528,10.51594/ijmer.v6i2.773,https://openalex.org/W4391898528,https://fepbl.com/index.php/ijmer/article/download/773/967,Engineering ethics; Ethical decision; Management science; Political science; Psychology,article,True,"Artificial Intelligence (AI) has rapidly become an integral part of decision-making processes across various industries, revolutionizing the way choices are made. This Review delves into the ethical considerations associated with the use of AI in decision-making, exploring the implications of algorithms, automation, and machine learning. The incorporation of AI in decision-making introduces a myriad of ethical concerns that demand careful scrutiny. The opacity of algorithms raises questions about transparency, accountability, and bias. Decision-making processes driven by AI can be complex and difficult to interpret, leading to challenges in understanding how and why specific choices are made. As a result, ethical concerns emerge regarding the potential lack of transparency and accountability, especially when these decisions impact individuals or societal groups. Bias in AI algorithms poses a critical ethical challenge. Machine learning models learn from historical data, and if that data is biased, the AI system may perpetuate and even exacerbate existing biases. Addressing this challenge requires meticulous examination of training data, algorithmic design, and ongoing monitoring to ensure fairness and mitigate discrimination. The increased reliance on AI in decision-making processes also raises concerns about accountability and responsibility. When AI systems make decisions, determining who is ultimately responsible for those decisions becomes a complex ethical issue. Establishing a framework for accountability is crucial to ensure that individuals, organizations, and developers share responsibility for the outcomes of AI-driven decisions. Moreover, ethical considerations extend to the broader societal impact of AI in decision-making. Issues such as job displacement, economic inequality, and the potential concentration of power in the hands of a few require careful ethical examination. Striking a balance between technological advancement and social responsibility is paramount to ensuring that AI benefits society as a whole. In conclusion, this review highlights the ethical implications of integrating AI into decision-making processes. It underscores the need for transparency, fairness, and accountability to address concerns related to bias, responsibility, and the broader societal impact of AI-driven decisions. Ethical frameworks must evolve alongside technological advancements to foster a responsible and equitable integration of AI in decision-making processes.&#x0D; Keywords: Ethical, Implications, AI, Decision Making, Process."
Bias and Fairness Addressing Discrimination in AI Systems,2024,Padmaja Pulivarthy; Pawan Whig,Advances in human and social aspects of technology book series,36,W4403508773,10.4018/979-8-3693-4147-6.ch005,https://openalex.org/W4403508773,,Transparency (behavior); Accountability; Dignity; Equity (law); Diversity (politics),book-chapter,False,"As artificial intelligence (AI) becomes increasingly pervasive in decision-making processes across various sectors, concerns about bias and fairness have risen to the forefront of ethical discussions. This chapter delves into the complex landscape of bias in AI systems, exploring its origins, manifestations, and implications for societal equity. We examine how biases can inadvertently infiltrate algorithms through data collection, preprocessing, and model training phases, leading to discriminatory outcomes against certain demographic groups. Moreover, we explore methodologies and frameworks aimed at mitigating bias, such as fairness-aware algorithms, bias detection techniques, and diversity-enhancing approaches. Ethical considerations and regulatory efforts are also scrutinized, highlighting the urgent need for transparency and accountability in AI development. By addressing these issues comprehensively, this chapter aims to contribute to the ongoing dialogue on fostering inclusive and equitable AI systems that uphold fundamental human rights and dignity."
Biased Bi-Population Evolutionary Algorithm for Energy-Efficient Fuzzy Flexible Job Shop Scheduling with Deteriorating Jobs,2024,Libao Deng; Yingjian Zhu; Yuanzhu Di; Lili Zhang,Complex System Modeling and Simulation,5,W4396752094,10.23919/csms.2023.0021,https://openalex.org/W4396752094,https://ieeexplore.ieee.org/ielx7/9420428/10525230/10525671.pdf,Fuzzy logic; Computer science; Mathematical optimization; Evolutionary algorithm; Flow shop scheduling,article,True,"There are many studies about flexible job shop scheduling problem with fuzzy processing time and deteriorating scheduling, but most scholars neglect the connection between them, which means the purpose of both models is to simulate a more realistic factory environment. From this perspective, the solutions can be more precise and practical if both issues are considered simultaneously. Therefore, the deterioration effect is treated as a part of the fuzzy job shop scheduling problem in this paper, which means the linear increase of a certain processing time is transformed into an internal linear shift of a triangle fuzzy processing time. Apart from that, many other contributions can be stated as follows. A new algorithm called reinforcement learning based biased bi-population evolutionary algorithm (RB2EA) is proposed, which utilizes Q-learning algorithm to adjust the size of the two populations and the interaction frequency according to the quality of population. A local enhancement method which combimes multiple local search stratgies is presented. An interaction mechanism is designed to promote the convergence of the bi-population. Extensive experiments are designed to evaluate the efficacy of RB2EA, and the conclusion can be drew that RB2EA is able to solve energy-efficient fuzzy flexible job shop scheduling problem with deteriorating jobs (EFFJSPD) efficiently."
Investigating the Impact of Bias in Web Search Algorithms: Implications for Digital Inequality,2023,Haffaz Aladeen,,2,W4360616945,10.31219/osf.io/dmkar,https://openalex.org/W4360616945,https://osf.io/dmkar/download,Personalization; Computer science; Inequality; Diversity (politics); Search engine,preprint,True,"Web search algorithms are essential tools for informationretrieval in the digital age. However, concerns about the pres-ence and impact of bias in these algorithms have grown sig-nificantly in recent years. This study aims to investigate theextent to which web search algorithms exhibit biases and toexplore the implications of these biases on digital inequality.To achieve this, we conducted a comparative analysis of thesearch results from major search engines, considering factorssuch as personalization, localization, and content diversity.Our findings reveal that biases in web search algorithmsare prevalent, leading to filter bubbles, echo chambers, andunequal access to information. Furthermore, we observed astrong correlation between biased search results and the rein-forcement of stereotypes, discrimination, and digital divide.This research highlights the importance of addressing algo-rithmic bias in order to promote equal access to informationand digital equity. Future work should focus on developingdebiasing techniques and ethical guidelines for web searchalgorithms to ensure fair and unbiased information retrieval,"
Bias and ethics of AI systems applied in auditing - A systematic review,2024,Wilberforce Murikah; Jeff Kimanga Nthenge; Faith Mueni Musyoka,Scientific African,65,W4399619293,10.1016/j.sciaf.2024.e02281,https://openalex.org/W4399619293,https://doi.org/10.1016/j.sciaf.2024.e02281,Accountability; Audit; Computer science; Due diligence; Transparency (behavior),review,True,"The integration of artificial intelligence into auditing shows great potential in enhancing automation and gaining insights from complex data. However, it also presents significant ethical challenges, including algorithmic biases, transparency, accountability, and fairness. This study aimed to investigate the sources of bias and risks posed by AI systems applied in auditing and the complex downstream interactions and effects they have. The study also explored the technical and ethical guardrails proposed and recommendations for translating principles into auditing practice. A systematic methodology was employed to acquire relevant studies across scientific databases. This involved a three-step process, including a targeted search query using Boolean operators and snowballing to yield 310 preliminary publications. A systematic review process was then conducted to identify 123 relevant articles focused on AI's implications for auditing, accounting, finance, or assurance contexts. Finally, screening and filtering on research quality distilled 83 high-quality publications from the year 2018 to 2023 spanning computer science, accounting, management science, and ethics disciplines. The analysis revealed five primary sources driving technical and human biases: data deficiencies, demographic homogeneity, spurious correlations, improper comparators, and cognitive biases. It also highlighted wider issues, such as trade-offs between efficiency and diligence, erosion of human skills and judgement, data dependence risks, and privacy violations from uncontrolled personal data exploitation. The study found promising remedies, including causal modeling to enable auditors to uncover subtle biases, representative algorithmic testing to evaluate fairness, periodic auditing of AI systems, human oversight alongside automation, and embedding ethical values like fairness and accountability into system design. The study concludes that auditors play a crucial role in assessing and ensuring AI's reliable and socially beneficial integration. It recommends governance, risk assessment before deployment, ongoing performance monitoring, and policies fostering trust and collaboration to responsibly translate principles into auditing practice."
Human visual explanations mitigate bias in AI-based assessment of surgeon skills,2023,Dani Kiyasseh; Jasper Laca; Taseen F. Haque; Maxwell Otiato; Brian J. Miles; Christian von Wagner; Daniel A. Donoho; Quoc‐Dien Trinh; Animashree Anandkumar; Andrew J. Hung,npj Digital Medicine,37,W4362506590,10.1038/s41746-023-00766-2,https://openalex.org/W4362506590,https://www.nature.com/articles/s41746-023-00766-2.pdf,Credentialing; Credential; Leverage (statistics); Debiasing; Privilege (computing),article,True,
"Transforming clinical virology with AI, machine learning and deep learning: a comprehensive review and outlook",2023,Abhishek Padhi; Ashwini Agarwal; Shailendra K. Saxena; C.D.S. Katoch,VirusDisease,52,W4386928955,10.1007/s13337-023-00841-y,https://openalex.org/W4386928955,https://pmc.ncbi.nlm.nih.gov/articles/PMC10533451/pdf/13337_2023_Article_841.pdf,Transformative learning; Artificial intelligence; Deep learning; Data science; Computer science,review,True,
Exploring the potential of AI-driven optimization in enhancing network performance and efficiency,2024,Uchenna Joseph Umoga; Enoch Oluwademilade Sodiya; Ejike David Ugwuanyi; Boma Sonimitiem Jacks; Oluwaseun Augustine Lottu; Obinna Donald Daraojimba; Alexander Obaigbena,Magna Scientia Advanced Research and Reviews,62,W4392356715,10.30574/msarr.2024.10.1.0028,https://openalex.org/W4392356715,https://magnascientiapub.com/journals/msarr/sites/default/files/MSARR-2024-0028.pdf,Computer science,article,True,"The exponential growth of network complexity and data volume in modern digital ecosystems has underscored the need for innovative approaches to optimize network performance and efficiency. This paper delves into the potential of AI-driven optimization techniques in addressing this imperative. Leveraging artificial intelligence (AI) algorithms such as machine learning and deep learning, the study investigates how AI can revolutionize network management and operation to achieve higher levels of performance and reliability. Through a comprehensive review of existing literature and case studies, this paper elucidates the fundamental principles, methodologies, and applications of AI-driven optimization in diverse network environments. It examines how AI algorithms can analyze vast amounts of network data, identify patterns, and make data-driven decisions to optimize network configurations, routing protocols, and resource allocation strategies. Moreover, the study explores how AI-driven optimization can enhance network security, fault tolerance, and scalability by autonomously detecting and mitigating potential threats and vulnerabilities. The Review succinctly encapsulates the main findings and insights derived from the analysis, emphasizing the transformative potential of AI-driven optimization for network performance and efficiency enhancement. It underscores the benefits of AI-driven approaches in automating complex optimization tasks, reducing operational overhead, and adapting dynamically to changing network conditions and user demands. Additionally, the Review discusses the challenges and considerations associated with the implementation of AI-driven optimization techniques, including algorithmic bias, data privacy concerns, and ethical implications. In conclusion, the Review underscores the critical role of AI-driven optimization in addressing the evolving challenges of network management and operation. It advocates for continued research and development efforts aimed at harnessing the full potential of AI-driven optimization to unlock new levels of performance and efficiency in network infrastructures. By embracing AI-driven approaches, organizations can streamline network operations, improve user experience, and drive innovation in the digital era."
Adaptive bias-variance trade-off in advantage estimator for actor–critic algorithms,2023,Yurou Chen; Fengyi Zhang; Zhiyong Liu,Neural Networks,5,W4388049652,10.1016/j.neunet.2023.10.023,https://openalex.org/W4388049652,,Estimator; Variance (accounting); Bootstrapping (finance); Computer science; Algorithm,article,False,
Mitigating Age-related Bias in Predictive Policing Algorithms,2023,Ahmed S. Almasoud,Preprints.org,1,W4388960667,10.20944/preprints202311.1534.v1,https://openalex.org/W4388960667,https://www.preprints.org/manuscript/202311.1534/v1/download,Odds; Computer science; Parity (physics); Psychology; Econometrics,preprint,True,"This study addressed algorithmic bias in predictive policing, focusing on the Chicago Police Department&amp;#039;s Strategic Subject List (SSL) dataset. We specifically focused on identifying and mitigating age-related biases, a notably underexplored area in prior research. Our research introduced Conditional Score Recalibration as a bias mitigation strategy alongside the well-established Class Balancing technique. Conditional Score Recalibration involved reassessing and adjusting risk scores for individuals initially assigned moderately high-risk scores in the dataset. This recalibration marked such individuals as low risk if they met three conditions, namely: no prior arrests for violent offenses, no previous arrests for narcotic offenses, and having never been involved in shooting incidents. These fairness strategies were implemented on the Random Forest model, and the fairness metrics employed included Equality of Opportunity Difference, Average Odds Difference, and Demographic Parity. The results showed a significant improvement in model fairness, particularly for age biases, without compromising the model&amp;#039;s accuracy. These findings challenged the often-assumed trade-off between fairness and accuracy, underscoring the feasibility of achieving fairness without compromising accuracy."
Popularity Bias Analysis of Recommendation Algorithm Based on ABM Simulation,2023,Cizhou Yu; Dongsheng Li; Tun Lu; Yichuan Jiang,Communications in computer and information science,2,W4376485417,10.1007/978-981-99-2356-4_35,https://openalex.org/W4376485417,,Popularity; Computer science; Recommender system; Interactivity; Visualization,book-chapter,False,
Using Regularization to Identify Measurement Bias Across Multiple Background Characteristics: A Penalized Expectation–Maximization Algorithm,2024,William C. M. Belzak; Daniel J. Bauer,Journal of Educational and Behavioral Statistics,7,W4391532052,10.3102/10769986231226439,https://openalex.org/W4391532052,,Expectation–maximization algorithm; Algorithm; Regularization (linguistics); Computer science; Mathematics,article,False,"Testing for differential item functioning (DIF) has undergone rapid statistical developments recently. Moderated nonlinear factor analysis (MNLFA) allows for simultaneous testing of DIF among multiple categorical and continuous covariates (e.g., sex, age, ethnicity, etc.), and regularization has shown promising results for identifying DIF among many covariates. However, computationally inefficient estimation methods have hampered practical use of the regularized MNFLA method. We develop a penalized expectation–maximization (EM) algorithm with soft- and firm-thresholding to more efficiently estimate regularized MNLFA parameters. Simulation and empirical results show that, compared to previous implementations of regularized MNFLA, the penalized EM algorithm is faster, more flexible, and more statistically principled. This method also yields similar recovery of DIF relative to previous implementations, suggesting that regularized DIF detection remains a preferred approach over traditional methods of identifying DIF."
RRT Autonomous Detection Algorithm Based on Multiple Pilot Point Bias Strategy and Karto SLAM Algorithm,2024,Lieping Zhang; Xiaoxu Shi; Liu Tang; Yilin Wang; Jiansheng Peng; Jianchu Zou,"Computers, materials & continua/Computers, materials & continua (Print)",4,W4391480431,10.32604/cmc.2024.047235,https://openalex.org/W4391480431,https://www.techscience.com/cmc/online/detail/19880/pdf,Algorithm; Computer science; Point (geometry); Artificial intelligence; Mathematics,article,True,"A Rapid-exploration Random Tree (RRT) autonomous detection algorithm based on the multi-guide-node deflection strategy and Karto Simultaneous Localization and Mapping (SLAM) algorithm was proposed to solve the problems of low efficiency of detecting frontier boundary points and drift distortion in the process of map building in the traditional RRT algorithm in the autonomous detection strategy of mobile robot.Firstly, an RRT global frontier boundary point detection algorithm based on the multi-guide-node deflection strategy was put forward, which introduces the reference value of guide nodes' deflection probability into the random sampling function so that the global search tree can detect frontier boundary points towards the guide nodes according to random probability.After that, a new autonomous detection algorithm for mobile robots was proposed by combining the graph optimization-based Karto SLAM algorithm with the previously improved RRT algorithm.The algorithm simulation platform based on the Gazebo platform was built.The simulation results show that compared with the traditional RRT algorithm, the proposed RRT autonomous detection algorithm can effectively reduce the time of autonomous detection, plan the length of detection trajectory under the condition of high average detection coverage, and complete the task of autonomous detection mapping more efficiently.Finally, with the help of the ROS-based mobile robot experimental platform, the performance of the proposed algorithm was verified in the real environment of different obstacles.The experimental results show that in the actual environment of simple and complex obstacles, the proposed RRT autonomous detection algorithm was superior to the traditional RRT autonomous detection algorithm in the time of detection, length of detection trajectory, and average coverage, thus improving the efficiency and accuracy of autonomous detection."
"Human Resources Analytics for Public Personnel Management: Concepts, Cases, and Caveats",2023,Wonhyuk Cho; Seeyoung Choi; Hemin Choi,Administrative Sciences,54,W4318777251,10.3390/admsci13020041,https://openalex.org/W4318777251,https://www.mdpi.com/2076-3387/13/2/41/pdf?version=1675156791,Analytics; Public sector; Thematic analysis; Scope (computer science); Business intelligence,article,True,"The advancement of data technology such as machine learning and artificial intelligence has broadened the scope of human resources (HR) analytics, commonly referred to as “people analytics.” This field has seen significant growth in recent years as organizations increasingly rely on algorithm-based predictive tools for HR-related decision making. However, its application in the public sector is not yet fully understood. This study examined the concepts and practices of HR analytics through a thematic review, and proposed a five-step process (define, collect, analyze, share, and reflect) for implementation in the public sector—the process aims to assist with the integration of HR analytics in public personnel management practices. By analyzing cases in both the public and private sectors, this study identified key lessons for functional areas such as workforce planning, recruitment, HR development, and performance management. This research also identified the necessary conditions for introducing HR analytics in public organizations, including data management, staff capabilities, and acceptance, and discussed the potential challenges of privacy, integrity, algorithmic bias, and publicness."
Bias-compensated sign subband adaptive filtering algorithm with individual-weighting-factors: Performance analysis and improvement,2023,Dongxu Liu; Haiquan Zhao,Digital Signal Processing,4,W4323041532,10.1016/j.dsp.2023.103981,https://openalex.org/W4323041532,,Weighting; Algorithm; Convergence (economics); Sign (mathematics); Computer science,article,False,
Algorithmic Fairness in Financial Decision-Making: Detection and Mitigation of Bias in Credit Scoring Applications,2024,The-Chuong Trinh; Daiyang Zhang,Journal of Advanced Computing Systems,3,W4411762796,10.69987/jacs.2024.40204,https://openalex.org/W4411762796,https://doi.org/10.69987/jacs.2024.40204,Computer science; Business; Finance; Actuarial science,article,True,"This paper examines algorithmic fairness in financial decision-making systems, specifically addressing bias detection and mitigation strategies in credit scoring applications. The research investigates how machine learning algorithms deployed in credit evaluation can perpetuate or amplify existing societal biases, resulting in discriminatory outcomes for marginalized communities. Through comprehensive analysis of statistical approaches, advanced machine learning techniques, and fairness metrics, this study quantifies disparate impacts across demographic groups in contemporary credit scoring systems. The research demonstrates that pre-existing biases embedded in historical lending data can produce persistent discriminatory patterns when translated into algorithmic decision frameworks. Experimental results indicate that bias mitigation techniques, including pre-processing methods (reweighing, data augmentation), in-processing approaches (fairness constraints, adversarial debiasing), and post-processing interventions (threshold optimization, calibration) can reduce disparity measures by 15-45% while maintaining acceptable performance trade-offs. The proposed fairness-aware framework integrates multiple complementary techniques across the model development lifecycle, achieving demographic parity improvements of 23% on average across tested datasets, with accuracy reductions limited to 3-7%. The research highlights the necessity of comprehensive fairness evaluation protocols that address multiple dimensions of equity while satisfying regulatory requirements and business imperatives. These findings contribute to the development of more equitable financial technologies that promote inclusive access to credit while maintaining appropriate risk assessment capabilities."
Shaping the Future of Healthcare: Ethical Clinical Challenges and Pathways to Trustworthy AI,2025,Polat Göktaş; Andrzej Grzybowski,Journal of Clinical Medicine,92,W4407998253,10.3390/jcm14051605,https://openalex.org/W4407998253,https://www.mdpi.com/2077-0383/14/5/1605/pdf?version=1740638375,Transparency (behavior); Health care; Accountability; Corporate governance; Sustainability,article,True,"Background/Objectives: Artificial intelligence (AI) is transforming healthcare, enabling advances in diagnostics, treatment optimization, and patient care. Yet, its integration raises ethical, regulatory, and societal challenges. Key concerns include data privacy risks, algorithmic bias, and regulatory gaps that struggle to keep pace with AI advancements. This study aims to synthesize a multidisciplinary framework for trustworthy AI in healthcare, focusing on transparency, accountability, fairness, sustainability, and global collaboration. It moves beyond high-level ethical discussions to provide actionable strategies for implementing trustworthy AI in clinical contexts. Methods: A structured literature review was conducted using PubMed, Scopus, and Web of Science. Studies were selected based on relevance to AI ethics, governance, and policy in healthcare, prioritizing peer-reviewed articles, policy analyses, case studies, and ethical guidelines from authoritative sources published within the last decade. The conceptual approach integrates perspectives from clinicians, ethicists, policymakers, and technologists, offering a holistic “ecosystem” view of AI. No clinical trials or patient-level interventions were conducted. Results: The analysis identifies key gaps in current AI governance and introduces the Regulatory Genome—an adaptive AI oversight framework aligned with global policy trends and Sustainable Development Goals. It introduces quantifiable trustworthiness metrics, a comparative analysis of AI categories for clinical applications, and bias mitigation strategies. Additionally, it presents interdisciplinary policy recommendations for aligning AI deployment with ethical, regulatory, and environmental sustainability goals. This study emphasizes measurable standards, multi-stakeholder engagement strategies, and global partnerships to ensure that future AI innovations meet ethical and practical healthcare needs. Conclusions: Trustworthy AI in healthcare requires more than technical advancements—it demands robust ethical safeguards, proactive regulation, and continuous collaboration. By adopting the recommended roadmap, stakeholders can foster responsible innovation, improve patient outcomes, and maintain public trust in AI-driven healthcare."
Innovative recruitment strategies in the IT sector: A review of successes and failures,2024,Funmilayo Aribidesi Ajayi; Chioma Ann Udeh,Magna Scientia Advanced Research and Reviews,43,W4394064896,10.30574/msarr.2024.10.2.0057,https://openalex.org/W4394064896,https://magnascientiapub.com/journals/msarr/sites/default/files/MSARR-2024-0057.pdf,Business; Engineering ethics; Engineering,review,True,"This study systematically reviews and analyzes the impact of innovative recruitment strategies on diversity and inclusion within the Information Technology (IT) sector. Given the rapid technological advancements and the increasing demand for skilled professionals, IT firms are at the forefront of adopting novel recruitment practices. The main objective of this research is to explore how these innovations influence the diversity of the IT workforce and to identify best practices for promoting inclusivity. Employing a systematic literature review and content analysis, the study examines peer-reviewed articles, industry reports, and relevant grey literature published within the last decade. Key insights reveal a significant shift towards the use of artificial intelligence (AI) and data analytics in recruitment processes, aiming to enhance efficiency and reduce biases. However, challenges such as potential algorithmic bias, privacy concerns, and the need for human oversight in automated recruitment are identified. The study concludes with actionable recommendations for IT firms, recruiters, and policymakers to ensure that recruitment innovations contribute positively to workforce diversity and inclusivity. It advocates for a balanced approach that leverages technology while addressing ethical considerations and promoting equal opportunities for underrepresented groups. Future research directions include exploring the long-term effects of these recruitment strategies on organizational performance and further examining the ethical implications of AI in recruitment. This study contributes to a deeper understanding of the evolving landscape of IT recruitment and its implications for diversity and inclusion initiatives."
Improved Multilayer Perceptron Neural Networks Weights and Biases Based on The Grasshopper optimization Algorithm to Predict Student Performance on Ambient Learning,2023,Mercy K. Michira; Richard Rimiru; Waweru Mwangi,,4,W4379352342,10.1145/3583788.3583797,https://openalex.org/W4379352342,https://dl.acm.org/doi/pdf/10.1145/3583788.3583797,Perceptron; Artificial neural network; Computer science; Algorithm; Backpropagation,article,True,The classification accuracy of a multi-layer Perceptron Neural Networks depends on the selection of its parameters such the connection weights and biases. Generating an optimal value of these parameters requires a suitable algorithm to train the multilayer perceptron neural networks. This paper presents swam based Grasshopper optimization algorithm that optimizes the connection weights and biases of Multilayer Perceptron Neural Network. Grasshopper optimization algorithm is a swarm-based metaheuristic algorithm applied for accurate learning of Multilayer Perceptron Neural Networks. The proposed Multilayer Layer Perceptron Neural Networks based on the Grasshopper Optimization Algorithm was validated using a Genetic algorithm and Backpropagation algorithm this algorithm has proved to perform satisfactorily performance by escaping local optimal and its fast convergence.
Ethical Considerations and Challenges in the Integration of Artificial Intelligence in Education: A Systematic Review,2024,Muhammad Tahir Khan Farooqi; Ishaq Amanat; Sher Muhammad Awan,Journal of excellence in management sciences.,29,W4402841691,10.69565/jems.v3i4.314,https://openalex.org/W4402841691,https://doi.org/10.69565/jems.v3i4.314,Engineering ethics; Psychology; Management science; Engineering,review,True,"This systematic review examines those challenges in light of data privacy, algorithmic bias, ethical implications, technological hurdles, and acceptance of AI by educators and students. First, data privacy should be a primary concern, as AI systems require extensive data, bringing up the potential for breach and misuse. Secondly, there must be a robust mechanism concerning data protection and against the application of GDPR. Another critical point is algorithm bias: biased training data sets may lead to discriminative decisions that will increase inequalities in education. It talks about AI's impact on teachers and classroom dynamics because the takeover of responsibilities may lower the intensity of necessary human contact. From a technical perspective, there is so much infrastructure and expertise required that too many educational institutions lack, especially in developing countries. In addition, educators themselves may feel that the change resists and fears job loss and therefore acts as a deterrent to AI integration. The review underscores the imperative for extensive training of teachers to support enabling the integration of AI. It now demands a collaborative effort on the part of all stakeholders to maximize the gains and reduce the drawbacks of AI in educational aspects. Continuous research in, policy-making for, and ethical guidelines on AI are required to benefit all aspects of education equitably and effectively."
Biased random-key genetic algorithms: A tutorial with applications,2024,Thiago F. Noronha; Celso C. Ribeiro,,3,W4401285165,10.1145/3665065.3665083,https://openalex.org/W4401285165,,Key (lock); Computer science; Algorithm; Theoretical computer science; Computer security,article,False,
"A critical review towards artificial general intelligence: Challenges, ethical considerations, and the path forward",2024,Sedat Sonko; Adebunmi Okechukwu Adewusi; Ogugua Chimezie; Shedrack Onwusinkwue; Akoh Atadoga,World Journal of Advanced Research and Reviews,64,W4392894584,10.30574/wjarr.2024.21.3.0817,https://openalex.org/W4392894584,https://wjarr.com/sites/default/files/WJARR-2024-0817.pdf,Transparency (behavior); Accountability; Engineering ethics; Computer science; Scrutiny,review,True,"The pursuit of Artificial General Intelligence (AGI) has captivated researchers and industry leaders alike, promising a future where machines possess human-like cognitive abilities. However, this ambitious endeavor is fraught with multifaceted challenges and ethical dilemmas that necessitate careful examination. This critical review surveys the landscape of AGI research, identifying key hurdles and ethical considerations while outlining potential pathways forward. Firstly, technical challenges loom large on the path to AGI. These encompass fundamental problems such as developing robust learning algorithms capable of generalizing across diverse domains, as well as engineering systems that can exhibit adaptive and autonomous behavior akin to human intelligence. Additionally, ensuring the safety and reliability of AGI systems presents a formidable obstacle, with concerns ranging from algorithmic bias to the potential for catastrophic outcomes in unanticipated scenarios. Ethical considerations permeate every facet of AGI development and deployment. Questions of accountability, transparency, and control surface as central concerns, as the implications of relinquishing decision-making authority to autonomous systems raise profound ethical dilemmas. Moreover, the socio-economic ramifications of widespread AGI adoption, including job displacement and inequality, demand careful scrutiny and proactive mitigation strategies. Navigating these challenges requires a concerted effort from interdisciplinary stakeholders. Collaboration between computer scientists, ethicists, policymakers, and the public is essential to establish robust frameworks for the responsible development and deployment of AGI. Moreover, fostering an inclusive dialogue that prioritizes ethical principles and societal values is paramount in shaping a future where AGI augments human capabilities while safeguarding against potential risks. While the pursuit of AGI holds immense promise, its realization demands a holistic approach that addresses technical challenges alongside ethical considerations. By charting a path forward that prioritizes safety, transparency, and ethical governance, we can harness the transformative potential of AGI while ensuring its alignment with human values and interests."
IoT Service Composition — An Estimation of Distribution Algorithm with Adaptive Bias,2023,Fengyang Sun; Hui Ma; Gang Chen; Sven Hartmann,,2,W4387005615,10.1109/cec53210.2023.10254066,https://openalex.org/W4387005615,,Estimation of distribution algorithm; Computer science; Probabilistic logic; Heuristic; Stability (learning theory),article,False,"Service composition in Internet of Things (SCIoT), as an emerging topic in service computing, aims to select optimal services to complete user requests according to various user requirements such as minimizing energy consumption and response time. To solve this NP-hard problem, numerous heuristic methods, e.g., local search and population-based algorithms, have been proposed, wherein Estimation of Distribution Algorithm (EDA) gains increasing attention because of its explicit global probabilistic nature. However, existing EDAs increase solution diversity by using fixed bias, yet interfere the stability of the learned distribution in the later stage of optimization. Therefore, this paper proposes an EDA with an adaptive bias strategy (EDA-AdaBias) to solve the service composition in IoT problem. The decreasing bias value is added onto the probability values for all choices of each solution variable over generations, which improves diversity of sampled solutions and avoids dramatic change of constructed distribution. Experiments indicate that EDA-AdaBias presents promising performance compared to other competitive methods on this problem."
Optimizing public transport system using biased random-key genetic algorithm,2024,João Luiz Alves Oliveira; André L. L. Aquino; Rian G. S. Pinheiro; Bruno Nogueira,Applied Soft Computing,3,W4394566100,10.1016/j.asoc.2024.111578,https://openalex.org/W4394566100,,Computer science; Key (lock); Genetic algorithm; Algorithm; Machine learning,article,False,
ETHICAL IMPLICATIONS OF AI IN FINANCIAL DECISION – MAKING: A REVIEW WITH REAL WORLD APPLICATIONS,2024,Oluwatobi Opeyemi Adeyelu; Chinonye Esther Ugochukwu; Mutiu Alade Shonibare,International Journal of Applied Research in Social Sciences,24,W4394883974,10.51594/ijarss.v6i4.1033,https://openalex.org/W4394883974,https://fepbl.com/index.php/ijarss/article/download/1033/1256,Business; Finance,review,True,"This study delves into the ethical implications of Artificial Intelligence (AI) in financial decision-making, exploring the transformative impact of AI technologies on the financial services sector. Through a comprehensive literature review, the research highlights the dual nature of AI's integration into finance, showcasing both its potential to enhance operational efficiency and decision accuracy and the ethical challenges it introduces. These challenges include concerns over data privacy, algorithmic bias, and the potential for systemic risks, underscoring the need for robust ethical frameworks and regulatory standards. The study emphasizes the importance of a multidisciplinary approach to AI development and deployment, advocating for collaboration among technologists, ethicists, policymakers, and end-users to ensure that AI technologies are aligned with societal values and ethical principles. Future directions for research are identified, focusing on the development of adaptive ethical guidelines, methodologies for embedding ethical principles into AI systems, and the investigation of AI's long-term impact on market dynamics and consumer behaviour. This research contributes valuable insights into the ethical integration of AI in finance, offering recommendations for ensuring that AI technologies are utilized in a manner that is both ethically sound and conducive to the advancement of the financial services industry.&#x0D; Keywords: Artificial Intelligence, Financial Decision-Making, Ethical Implications, Algorithmic Bias, Data Privacy, Regulatory Standards, Multidisciplinary Approach."
Correction: Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms,2023,Benedetta Giovanola; Simona Tiribelli,AI & Society,6,W4384934767,10.1007/s00146-023-01722-0,https://openalex.org/W4384934767,https://link.springer.com/content/pdf/10.1007/s00146-023-01722-0.pdf,Health care; Performing arts; Artificial intelligence; Computer science; Machine learning,article,True,
ChatGPT: friend or foe?,2023,The Lancet Digital Health,The Lancet Digital Health,249,W4319301633,10.1016/s2589-7500(23)00023-7,https://openalex.org/W4319301633,https://doi.org/10.1016/s2589-7500(23)00023-7,Harm; Realm; Computer science; Scripting language; Health care,editorial,True,"You would have been hard-pressed to miss the storm surrounding ChatGPT (Chat Generative Pre-trained Transformer) over the past few months. News outlets and social media have been abuzz with reports on the chatbot developed by OpenAI. In response to a written prompt, ChatGPT can compose emails, write computer code, and even craft movie scripts. Researchers have also demonstrated its competency to pass medical licensing exams. But excitement has been matched by a swathe of ethical concerns that could—and perhaps should—limit its adoption ChatGPT is powered by a refined version of the large language model (LLM) GPT-3.5. Its base model GPT-3 was trained on articles, websites, books, and written conversations, but a process of fine-tuning (including optimisation for dialogue) enables ChatGPT to respond to prompts in a conversational way. In the realm of health care, Sajan B Patel and Kyle Lam illustrated ChatGPT's ability to generate a patient discharge summary from a brief prompt. Automating this process could reduce delays in discharge from secondary care without compromising on detail, freeing up valuable time for doctors to invest in patient care and developmental training. A separate study also tested its ability to simplify radiology reports, with the generated reports being deemed overall factually correct, complete, and with low perceived risk of harm to patients. But in both cases, errors were evident. In the discharge summary example provided by Patel and Lam, ChatGPT added extra information to the summary that was not included in their prompt. Likewise, the radiology report study identified potentially harmful mistakes such as missing key medical findings. Such errors signal that if implemented in clinical practice, manual checks of automated outputs would be required. The limitations of ChatGPT are known. By OpenAI's own admission, ChatGPT's output can be incorrect or biased, such as citing article references that do not exist or perpetuating sexist stereotypes. It could also respond to harmful instructions, such as to generate malware. OpenAI set up guardrails to minimise the risks, but users have found ways around these, and as ChatGPT's outputs could be used to train future iterations of the model, these errors might be recycled and amplified. OpenAI have asked users to report inappropriate responses in order to help improve the model, but this has been met with criticism, as it's often people disproportionately affected by algorithmic bias (such as those from marginalised communities) who are expected to help find solutions. Michael Liebrenz and colleagues opine that although ChatGPT could serve to democratise knowledge sharing as it can receive and output text in multiple languages (beneficial for non-native speakers publishing in English), inaccuracies in generated text could fuel the spread of misinformation. These concerns have serious implications for the integrity of the scientific record, given the risk of introducing not only errors but also plagiarised content into publications. This could result in future research or health policy decisions being made on the basis of false information. Last month, the World Association of Medical Editors published its recommendations on the use of ChatGPT and other chatbots in scholarly publications, one of which is that journal editors need new tools to detect AI-generated or modified content. Indeed, an AI output detector was shown to be better at distinguishing between original and ChatGPT-generated research article abstracts than a plagiarism detector and human reviewers, but did falsely flag an original abstract as being “fake”. Technology is evolving, and editorial policies need to evolve too. Elsevier has introduced a new policy on the use of AI and AI-assisted technologies in scientific writing, stipulating that use should be limited to improving readability and language of the work, and should be declared in the manuscript; authors should do manual checks of any AI-generated output; and these tools should not be listed or cited as an author or co-author as they cannot take on the responsibilities that authorship entails (such as being accountable for the published work). Widespread use of ChatGPT is seemingly inevitable but in its current iteration careless, unchecked use could be a foe to both society and scholarly publishing. More forethought and oversight on model training are needed, as is investment in robust AI output detectors. ChatGPT is a game changer, but we're not quite ready to play."
Time–Frequency Signal Integrity Monitoring Algorithm Based on Temperature Compensation Frequency Bias Combination Model,2024,Yu Guo; Zongnan Li; Hang Gong; Jing Peng; Gang Ou,Remote Sensing,3,W4394963276,10.3390/rs16081453,https://openalex.org/W4394963276,https://www.mdpi.com/2072-4292/16/8/1453/pdf?version=1713537964,Computer science; Robustness (evolution); Signal integrity; Time–frequency analysis; Adaptability,article,True,"To ensure the long-term stable and uninterrupted service of satellite navigation systems, the robustness and reliability of time–frequency systems are crucial. Integrity monitoring is an effective method to enhance the robustness and reliability of time–frequency systems. Time–frequency signals are fundamental for integrity monitoring, with their time differences and frequency biases serving as essential indicators. These indicators are influenced by the inherent characteristics of the time–frequency signals, as well as the links and equipment they traverse. Meanwhile, existing research primarily focuses on only monitoring the integrity of the time–frequency signals’ output by the atomic clock group, neglecting the integrity monitoring of the time–frequency signals generated and distributed by the time–frequency signal generation and distribution subsystem. This paper introduces a time–frequency signal integrity monitoring algorithm based on the temperature compensation frequency bias combination model. By analyzing the characteristics of time difference measurements, constructing the temperature compensation frequency bias combination model, and extracting and monitoring noise and frequency bias features from the time difference measurements, the algorithm achieves comprehensive time–frequency signal integrity monitoring. Experimental results demonstrate that the algorithm can effectively detect, identify, and alert users to time–frequency signal faults. Additionally, the model and the integrity monitoring parameters developed in this paper exhibit high adaptability, making them directly applicable to the integrity monitoring of time–frequency signals across various links. Compared with traditional monitoring algorithms, the algorithm proposed in this paper greatly improves the effectiveness, adaptability, and real-time performance of time–frequency signal integrity monitoring."
Analytical quaternion-based bias estimation algorithm for fast and accurate stationary gyro-compassing,2024,Hamed Mohammadkarimi; Saadat Pour Mozafari; Mohammad Alizadeh,Scientific Reports,3,W4400463011,10.1038/s41598-024-66282-9,https://openalex.org/W4400463011,https://www.nature.com/articles/s41598-024-66282-9.pdf,Quaternion; Azimuth; Kalman filter; Computer science; Inertial navigation system,article,True,"Abstract This work introduces a novel approach to Strapdown Inertial Navigation System (SINS) alignment, distinct from recursive methods like Kalman filtering. The proposed methodology expedites bias error calculations by utilizing quaternion-based analytical relationships, which bypasses the slow convergence behavior associated with recursive algorithms, particularly in azimuth angle error estimation. In addition, the proposed approach demonstrates comparable accuracy to traditional fine alignment methods. Simulations and experiments validate that in contrast to the 10-min time requirement of traditional fine alignment methods (for azimuth angle estimation in stationary conditions), the proposed approach achieves the same accuracy within 20 s. However, limitations exist as the algorithm is applicable only in stationary conditions, and necessitating a high-grade IMU capable of measuring the earth’s rotation rate."
Biased regression algorithms in the quaternion domain,2024,Rosa M. Fernández-Alcalá; J.D. Jiménez-López; Jesús Navarro-Moreno; Juan Carlos Ruiz-Molina,Journal of the Franklin Institute,2,W4392906913,10.1016/j.jfranklin.2024.106785,https://openalex.org/W4392906913,https://doi.org/10.1016/j.jfranklin.2024.106785,Quaternion; Algorithm; Regression; Domain (mathematical analysis); Computer science,article,True,"The ill-conditioned matrix problem in quaternion linear regression models is addressed in this paper and several dimension-reduction based regression methods for circumventing this problem are suggested. The algorithms are formulated in a general way and can be easily adapted to different scenarios: widely linear, semi-widely linear and strictly linear processing, in accordance with the properness properties presented by quaternion random vectors. A comparison with existing solutions is carried out by using both laboratory data and a color face database."
Microsoft Copilot and Anthropic Claude AI in education and library service,2024,Adebowale Jeremy Adetayo; Mariam Oyinda Aborisade; Basheer Abiodun Sanni,Library Hi Tech News,31,W4390912975,10.1108/lhtn-01-2024-0002,https://openalex.org/W4390912975,,Computer science; Transparency (behavior); Context (archaeology); Special education; Corporate governance,article,False,"Purpose This study aims to explore the collaborative potential of Microsoft Copilot and Anthropic Claude AI as an assistive technology in education and library services. The research delves into technical architectures and various use cases for both tools, proposing integration strategies within educational and library environments. The paper also addresses challenges such as algorithmic bias, hallucination and data rights. Design/methodology/approach The study used a literature review approach combined with the proposal of integration strategies across education and library settings. Findings The collaborative framework between Copilot and Claude AI offers a comprehensive solution for transforming education and library services. The study identifies the seamless combination of real-time internet access, information retrieval and advanced comprehension features as key findings. In addition, challenges such as algorithmic bias and data rights are addressed, emphasizing the need for responsible AI governance, transparency and continuous improvement. Originality/value Contribute to the field by exploring the unique collaborative framework of Copilot and Claude AI in a specific context, emphasizing responsible AI governance and addressing existing gaps."
Biased thermodynamics can explain the behaviour of smart optimization algorithms that work above the dynamical threshold,2023,Angelo Giorgio Cavaliere; Federico Ricci‐Tersenghi,arXiv (Cornell University),3,W4361193629,10.48550/arxiv.2303.14879,https://openalex.org/W4361193629,https://arxiv.org/pdf/2303.14879,Ergodicity; Measure (data warehouse); Constraint satisfaction problem; Dynamical systems theory; Mathematics,preprint,True,"Random constraint satisfaction problems can display a very rich structure in the space of solutions, with often an ergodicity breaking -- also known as clustering or dynamical -- transition preceding the satisfiability threshold when the constraint-to-variables ratio $α$ is increased. However, smart algorithms start to fail finding solutions in polynomial time at some threshold $α_{\rm alg}$ which is algorithmic dependent and generally bigger than the dynamical one $α_d$. The reason for this discrepancy is due to the fact that $α_d$ is traditionally computed according to the uniform measure over all the solutions. Thus, while bounding the region where a uniform sampling of the solutions is easy, it cannot predict the performance of off-equilibrium processes, that are still able of finding atypical solutions even beyond $α_d$. Here we show that a reconciliation between algorithmic behaviour and thermodynamic prediction is nonetheless possible at least up to some threshold $α_d^{\rm opt}\geqα_d$, which is defined as the maximum value of the dynamical threshold computed on all possible probability measures over the solutions. We consider a simple Monte Carlo-based optimization algorithm, which is restricted to the solution space, and we demonstrate that sampling the equilibrium distribution of a biased measure improving on $α_d$ is still possible even beyond the ergodicity breaking point for the uniform measure, where other algorithms hopelessly enter the out-of-equilibrium regime. The conjecture we put forward is that many smart algorithms sample the solution space according to a biased measure: once this measure is identified, the algorithmic threshold is given by the corresponding ergodicity-breaking transition."
"The integration of artificial intelligence into clinical medicine: Trends, challenges, and future directions",2025,Prasanna Sakthi Aravazhi; Praveen Thenraj Gunasekaran; N. Benjamin; Andy Thai; Kiran Kishor Chandrasekar; Nikhil Deep Kolanu; Priyadarshi Prajjwal; Yogesh Tekuru; L. Brito; Pugazhendi Inban,Disease-a-Month,25,W4408858259,10.1016/j.disamonth.2025.101882,https://openalex.org/W4408858259,,Artificial intelligence; Data science; Computer science,review,False,
Predictive policing and algorithmic fairness,2023,Tzu-Wei Hung; Chun-Ping Yen,Synthese,28,W4379473793,10.1007/s11229-023-04189-0,https://openalex.org/W4379473793,https://link.springer.com/content/pdf/10.1007/s11229-023-04189-0.pdf,Philosophy of language; Causation; Context (archaeology); Corporate governance; Predictive power,article,True,"Abstract This paper examines racial discrimination and algorithmic bias in predictive policing algorithms (PPAs), an emerging technology designed to predict threats and suggest solutions in law enforcement. We first describe what discrimination is in a case study of Chicago’s PPA. We then explain their causes with Broadbent’s contrastive model of causation and causal diagrams. Based on the cognitive science literature, we also explain why fairness is not an objective truth discoverable in laboratories but has context-sensitive social meanings that need to be negotiated through democratic processes. With the above analysis, we next predict why some recommendations given in the bias reduction literature are not as effective as expected. Unlike the cliché highlighting equal participation for all stakeholders in predictive policing, we emphasize power structures to avoid hermeneutical lacunae. Finally, we aim to control PPA discrimination by proposing a governance solution—a framework of a social safety net."
Revolutionizing Healthcare: How Machine Learning is Transforming Patient Diagnoses - a Comprehensive Review of AI's Impact on Medical Diagnosis,2023,Ahmad Yousaf Gill; Ayesha Saeed; Saad Rasool; Ali Husnain; Hafiz Khawar Hussain,Journal Of World Science,46,W4388001743,10.58344/jws.v2i10.449,https://openalex.org/W4388001743,https://jws.rivierapublishing.id/index.php/jws/article/download/449/1018,Health care; Transformative learning; Compassion; Autonomy; Engineering ethics,review,True,"The integration of machine learning into healthcare heralds a new era where the convergence of technology and human compassion reshapes the very essence of healing. This monumental shift transcends mere technological advancement; it represents a profound evolution in patient care. By unraveling intricate patterns within medical data, machine learning empowers healthcare professionals with early disease detection and precise risk assessment, augmenting human intuition rather than replacing it. This synergy between AI-driven insights and human expertise has led to remarkable achievements, from redefining radiological interpretations to foreseeing infectious disease outbreaks, painting a future where healthcare is not only precise but profoundly patient-centered. Yet, amidst these groundbreaking advancements, ethical considerations stand as pillars guiding responsible innovation. Upholding patient autonomy, ensuring data privacy, and addressing algorithmic bias are essential to maintain trust and integrity. As we navigate this transformative path, the promise of a healthcare landscape where healing becomes a symphony of technology and tradition becomes evident. It is a future where the well-being and hopes of millions are at the core, promising a brighter, more compassionate tomorrow for healthcare, where every diagnosis, treatment, and act of care resonates with the harmony of human expertise and technological marvels."
Navigating the Ethical Challenges of Artificial Intelligence in Higher Education: An Analysis of Seven Global AI Ethics Policies,2023,Zouhaier Slimi; Beatriz Villarejo-Carballido,TEM Journal,127,W4381570295,10.18421/tem122-02,https://openalex.org/W4381570295,https://www.temjournal.com/content/122/TEMJournalMay2023_590_602.pdf,Accountability; Transparency (behavior); Software deployment; Openness to experience; Documentation,article,True,"AI use in higher education raises ethical concerns that must be addressed. Biased algorithms pose a significant threat, especially if used in admission or grading processes, as they could have devastating effects on students. Another issue is the displacement of human educators by AI systems, and there are concerns about transparency and accountability as AI becomes more integrated into decision-making processes. This paper examined three AI objectives related higher education: biased algorithms, AI and decision-making, and human displacement. Discourse analysis of seven AI ethics policies was conducted, including those from UNESCO, China, the European Commission, Google, MIT, Sanford HAI, and Carnegie Mellon. The findings indicate that stakeholders must work together to address these challenges and ensure responsible AI deployment in higher education while maximizing its benefits. Fair use and protecting individuals, especially those with vulnerable characteristics, are crucial. Gender bias must be avoided in algorithm development, learning data sets, and AI decision-making. Data collection, labeling, and algorithm documentation must be of the highest quality to ensure traceability and openness. Universities must study the ethical, social, and policy implications of AI to ensure responsible development and deployment. The AI ethics policies stress responsible AI development and deployment, with a focus on transparency and accountability. Making AI systems more transparent and answerable may reduce the adverse effects of displacement. In conclusion, AI must be considered ethically in higher education, and stakeholders must ensure that AI is used responsibly, fairly, and in a way that maximizes its benefits while minimizing its risks."
Social Bias in AI: Re-coding Innovation through Algorithmic Political Capitalism,2025,S. Carter; John G. Dale,AI & Society,1,W4413135319,10.1007/s00146-025-02540-2,https://openalex.org/W4413135319,https://link.springer.com/content/pdf/10.1007/s00146-025-02540-2.pdf,Capitalism; Transparency (behavior); Accountability; Sociotechnical system; Corporate governance,article,True,"Abstract This research examines the social dynamics underpinning algorithmic bias, proposing a framework for addressing these issues through the lens of algorithmic political capitalism. We explore how socio-technical-ecological relations of power often reproduce harmful algorithmic effects, including social bias, data exploitation in the knowledge economy, prejudiced predictions, and unexamined user biases that obscure power asymmetries and harm society. Building on complexity theory, particularly Morçöl’s definition of public policy as a dynamic system with co-evolving relationships between actors and systems, we analyze the challenges and opportunities to mitigate these harms within a multilayered framework. Our framework extends Keller and Block’s concept of ‘technology-dependent political capitalism’, incorporating mechanisms to ensure government assistance is conditional, allowing bicameral governance in supported corporations, and empowering local and state authorities to hold organizations accountable. Finally, we highlight the crucial roles of transparency, accountability, and democratization in fostering meaningful innovation, and argue that addressing algorithmic bias and the inequities of the knowledge economy requires a nuanced understanding of the interplay between public policy, technological systems, and societal structures. Our proposals aim to reshape the socio-technical-ecological landscape, creating conditions for algorithmic innovation that align with democratic values and equitable societal progress, while mitigating systemic violence."
“Inventor’s Bias” at Work: When Low-Performing Algorithms Seem Fair,2023,Maya J. Cratsley; Nathanael J. Fast,International Journal of Human-Computer Interaction,2,W4381857282,10.1080/10447318.2023.2224954,https://openalex.org/W4381857282,,Perception; Context (archaeology); Product (mathematics); Phenomenon; Politics,article,False,"This article introduces the ""Inventor's Bias Effect,"" the propensity for inventors to be over-optimistic about the positive features and uses of the products they create. We explore this phenomenon in the context of decision-making algorithms by conducting two online studies (N = 1001) where subjects were asked to either create or evaluate an AI-based tool that can automate human resource decisions in an organization. Study 1 revealed that individuals in the role of inventor perceived a low-performing algorithm they created as fairer relative to the ratings of other stakeholders (CEOs, employees, and the general public). The tendency for these ""inventors"" to personally identify with the products they created mediated this effect. Study 2 showed that inventors' perceptions of fairness of the algorithms they created translated into an increased desire for the organization to continue using their product, even though it was inaccurate for a third of all decisions. This research demonstrates how stakeholders' relations to algorithms may encourage biased decision making and highlights the need for caution in organizational and political decision-making processes."
A Fairness Approach to Mitigating Racial Bias of Credit Scoring Models by Decision Tree and the Reweighing Fairness Algorithm,2023,Jen–Ying Shih; Ze-Han Chin,,3,W4383503216,10.1109/iceib57887.2023.10170339,https://openalex.org/W4383503216,,Computer science; Decision tree; Credit score; Algorithm; Fairness measure,article,False,"Credit scoring models have been widely applied by financial institutions, Peer to Peer (P2P) lending service providers, and Buy Now Pay Later (BNPL) service providers to evaluate their customers' financial status. Therefore, it has a large impact on consumer financing activities. However, unfair evaluation may occur as the development of credit scoring models contains biased judgments (e.g., racial bias), which deteriorates users' credit access ability. Thus, we study the feasibility of mitigating racial bias in developing a credit scoring model. By using a data set provided by a P2P lending platform, LendingClub, we integrated the C5.0 decision tree algorithm and the reweighing fairness algorithm to develop credit scoring models with cost-sensitive modeling concepts. Multi-class fair credit scoring evaluation was also studied in terms of performance indices, including accuracy, average cost, and unfairness metrics. The results demonstrated that the reweighing fairness algorithm reduced the unfairness and average cost of models. In addition, combining the fairness algorithm and cost-sensitive modeling minimized the average cost of models while maintaining the functionality of the fairness algorithm."
"Province of Origin, Decision‐Making Bias, and Responses to Bureaucratic Versus Algorithmic Decision‐Making",2025,Ge Wang; Zhejun Zhang; Shenghua Xie; Yue Guo,Public Administration Review,1,W4406869107,10.1111/puar.13928,https://openalex.org/W4406869107,,Bureaucracy; Political science; Law; Politics,article,False,"ABSTRACT As algorithmic decision‐making (ADM) becomes prevalent in certain public sectors, its interaction with traditional bureaucratic decision‐making (BDM) evolves, especially in contexts shaped by regional identities and decision‐making biases. To explore these dynamics, we conducted two survey experiments within traffic enforcement scenarios, involving 4816 participants across multiple provinces. Results indicate that non‐native residents perceived ADM as fairer and more acceptable than BDM when they did not share a province of origin with local bureaucrats. Both native and non‐native residents showed a preference for ADM in the presence of bureaucratic and algorithmic biases but preferred BDM when such biases were absent. When bureaucratic and algorithmic biases coexisted, the lack of a shared province of origin further reinforced non‐native residents' perception of ADM as fairer and more acceptable than BDM. Our findings reveal the complex interplay among province of origin, decision‐making biases, and responses to different decision‐making approaches."
Biased random-key genetic algorithm for the job sequencing and tool switching problem with non-identical parallel machines,2023,Leonardo C.R. Soares; Marco Antonio Moreira Carvalho,Computers & Operations Research,4,W4389670605,10.1016/j.cor.2023.106509,https://openalex.org/W4389670605,,Benchmark (surveying); Minification; Computer science; Key (lock); Job shop scheduling,article,False,
Mass media impact on opinion evolution in biased digital environments: a bounded confidence model,2023,Valentina Pansanella; Alina Sîrbu; János Kertész; Giulio Rossetti,Scientific Reports,16,W4386457915,10.1038/s41598-023-39725-y,https://openalex.org/W4386457915,https://www.nature.com/articles/s41598-023-39725-y.pdf,Mainstream; Computer science; Social media; Population; Radicalization,article,True,"Abstract People increasingly shape their opinions by accessing and discussing content shared on social networking websites. These platforms contain a mixture of other users’ shared opinions and content from mainstream media sources. While online social networks have fostered information access and diffusion, they also represent optimal environments for the proliferation of polluted information and contents, which are argued to be among the co-causes of polarization/radicalization phenomena. Moreover, recommendation algorithms - intended to enhance platform usage - likely augment such phenomena, generating the so-called Algorithmic Bias . In this work, we study the effects of the combination of social influence and mass media influence on the dynamics of opinion evolution in a biased online environment, using a recent bounded confidence opinion dynamics model with algorithmic bias as a baseline and adding the possibility to interact with one or more media outlets, modeled as stubborn agents. We analyzed four different media landscapes and found that an open-minded population is more easily manipulated by external propaganda - moderate or extremist - while remaining undecided in a more balanced information environment. By reinforcing users’ biases, recommender systems appear to help avoid the complete manipulation of the population by external propaganda."
A Novel Bias-TSP Algorithm for Maritime Patrol,2023,Geraldo Mulato de Lima Filho; Angelo Pássaro; Guilherme Moura Delfino; Herman Monsuur,IEEE Access,2,W4323065993,10.1109/access.2023.3252013,https://openalex.org/W4323065993,https://ieeexplore.ieee.org/ielx7/6287639/6514899/10058498.pdf,Computer science; Algorithm,article,True,"This work aims to develop a search planning strategy to be used by a drone equipped with an inverse synthetic-aperture radar (ISAR) and an electro-optical sensor. After describing the specifics of our maritime scenario, we discuss four methodologies that can be used to find vessels involved in illegal fishing activities as quickly as possible. In addition to the clustering of the vessels, determined by the drone&#x2019;s electro-optical sensor range, we introduce a novel technique to bias a traveling salesman problem (TSP) tour. This bias is based on deliberately increasing distances to vessels that are classified as probable fishing vessels. This increase in distance is meant to prioritize visits to probable fishing vessels. Vessels are classified based on their length. The classification result and the vessel clustering are available before the actual planning of the tour. Simulations of scenarios in which we have a few vessels fishing illegally show that the novel technique, the bias-TSP, combined with a tour orientation based on operational considerations, outperforms the classic TSP: the mean distance traveled to find all the vessels involved in illegal fishing activities is reduced by at least 35&#x2013;50%. We also show that different drone take-off locations significantly impact the results."
How Public Officials Perceive Algorithmic Discretion: A Study of Status Quo Bias in Policing,2025,Muhammad Afzal; Panos Panagiotopoulos,Public Administration Review,2,W4409372777,10.1111/puar.13957,https://openalex.org/W4409372777,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/puar.13957,Status quo; Discretion; Status quo bias; Political science; Public administration,article,True,"ABSTRACT Algorithms are disrupting established decision‐making practices in public administration. A key area of interest lies in algorithmic discretion or how public officials use algorithms to exercise discretion. The article develops a framework to explain algorithmic discretion by drawing on status quo bias theory and bureaucratic discretion. A study with police officers in the UK shows that—while officers still value their discretion—it is resistance via the aspects of status quo bias that accounts for a more substantial explanation. Transition costs, loss aversion, and performance uncertainty determine resistance and, in turn, reluctance to delegate discretion to algorithms. The study contributes to public administration research that demonstrates the influence of cognitive biases in the increasing use of algorithms in areas like policing. The article concludes with recommendations for embedding algorithmic discretion into the professional development of public officials to mitigate sources of status quo bias."
Equity and Bias in AI Educational Tools: A Critical Examination of Algorithmic Decision-Making in Classrooms,2025,Shumaila Farheen; Azhar Abbas Cheema; Rooh Ullah; Marium Minhas Bandeali,The critical review of social sciences studies,1,W4412011869,10.59075/zqmnpa62,https://openalex.org/W4412011869,https://thecrsss.com/index.php/Journal/article/download/665/691,Equity (law); Computer science; Mathematics education; Actuarial science; Psychology,article,True,"The integration of artificial intelligence (AI) into education raises significant concerns about algorithmic bias and its impact on equity. This study examines the linkage between teachers' perceptions of AI bias and their concerns for fairness in the classroom while also assessing the influence of teaching level and AI familiarity. Through a cross-sectional survey of 270 educators in Punjab, Pakistan, we utilized quantitative methods, including correlation and regression analysis, to test our hypotheses. Our findings reveal a strong positive association between perceived algorithmic bias and threats to educational equity. We found that school teachers were significantly more aware of AI bias than university faculty. Furthermore, greater teacher familiarity with AI tools correlated with a more nuanced understanding of their potential biases. These results underscore the urgent need to address the ""black box"" nature of educational AI. The study provides empirical evidence that, without careful regulation and comprehensive teacher training, AI-driven tools risk perpetuating and even exacerbating existing educational inequalities, aligning with global concerns about the ethical deployment of AI in sensitive sectors."
Bias in medical AI: Implications for clinical decision-making,2024,James M. Cross; Michael A. Choma; John A. Onofrey,PLOS Digital Health,167,W4404134492,10.1371/journal.pdig.0000651,https://openalex.org/W4404134492,https://doi.org/10.1371/journal.pdig.0000651,Medical decision making; Clinical decision making; Psychology; Computer science; Artificial intelligence,review,True,"Biases in medical artificial intelligence (AI) arise and compound throughout the AI lifecycle. These biases can have significant clinical consequences, especially in applications that involve clinical decision-making. Left unaddressed, biased medical AI can lead to substandard clinical decisions and the perpetuation and exacerbation of longstanding healthcare disparities. We discuss potential biases that can arise at different stages in the AI development pipeline and how they can affect AI algorithms and clinical decision-making. Bias can occur in data features and labels, model development and evaluation, deployment, and publication. Insufficient sample sizes for certain patient groups can result in suboptimal performance, algorithm underestimation, and clinically unmeaningful predictions. Missing patient findings can also produce biased model behavior, including capturable but nonrandomly missing data, such as diagnosis codes, and data that is not usually or not easily captured, such as social determinants of health. Expertly annotated labels used to train supervised learning models may reflect implicit cognitive biases or substandard care practices. Overreliance on performance metrics during model development may obscure bias and diminish a model’s clinical utility. When applied to data outside the training cohort, model performance can deteriorate from previous validation and can do so differentially across subgroups. How end users interact with deployed solutions can introduce bias. Finally, where models are developed and published, and by whom, impacts the trajectories and priorities of future medical AI development. Solutions to mitigate bias must be implemented with care, which include the collection of large and diverse data sets, statistical debiasing methods, thorough model evaluation, emphasis on model interpretability, and standardized bias reporting and transparency requirements. Prior to real-world implementation in clinical settings, rigorous validation through clinical trials is critical to demonstrate unbiased application. Addressing biases across model development stages is crucial for ensuring all patients benefit equitably from the future of medical AI."
Definition drives design: Disability models and mechanisms of bias in AI technologies,2023,Denis Newman-Griffis; Jessica Sage Rauchberg; Rahaf Alharbi; Louise Hickman; Harry Hochheiser,First Monday,34,W4319845028,10.5210/fm.v28i1.12903,https://openalex.org/W4319845028,https://firstmonday.org/ojs/index.php/fm/article/download/12903/10797,Computer science; Transparency (behavior); Data science; Implementation; Software deployment,article,True,"The increasing deployment of artificial intelligence (AI) tools to inform decision-making across diverse areas including healthcare, employment, social benefits, and government policy, presents a serious risk for disabled people, who have been shown to face bias in AI implementations. While there has been significant work on analysing and mitigating algorithmic bias, the broader mechanisms of how bias emerges in AI applications are not well understood, hampering efforts to address bias where it begins. In this article, we illustrate how bias in AI-assisted decision-making can arise from a range of specific design decisions, each of which may seem self-contained and non-biasing when considered separately. These design decisions include basic problem formulation, the data chosen for analysis, the use the AI technology is put to, and operational design elements in addition to the core algorithmic design. We draw on three historical models of disability common to different decision-making settings to demonstrate how differences in the definition of disability can lead to highly distinct decisions on each of these aspects of design, leading in turn to AI technologies with a variety of biases and downstream effects. We further show that the potential harms arising from inappropriate definitions of disability in fundamental design stages are further amplified by a lack of transparency and disabled participation throughout the AI design process. Our analysis provides a framework for critically examining AI technologies in decision-making contexts and guiding the development of a design praxis for disability-related AI analytics. We put forth this article to provide key questions to facilitate disability-led design and participatory development to produce more fair and equitable AI technologies in disability-related contexts."
A Biased-Randomized Discrete Event Algorithm to Improve the Productivity of Automated Storage and Retrieval Systems in the Steel Industry,2024,Mattia Neroni; Massimo Bertolini; Ángel A. Juan,Algorithms,3,W4391023723,10.3390/a17010046,https://openalex.org/W4391023723,https://www.mdpi.com/1999-4893/17/1/46/pdf?version=1705678616,Computer science; Simulated annealing; Heuristics; RSS; Job shop scheduling,article,True,"In automated storage and retrieval systems (AS/RSs), the utilization of intelligent algorithms can reduce the makespan required to complete a series of input/output operations. This paper introduces a simulation optimization algorithm designed to minimize the makespan in a realistic AS/RS commonly found in the steel sector. This system includes weight and quality constraints for the selected items. Our hybrid approach combines discrete event simulation with biased-randomized heuristics. This combination enables us to efficiently address the complex time dependencies inherent in such dynamic scenarios. Simultaneously, it allows for intelligent decision making, resulting in feasible and high-quality solutions within seconds. A series of computational experiments illustrates the potential of our approach, which surpasses an alternative method based on traditional simulated annealing."
Ethical Considerations of AI Implementation in the Library Era,2024,N. Rajkumar; C. Viji; A. Mohanraj; K. R. Senthilkumar; R. Jagajeevan; Judeson Antony Kovilpillai,Advances in library and information science (ALIS) book series,29,W4397003176,10.4018/979-8-3693-5593-0.ch007,https://openalex.org/W4397003176,,Engineering ethics; Library science; Computer science; Engineering,book-chapter,False,"As the mixture of artificial intelligence (AI) continues to permeate several sectors, ethical considerations have ended up a focus in ensuring responsible and sustainable AI deployment. This virtual library explores the multifaceted moral dimensions related to AI implementation. The gathering of scholarly articles and studies papers delves into key moral problems, spanning troubles which includes bias and fairness, transparency, responsibility, privacy, and societal impact. The number one section of the virtual library addresses the undertaking of algorithmic bias and fairness, reading how biases in AI systems can perpetuate societal inequalities. Various methods to mitigating bias and selling fairness in AI algorithms are explored, providing insights into the improvement of more equitable AI programs. Transparency and duty are the focal factors of the second one segment, emphasizing the need for clean conversation of AI decision-making techniques and mechanisms for holding AI systems answerable for their movements."
A systematic bias in template-based radial velocity extraction algorithms,2025,A.M. Silva; N. C. Santos; J. P. Faria; J. H. C. Martins; E. Cristo; S. G. Sousa; Pablo Teixeira Viana; É. Artigau; K. Al Moulla; A. Castro-González; D. F. M. Folha; P. Figueira; Tobias M. Schmidt; Fabrizio Pepe; X. Dumusque; O. D. S. Demangeon; T.L. Campante; X. Delfosse; Bachar Wehbé; J. Lillo-Box; A. R. Costa Silva; J. Rodrigues; J. I. Gónzalez Hernández; T. Azevedo Silva; S. Cristiani; H. M. Tabernero; Ε. Πάλλη; B. Lavie; A. Suárez Mascareño; P. Di Marcantonio; Alexandre Cabral; C. J. A. P. Martins; N. J. Nunes; A. Sozzetti,Astronomy and Astrophysics,2,W4412073240,10.1051/0004-6361/202554955,https://openalex.org/W4412073240,https://doi.org/10.1051/0004-6361/202554955,Physics; Radial velocity; Astrophysics; Algorithm; Extraction (chemistry),article,True,"Context . The radial velocity (RV) method plays a key role in modern-day astrophysics. One of the most common techniques for extracting precise RVs from state-of-the-art spectrographs is template-matching (TM) algorithms. They have been shown to perform better than a cross-correlation function (CCF) approach in cases of cooler stars (e.g. M dwarfs) and multiple implementations have appeared over the past years. More recently, line-by-line (LBL) approaches offer an alternative avenue to extract RVs by analyzing individual spectral lines. Aims . In this paper, we identify and explore a previously unidentified, multi-meter-per-second, systematic correlation between time and RVs inferred through TM and LBL methods. We evaluate the influence of the data-driven stellar template in the RV bias and hypothesise on the possible sources of this effect. Methods . We used the s-BART pipeline to extract RVs from three different datasets gathered over four nights of ESPRESSO and HARPS observations. We demonstrate that the effect can be recovered on a larger sample of 19 targets, totalling 4124 ESPRESSO observations over 38 nights. We also showcase the presence of the bias in RVs extracted with the SERVAL and ARVE pipelines. Lastly, we explore the construction of the stellar template over the five years of ESPRESSO observations of HD 10700, totalling more than 2000 observations. Results . We find that a systematic quasi-linear bias affects the RV extraction with slopes that vary from —0.3 ms −1 h −1 to —52 m s −1 h −1 in our sample. This trend is not observed in CCF RVs and only appears when all observations of a given star are collected within a short time period (timescales of hours). We show that this systematic contamination exists in the RV time series of two different template-matching pipelines and one line-by-line pipeline, and it is agnostic to the spectrograph. We also find that this effect is linked to the construction of the stellar template, as we were able to mitigate it through a careful selection of the observations used to construct it. Our results suggest that a contamination of micro-telluric features, coupled with other sources of correlated noise, could be the driving factor of this effect. We also show that this effect does not impact the usual usage of template-matching for the detection and characterisation of exoplanets. However, the short-timescale science cases, such as asteroseismology as well as transit and atmospheric characterisation, can be severely affected."
"Fairness in Recommendation: Foundations, Methods, and Applications",2023,Yunqi Li; Hanxiong Chen; Shuyuan Xu; Yingqiang Ge; Juntao Tan; Shuchang Liu; Yongfeng Zhang,ACM Transactions on Intelligent Systems and Technology,64,W4385302663,10.1145/3610302,https://openalex.org/W4385302663,,Recommender system; Computer science; Ranking (information retrieval); Quality (philosophy); Domain (mathematical analysis),article,False,"As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision-making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This survey focuses on the foundations for fairness in recommendation literature. It first presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking to provide a general overview of fairness research, as well as introduce the more complex situations and challenges that need to be considered when studying fairness in recommender systems. After that, the survey will introduce fairness in recommendation with a focus on the taxonomies of current fairness definitions, the typical techniques for improving fairness, as well as the datasets for fairness studies in recommendation. The survey also talks about the challenges and opportunities in fairness research with the hope of promoting the fair recommendation research area and beyond."
"Transforming Healthcare with AI: Promises, Pitfalls, and Pathways Forward",2024,Ali Shuaib,International Journal of General Medicine,49,W4396560157,10.2147/ijgm.s449598,https://openalex.org/W4396560157,https://www.dovepress.com/getfile.php?fileID=98729,Medicine; Health care; Critical pathways; Data science; Computational biology,article,True,"This perspective paper provides a comprehensive examination of artificial intelligence (AI) in healthcare, focusing on its transformative impact on clinical practices, decision-making, and physician-patient relationships. By integrating insights from evidence, research, and real-world examples, it offers a balanced analysis of AI's capabilities and limitations, emphasizing its role in streamlining administrative processes, enhancing patient care, and reducing physician burnout while maintaining a human-centric approach in medicine. The research underscores AI's capacity to augment clinical decision-making and improve patient interactions, but it also highlights the variable impact of AI in different healthcare settings. The need for context-specific adaptations and careful integration of AI technologies into existing healthcare workflows is emphasized to maximize benefits and minimize unintended consequences. Significant attention is given to the implications of AI on the roles and competencies of healthcare professionals. The emergence of AI necessitates new skills in data literacy and technology use, prompting a shift in educational curricula towards digital health and AI training. Ethical considerations are a pivotal aspect of the discussion. The paper explores the challenges posed by data privacy concerns, algorithmic biases, and ensuring equitable access to AI-driven healthcare. It advocates for the development of comprehensive ethical frameworks and ongoing research to guide the responsible use of AI in healthcare. Conclusively, the paper advocates for a balanced approach to AI adoption in healthcare, highlighting the importance of ongoing research, strategic implementation, and the synergistic combination of human expertise with AI technologies for optimal patient care."
Bias-based Denoising Causal Recommendation Algorithm,2023,Yang Xu,Frontiers in Computing and Intelligent Systems,1,W4366783237,10.54097/fcis.v3i2.6909,https://openalex.org/W4366783237,https://drpress.org/ojs/index.php/fcis/article/download/6909/6698,Computer science; Causal inference; Collaborative filtering; Inference; Noise (video),article,True,"Traditional recommendation algorithms, such as collaborative filtering, make recommendations by learning the relevant relationships between users and items. However, considering only the relationships without considering the underlying causal mechanisms would be unfair, uninterpretable, and would lead to bias. In this paper, we propose bias-based denoising causal recommendation algorithm (BDCR) . First, the method dynamically transforms the explicit user-item feedback into implicit feedback with an embedded representation. Then, a truncation function based on causal inference is constructed to remove false positive noise. In addition, traditional recommendations and denoised causal recommendations are aggregated to obtain predictive scores. Finally, experimental results on two real datasets show that the BDCR algorithm outperforms the classical algorithm in terms of recall and NDCG metrics."
Face the Facts: Using Face Averaging to Visualize Gender-by-Race Bias in Facial Analysis Algorithms,2024,Kentrell Owens; Erin Freiburger; Ryan Hutchings; Mattea Sim; Kurt Hugenberg; Franziska Roesner; Tadayoshi Kohno,Proceedings of the AAAI/ACM Conference on AI Ethics and Society,1,W4404518291,10.1609/aies.v7i1.31707,https://openalex.org/W4404518291,,Face (sociological concept); Race (biology); Algorithm; Artificial intelligence; Computer science,article,False,"We applied techniques from psychology --- typically used to visualize human bias --- to facial analysis systems, providing novel approaches for diagnosing and communicating algorithmic bias. First, we aggregated a diverse corpus of human facial images (N=1492) with self-identified gender and race. We tested four automated gender recognition (AGR) systems and found that some exhibited intersectional gender-by-race biases. Employing a technique developed by psychologists --- face averaging --- we created composite images to visualize these systems' outputs. For example, we visualized what an ""average woman"" looks like, according to a system's output. Second, we conducted two online experiments wherein participants judged the bias of hypothetical AGR systems. The first experiment involved participants (N=228) from a convenience sample. When depicting the same results in different formats, facial visualizations communicated bias to the same magnitude as statistics. In the second experiment with only Black participants (N=223), facial visualizations communicated bias significantly more than statistics, suggesting that face averages are meaningful for communicating algorithmic bias."
Smart Smile: Revolutionizing Dentistry With Artificial Intelligence,2023,Ashwini Dhopte; Hiroj Bagde,Cureus,76,W4382651942,10.7759/cureus.41227,https://openalex.org/W4382651942,https://assets.cureus.com/uploads/review_article/pdf/167748/20230701-8816-tv79o7.pdf,Medicine; Transformative learning; Virtual reality; Robotics; Artificial intelligence,review,True,"Artificial intelligence (AI) has emerged as a transformative technology in various industries, and its potential in dentistry is gaining significant attention. This abstract explores the future prospects of AI in dentistry, highlighting its potential to revolutionize clinical practice, improve patient outcomes, and enhance the overall efficiency of dental care. The application of AI in dentistry encompasses several key areas, including diagnosis, treatment planning, image analysis, patient management, and personalized care. AI algorithms have shown promising results in the automated detection and diagnosis of dental conditions, such as caries, periodontal diseases, and oral cancers, aiding clinicians in early intervention and improving treatment outcomes. Furthermore, AI-powered treatment planning systems leverage machine learning techniques to analyze vast amounts of patient data, considering factors like medical history, anatomical variations, and treatment success rates. These systems provide dentists with valuable insights and support in making evidence-based treatment decisions, ultimately leading to more predictable and tailored treatment approaches. While the potential of AI in dentistry is immense, it is essential to address certain challenges, including data privacy, algorithm bias, and regulatory considerations. Collaborative efforts between dental professionals, AI experts, and policymakers are crucial to developing robust frameworks that ensure the responsible and ethical implementation of AI in dentistry. Moreover, AI-driven robotics has introduced innovative approaches to dental surgery, enabling precise and minimally invasive procedures, and ultimately reducing patient discomfort and recovery time. Virtual reality (VR) and augmented reality (AR) applications further enhance dental education and training, allowing dental professionals to refine their skills in a realistic and immersive environment. AI holds tremendous promise in shaping the future of dentistry. Through its ability to analyze vast amounts of data, provide accurate diagnoses, facilitate treatment planning, improve image analysis, streamline patient management, and enable personalized care, AI has the potential to enhance dental practice and significantly improve patient outcomes. Embracing this technology and its future development will undoubtedly revolutionize the field of dentistry, fostering a more efficient, precise, and patient-centric approach to oral healthcare. Overall, AI represents a powerful tool that has the potential to revolutionize various aspects of society, from improving healthcare outcomes to optimizing business operations. Continued research, development, and responsible implementation of AI technologies will shape our future, unlocking new possibilities and transforming the way we live and work."
Research on the Bias Sampling RRT Algorithm for Supermarket Chain Distribution Routes under the O2O Model,2023,Zheng Shi,Frontiers in Computing and Intelligent Systems,2,W4388399257,10.54097/fcis.v5i2.12444,https://openalex.org/W4388399257,https://drpress.org/ojs/index.php/fcis/article/download/12444/12113,Sampling (signal processing); Terrain; Path (computing); Computer science; Transformation (genetics),article,True,"The purpose of this article is to propose the Bias Sampling RRT algorithm and use it as an optimization algorithm for supermarket chain distribution routes under the O2O model. As a retail store in the transformation and upgrading of chain stores, the actual terrain factors in distribution directly affect the timely delivery of goods from online to offline. The Bias Sampling RRT algorithm, as a path search method for time window vehicle routing problems, can find the optimal path that meets time window constraints. The applicability and effectiveness of the Bias Sampling RRT algorithm have been demonstrated through map simulation calculations. The simulation results show that compared with the RRT method, the Bias Sampling RRT algorithm has a shorter distribution path and shorter distribution time. This method is very suitable for the distribution activities of chain supermarkets or single store retail enterprises in the complex transformation and upgrading of actual terrain."
Reducing the incidence of biased algorithmic decisions through feature importance transparency: an empirical study,2024,Sepideh Ebrahimi; Esraa Abdelhalim; Khaled Hassanein; Milena Head,European Journal of Information Systems,4,W4402130453,10.1080/0960085x.2024.2395531,https://openalex.org/W4402130453,https://www.tandfonline.com/doi/pdf/10.1080/0960085X.2024.2395531?needAccess=true,Transparency (behavior); Computer science; Strategic information system; Empirical research; Feature (linguistics),article,True,
Examining bias perpetuation in academic search engines: An algorithm audit of Google and Semantic Scholar,2024,Celina Kacperski; Mona Bielig; Mykola Makhortykh; Maryna Sydorova; Roberto Ulloa,First Monday,4,W4404038469,10.5210/fm.v29i11.13730,https://openalex.org/W4404038469,https://firstmonday.org/ojs/index.php/fm/article/download/13730/11713,Audit; Computer science; Search engine; Information retrieval; World Wide Web,article,True,"Researchers rely on academic Web search engines to find scientific sources, but search engine mechanisms may selectively present content that aligns with biases embedded in queries. This study examines whether confirmation biased queries prompted into Google Scholar and Semantic Scholar will yield results aligned with a query’s bias. Six queries (topics across health and technology domains such as ‘vaccines’, ‘Internet use’) were analyzed for disparities in search results. We confirm that biased queries (targeting ‘benefits’ or ‘risks’) affect search results in line with bias, with technology-related queries displaying more significant disparities. Overall, Semantic Scholar exhibited fewer disparities than Google Scholar. Topics rated as more polarizing did not consistently show more disparate results. Academic search results that perpetuate confirmation bias have strong implications for both researchers and citizens searching for evidence. More research is needed to explore how scientific inquiry and academic search engines interact."
Robust Bias-Compensated CR-NSAF Algorithm: Design and Performance Analysis,2024,Pengwei Wen; Bolin Wang; Boyang Qu; Sheng Zhang; Haiquan Zhao; Jing Liang,IEEE Transactions on Systems Man and Cybernetics Systems,3,W4404469651,10.1109/tsmc.2024.3491188,https://openalex.org/W4404469651,,Computer science; Algorithm; Control theory (sociology); Artificial intelligence; Control (management),article,False,
AI-powered techniques in anatomical imaging: Impacts on veterinary diagnostics and surgery,2024,A. S. Vickram; Shofia Saghya Infant; Priyanka Choudhary; Hitesh Chopra,Annals of Anatomy - Anatomischer Anzeiger,23,W4404553738,10.1016/j.aanat.2024.152355,https://openalex.org/W4404553738,,Medical physics; Medicine; Veterinary medicine,review,False,
Parallel and bias-free RSA algorithm for maximal Poisson-sphere sampling,2024,Marc Josien; Raphaël Prat,Computer Physics Communications,3,W4401854267,10.1016/j.cpc.2024.109354,https://openalex.org/W4401854267,,Poisson distribution; Algorithm; Sampling (signal processing); Mathematics; Computer science,article,False,
Hybrid-biased genetic algorithm for packing unequal rectangles into a fixed-size circle,2024,Qiang Luo; Yunqing Rao; Piaoruo Yang; Xusheng Zhao,Computers & Operations Research,2,W4399207519,10.1016/j.cor.2024.106716,https://openalex.org/W4399207519,,Circle packing; Combinatorics; Mathematics; Algorithm; Genetic algorithm,article,False,
Adaptive Learning through Artificial Intelligence,2023,Meet Joshi,SSRN Electronic Journal,29,W4385326031,10.2139/ssrn.4514887,https://openalex.org/W4385326031,https://doi.org/10.2139/ssrn.4514887,Artificial intelligence; Computer science; Adaptive learning; Cognitive science; Psychology,article,True,
Opportunities and Challenges of Integrating Generative Artificial Intelligence in Education,2024,Rommel AlAli; Yousef Wardat,International Journal of Religion,48,W4396872845,10.61707/8y29gv34,https://openalex.org/W4396872845,https://ijor.co.uk/ijor/article/download/4397/2263,Generative grammar; Artificial intelligence; Psychology; Cognitive science; Computer science,article,True,"This paper thoroughly examines both the opportunities and obstacles associated with integrating Generative Artificial Intelligence (AI) into educational settings. It explores how Generative AI has the potential to enrich learning experiences, customize education for individuals, and foster creativity. However, it also confronts several challenges including ethical dilemmas, safeguarding data privacy, mitigating algorithmic biases, and reshaping the role of educators. Through a synthesis of theoretical frameworks and empirical research, the paper offers valuable insights into effective strategies for navigating these challenges. It emphasizes the importance of establishing ethical guidelines, ensuring transparency in algorithms, and adopting inclusive design principles during AI integration. Furthermore, the paper underscores the importance of providing educators with adequate training and professional development opportunities to effectively utilize AI tools. Additionally, it advocates for ongoing dialogue among stakeholders—such as educators, policymakers, technologists, and students—to steer responsible AI integration in education. Ultimately, the paper advocates for a collaborative approach that prioritizes human-centric values, equity, and diversity. While Generative AI holds promise for revolutionizing educational practices, its integration requires thoughtful consideration of ethical, social, and pedagogical implications. Through proactive collaboration and partnership, educators can leverage AI's potential to create more immersive, tailored, and equitable learning environments."
AI revolution in healthcare and medicine and the (re-)emergence of inequalities and disadvantages for ageing population,2023,Justyna Stypińska; Annette Franke,Frontiers in Sociology,30,W4317717912,10.3389/fsoc.2022.1038854,https://openalex.org/W4317717912,https://www.frontiersin.org/articles/10.3389/fsoc.2022.1038854/pdf,Health care; Harm; Inequality; Population; Population ageing,article,True,"AI systems in medicine and healthcare are being extensively explored in prevention, diagnosis, novel drug designs and after-care. The application of AI technology in healthcare systems promises impressive outcomes such as equalising healthcare, reducing mortality rate and human error, reducing medical costs, as well as reducing reliance on social services. In the light of the WHO “Decade of Healthy Ageing”, AI applications are designed as digital innovations to support the quality of life for older persons. However, the emergence of evidence of different types of algorithmic bias in AI applications, ageism in the use of digital devices and platforms, as well as age bias in digital data suggests that the use of AI might have discriminatory effects on older population or even cause harm. This paper addresses the issue of age biases and age discrimination in AI applications in medicine and healthcare systems and try to identify main challenges in this area. It will reflect on the potential of AI applications to amplify the already existing health inequalities by discussing two levels where potential negative impact of AI on age inequalities might be observed. Firstly, we will address the technical level of age bias in algorithms and digital datasets (especially health data). Secondly, we will discuss the potential disparate outcomes of automatic decision-making systems (ADMs) used in healthcare on the older population. These examples will demonstrate, although only partially, how AI systems may create new structures of age inequalities and novel dimensions of exclusion in healthcare and medicine."
"MiniMed 780G System Outperforms Other Automated Insulin Systems Due to Algorithm Design, Not Bias: Response to Inaccurate Allegations",2024,Tim van den Heuvel; Javier Castañeda; Isabeau Thijs; Arcelia Arrieta; Lou Lintereur; John Shin; Ohad Cohen,Diabetes Technology & Therapeutics,6,W4393526032,10.1089/dia.2024.0121,https://openalex.org/W4393526032,,Medicine; Algorithm; Diabetes mellitus; Computer science; Endocrinology,letter,False,Not applicable (letter to the editor)
Ethical Considerations and Fairness in the Use of Artificial Intelligence for Neuroradiology,2023,Christopher G. Filippi; Joel M. Stein; Zihao Wang; Spyridon Bakas; Yichuan Liu; Peter Chang; Yvonne W. Lui; Christopher P. Hess; Daniel P. Barboriak; Adam E. Flanders; Max Wintermark; Greg Zaharchuk; Ona Wu,American Journal of Neuroradiology,16,W4386324990,10.3174/ajnr.a7963,https://openalex.org/W4386324990,https://www.ajnr.org/content/ajnr/early/2023/08/31/ajnr.A7963.full.pdf,Neuroradiology; Software deployment; Workflow; Accountability; Harm,review,True,"In this review, concepts of algorithmic bias and fairness are defined qualitatively and mathematically. Illustrative examples are given of what can go wrong when unintended bias or unfairness in algorithmic development occurs. The importance of explainability, accountability, and transparency with respect to artificial intelligence algorithm development and clinical deployment is discussed. These are grounded in the concept of ""primum no nocere"" (first, do no harm). Steps to mitigate unfairness and bias in task definition, data collection, model definition, training, testing, deployment, and feedback are provided. Discussions on the implementation of fairness criteria that maximize benefit and minimize unfairness and harm to neuroradiology patients will be provided, including suggestions for neuroradiologists to consider as artificial intelligence algorithms gain acceptance into neuroradiology practice and become incorporated into routine clinical workflow."
"Artificial intelligence in nursing: Current trends, possibilities and pitfalls",2024,Sirwan Khalid Ahmed,Journal of Medicine Surgery and Public Health,58,W4391848861,10.1016/j.glmedi.2024.100072,https://openalex.org/W4391848861,https://doi.org/10.1016/j.glmedi.2024.100072,Transformative learning; Transparency (behavior); Health care; Workflow; Accountability,article,True,
"A systematic review of trustworthy and explainable artificial intelligence in healthcare: Assessment of quality, bias risk, and data fusion",2023,A. S. Albahri; Ali M. Duhaim; Mohammed A. Fadhel; Alhamzah Alnoor; Noor S. Baqer; Laith Alzubaidi; O. S. Albahri; A. H. Alamoodi; Jinshuai Bai; Asma Salhi; José Santamaría; Chun Ouyang; Ashish Gupta; Yuantong Gu; Muhammet Deveci,Information Fusion,550,W4324135233,10.1016/j.inffus.2023.03.008,https://openalex.org/W4324135233,,Health care; Transparency (behavior); Objectivity (philosophy); Computer science; Quality (philosophy),review,False,
Effective Strategies for Mitigating Bias in Hiring Algorithms: A Comparative Analysis,2023,,Journal of Artificial Intelligence Machine Learning and Data Science,1,W4388298463,10.51219/jaimld/yusuf-jazakallah/16,https://openalex.org/W4388298463,https://doi.org/10.51219/jaimld/yusuf-jazakallah/16,Algorithm; Computer science,article,True,"Bias in hiring algorithms is a critical issue that has been widely recognized in recent years.As more companies rely on automated candidate selection processes, it is essential to develop fair and equitable recruitment practices that ensure equal opportunities for all candidates.The objective of this research paper is to propose a comprehensive framework for mitigating bias in hiring algorithms.By utilizing a combination of machine learning techniques, statistical analysis, and ethical considerations, the study aims to identify, measure, and mitigate both overt and subtle forms of bias present in these algorithms.This paper's findings underscore the significance of employing de-biasing strategies to ensure diversity and inclusion in the workplace.In this introduction, we will discuss the critical issue of bias mitigation in hiring algorithms, the importance of fair and equitable recruitment practices, and the objective of the study.We will also provide an overview of the research methodology, the measurement of bias, and the proposed mitigation strategies.Finally, we will summarize the key findings and the proposed framework for reducing bias in hiring algorithms."
Stochastic Optimization Algorithms for Problems with Controllable Biased Oracles,2023,Yin Liu; Sam Davanloo Tajbakhsh,arXiv (Cornell University),1,W4380715281,10.48550/arxiv.2306.07810,https://openalex.org/W4380715281,https://arxiv.org/pdf/2306.07810,Computation; Oracle; Stochastic optimization; Optimization problem; Computer science,preprint,True,"Motivated by multiple emerging applications in machine learning, we consider an optimization problem in a general form where the gradient of the objective function is available through a biased stochastic oracle. We assume a bias-control parameter can reduce the bias magnitude, however, a lower bias requires more computation/samples. For instance, for two applications on stochastic composition optimization and policy optimization for infinite-horizon Markov decision processes, we show that the bias follows a power law and exponential decay, respectively, as functions of their corresponding bias control parameters. For problems with such gradient oracles, the paper proposes stochastic algorithms that adjust the bias-control parameter throughout the iterations. We analyze the nonasymptotic performance of the proposed algorithms in the nonconvex regime and establish their sample or bias-control computation complexities to obtain a stationary point. Finally, we numerically evaluate the performance of the proposed algorithms over three applications."
Influence of symbolic content on recommendation bias: analyzing YouTube’s algorithm during Taiwan’s 2024 election,2025,Mert Can Çakmak; Nitin Agarwal,Applied Network Science,1,W4411566577,10.1007/s41109-025-00713-y,https://openalex.org/W4411566577,https://appliednetsci.springeropen.com/counter/pdf/10.1007/s41109-025-00713-y,Content (measure theory); Computer science; Content analysis; Algorithm; Mathematics,article,True,"Abstract This study investigates the role of symbolic content, including social, cultural, and political imagery, in shaping algorithmic biases within YouTube’s recommendation system, using the 2024 Taiwanese presidential election as a case study. Leveraging classification methodology and a dataset of 15,600 videos collected via a rigorous multiphase keyword expansion, our research employs a novel combination of social network analysis, statistical metrics, and generative AI-based content evaluation to examine the propagation dynamics, community formation, and topic relevance of both symbolic and non-symbolic content. Our analysis reveals a dual dynamic: symbolic content fosters tightly integrated, cohesive communities characterized by strong thematic consistency and deeper topic relevance, yet exhibits limited network-wide visibility, while non-symbolic content achieves broader connectivity by often serving as crucial bridges within recommendation networks. Building on prior research documenting the influential role of symbols in political mobilization and online misinformation, we further assess how symbolic imagery interacts with algorithmic recommendation processes. Our findings underscore that algorithmic biases may inadvertently reinforce echo chambers and limit content diversity, highlighting the need for recommendation systems that balance content relevance with community-specific thematic coherence. These insights offer valuable guidance for policymakers, platform designers, and content creators striving for equitable content representation in the digital era."
Bias-field digitized counterdiabatic quantum algorithm for higher-order binary optimization,2025,Sebastián V. Romero; Anne-Maria Visuri; Alejandro Gomez Cadavid; Anton Simen; E. Solano; Narendra N. Hegade,Communications Physics,2,W4413448855,10.1038/s42005-025-02270-3,https://openalex.org/W4413448855,https://www.nature.com/articles/s42005-025-02270-3.pdf,Binary number; Algorithm; Field (mathematics); Order (exchange); Computer science,article,True,"Abstract Combinatorial optimization plays a crucial role in many industrial applications. While classical computing often struggles with complex instances, quantum optimization emerges as a promising alternative. Here, we present an enhanced bias-field digitized counterdiabatic quantum optimization (BF-DCQO) algorithm to address higher-order unconstrained binary optimization (HUBO). We apply BF-DCQO to a HUBO problem featuring three-local terms in the Ising spin-glass model, validated experimentally using 156 qubits on an IBM quantum processor. In the studied instances, our results outperform standard methods such as the quantum approximate optimization algorithm, quantum annealing, simulated annealing, and Tabu search. Furthermore, we provide numerical evidence of the feasibility of a similar HUBO problem on a 433-qubit Osprey-like quantum processor. Finally, we solve denser instances of the MAX 3-SAT problem in an IonQ emulator. Our results show that BF-DCQO offers an effective path for solving large-scale HUBO problems on current and near-term quantum processors."
Role and Challenges of ChatGPT and Similar Generative Artificial Intelligence in Business Management,2023,Nitin Liladhar Rane,SSRN Electronic Journal,64,W4388152605,10.2139/ssrn.4603227,https://openalex.org/W4388152605,https://doi.org/10.2139/ssrn.4603227,Generative grammar; Business management; Artificial intelligence; Business; Knowledge management,article,True,
Scaffolding Children’s Sensemaking around Algorithmic Fairness,2023,Jean Salac; Rotem Landesman; Stefania Druga; Amy J. Ko,,10,W4380479537,10.1145/3585088.3589379,https://openalex.org/W4380479537,https://dl.acm.org/doi/pdf/10.1145/3585088.3589379,Situated; Sensemaking; Distrust; Agency (philosophy); Computer science,article,True,"Prior research has investigated children's perceptions of algorithmic bias, but provides little guidance on engaging children in conversations on algorithmic bias that center their agency and well-being. To address this, we developed discussions and design activities based on three scenarios of algorithmic (un)fairness. We conducted these discussions and activities with 16 children (ages 8-12) in the US, and examined our data using qualitative thematic analysis. Grounded in lived experiences and situated knowledge, participants were capable of reasoning around both explicit and implicit effects of algorithmic bias. Participants also expressed distrust of technology, doubting technology's abilities and preferring human approaches to resolve unfairness. This work contributes (1) a more nuanced understanding of children's situated reasoning of technology, suggesting their potential for critical engagement and (2) a blueprint for engaging children in scaffolded yet open-ended sensemaking around algorithmic fairness, informing the design of tools, curricula, and other learning experiences for children."
"The Transformative Role of Artificial Intelligence in Dentistry: A Comprehensive Overview Part 2: The Promise and Perils, and the International Dental Federation Communique",2025,Nozimjon Tuygunov; Lakshman P. Samaranayake; Zohaib Khurshid; Paak Rewthamrongsris; Falk Schwendicke; Thanaphum Osathanon; Noor Azlin Yahya,International Dental Journal,46,W4407932918,10.1016/j.identj.2025.02.006,https://openalex.org/W4407932918,https://doi.org/10.1016/j.identj.2025.02.006,Transformative learning; Dentistry; Political science; Medicine; Engineering ethics,review,True,"In the final part of this two part article on artificial intelligence (AI) in dentistry we review its transformative role, focusing on AI in dental education, patient communications, challenges of integration, strategies to overcome barriers, ethical considerations, and finally, the recently released International Dental Federation (FDI) Communique (white paper) on AI in Dentistry. AI in dental education is highlighted for its potential in enhancing theoretical and practical dimensions, including patient telemonitoring and virtual training ecosystems. Challenges of AI integration in dentistry are outlined, such as data availability, bias, and human accountability. Strategies to overcome these challenges include promoting AI literacy, establishing regulations, and focusing on specific AI implementations. Ethical considerations in AI integration within dentistry, such as patient privacy and algorithm bias, are emphasized. The need for clear guidelines and ongoing evaluation of AI systems is crucial. The FDI White Paper on AI in Dentistry provides insights into the significance of AI in oral care, dental education, and research, along with standards for governance. It discusses AI's impact on individual patients, community health, dental education, and research. The paper addresses biases, limited generalizability, accessibility, and regulatory requirements for AI in dental practice. In conclusion, AI plays a significant role in modern dental care, offering benefits in diagnosis, treatment planning, and decision-making. While facing challenges, strategic initiatives focusing on AI literacy, regulations, and targeted implementations can help overcome barriers and maximize the potential of AI in dentistry. Ethical considerations and ongoing evaluation are essential for ensuring responsible, effective and efficacious deployment of AI technologies in dental ecosystem."
Fairness in Healthcare: Assessing Data Bias and Algorithmic Fairness,2024,Fariba Dehghani; Nikita Malik; Jenn-Wei Lin; Sayeh Bayat; Mariana Bento,,2,W4405304237,10.1109/sipaim62974.2024.10783630,https://openalex.org/W4405304237,,Fairness measure; Health care; Computer science; Max-min fairness; Economics,article,False,
Bias Compensation for Kernel Least-Mean-Square Algorithms,2025,Ying‐Ren Chien; Liu Jin-ling; En-Ting Lin; Guobing Qian,IEEE Sensors Letters,2,W4408792212,10.1109/lsens.2025.3553594,https://openalex.org/W4408792212,,Algorithm; Square (algebra); Kernel (algebra); Computer science; Mathematics,article,False,
A Simple Bias Reduction Algorithm for RNA Sequencing Datasets,2023,Christopher Thron; Hannah E. Bergom; Ella Boytim; Mienie Roberts; Justin H. Hwang; Farhad Jafari,bioRxiv (Cold Spring Harbor Laboratory),2,W4388141697,10.1101/2023.10.31.564992,https://openalex.org/W4388141697,https://www.biorxiv.org/content/biorxiv/early/2023/11/01/2023.10.31.564992.full.pdf,Computational biology; Gene; Biology; RNA; Computer science,preprint,True,"Abstract RNA sequencing (RNA-seq) is the conventional genome-scale approach used to capture the expression levels of all detectable genes in a biological sample. This is now regularly used in the clinical diagnostic space for cancer patients. While the information gained is intended to impact treatment decisions, numerous technical and quality issues remain. This includes inaccuracies in the dissemination of gene-gene relationships. For such reasons, clinical decisions are still mostly driven by DNA biomarkers, such as gene mutations or fusions. In this study, we aimed to correct for systemic bias based on RNA-sequencing platforms in order to improve our understanding of the gene-gene relationships. To do so, we examined standard pre-processed RNA-seq datasets obtained from three studies conducted by two consortium efforts including The Cancer Genome Atlas (TCGA) and Stand Up 2 Cancer (SU2C). We particularly examined the TCGA Bladder Cancer (n = 408) and Prostate Cancer (n = 498) studies as well as the SU2C Prostate Cancer study (n = 208). Using various statistical tests, we detected expression-level dependent, per-sample biases in all datasets. Using simulations, we show that these biases corrupt the results of t -tests designed to identify expression level differences between subpopulations. Importantly, these biases introduce large errors into estimates of gene-gene correlations. To mitigate these biases, we introduce Local Leveling as a novel mathematical approach that transforms count level data and corrects these observed biases. Local Leveling specifically corrects for the bias due to the inherent differential detection of transcripts that is driven by differential expression levels. Based on standard forms of count data (Raw counts, transcripts per million, fragments per kilobase of exon per million), we demonstrate that local leveling effectively removes the observed per-sample biases, and improves the accuracy in simulated statistical tests. Importantly, this led to systemic changes of gene-gene relationships when examining the correlation of key oncogenes, such as the Androgen Receptor, with all other detectable genes. Altogether, Local Leveling improves our capacity towards understanding gene-gene relationships, which may lead to novel ways to utilize the information derived from clinical tests."
Fairness and Bias in Truth Discovery Algorithms: An Experimental Analysis,2023,Simone Lazier; Saravanan Thirumuruganathan; Hadis Anahideh,arXiv (Cornell University),1,W4367060658,10.48550/arxiv.2304.12573,https://openalex.org/W4367060658,https://arxiv.org/pdf/2304.12573,Crowdsourcing; Computer science; Majority rule; Voting; Panacea (medicine),preprint,True,"Machine learning (ML) based approaches are increasingly being used in a number of applications with societal impact. Training ML models often require vast amounts of labeled data, and crowdsourcing is a dominant paradigm for obtaining labels from multiple workers. Crowd workers may sometimes provide unreliable labels, and to address this, truth discovery (TD) algorithms such as majority voting are applied to determine the consensus labels from conflicting worker responses. However, it is important to note that these consensus labels may still be biased based on sensitive attributes such as gender, race, or political affiliation. Even when sensitive attributes are not involved, the labels can be biased due to different perspectives of subjective aspects such as toxicity. In this paper, we conduct a systematic study of the bias and fairness of TD algorithms. Our findings using two existing crowd-labeled datasets, reveal that a non-trivial proportion of workers provide biased results, and using simple approaches for TD is sub-optimal. Our study also demonstrates that popular TD algorithms are not a panacea. Additionally, we quantify the impact of these unfair workers on downstream ML tasks and show that conventional methods for achieving fairness and correcting label biases are ineffective in this setting. We end the paper with a plea for the design of novel bias-aware truth discovery algorithms that can ameliorate these issues."
Causal effect of racial bias in machine learning algorithms affecting user persuasiveness &amp;amp; decision-making: An Empirical Study,2023,Kinshuk Sengupta; Praveen Ranjan Srivast,Research Square (Research Square),5,W4318540816,10.21203/rs.3.rs-2509731/v1,https://openalex.org/W4318540816,https://www.researchsquare.com/article/rs-2509731/latest.pdf,Counterfactual thinking; Artificial intelligence; Computer science; Harm; Outcome (game theory),preprint,True,"Abstract Language data and models demonstrate various types of bias, be it ethnic, religious, gender, or socioeconomic. AI/NLP models, when trained on the racially biased dataset, AI/NLP models instigate poor model explainability, influence user experience during decision making, and thus further magnify societal biases, raising profound ethical implications for society. The motivation of the study is to investigate how AI systems imbibe bias introduced in data and further produce unexplainable discriminatory outcomes. This implication of the study will aid in understanding how machine learning models imbibe bias from data and try to influence individuals' articulateness of system outcomes due to bias features. The design of the experiment involves studying the counterfactual impact of racial bias features present in language datasets and its associated effect on the model outcome. A mixed research methodology is adopted to investigate the cross implication of biased model outcome on user experience effect on decision-making through controlled lab experimentation. The findings provide foundation support for correlating the importance of carry-over an artificial intelligence model solving NLP task due to biased concept presented in the dataset. Further, the research outcomes justify the negative influence on users' persuasiveness 1 that leads to altering the decision-making quotient of an individual when trying to rely on the model outcome to act. The paper bridges the gap across the harm caused in establishing poor user trustworthiness due to an inequitable system design and provides strong support for researchers, policymakers, and data scientists to build responsible AI frameworks within organizations. 1 Refer to abbreviation in appendix for more explanation"
Fairness in Deep Learning: A Survey on Vision and Language Research,2023,Otávio Parraga; Martin D. Móre; Christian Mattjie; Nathan Gavenski; Lucas Silveira Kupssinskü; Adilson Medronha; Luis V. Moura; Gabriel S. Simões; Rodrigo C. Barros,ACM Computing Surveys,27,W4390002645,10.1145/3637549,https://openalex.org/W4390002645,https://dl.acm.org/doi/pdf/10.1145/3637549,Debiasing; Computer science; Artificial intelligence; Artificial neural network; Taxonomy (biology),review,True,"Despite being responsible for state-of-the-art results in several computer vision and natural language processing tasks, neural networks have faced harsh criticism due to some of their current shortcomings. One of them is that neural networks are correlation machines prone to model biases within the data instead of focusing on actual useful causal relationships. This problem is particularly serious in application domains affected by aspects such as race, gender, and age. To prevent models from incurring unfair decision-making, the AI community has concentrated efforts on correcting algorithmic biases, giving rise to the research area now widely known as fairness in AI . In this survey paper, we provide an in-depth overview of the main debiasing methods for fairness-aware neural networks in the context of vision and language research. We propose a novel taxonomy that builds upon previous proposals but is tailored for deep learning research to better organize the literature on debiasing methods for fairness. We review all important neural-based methods and evaluation metrics while discussing the current challenges, trends, and important future work directions for the interested researcher and practitioner."
Individual bias and fluctuations in collective decision making: from algorithms to Hamiltonians,2023,Petro Sarkanych; Mariana Krasnytska; Luis Gómez-Nava; Paweł Romańczuk; Yurij Holovatch,Physical Biology,2,W4377013935,10.1088/1478-3975/acd6ce,https://openalex.org/W4377013935,https://doi.org/10.1088/1478-3975/acd6ce,Statistical physics; Probabilistic logic; Hamiltonian (control theory); Analogy; Mathematics,article,True,"Abstract In this paper, we reconsider the spin model suggested recently to understand some features of collective decision making among higher organisms (Hartnett et al 2016 Phys. Rev. Lett. 116 038701). Within the model, the state of an agent i is described by the pair of variables corresponding to its opinion <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" overflow=""scroll""> <mml:msub> <mml:mi>S</mml:mi> <mml:mi>i</mml:mi> </mml:msub> <mml:mo>=</mml:mo> <mml:mo>±</mml:mo> <mml:mn>1</mml:mn> </mml:math> and a bias ω i toward any of the opposing values of S i . Collective decision making is interpreted as an approach to the equilibrium state within the nonlinear voter model subject to a social pressure and a probabilistic algorithm. Here, we push such a physical analogy further and give the statistical physics interpretation of the model, describing it in terms of the Hamiltonian of interaction and looking for the equilibrium state via explicit calculation of its partition function. We show that, depending on the assumptions about the nature of social interactions, two different Hamiltonians can be formulated, which can be solved using different methods. In such an interpretation the temperature serves as a measure of fluctuations, not considered before in the original model. We find exact solutions for the thermodynamics of the model on the complete graph. The general analytical predictions are confirmed using individual-based simulations. The simulations also allow us to study the impact of system size and initial conditions on the collective decision making in finite-sized systems, in particular, with respect to convergence to metastable states."
Ethical Implications and Principles of Using Artificial Intelligence Models in the Classroom: A Systematic Literature Review.,2024,Tang Lin; Yu-Sheng Su,International Journal of Interactive Multimedia and Artificial Intelligence,31,W4392307504,10.9781/ijimai.2024.02.010,https://openalex.org/W4392307504,https://doi.org/10.9781/ijimai.2024.02.010,Computer science; Artificial intelligence; Data science; Management science; Cognitive science,article,True,"The increasing use of artificial intelligence (AI) models in the classroom not only brings a large number of benefits, but also has a variety of ethical implications. To provide effective education, it is now necessary to understand the ethical implications of using AI models in the classroom, and the principles for avoiding and addressing these ethical implications. However, existing research on the ethical implications of using AI models in the classroom is rather sparse, and a holistic overview is lacking. Therefore, this study seeks to offer an overview of research on the ethical implications, ethical principles and the future research directions and practices of using AI models in the classroom through a systematic literature review. Out of 1,445 initially identified publications between 2013 and 2023, 32 articles were included for final coding analysis, identified using explicit inclusion and exclusion criteria. The findings revealed five main ethical implications, namely algorithmic bias and discrimination, data privacy leakage, lack of transparency, decreased autonomy, and academic misconduct, with algorithmic bias being the most prominent (i.e., the number of existing studies is the most), followed by privacy leakage, whereas decreased autonomy and academic misconduct were relatively understudied; and six main ethical principles, namely fairness, privacy, transparency, accountability, autonomy and beneficence, with fairness being the most prominent ethical principle (i.e., the number of existing studies is the most), followed by privacy, while autonomy and beneficence were relatively understudied. Future directions of research are given, and guidelines for future practice are provided: (1) further substantive discussion, understanding and solution of ethical implications are required; (2) the precise mechanism of ethical principles of using AI models in the classroom remains to be elucidated and extended to the implementation phase; and (3) the ethical implications of the use of AI models in the classroom require accurate assessment."
Aware and Critical Navigation in the Media Landscape: (Un)biased Algorithms and the Need for New Media Literacy in the Era of Artificial Intelligence and Digital Media,2023,Aneta Risteska,KAIROS Media and Communication Review,4,W4413265046,10.64370/tsnh6944,https://openalex.org/W4413265046,https://doi.org/10.64370/tsnh6944,Media literacy; Social media; Literacy; Transparency (behavior); Accountability,article,True,"As technology advances rapidly, media literacy education plays a crucial role in supplying individuals with the skills and knowledge to navigate the complex media landscape. The article examines the ethical implications of AI algorithms highlighting the importance of critical awareness among users. AI-driven recommendation systems have considerable influence over individuals' information consumption and worldview, which requires media literacy education to foster a deep understanding of biases, limitations and potential risks associated with these algorithms. This paper points to the need for ethical behaviour to govern AI algorithms, ensuring transparency, accountability and fairness in content curation. Additionally, the article brings examples that indicate how algorithms work and what consequences they can leave in our social life and actions if we do not create them according to certain ethical values, or if we consume their messages without critical awareness. New media literacy education should empower individuals to make informed decisions about their privacy and develop a critical stance toward data collection practices. Concepts such as informed consent, data anonymity, and the implications of targeted advertising should be addressed in media literacy education. Furthermore, the paper emphasizes the responsibilities of media literacy educators themselves. Teachers and institutions must ensure that media literacy programs promote inclusivity, diversity, and a global vision. By incorporating ethical frameworks into the curriculum, educators can cultivate responsible digital citizenship and encourage critical thinking about the social impact of AI and digital media. Media literacy education in the context of AI and digital media must address the ethical dimensions inherent in these technologies. By equipping individuals with the necessary tools to critically analyse algorithms, navigate data privacy concerns, and foster responsible digital citizenship, media literacy education can facilitate an informed and ethical engagement with AI and digital media."
Artificial Intelligence and Inequality: Challenges and Opportunities,2024,Milad Shahvaroughi Farahani; Ghazal Ghasemi,,59,W4391997180,10.32388/7hwuz2,https://openalex.org/W4391997180,https://doi.org/10.32388/7hwuz2,Inequality; Computer science; Mathematics; Mathematical analysis,preprint,True,"Integrating artificial intelligence (AI) technologies into various aspects of society has sparked both excitement and concern regarding its potential impact on inequality. This abstract provides an overview of the key issues surrounding AI and inequality, exploring the challenges and opportunities arising from the widespread adoption of AI systems. Firstly, we examine how AI technologies have the potential to exacerbate existing inequalities across various domains, including labor markets, education, healthcare, and access to services. AI-driven automation may lead to job displacement and wage polarization, widening the gap between high-skilled and low-skilled workers. Moreover, algorithmic biases embedded in AI systems can perpetuate discrimination and inequity, particularly against marginalized communities. However, alongside these challenges, AI also presents opportunities to address inequality and promote inclusivity. AI-powered innovations have the potential to enhance efficiency, accessibility, and affordability in sectors such as healthcare, education, and financial services, thereby reducing disparities in access to essential resources and opportunities. Additionally, initiatives focused on ethical AI development and responsible AI governance can mitigate the negative impacts of AI on inequality by promoting fairness, transparency, and accountability in algorithmic decision-making processes. In conclusion, while AI has the potential to both exacerbate and mitigate inequality, its ultimate impact depends on the choices we make in designing, deploying, and governing AI systems. By prioritizing equity, social justice, and human welfare in AI development and implementation, we can harness the transformative power of AI to create a more equitable and inclusive society."
Bias in Green AI Addressing Disparities in Data and Algorithms,2025,Avtar Krishan Trehan,Advances in environmental engineering and green technologies book series,1,W4407435484,10.4018/979-8-3693-9471-7.ch005,https://openalex.org/W4407435484,,Computer science; Algorithm,book-chapter,False,"Green AI, a paradigm focused on sustainable and energy-efficient artificial intelligence, holds immense promise for advancing environmental and social equity goals. However, biases inherent in data and algorithms can perpetuate or even exacerbate existing disparities, undermining these objectives. This chapter explores the critical intersection of bias and Green AI, examining how inequities arise in the development and deployment of AI systems designed for sustainability. By analyzing case studies and recent research, the chapter highlights the implications of biased datasets, algorithmic decisions, and accessibility gaps. It also proposes strategies for mitigating bias, such as ethical AI frameworks, diverse data collection practices, and community-driven approaches. Addressing these challenges is essential to ensure that Green AI contributes to an equitable and inclusive future while meeting its sustainability goals."
Loss modeling with the size-biased lognormal mixture and the entropy regularized EM algorithm,2024,Taehan Bae; Tatjana Miljkovic,Insurance Mathematics and Economics,2,W4396888684,10.1016/j.insmatheco.2024.05.003,https://openalex.org/W4396888684,https://doi.org/10.1016/j.insmatheco.2024.05.003,Log-normal distribution; Mixture model; Erlang (programming language); Mathematics; Expectation–maximization algorithm,article,True,"The Erlang mixture with a common scale parameter is one of many popular models for modeling insurance losses. However, the actuarial literature recognizes and discusses some limitations of aforementioned model in approximate heavy-tailed distributions. In this paper, a size-biased left-truncated Lognormal (SB-ltLN) mixture is proposed as a robust alternative to the Erlang mixture for modeling left-truncated insurance losses with a heavy tail. The weak denseness property of the weighted Lognormal mixture is studied along with the tail behavior. Explicit analytical solutions are derived for moments and Tail Value at Risk based on the proposed model. An extension of the regularized expectation–maximization (REM) algorithm with Shannon's entropy weights (ewREM) is introduced for parameter estimation and variability assessment. The Operational Riskdata eXchange's left-truncated internal fraud loss data set is used to illustrate applications of the proposed model. Finally, the results of a simulation study show promising performance of the proposed SB-ltLN mixture in different simulation settings."
Finding the white male: The prevalence and consequences of algorithmic gender and race bias in political Google searches,2024,Tobias Rohrbach; Mykola Makhortykh; Maryna Sydorova,arXiv (Cornell University),2,W4396606341,10.48550/arxiv.2405.00335,https://openalex.org/W4396606341,https://arxiv.org/pdf/2405.00335,Race (biology); Politics; White (mutation); Gender bias; Political science,preprint,True,"Search engines like Google have become major information gatekeepers that use artificial intelligence (AI) to determine who and what voters find when searching for political information. This article proposes and tests a framework of algorithmic representation of minoritized groups in a series of four studies. First, two algorithm audits of political image searches delineate how search engines reflect and uphold structural inequalities by under- and misrepresenting women and non-white politicians. Second, two online experiments show that these biases in algorithmic representation in turn distort perceptions of the political reality and actively reinforce a white and masculinized view of politics. Together, the results have substantive implications for the scientific understanding of how AI technology amplifies biases in political perceptions and decision-making. The article contributes to ongoing public debates and cross-disciplinary research on algorithmic fairness and injustice."
Machine learning software for optimizing SME social media marketing campaigns,2024,Wagobera Edgar Kedi; Chibundom Ejimuda; Courage Idemudia; Tochukwu Ignatius Ijomah,Computer Science & IT Research Journal,35,W4400999562,10.51594/csitrj.v5i7.1349,https://openalex.org/W4400999562,https://fepbl.com/index.php/csitrj/article/download/1349/1581,Social media; Business; Social media marketing; Software; Marketing,article,True,"This review paper explores the transformative role of machine learning in optimizing social media marketing strategies for small and medium-sized enterprises (SMEs). It begins by highlighting the significance of social media marketing for SMEs, outlining the historical context of traditional marketing strategies, and examining current trends and emerging machine learning applications. The paper delves into the technical challenges of implementing machine learning, such as data quality, algorithm complexity, and system integration, as well as ethical concerns surrounding data privacy and algorithmic bias. SME-specific limitations are also discussed, including budget constraints and lack of technical expertise. Future directions focus on emerging technologies like deep learning and reinforcement learning, offering practical recommendations for SMEs to leverage these advancements effectively. The conclusion emphasizes the importance of embracing machine learning to achieve sustainable growth and competitive advantage in the digital marketplace. Keywords: Machine Learning, Social Media Marketing, SMEs, Data Privacy, Audience Targeting."
Mitigating the risk of artificial intelligence bias in cardiovascular care,2024,Ariana Mihan; Ambarish Pandey; Harriette G.C. Van Spall,The Lancet Digital Health,22,W4401983318,10.1016/s2589-7500(24)00155-9,https://openalex.org/W4401983318,https://doi.org/10.1016/s2589-7500(24)00155-9,Computer science; Artificial intelligence; Risk analysis (engineering); Medicine,review,True,"Digital health technologies can generate data that can be used to train artificial intelligence (AI) algorithms, which have been particularly transformative in cardiovascular health-care delivery. However, digital and health-care data repositories that are used to train AI algorithms can introduce bias when data are homogeneous and health-care processes are inequitable. AI bias can also be introduced during algorithm development, testing, implementation, and post-implementation processes. The consequences of AI algorithmic bias can be considerable, including missed diagnoses, misclassification of disease, incorrect risk prediction, and inappropriate treatment recommendations. This bias can disproportionately affect marginalised demographic groups. In this Series paper, we provide a brief overview of AI applications in cardiovascular health care, discuss stages of algorithm development and associated sources of bias, and provide examples of harm from biased algorithms. We propose strategies that can be applied during the training, testing, and implementation of AI algorithms to mitigate bias so that all those at risk for or living with cardiovascular disease might benefit equally from AI."
A Biased Random Key Genetic Algorithm for Solving the Longest Common Square Subsequence Problem,2024,Jaume Reixach; Christian Blum; Marko Djukanović; Günther R. Raidl,IEEE Transactions on Evolutionary Computation,2,W4399571756,10.1109/tevc.2024.3413150,https://openalex.org/W4399571756,,Longest common subsequence problem; Key (lock); Algorithm; Longest increasing subsequence; Genetic algorithm,article,False,"This article considers the longest common square subsequence (LCSqS) problem, a variant of the longest common subsequence (LCS) problem in which solutions must be square strings. A square string can be expressed as the concatenation of a string with itself. The LCSqS problem has applications in bioinformatics, for discovering internal similarities between molecular structures. We propose a metaheuristic approach, a biased random key genetic algorithm (BRKGA) hybridized with a beam search (BS) from the literature. Our approach is based on reducing the LCSqS problem to a set of promising LCS problems. This is achieved by cutting each input string into two parts first and then evaluating such a transformed instance by solving the LCS problem for the obtained overall set of strings. The task of the BRKGA is, hereby, to find a set of good cut points for the input strings. For this purpose, the search is carefully biased by problem-specific greedy information. For each cut point vector, the resulting LCS problem is approximately solved by the existing BS approach. The proposed algorithm is evaluated against a previously proposed state-of-the-art variable neighborhood search (VNS) on random uniform instances from the literature, new nonuniform instances, and a real-world instance set consisting of DNA strings. The results underscore the importance of our work, as our novel approach outperforms former state-of-the-art with statistical significance. Particularly, they evidence the limitations of the VNS when solving nonuniform instances, for which our method shows superior performance."
Harnessing the Power of AI: A Comprehensive Review of Its Impact and Challenges in Nursing Science and Healthcare,2023,Seema Yelne; Minakshi Chaudhary; Karishma Dod; Akhtaribano Sayyad; Ranjana Sharma,Cureus,150,W4388895516,10.7759/cureus.49252,https://openalex.org/W4388895516,https://assets.cureus.com/uploads/review_article/pdf/206741/20231122-10807-1n3hf8c.pdf,Transformative learning; Health care; Medicine; Engineering ethics; Applications of artificial intelligence,review,True,
Bias and Non-Diversity of Big Data in Artificial Intelligence: Focus on Retinal Diseases,2023,Cris Martin P. Jacoba; Leo Anthony Celi; Anja Lorch; Ward Fickweiler; Lucia Sobrin; Judy Wawira Gichoya; Lloyd Paul Aiello; Paolo S. Silva,Seminars in Ophthalmology,32,W4317359848,10.1080/08820538.2023.2168486,https://openalex.org/W4317359848,,Diversity (politics); Health care; Big data; Medicine; Inequality,article,False,"Artificial intelligence (AI) applications in healthcare will have a potentially far-reaching impact on patient care, however issues regarding algorithmic bias and fairness have recently surfaced. There is a recognized lack of diversity in the available ophthalmic datasets, with 45% of the global population having no readily accessible representative images, leading to potential misrepresentations of their unique anatomic features and ocular pathology. AI applications in retinal disease may show less accuracy with underrepresented populations that may further widen the gap of health inequality if left unaddressed. Beyond disease symptomatology, social determinants of health must be integrated into our current paradigms of disease understanding, with the goal of more personalized care. AI has the potential to decrease global healthcare inequality, but it will need to be based on a more diverse, transparent and responsible use of healthcare data."
BrkgaCuda 2.0: a framework for fast biased random-key genetic algorithms on GPUs,2024,Bruno Oliveira; Eduardo C. Xavier; Edson Borin,Soft Computing,3,W4404695137,10.1007/s00500-024-10336-7,https://openalex.org/W4404695137,,Computer science; Key (lock); Parallel computing; Genetic algorithm; Algorithm,article,False,
"Simplicity bias, algorithmic probability, and the random logistic map",2023,Boumediene Hamzi; Kamaludin Dingle,arXiv (Cornell University),1,W4390529213,10.48550/arxiv.2401.00593,https://openalex.org/W4390529213,https://arxiv.org/pdf/2401.00593,Simplicity; Noise (video); Computer science; Algorithm; Extrapolation,preprint,True,"Simplicity bias is an intriguing phenomenon prevalent in various input-output maps, characterized by a preference for simpler, more regular, or symmetric outputs. Notably, these maps typically feature high-probability outputs with simple patterns, whereas complex patterns are exponentially less probable. This bias has been extensively examined and attributed to principles derived from algorithmic information theory and algorithmic probability. In a significant advancement, it has been demonstrated that the renowned logistic map and other one-dimensional maps exhibit simplicity bias when conceptualized as input-output systems. Building upon this work, our research delves into the manifestations of simplicity bias within the random logistic map, specifically focusing on scenarios involving additive noise. We discover that simplicity bias is observable in the random logistic map for specific ranges of $μ$ and noise magnitudes. Additionally, we find that this bias persists even with the introduction of small measurement noise, though it diminishes as noise levels increase. Our studies also revisit the phenomenon of noise-induced chaos, particularly when $μ=3.83$, revealing its characteristics through complexity-probability plots. Intriguingly, we employ the logistic map to illustrate a paradoxical aspect of data analysis: more data adhering to a consistent trend can occasionally lead to \emph{reduced} confidence in extrapolation predictions, challenging conventional wisdom. We propose that adopting a probability-complexity perspective in analyzing dynamical systems could significantly enrich statistical learning theories related to series prediction and analysis. This approach not only facilitates a deeper understanding of simplicity bias and its implications but also paves the way for novel methodologies in forecasting complex systems behavior."
Using conventional framing to offset bias against algorithmic errors,2025,Hamza Tariq; Jonathan A. Fugelsang; Derek J. Koehler,Judgment and Decision Making,1,W4409516416,10.1017/jdm.2025.8,https://openalex.org/W4409516416,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/E07D28133525298F30DC81B06887FAF3/S1930297525000087a.pdf/div-class-title-using-conventional-framing-to-offset-bias-against-algorithmic-errors-div.pdf,Framing (construction); Offset (computer science); Computer science; Social psychology; Psychology,article,True,"Abstract Prior research has shown that people judge algorithmic errors more harshly than identical mistakes made by humans—a bias known as algorithm aversion. We explored this phenomenon across two studies ( N = 1199), focusing on the often-overlooked role of conventionality when comparing human versus algorithmic errors by introducing a simple conventionality intervention. Our findings revealed significant algorithm aversion when participants were informed that the decisions described in the experimental scenarios were conventionally made by humans. However, when participants were told that the same decisions were conventionally made by algorithms, the bias was significantly reduced—or even completely offset. This intervention had a particularly strong influence on participants’ recommendations of which decision-maker should be used in the future—even revealing a bias against human error makers when algorithms were framed as the conventional choice. These results suggest that the existing status quo plays an important role in shaping people’s judgments of mistakes in human–algorithm comparisons."
AI SOLUTIONS FOR DEVELOPMENTAL ECONOMICS: OPPORTUNITIES AND CHALLENGES IN FINANCIAL INCLUSION AND POVERTY ALLEVIATION,2024,Temitayo Oluwaseun Jejeniwa; Noluthando Zamanjomane Mhlongo; Titilola Olaide Jejeniwa,International Journal of Advanced Economics,36,W4395669997,10.51594/ijae.v6i4.1073,https://openalex.org/W4395669997,https://fepbl.com/index.php/ijae/article/download/1073/1297,Financial inclusion; Poverty; Inclusion (mineral); Economics; Development economics,article,True,"AI presents immense potential in addressing the complex challenges of developmental economics, particularly in the realms of financial inclusion and poverty alleviation. This abstract explores the opportunities and challenges associated with integrating AI solutions in these critical areas. Financial inclusion, essential for sustainable development, remains hampered by barriers such as limited access to banking services, socioeconomic disparities, and regulatory constraints. AI offers innovative approaches through data analytics and prediction models, enabling tailored financial services, risk assessment, and personalized interventions. However, the implementation of AI solutions poses significant challenges, including concerns regarding data privacy, ethical implications such as algorithmic bias, and accessibility issues in underserved regions. Through case studies and best practices, lessons can be gleaned to inform future initiatives, emphasizing the importance of adaptable policy frameworks, collaboration, and impact assessment. Looking ahead, emerging AI technologies like blockchain and enhanced regulatory measures hold promise, necessitating cross-sector partnerships and a concerted effort to harness AI's transformative potential for sustainable development and inclusive growth. Keywords: Developmental Economics, Financial Inclusion, Poverty Alleviation, AI Solutions, Challenges, Opportunities."
"Artificial Intelligence in Head and Neck Cancer: Innovations, Applications, and Future Directions",2024,Tuan D. Pham; Muy‐Teck Teh; Domniki Chatzopoulou; Simon Holmes; Paul Coulthard,Current Oncology,36,W4402314224,10.3390/curroncol31090389,https://openalex.org/W4402314224,https://doi.org/10.3390/curroncol31090389,Artificial intelligence; Applications of artificial intelligence; Medicine; Precision medicine; Deep learning,review,True,"Artificial intelligence (AI) is revolutionizing head and neck cancer (HNC) care by providing innovative tools that enhance diagnostic accuracy and personalize treatment strategies. This review highlights the advancements in AI technologies, including deep learning and natural language processing, and their applications in HNC. The integration of AI with imaging techniques, genomics, and electronic health records is explored, emphasizing its role in early detection, biomarker discovery, and treatment planning. Despite noticeable progress, challenges such as data quality, algorithmic bias, and the need for interdisciplinary collaboration remain. Emerging innovations like explainable AI, AI-powered robotics, and real-time monitoring systems are poised to further advance the field. Addressing these challenges and fostering collaboration among AI experts, clinicians, and researchers is crucial for developing equitable and effective AI applications. The future of AI in HNC holds significant promise, offering potential breakthroughs in diagnostics, personalized therapies, and improved patient outcomes."
"The YOLO Framework: A Comprehensive Review of Evolution, Applications, and Benchmarks in Object Detection",2024,Momina Liaqat Ali; Zhou Zhang,Preprints.org,60,W4403712112,10.20944/preprints202410.1785.v1,https://openalex.org/W4403712112,https://www.preprints.org/manuscript/202410.1785/v1/download,Computer science; Object (grammar); Data science; Artificial intelligence; Systems engineering,review,True,"This paper presents a comprehensive review of the You Only Look Once (YOLO) framework, a transformative one-stage object detection algorithm renowned for its remarkable balance between speed and accuracy. Since its inception, YOLO has evolved significantly, with versions spanning from YOLOv1 to the most recent YOLOv11, each introducing pivotal innovations in feature extraction, bounding box prediction, and optimization techniques. These advancements, particularly in the backbone, neck, and head components, have positioned YOLO as a leading solution for real-time object detection across a variety of domains. In this review, we explore YOLO&amp;#039;s diverse applications, including its critical role in medical imaging for COVID-19 detection, breast cancer identification, and tumor localization, where it has significantly enhanced diagnostic efficiency. YOLO&amp;#039;s robust performance in autonomous vehicles is also highlighted, as it excels in challenging conditions like fog, rain, and low-light environments, thereby contributing to improved road safety and autonomous driving systems. In the agricultural sector, YOLO has transformed precision farming by enabling early detection of pests, diseases, and crop health issues, promoting more sustainable farming practices. Additionally, we provide an in-depth performance analysis of YOLO models—such as YOLOv9, YOLO-NAS, YOLOv10, and YOLOv11—across multiple benchmark datasets. This analysis compares their suitability for a range of applications, from lightweight embedded systems to high-resolution, complex object detection tasks. The paper also addresses YOLO&amp;#039;s challenges, such as occlusion, small object detection, and dataset biases, while discussing recent advancements that aim to mitigate these limitations. Moreover, we examine the ethical implications of YOLO&amp;#039;s deployment, particularly in surveillance and monitoring applications, raising concerns about privacy, algorithmic biases, and the potential to perpetuate societal inequities. These ethical considerations are critical in domains like law enforcement, where biased object detection models can have serious repercussions. Through this detailed review of YOLO&amp;#039;s technical advancements, applications, performance, and ethical challenges, this paper serves as a valuable resource for researchers, developers, and policymakers looking to understand YOLO’s current capabilities and future directions in the evolving field of object detection."
The Impact of NBA Implementation Across Engineering Disciplines,2024,S. Saravanan; J. Chandrasekar; S. Satheesh Kumar; Pavan Patel; J. Maria Shanthi; Sampath Boopathi,Advances in higher education and professional development book series,25,W4399452311,10.4018/979-8-3693-1666-5.ch010,https://openalex.org/W4399452311,,Transformative learning; Commodification; Entertainment; Equity (law); Engineering,book-chapter,False,"NBA principles and technologies have significantly impacted engineering disciplines, leading to social changes and reshaping industries. NBA-inspired innovations have influenced materials science, data analytics, public health, urban design, and entertainment experiences. The study examines equity, access, and ethical dimensions of NBA-driven engineering innovations, focusing on data privacy, algorithmic bias, and commodification of athlete performance. The goal is to foster inclusivity, equity, and sustainable development, fostering collaboration between the sports industry and engineering disciplines. This approach emphasizes the transformative power of sports-driven innovation in engineering for societal betterment."
Collaborative filtering algorithms are prone to mainstream-taste bias,2023,Pantelis P. Analytis; Philipp Hager,,1,W4386729270,10.1145/3604915.3608825,https://openalex.org/W4386729270,,Collaborative filtering; Computer science; Recommender system; Mainstream; Variation (astronomy),article,False,"Collaborative filtering has been a dominant approach in the recommender systems community since the early 1990s. Collaborative filtering (and other) algorithms, however, have been predominantly evaluated by aggregating results across users or user groups. These performance averages hide large disparities: an algorithm may perform very well for some users (or groups) and poorly for others. We show that performance variation is large and systematic. In experiments on three large-scale datasets and using an array of collaborative filtering algorithms, we demonstrate large performance disparities across algorithms, datasets and metrics for different users. We then show that two key features that characterize users, their mean taste similarity and dispersion in taste similarity with other users, can systematically explain performance variation better than previously identified features. We use these two features to visualize algorithm performance for different users and we point out that this mapping can capture different categories of users that have been proposed before. Our results demonstrate an extensive mainstream-taste bias in collaborative filtering algorithms, which implies a fundamental fairness limitation that needs to be mitigated."
Gender bias perpetuation and mitigation in AI technologies: challenges and opportunities,2023,Sinead O’Connor; Helen K. Liu,AI & Society,88,W4382293074,10.1007/s00146-023-01675-4,https://openalex.org/W4382293074,https://link.springer.com/content/pdf/10.1007/s00146-023-01675-4.pdf,Software deployment; Accountability; Emerging technologies; Categorization; Neutrality,article,True,"Abstract Across the world, artificial intelligence (AI) technologies are being more widely employed in public sector decision-making and processes as a supposedly neutral and an efficient method for optimizing delivery of services. However, the deployment of these technologies has also prompted investigation into the potentially unanticipated consequences of their introduction, to both positive and negative ends. This paper chooses to focus specifically on the relationship between gender bias and AI, exploring claims of the neutrality of such technologies and how its understanding of bias could influence policy and outcomes. Building on a rich seam of literature from both technological and sociological fields, this article constructs an original framework through which to analyse both the perpetuation and mitigation of gender biases, choosing to categorize AI technologies based on whether their input is text or images. Through the close analysis and pairing of four case studies, the paper thus unites two often disparate approaches to the investigation of bias in technology, revealing the large and varied potential for AI to echo and even amplify existing human bias, while acknowledging the important role AI itself can play in reducing or reversing these effects. The conclusion calls for further collaboration between scholars from the worlds of technology, gender studies and public policy in fully exploring algorithmic accountability as well as in accurately and transparently exploring the potential consequences of the introduction of AI technologies."
Active learning with human heuristics: an algorithm robust to labeling bias,2024,Sriram Ravichandran; Nandan Sudarsanam; Balaraman Ravindran; Konstantinos V. Katsikopoulos,Frontiers in Artificial Intelligence,1,W4404509705,10.3389/frai.2024.1491932,https://openalex.org/W4404509705,https://doi.org/10.3389/frai.2024.1491932,Heuristics; Computer science; Oracle; Machine learning; Artificial intelligence,article,True,"Active learning enables prediction models to achieve better performance faster by adaptively querying an oracle for the labels of data points. Sometimes the oracle is a human, for example when a medical diagnosis is provided by a doctor. According to the behavioral sciences, people, because they employ heuristics, might sometimes exhibit biases in labeling. How does modeling the oracle as a human heuristic affect the performance of active learning algorithms? If there is a drop in performance, can one design active learning algorithms robust to labeling bias? The present article provides answers. We investigate two established human heuristics (fast-and-frugal tree, tallying model) combined with four active learning algorithms (entropy sampling, multi-view learning, conventional information density, and, our proposal, inverse information density) and three standard classifiers (logistic regression, random forests, support vector machines), and apply their combinations to 15 datasets where people routinely provide labels, such as health and other domains like marketing and transportation. There are two main results. First, we show that if a heuristic provides labels, the performance of active learning algorithms significantly drops, sometimes below random. Hence, it is key to design active learning algorithms that are robust to labeling bias. Our second contribution is to provide such a robust algorithm. The proposed inverse information density algorithm, which is inspired by human psychology, achieves an overall improvement of 87% over the best of the other algorithms. In conclusion, designing and benchmarking active learning algorithms can benefit from incorporating the modeling of human heuristics."
Mitigating Bias in Clinical Machine Learning Models,2024,Julio C. Perez-Downes; Andrew S. Tseng; Keith Mcconn; Sara Elattar; Olayemi Sokumbi; Ronnie Sebro; Megan Allyse; Bryan Dangott; Rickey E. Carter; Demilade Adedinsewo,Current Treatment Options in Cardiovascular Medicine,12,W4391722986,10.1007/s11936-023-01032-0,https://openalex.org/W4391722986,,Machine learning; Artificial intelligence; Medicine; Health care; Precision medicine,article,False,
"Navigating and reviewing ethical dilemmas in AI development: Strategies for transparency, fairness, and accountability",2024,Olatunji Akinrinola; Chinwe Chinazo Okoye; Onyeka Chrisanctus Ofodile; Chinonye Esther Ugochukwu,GSC Advanced Research and Reviews,128,W4392621839,10.30574/gscarr.2024.18.3.0088,https://openalex.org/W4392621839,https://gsconlinepress.com/journals/gscarr/sites/default/files/GSCARR-2024-0088.pdf,Accountability; Transparency (behavior); Psychology; Engineering ethics; Political science,article,True,"As artificial intelligence (AI) continues to permeate various aspects of our lives, the ethical challenges associated with its development become increasingly apparent. This paper navigates and reviews the ethical dilemmas in AI development, focusing on strategies to promote transparency, fairness, and accountability. The rapid growth of AI technology has given rise to concerns related to bias, lack of transparency, and the need for clear accountability mechanisms. In this exploration, we delve into the intricate ethical landscape of AI, examining issues such as bias and fairness, lack of transparency, and the challenges associated with accountability. To address these concerns, we propose strategies for transparency, including the implementation of Explainable AI (XAI), advocating for open data sharing, and embracing ethical AI frameworks. Furthermore, we explore strategies to promote fairness in AI algorithms, emphasizing the importance of fairness metrics, diverse training data, and continuous monitoring for iterative improvement. Additionally, the paper delves into strategies to ensure accountability in AI development, considering regulatory measures, ethical AI governance, and the incorporation of human-in-the-loop approaches. To provide practical insights, case studies and real-world examples are analyzed to distill lessons learned and best practices. The paper concludes with a comprehensive overview of the proposed strategies, emphasizing the importance of balancing innovation with ethical responsibility in the evolving landscape of AI development. This work contributes to the ongoing discourse on AI ethics, offering a roadmap for navigating the challenges and fostering responsible AI development practices."
Bias-Compensated PNLMS Algorithm With Multi-Segment Function for Noisy Input,2025,Zhan Jin; Zhonghao Yang; Qianjin Li; Linfeng Ma,IEEE Access,1,W4407948927,10.1109/access.2025.3546064,https://openalex.org/W4407948927,https://doi.org/10.1109/access.2025.3546064,Algorithm; Computer science; Function (biology); Pattern recognition (psychology); Artificial intelligence,article,True,"To address the estimation bias caused by ignoring input noise in existing adaptive filtering algorithms, a new proportionate-type algorithm is proposed in this paper. First, a bias-compensation term is derived based on an unbiased criterion when constructing the cost function of the algorithm to achieve unbiased estimation. Next, this bias-compensation term is integrated into the mu-law PNLMS (MPNLMS) algorithm to design the bias-compensated PNLMS combined with the multi-segment function (BC-MS-PNLMS) algorithm. Simulation results for echo paths and underwater channels demonstrate that the BC-MS-PNLMS algorithm outperforms other sparse-type algorithms in sparse experimental environments."
Ethical and regulatory considerations in the use of AI and machine learning in nursing: A systematic review,2025,Suheb Mohammed; Yasmine M Osman; Ateya Megahed Ibrahim; Mostafa Shaban,International Nursing Review,18,W4408244061,10.1111/inr.70010,https://openalex.org/W4408244061,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/inr.70010,Transparency (behavior); Safeguarding; Autonomy; Accountability; Nursing,review,True,"Abstract Aim This study systematically explores the ethical and regulatory considerations surrounding the integration of artificial intelligence (AI) and machine learning (ML) in nursing practice, with a focus on patient autonomy, data privacy, algorithmic bias, and accountability. Background AI and ML are transforming nursing practice by enhancing clinical decision‐making and operational efficiency. However, these technologies present significant ethical challenges related to ensuring patient autonomy, safeguarding data privacy, mitigating algorithmic bias, and ensuring transparency in decision‐making processes. Current frameworks are not sufficiently tailored to nursing‐specific contexts. Methods A systematic review was conducted, adhering to PRISMA guidelines. Six major databases were searched for studies published between 2000 and 2024. Seventeen studies met the inclusion criteria and were included in the final analysis. Results Five key themes emerged from the review: enhancement of clinical decision‐making, promotion of ethical awareness, support for routine nursing tasks, challenges in algorithmic bias, and the importance of public engagement in regulatory frameworks. The review identified critical gaps in nursing‐specific ethical guidelines and regulatory oversight for AI integration in practice. Discussion AI technologies offer substantial benefits for nursing, particularly in decision‐making and task efficiency. However, these advantages must be balanced against ethical concerns, including the protection of patient rights, algorithmic transparency, and bias mitigation. Current regulatory frameworks require adaptation to meet the ethical needs of nursing. Conclusion and implications for nursing and health policy The findings emphasize the need for the development of nursing‐specific ethical guidelines and robust regulatory frameworks to ensure the responsible integration of AI technologies into nursing practice. AI integration must uphold ethical principles while enhancing the quality of care."
Transformative AI in human resource management: enhancing workforce planning with topic modeling,2024,Murale Venugopal; Vandana Madhavan; Rajiv Prasad; Raghu Raman,Cogent Business & Management,21,W4404772152,10.1080/23311975.2024.2432550,https://openalex.org/W4404772152,https://doi.org/10.1080/23311975.2024.2432550,Transformative learning; Workforce; Knowledge management; Business; Workforce planning,article,True,"This study explores the transformative role of artificial intelligence (AI) in human resource management (HRM), focusing on key functions such as recruitment, retention, and performance management. A comprehensive review was carried out PRISMA framework and BERTopic model on AI and HRM‑related keywords. The resulting publications were analyzed to extract meaningful topics. AI‑driven tools streamline candidate screening and interview analysis, significantly enhancing hiring efficiency and decision‑making accuracy. Concerns about algorithmic bias highlight the need for robust governance frameworks to ensure transparency and fairness in AI‑driven processes. The study emphasizes the importance of aligning AI adoption with Organizational Development principles to foster inclusivity and organizational justice. The integration of AI in performance management facilitates real‑time, objective performance assessments, although overreliance on such technologies can affect employee trust and engagement. Despite these advances, the study highlights ethical concerns surrounding data privacy and the potential for algorithmic bias. Addressing these challenges requires the implementation of comprehensive ethical frameworks to promote fairness and inclusivity in AI‑HRM applications. Strategically, AI transforms HR from a reactive function to a proactive, data‑driven partner aligned with long‑term organizational goals. Successful AI integration depends on governance mechanisms that uphold ethical standards, foster employee trust, and ensure transparency, enabling organizations to fully leverage AI’s potential in enhancing workforce management."
Beyond traditional interviews: Psychometric analysis of asynchronous video interviews for personality and interview performance evaluation using machine learning,2024,Antonis Koutsoumpis; Sina Ghassemi; Janneke K. Oostrom; Djurre Holtrop; Ward van Breda; Tianyi Zhang; Reinout E. de Vries,Computers in Human Behavior,32,W4390479873,10.1016/j.chb.2023.108128,https://openalex.org/W4390479873,https://doi.org/10.1016/j.chb.2023.108128,Psychology; Asynchronous communication; Applied psychology; Personality; Social psychology,article,True,
Impact on bias mitigation algorithms to variations in inferred sensitive attribute uncertainty,2025,Yanchen Wang; Lisa Singh,Frontiers in Artificial Intelligence,1,W4408191332,10.3389/frai.2025.1520330,https://openalex.org/W4408191332,https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1520330/pdf,Computer science; Debiasing; Inference; Trustworthiness; Data mining,article,True,"Concerns about the trustworthiness, fairness, and privacy of AI systems are growing, and strategies for mitigating these concerns are still in their infancy. One approach to improve trustworthiness and fairness in AI systems is to use bias mitigation algorithms. However, most bias mitigation algorithms require data sets that contain sensitive attribute values to assess the fairness of the algorithm. A growing number of real world data sets do not make sensitive attribute information readily available to researchers. One solution is to infer the missing sensitive attribute information and apply an existing bias mitigation algorithm using this inferred knowledge. While researchers are beginning to explore this question, it is still unclear how robust existing bias mitigation algorithms are to different levels of inference accuracy. This paper explores this question by investigating the impact of different levels of accuracy of the inferred sensitive attribute on the performance of different bias mitigation strategies. We generate variation in sensitive attribute accuracy using both simulation and construction of neural models for the inference task. We then assess the quality of six bias mitigation algorithms that are deployed across different parts of our learning life cycle: pre-processing, in-processing, and post-processing. We find that the disparate impact remover is the least sensitive bias mitigation strategy and that if we apply the bias mitigation algorithms using an inferred sensitive attribute with reasonable accuracy, the fairness scores are higher than the best standard model and the balanced accuracy is similar to that of the standard model. These findings open the door for improving fairness of black box AI systems using some bias mitigation strategies."
Age-related bias and artificial intelligence: a scoping review,2023,Charlene H. Chu; Simon Donato‐Woodger; Shehroz S. Khan; Rune Nyrup; Kathleen Leslie; Alexandra Lyn; Tianyu Shi; Andria Bianchi; Samira Abbasgholizadeh Rahimi; Amanda Grenier,Humanities and Social Sciences Communications,61,W4385952733,10.1057/s41599-023-01999-y,https://openalex.org/W4385952733,https://www.nature.com/articles/s41599-023-01999-y.pdf,Artificial intelligence; Grey literature; Gender bias; Computer science; Machine learning,review,True,"Abstract There are widespread concerns about bias and discriminatory output related to artificial intelligence (AI), which may propagate social biases and disparities. Digital ageism refers to ageism reflected design, development, and implementation of AI systems and technologies and its resultant data. Currently, the prevalence of digital ageism and the sources of AI bias are unknown. A scoping review informed by the Arksey and O’Malley methodology was undertaken to explore age-related bias in AI systems, identify how AI systems encode, produce, or reinforce age-related bias, what is known about digital ageism, and the social, ethical and legal implications of age-related bias. A comprehensive search strategy that included five electronic bases and grey literature sources including legal sources was conducted. A framework of machine learning biases spanning from data to user by Mehrabi et al. is used to present the findings (Mehrabi et al. 2021). The academic search resulted in 7595 articles that were screened according to the inclusion criteria, of which 307 were included for full-text screening, and 49 were included in this review. The grey literature search resulted in 2639 documents screened, of which 235 were included for full text screening, and 25 were found to be relevant to the research questions pertaining to age and AI. As a result, a total of 74 documents were included in this review. The results show that the most common AI applications that intersected with age were age recognition and facial recognition systems. The most frequent machine learning algorithms used were convolutional neural networks and support vector machines. Bias was most frequently introduced in the early ‘data to algorithm’ phase in machine learning and the ‘algorithm to user’ phase specifically with representation bias ( n = 33) and evaluation bias ( n = 29), respectively (Mehrabi et al. 2021). The review concludes with a discussion of the ethical implications for the field of AI and recommendations for future research."
Artificial intelligence in environmental health and public safety: A comprehensive review of USA strategies,2023,Adedayo Adefemi; Emmanuel Adikwu Ukpoju; Oladipo Olugbenga Adekoya; Ayodeji Abatan; Abimbola Oluwatoyin Adegbite,World Journal of Advanced Research and Reviews,44,W4390155689,10.30574/wjarr.2023.20.3.2591,https://openalex.org/W4390155689,https://wjarr.com/sites/default/files/WJARR-2023-2591.pdf,Public health; Resilience (materials science); Emergency management; Environmental monitoring; Big data,review,True,"This study explores the transformative role of artificial intelligence (AI) in environmental health and public safety within the USA, focusing on pollution monitoring, emergency response, and sustainable practices for public. With the growing challenges posed by climate change, pollution, and emerging public health threats, the integration of Artificial Intelligence (AI) in environmental health and public safety strategies has become imperative. This comprehensive review explores the diverse array of AI applications implemented in the United States to address environmental issues and enhance public safety measures. The paper analyzes the multifaceted role of AI across various domains, including air and water quality monitoring, disease surveillance, disaster response, and infrastructure resilience. The advancements in AI technologies that have revolutionized data collection, analysis, and prediction in environmental health are examined. Machine learning algorithms, sensor networks, and satellite imagery are examined as tools for real-time monitoring and early detection of environmental hazards. Additionally, the paper investigates the integration of AI in public health surveillance systems, showcasing how predictive analytics and data-driven models contribute to the identification and containment of infectious diseases. Furthermore, the study sheds light on the incorporation of AI in disaster management, emphasizing the role of predictive modeling and risk assessment in optimizing emergency response strategies. The implementation of smart city technologies and intelligent infrastructure systems is discussed, highlighting how AI contributes to enhancing public safety and minimizing the impact of natural disasters. The review also critically evaluates the ethical, legal, and privacy considerations associated with the widespread adoption of AI in environmental health and public safety initiatives. It addresses concerns related to data security, algorithmic biases, and the need for transparent and accountable governance frameworks. Through an in-depth analysis of case studies, policies, and initiatives, this review provides insights into the successes and challenges of AI implementation in the USA. It concludes with recommendations for future research directions and policy considerations to ensure the responsible and effective integration of AI technologies in safeguarding environmental health and public safety. The findings presented in this review contribute to the broader discourse on leveraging AI for sustainable and resilient communities in the face of evolving environmental and public health challenges."
"Beyond bias: algorithmic machines, discrimination law and the analogy trap",2023,Raphaële Xenidis,Transnational Legal Theory,1,W4392361568,10.1080/20414005.2024.2307200,https://openalex.org/W4392361568,,Analogy; Trap (plumbing); Law; Law and economics; Computer science,article,False,"This article shows how the 'challenge of regulatory connection' in an increasingly algorithmic society triggers legal responses guided by analogies between the 'human' and the 'digital' realms. Focusing on non-discrimination law, it argues that analogical reasoning, however, masks how the affordances of algorithmic systems challenge the epistemological foundations of the law. This article unpacks the 'analogy' black box to shed light on the normative implications of its abstraction operations. It articulates three central 'traps' that result from attempts to transpose discrimination laws to algorithmic machines and their biases. Thereby, it shows how algorithmic artefacts mediate, displace and erode the regulatory objects, forms of subjectivity and modes of reasoning traditionally constructed by the law. The aim is to expose how algorithmic rationality unsettles the patterns of power distribution and the allocation of burdens and benefits enacted by legal techniques and to give visibility to alternative normative options."
Reducing Symbiosis Bias Through Better A/B Tests of Recommendation Algorithms,2023,David Holtz; Jennifer Brennan; Jean Pouget-Abadie,arXiv (Cornell University),1,W4386755597,10.48550/arxiv.2309.07107,https://openalex.org/W4386755597,https://arxiv.org/pdf/2309.07107,Computer science; Randomized experiment; Algorithm; Value (mathematics); Cluster (spacecraft),preprint,True,"It is increasingly common in digital environments to use A/B tests to compare the performance of recommendation algorithms. However, such experiments often violate the stable unit treatment value assumption (SUTVA), particularly SUTVA's ""no hidden treatments"" assumption, due to the shared data between algorithms being compared. This results in a novel form of bias, which we term ""symbiosis bias,"" where the performance of each algorithm is influenced by the training data generated by its competitor. In this paper, we investigate three experimental designs--cluster-randomized, data-diverted, and user-corpus co-diverted experiments--aimed at mitigating symbiosis bias. We present a theoretical model of symbiosis bias and simulate the impact of each design in dynamic recommendation environments. Our results show that while each design reduces symbiosis bias to some extent, they also introduce new challenges, such as reduced training data in data-diverted experiments. We further validate the existence of symbiosis bias using data from a large-scale A/B test conducted on a global recommender system, demonstrating that symbiosis bias affects treatment effect estimates in the field. Our findings provide actionable insights for researchers and practitioners seeking to design experiments that accurately capture algorithmic performance without bias in treatment effect estimates introduced by shared data."
Applying artificial intelligence in healthcare: lessons from the COVID-19 pandemic,2023,Sreejith Balasubramanian; Vinaya Shukla; Nazrul Islam; Arvind Upadhyay; Linh Duong,International Journal of Production Research,35,W4387307469,10.1080/00207543.2023.2263102,https://openalex.org/W4387307469,https://www.tandfonline.com/doi/pdf/10.1080/00207543.2023.2263102?needAccess=true,Health care; Pandemic; Stakeholder; Big data; Knowledge management,article,True,"The COVID-19 pandemic exposed vulnerabilities in global healthcare systems and highlighted the need for innovative, technology-driven solutions like Artificial Intelligence (AI). However, previous research on the topic has been limited and fragmented, leading to an incomplete understanding of the ‘what’, ‘where’ and ‘how’ of its application, as well as its associated benefits and challenges. This study proposes a comprehensive AI framework for healthcare and assesses its effectiveness within the UAE’s healthcare sector. It provides valuable insights into AI applications for healthcare stakeholders that range from the molecular to the population level. The study covers the different computational techniques employed, from machine learning to computer vision, and the various types of data inputs fed into these techniques, including clinical, epidemiological, locational, behavioural and genomic data. Additionally, the research highlights AI’s capacity to enhance healthcare’s operational, quality-related and social outcomes, and recognises regulatory policies, technological infrastructure, stakeholder cooperation and innovation readiness as key facilitators of AI adoption. Lastly, we stress the importance of addressing challenges such as data privacy, security, generalisability and algorithmic bias. Our findings are relevant beyond the pandemic in facilitating the development of AI-related policy interventions and support mechanisms for building resilient healthcare sector that can withstand future challenges."
Scheduling two-stage healthcare appointment systems via a knowledge-based biased random-key genetic algorithm,2025,Fajun Yang; Chao Li; Feng Wang; Zhi Yang; Kaizhou Gao,Swarm and Evolutionary Computation,4,W4407273792,10.1016/j.swevo.2025.101864,https://openalex.org/W4407273792,,Computer science; Key (lock); Scheduling (production processes); Algorithm; Genetic algorithm,article,False,
Epistemically violent biases in artificial intelligence design: the case of DALLE-E 2 and Starry AI,2023,Blessing Mbalaka,Digital Transformation and Society,20,W4382403160,10.1108/dts-01-2023-0003,https://openalex.org/W4382403160,https://www.emerald.com/insight/content/doi/10.1108/DTS-01-2023-0003/full/pdf?title=epistemically-violent-biases-in-artificial-intelligence-design-the-case-of-dalle-e-2-and-starry-ai,Offensive; Generator (circuit theory); Diversity (politics); Inclusion (mineral); Computer science,article,True,"Purpose The paper aims to expand on the works well documented by Joy Boulamwini and Ruha Benjamin by expanding their critique to the African continent. The research aims to assess if algorithmic biases are prevalent in DALL-E 2 and Starry AI. The aim is to help inform better artificial intelligence (AI) systems for future use. Design/methodology/approach The paper utilised a desktop study for literature and gathered data from Open AI’s DALL-E 2 text-to-image generator and StarryAI text-to-image generator. Findings The DALL-E 2 significantly underperformed when it was tasked with generating images of “An African Family” as opposed to images of a “Family”. The pictures lacked any conceivable detail as compared to the latter of this comparison. The StarryAI significantly outperformed the DALL-E 2 and rendered visible faces. However, the accuracy of the culture portrayed was poor. Research limitations/implications Because of the chosen research approach, the research results may lack generalisability. Therefore, researchers are encouraged to test the proposed propositions further. The implications, however, are that more inclusion is warranted to help address the issue of cultural inaccuracies noted in a few of the paper’s experiments. Practical implications The paper is useful for advocates who advocate for algorithmic equality and fairness by highlighting evidence of the implications of systemic-induced algorithmic bias. Social implications The reduction in offensive racism and more socially appropriate AI can be a better product for commercialisation and general use. If AI is trained on diversity, it can lead to better applications in contemporary society. Originality/value The paper’s use of DALL-E 2 and Starry AI is an under-researched area, and future studies on this matter are welcome."
Bias-aware training and evaluation of link prediction algorithms in network biology,2025,Serhan Yılmaz; Kaan Yorgancıoğlu; Mehmet Koyutürk,Proceedings of the National Academy of Sciences,1,W4411178498,10.1073/pnas.2416646122,https://openalex.org/W4411178498,https://doi.org/10.1073/pnas.2416646122,Link (geometry); Computer science; Training (meteorology); Algorithm; Artificial intelligence,article,True,"For biomedical applications, new link prediction algorithms are continuously being developed. These algorithms are typically evaluated computationally, using test sets generated by sampling the edges uniformly at random. However, as we demonstrate, this evaluation approach introduces a bias toward “rich nodes,” i.e., those with higher degrees in the network. More concerningly, this bias persists even when different network snapshots are used for evaluation, as recommended in the machine learning community. This creates a cycle in research where newly developed algorithms generate more knowledge on well-studied biological entities while understudied entities are commonly overlooked. To overcome this issue, we propose a weighted validation setting specifically focusing on low-degree nodes and present AWARE strategies to facilitate bias-aware training and evaluation of link prediction algorithms. These strategies can help researchers gain better insights from computational evaluations and promote the development of new algorithms focusing on novel findings and understudied proteins."
AI-DRIVEN PREDICTIVE ANALYTICS IN RETAIL: A REVIEW OF EMERGING TRENDS AND CUSTOMER ENGAGEMENT STRATEGIES,2024,David Iyanuoluwa Ajiga; Ndubuisi Leonard Ndubuisi; Onyeka Franca Asuzu; Oluwaseyi Rita Owolabi; Tula Sunday Tubokirifuruar; Rhoda Adura Adeleye,International Journal of Management & Entrepreneurship Research,54,W4391898540,10.51594/ijmer.v6i2.772,https://openalex.org/W4391898540,https://doi.org/10.51594/ijmer.v6i2.772,Customer engagement; Analytics; Predictive analytics; Data science; Business,review,True,"As the retail landscape undergoes a profound transformation in the era of digitalization, the integration of Artificial Intelligence (AI) and predictive analytics has emerged as a pivotal force reshaping the industry. This paper provides a comprehensive review of the latest trends in AI-driven predictive analytics within the retail sector and explores innovative customer engagement strategies that leverage these advanced technologies. The review begins by elucidating the foundational concepts of AI and predictive analytics, highlighting their synergistic role in forecasting consumer behavior, demand patterns, and market trends. The paper then delves into the emerging trends, such as machine learning algorithms, natural language processing, and computer vision, that are revolutionizing the way retailers harness data for strategic decision-making. In addition to outlining technological advancements, the paper emphasizes the crucial role of data quality and ethical considerations in the implementation of AI-driven predictive analytics. It examines the challenges associated with privacy concerns, algorithmic bias, and the need for transparent AI models to ensure responsible and fair use of customer data. Furthermore, the paper explores a spectrum of customer engagement strategies enabled by AI-driven predictive analytics. From personalized shopping experiences and targeted marketing campaigns to dynamic pricing and inventory optimization, retailers are deploying innovative approaches to enhance customer satisfaction and loyalty. The review also discusses case studies of successful AI implementations in leading retail enterprises, showcasing tangible benefits such as improved operational efficiency, increased sales, and enhanced customer retention. These real-world examples illustrate the transformative impact of AI-driven predictive analytics on diverse aspects of the retail value chain. By examining emerging trends and customer engagement strategies, it serves as a valuable resource for industry professionals, researchers, and policymakers seeking to navigate the evolving landscape of AI in the retail sector.&#x0D; Keywords: AI-driven Predictive Analytics, Retail Industry, Customer Engagement Strategies, Machine Learning Algorithms, Natural Language Processing."
Sampling lattice points in a polytope: a Bayesian biased algorithm with random updates,2024,Miles Bakenhus; Sonja Petrović,Algebraic Statistics,1,W4397005953,10.2140/astat.2024.15.61,https://openalex.org/W4397005953,https://doi.org/10.2140/astat.2024.15.61,Polytope; Bayesian probability; Algorithm; Lattice (music); Gibbs sampling,article,True,"The set of nonnegative integer lattice points in a polytope, also known as the fiber of a linear map, makes an appearance in several applications including optimization and statistics.We address the problem of sampling from this set using three ingredients: an easy-to-compute lattice basis of the constraint matrix, a biased sampling algorithm with a Bayesian framework, and a step-wise selection method.The bias embedded in our algorithm updates sampler parameters to improve fiber discovery rate at each step chosen from previously discovered elements.We showcase the performance of the algorithm on several examples, including fibers that are out of reach for the state-of-the-art Markov bases samplers."
Sampling lattice points in a polytope: a Bayesian biased algorithm with random updates,2023,Miles Bakenhus; Sonja Petrović,arXiv (Cornell University),1,W4383473918,10.48550/arxiv.2307.02428,https://openalex.org/W4383473918,https://arxiv.org/pdf/2307.02428,Polytope; Algorithm; Lattice (music); Bayesian probability; Sampling (signal processing),preprint,True,"The set of nonnegative integer lattice points in a polytope, also known as the fiber of a linear map, makes an appearance in several applications including optimization and statistics. We address the problem of sampling from this set using three ingredients: an easy-to-compute lattice basis of the constraint matrix, a biased sampling algorithm with a Bayesian framework, and a step-wise selection method. The bias embedded in our algorithm updates sampler parameters to improve fiber discovery rate at each step chosen from previously discovered elements. We showcase the performance of the algorithm on several examples, including fibers that are out of reach for the state-of-the-art Markov bases samplers."
Racial and Ethnic Disparities in Brain Age Algorithm Performance: Investigating Bias Across Six Popular Methods,2025,Dorthea J Adkins; Jamie L. Hanson,bioRxiv (Cold Spring Harbor Laboratory),1,W4414564280,10.1101/2025.09.18.25336117,https://openalex.org/W4414564280,,,article,False,"Abstract Brain age algorithms, which estimate biological aging from neuroimaging data, are increasingly used as biomarkers for health and disease. However, most algorithms are trained on datasets with limited racial and ethnic diversity, raising concerns about potential algorithmic bias that could exacerbate health disparities. To probe this potential, we evaluated six popular brain age algorithms using data from the Health and Aging Brain Study–Health Disparities (HABS-HD), comprising 1,123 White American, 1,107 Hispanic American, and 678 African American participants, ages ≥50. Comparing correlations between brain age and chronological age across racial/ethnic groups, relations were consistently weaker for African American participants compared to White and Hispanic American participants across most algorithms (ranging from r=0.51-0.85 for African Americans vs. r=0.57-0.89 for other groups). We also examined error for brain age v. chronological age and found significant differences in median errors across racial/ethnic groups, though specific patterns varied by algorithm. Sensitivity models weighting for age, sex, and scan quality noted similar patterns, with all algorithms maintaining significant differences in correlation or median prediction error between groups. Our findings reveal systematic performance differences in brain age algorithms across racial and ethnic groups, with most algorithms consistently showing reduced algorithm accuracy for African American and/or Hispanic-American participants. These biases, which are likely introduced at multiple stages of algorithm development, could impact clinical utility and diagnostic accuracy. Results highlight the urgent need for more inclusive algorithm development and validation to ensure equitable healthcare applications of neuroimaging biomarkers."
Algorithmic Unfairness through the Lens of EU Non-Discrimination Law,2023,Hilde Weerts; Raphaële Xenidis; Fabien Tarissan; Henrik Palmer Olsen; Mykola Pechenizkiy,,20,W4378446601,10.1145/3593013.3594044,https://openalex.org/W4378446601,https://dl.acm.org/doi/pdf/10.1145/3593013.3594044,Lens (geology); Computer science; Law; Political science; Optics,article,True,"Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set out the normative underpinnings of fairness metrics and technical interventions and compare these to the legal reasoning of the Court of Justice of the EU. Specifically, we show how normative assumptions often remain implicit in both disciplinary approaches and explain the ensuing limitations of current AI practice and non-discrimination law. We conclude with implications for AI practitioners and regulators."
Artificial intelligence in intelligent tutoring systems toward sustainable education: a systematic review,2023,Chien-Chang Lin; Anna Y.Q. Huang; Owen H.T. Lu,Smart Learning Environments,302,W4386223224,10.1186/s40561-023-00260-y,https://openalex.org/W4386223224,https://slejournal.springeropen.com/counter/pdf/10.1186/s40561-023-00260-y,Software deployment; Computer science; Knowledge management; Sustainable development; Political science,review,True,"Abstract Sustainable education is a crucial aspect of creating a sustainable future, yet it faces several key challenges, including inadequate infrastructure, limited resources, and a lack of awareness and engagement. Artificial intelligence (AI) has the potential to address these challenges and enhance sustainable education by improving access to quality education, creating personalized learning experiences, and supporting data-driven decision-making. One outcome of using AI and Information Technology (IT) systems in sustainable education is the ability to provide students with personalized learning experiences that cater to their unique learning styles and preferences. Additionally, AI systems can provide teachers with data-driven insights into student performance, emotions, and engagement levels, enabling them to tailor their teaching methods and approaches or provide assistance or intervention accordingly. However, the use of AI and IT systems in sustainable education also presents challenges, including issues related to privacy and data security, as well as potential biases in algorithms and machine learning models. Moreover, the deployment of these systems requires significant investments in technology and infrastructure, which can be a challenge for educators. In this review paper, we will provide different perspectives from educators and information technology solution architects to connect education and AI technology. The discussion areas include sustainable education concepts and challenges, technology coverage and outcomes, as well as future research directions. By addressing these challenges and pursuing further research, we can unlock the full potential of these technologies and support a more equitable and sustainable education system."
When mitigating bias is unfair: multiplicity and arbitrariness in algorithmic group fairness,2023,Nataša Krčo; Thibault Laugel; Jean-Michel Loubès; Marcin Detyniecki,arXiv (Cornell University),2,W4321015313,10.48550/arxiv.2302.07185,https://openalex.org/W4321015313,https://arxiv.org/pdf/2302.07185,Debiasing; Odds; Arbitrariness; Computer science; Selection bias,preprint,True,"Most research on fair machine learning has prioritized optimizing criteria such as Demographic Parity and Equalized Odds. Despite these efforts, there remains a limited understanding of how different bias mitigation strategies affect individual predictions and whether they introduce arbitrariness into the debiasing process. This paper addresses these gaps by exploring whether models that achieve comparable fairness and accuracy metrics impact the same individuals and mitigate bias in a consistent manner. We introduce the FRAME (FaiRness Arbitrariness and Multiplicity Evaluation) framework, which evaluates bias mitigation through five dimensions: Impact Size (how many people were affected), Change Direction (positive versus negative changes), Decision Rates (impact on models' acceptance rates), Affected Subpopulations (who was affected), and Neglected Subpopulations (where unfairness persists). This framework is intended to help practitioners understand the impacts of debiasing processes and make better-informed decisions regarding model selection. Applying FRAME to various bias mitigation approaches across key datasets allows us to exhibit significant differences in the behaviors of debiasing methods. These findings highlight the limitations of current fairness criteria and the inherent arbitrariness in the debiasing process."
A multistart biased‐randomized algorithm for solving a three‐dimensional case picking problem with real‐life constraints,2023,Mattia Neroni; Ángel A. Juan; Massimo Bertolini,International Transactions in Operational Research,2,W4390345618,10.1111/itor.13421,https://openalex.org/W4390345618,https://hdl.handle.net/11380/1350890,Pallet; Vehicle routing problem; Mathematical optimization; Heuristic; Computer science,article,True,"Abstract This paper introduces the three‐dimensional case picking problem (3D‐CPP) and proposes a multistart biased‐randomized algorithm (BRA) to solve it. The 3D‐CPP combines two important topics in modern warehouse logistics: the pallet loading problem and the routing of pickers in manual warehouses. The proposed optimization procedure aims at minimizing the overall distance traveled by the pickers, and is achieved by combining a routing problem (i.e., the order in which picking positions are visited) with a loading problem (i.e., the way in which cases are placed onto the pallet). We also consider additional constraints regarding the weight, vertical support, and strength of the cases. In order to solve this problem, we first propose a constructive heuristic which combines routing and packing procedures. This initial heuristic is then extended into a multistart BRA by employing a skewed probability distribution to introduce a certain degree of randomness during the solution‐construction process. A series of computational experiments allow us to assess the quality of the proposed approach, through a comparison with other algorithms as well as using real‐life data provided by an industrial partner."
Smiling women pitching down: auditing representational and presentational gender biases in image-generative AI,2023,Luhang Sun; Mian Wei; Yibing Sun; Yoo Ji Suh; Liwei Shen; Sijia Yang,Journal of Computer-Mediated Communication,74,W4391463878,10.1093/jcmc/zmad045,https://openalex.org/W4391463878,https://academic.oup.com/jcmc/article-pdf/29/1/zmad045/56546560/zmad045.pdf,Presentational and representational acting; Audit; Psychology; Generative grammar; Artificial intelligence,article,True,"Abstract Generative Artificial Intelligence (AI) models like DALL·E 2 can interpret prompts and generate high-quality images that exhibit human creativity. Though public enthusiasm is booming, systematic auditing of potential gender biases in AI-generated images remains scarce. We addressed this gap by examining the prevalence of two occupational gender biases (representational and presentational biases) in 15,300 DALL·E 2 images spanning 153 occupations. We assessed potential bias amplification by benchmarking against the 2021 U.S. census data and Google Images. Our findings reveal that DALL·E 2 underrepresents women in male-dominated fields while overrepresenting them in female-dominated occupations. Additionally, DALL·E 2 images tend to depict more women than men with smiles and downward-pitching heads, particularly in female-dominated (versus male-dominated) occupations. Our algorithm auditing study demonstrates more pronounced representational and presentational biases in DALL·E 2 compared to Google Images and calls for feminist interventions to curtail the potential impacts of such biased AI-generated images on the media ecology."
Bias in the Algorithm: Issues Raised Due to Use of Facial Recognition in India,2024,Imad A. Basheer,Journal of Development Policy and Practice,1,W4404187464,10.1177/24551333241283992,https://openalex.org/W4404187464,,Facial recognition system; Computer science; Artificial intelligence; Speech recognition; Pattern recognition (psychology),article,False,"From helping you and me unlock our smartphones with a mere glance to enabling the police to identify criminals via CCTV footage, artificial intelligence (AI)-powered face recognition technology (FRT) is rapidly advancing by the day. According to reports, the Indian Railways will install FRT-based video surveillance systems in 983 railway stations across the country to ramp up security. However, the accuracy and reliability of FRT depend on the quality of the input images, the algorithms used and the size and quality of the reference database. Due to the scope for major imbalances in these elements, these algorithms have been found to be biased—largely against minority communities and women, thereby exacerbating already prevalent forms of societal discrimination. Scrutinising key research studies that collectively expose racial and gender bias prevalent in widely adopted FRT tools in the United States as well as India, this article analyses how bias and inaccuracy of these tools have led to poor outcomes and raised concerns when deployed by law enforcement agencies in India. Furthermore, this article traces the historical context of these biases and proposes debiasing measures that go beyond balancing datasets."
Scheduling technicians and tasks through an adaptive multi-objective biased random-key genetic algorithm,2024,Ricardo B. Damm; Antônio Augusto Chaves; José A. Riveaux; Débora P. Ronconi,Annals of Operations Research,3,W4403366585,10.1007/s10479-024-06325-6,https://openalex.org/W4403366585,,Computer science; Theory of computation; Key (lock); Scheduling (production processes); Genetic algorithm,article,False,
The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field,2023,Hossein Hassani; Emmanuel Sirimal Silva,Big Data and Cognitive Computing,346,W4361002760,10.3390/bdcc7020062,https://openalex.org/W4361002760,https://www.mdpi.com/2504-2289/7/2/62/pdf?version=1679985158,Computer science; Data science; Workflow; Field (mathematics); Artificial intelligence,article,True,"ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s architecture, including its ability to be fine-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the benefits outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workflows and is likely to become an increasingly important tool for intelligence augmentation in the field of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classification. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be fine-tuned for specific use cases, it may not perform well on certain tasks if it has not been specifically trained for them. Additionally, the output of ChatGPT may be difficult to interpret, which could pose challenges for decision-making in data science applications."
Ortho-Heterodox Biases and the Economist Algorithms of ChatGPT,2023,Oz Iazdi,Iberian Journal of the History of Economic Thought,1,W4390782957,10.5209/ijhe.91545,https://openalex.org/W4390782957,https://revistas.ucm.es/index.php/IJHE/article/download/91545/4564456567998,Heterodoxy; Impartiality; Orthodoxy; Positive economics; Perception,article,True,"Recommendations for economic policies can be based on different theoretical perspectives and may present hidden biases. Identifying these biases is challenging when they are embedded in recommendations from sources with high technological and social disruptive potential, where a good level of impartiality is expected, such as contemporary large language models. Thus, a questionnaire was administered to economists affiliated with the Brazilian academic community to assess their perception of orthodox/heterodox biases in economic policy recommendations derived from interactions with ChatGPT. The results showed that: i) there is still no consensus on the concepts of orthodoxy and heterodoxy in Brazil; ii) there are indications of a positive relationship between how self-proclaimed heterodox (orthodox) an economist is and how heterodox (orthodox) the perceived bias in an economic policy is; iii) it was not possible to identify a consistently orthodox or heterodox bias in ChatGPT's recommendations, which exhibited a good degree of impartiality."
Exploring patient perspectives on how they can and should be engaged in the development of artificial intelligence (AI) applications in health care,2023,Samira Adus; Jillian Macklin; Andrew D. Pinto,BMC Health Services Research,65,W4387957120,10.1186/s12913-023-10098-2,https://openalex.org/W4387957120,https://bmchealthservres.biomedcentral.com/counter/pdf/10.1186/s12913-023-10098-2,Health informatics; Health care; Modalities; Medicine; Focus group,article,True,"Abstract Background Artificial intelligence (AI) is a rapidly evolving field which will have implications on both individual patient care and the health care system. There are many benefits to the integration of AI into health care, such as predicting acute conditions and enhancing diagnostic capabilities. Despite these benefits potential harms include algorithmic bias, inadequate consent processes, and implications on the patient-provider relationship. One tool to address patients’ needs and prevent the negative implications of AI is through patient engagement. As it currently stands, patients have infrequently been involved in AI application development for patient care delivery. Furthermore, we are unaware of any frameworks or recommendations specifically addressing patient engagement within the field of AI in health care. Methods We conducted four virtual focus groups with thirty patient participants to understand of how patients can and should be meaningfully engaged within the field of AI development in health care. Participants completed an educational module on the fundamentals of AI prior to participating in this study. Focus groups were analyzed using qualitative content analysis. Results We found that participants in our study wanted to be engaged at the problem-identification stages using multiple methods such as surveys and interviews. Participants preferred that recruitment methodologies for patient engagement included both in-person and social media-based approaches with an emphasis on varying language modalities of recruitment to reflect diverse demographics. Patients prioritized the inclusion of underrepresented participant populations, longitudinal relationship building, accessibility, and interdisciplinary involvement of other stakeholders in AI development. We found that AI education is a critical step to enable meaningful patient engagement within this field. We have curated recommendations into a framework for the field to learn from and implement in future development. Conclusion Given the novelty and speed at which AI innovation is progressing in health care, patient engagement should be the gold standard for application development. Our proposed recommendations seek to enable patient-centered AI application development in health care. Future research must be conducted to evaluate the effectiveness of patient engagement in AI application development to ensure that both AI application development and patient engagement are done rigorously, efficiently, and meaningfully."
Evaluating the Potential of Artificial Intelligence in Orthopedic Surgery for Value-based Healthcare,2023,Aftab Tariq; Ahmad Yousaf Gill; Hafiz Khawar Hussain,International Journal of Multidisciplinary Sciences and Arts,29,W4382792584,10.47709/ijmdsa.v2i1.2394,https://openalex.org/W4382792584,https://jurnal.itscience.org/index.php/ijmdsa/article/download/2394/1851,Orthopedic surgery; Health care; Medicine; Analytics; Workforce,article,True,"&#x0D; &#x0D; &#x0D; &#x0D; The potential of artificial intelligence (AI) to transform value-based healthcare in the area of orthopedic surgery is examined in this research. Orthopedic surgeons and healthcare systems may improve patient outcomes, increase efficiency, and alter care delivery by combining AI algorithms, cutting-edge data analytics, and novel technology. Through case studies and success stories, the article provides a thorough study of the advantages and prospects provided by AI in orthopedic surgery. These instances demonstrate how AI has been successfully applied to several facets of orthopedic surgery, including as diagnosis, planning of the surgical course, surgical navigation, postoperative care, and resource allocation. The ethical and legal ramifications of using AI are also discussed in the study, with a focus on patient autonomy, privacy, accountability, and any potential effects on the healthcare workforce. The potential applications of AI in orthopedic surgery are examined, together with developments in preoperative planning, surgical robotics, remote monitoring, predictive analytics, personalised medicine, research, and innovation. The promise of AI in orthopedic surgery is obvious, despite issues with data quality, privacy, algorithm biases, and legal constraints. The ethical and appropriate application of AI technology in orthopedic surgery has the potential to significantly enhance patient outcomes, lower complications, boost efficiency, and change the way healthcare is provided. This study lays the groundwork for future study and application in the field of orthopedic surgery by offering insightful information on the role of AI in delivering value-based healthcare.&#x0D; &#x0D; &#x0D; &#x0D; &#x0D; &#x0D; &#x0D; &#x0D; &#x0D; &#x0D;"
Harnessing Artificial Intelligence,2024,Imogen Cooper,"Advances in educational marketing, administration, and leadership book series",23,W2898347082,10.4018/979-8-3693-7190-9.ch003,https://openalex.org/W2898347082,,Computer science; Artificial intelligence,book-chapter,False,"This provides a comprehensive exploration of Artificial Intelligence (AI) and its implications for education. Beginning with an introduction to the foundational principles of AI, the chapter delves into various aspects of AI, including machine learning, neural networks, and deep learning. It examines how AI is transforming industries and societies across healthcare, finance, and education, highlighting its potential to drive innovation and efficiency. Furthermore, the chapter addresses the ethical considerations associated with AI education, emphasizing the importance of data privacy, algorithmic bias, transparency, and accountability. By promoting ethical awareness and responsible decision-making, educators can empower students to navigate the AI-driven world ethically and responsibly."
"Justice, trust, and moral judgements when personnel selection is supported by algorithms",2023,Tina Feldkamp; Markus Langer; Leo Wies; Cornelius J. König,European Journal of Work and Organizational Psychology,20,W4321458265,10.1080/1359432x.2023.2169140,https://openalex.org/W4321458265,https://resolver.sub.uni-goettingen.de/purl?gro-2/132420,Judgement; Psychology; Perception; Social psychology; Selection (genetic algorithm),article,True,"Although algorithm-based systems are increasingly used as a decision-support for managers, there is still a lack of research on the effects of algorithm use and more specifically on potential algorithmic bias on decision-makers. To investigate how potential social bias in a recommendation outcome influences trust, fairness perceptions, and moral judgement, we used a moral dilemma scenario. Participants (N = 215) imagined being human resource managers responsible for personnel selection and receiving decision-support from either human colleagues or an algorithm-based system. They received an applicant preselection that was either gender-balanced or predominantly male. Although participants perceived algorithm-based support as less biased, they also perceived it as generally less fair and had less trust in it. This could be related to the finding that participants perceived algorithm-based systems as more consistent but also as less likely to uphold moral standards. Moreover, participants tended to reject algorithm-based preselection more often than human-based and were more likely to use utilitarian judgements when accepting it, which may indicate different underlying moral judgement processes."
Harnessing the Power of Large Language Models for Cybersecurity,2024,Hewa Majeed Zangana; Marwan Omar,"Advances in information security, privacy, and ethics book series",26,W4403969640,10.4018/979-8-3693-9311-6.ch001,https://openalex.org/W4403969640,,Computer security; Computer science; Power (physics); Physics; Quantum mechanics,book-chapter,False,"The LLMs not only have changed the overall nature of NPL but have also helped a lot in setting standards in cyber security. Within the confines of this review, the authors discuss the benefits, progressions, difficulties, as well as the future paths aimed to be taken in the cybersecurity field of LLMs. They delve into how LLMs help companies process unstructured textual data for text dangers detections, vulnerability assessments, and incident responses. In addition, they investigate the ethical and societal consequences of using LLMs for cybersecurity, facing challenges like algorithmic bias, privacy, and data safety. Besides that, they find that critical research questions in the crossroads of LLMs and cybersecurity language include unique assessing techniques and the improvement of algorithms to clarify the information. Through the development of many-faceted interdisciplinary cooperation and ethics-based considerations, we can maximize the opportunities LLMs present in the cyber world and build a more resilient and secure environment for everyone."
Unraveling the Ethical Enigma: Artificial Intelligence in Healthcare,2023,Madhan Jeyaraman; Sangeetha Balaji; Naveen Jeyaraman; Sankalp Yadav,Cureus,195,W4385724150,10.7759/cureus.43262,https://openalex.org/W4385724150,https://assets.cureus.com/uploads/review_article/pdf/178557/20230810-7795-1otiewn.pdf,Health care; Transparency (behavior); Accountability; Transformative learning; Big data,review,True,"The integration of artificial intelligence (AI) into healthcare promises groundbreaking advancements in patient care, revolutionizing clinical diagnosis, predictive medicine, and decision-making. This transformative technology uses machine learning, natural language processing, and large language models (LLMs) to process and reason like human intelligence. OpenAI's ChatGPT, a sophisticated LLM, holds immense potential in medical practice, research, and education. However, as AI in healthcare gains momentum, it brings forth profound ethical challenges that demand careful consideration. This comprehensive review explores key ethical concerns in the domain, including privacy, transparency, trust, responsibility, bias, and data quality. Protecting patient privacy in data-driven healthcare is crucial, with potential implications for psychological well-being and data sharing. Strategies like homomorphic encryption (HE) and secure multiparty computation (SMPC) are vital to preserving confidentiality. Transparency and trustworthiness of AI systems are essential, particularly in high-risk decision-making scenarios. Explainable AI (XAI) emerges as a critical aspect, ensuring a clear understanding of AI-generated predictions. Cybersecurity becomes a pressing concern as AI's complexity creates vulnerabilities for potential breaches. Determining responsibility in AI-driven outcomes raises important questions, with debates on AI's moral agency and human accountability. Shifting from data ownership to data stewardship enables responsible data management in compliance with regulations. Addressing bias in healthcare data is crucial to avoid AI-driven inequities. Biases present in data collection and algorithm development can perpetuate healthcare disparities. A public-health approach is advocated to address inequalities and promote diversity in AI research and the workforce. Maintaining data quality is imperative in AI applications, with convolutional neural networks showing promise in multi-input/mixed data models, offering a comprehensive patient perspective. In this ever-evolving landscape, it is imperative to adopt a multidimensional approach involving policymakers, developers, healthcare practitioners, and patients to mitigate ethical concerns. By understanding and addressing these challenges, we can harness the full potential of AI in healthcare while ensuring ethical and equitable outcomes."
A Hybrid Biased Random-Key Genetic Algorithm for the Container Relocation Problem,2024,Andresson da Silva Firmino; Valéria Cesário Times,Springer tracts in nature-inspired computing,1,W4391961568,10.1007/978-981-99-8107-6_4,https://openalex.org/W4391961568,,Relocation; Container (type theory); Key (lock); Genetic algorithm; Computer science,book-chapter,False,
Transforming Healthcare through AI: Unleashing the Power of Personalized Medicine,2023,Abdullah Khan,International Journal of Multidisciplinary Sciences and Arts,18,W4382792558,10.47709/ijmdsa.v2i1.2424,https://openalex.org/W4382792558,https://jurnal.itscience.org/index.php/ijmdsa/article/download/2424/1849,Health care; Workflow; Analytics; Big data; Openness to experience,article,True,"The healthcare sector now places a lot of emphasis on providing patients with personalized care that is catered to their unique requirements and features. A significant force behind the development and use of customized healthcare is artificial intelligence (AI). This essay examines the use of AI in personalized healthcare and how it can affect several facets of healthcare provision. The study starts out by talking about how AI is being used in diagnostics, emphasizing how machine learning algorithms and the examination of various datasets can improve diagnostic accuracy. It goes into more detail about how AI can be used to create tailored treatment plans, utilizing patient-specific data and predictive analytics to maximize therapeutic results and reduce side effects. Examined are the ethical issues surrounding AI in customized healthcare, such as data privacy, algorithmic bias, and the value of responsibility and openness. The integration of AI into clinical practice is also covered in the study, along with prospects for improving decision-making, streamlining workflow, and overall healthcare delivery effectiveness. The healthcare sector now places a lot of emphasis on providing patients with personalized care that is catered to their unique requirements and features. A significant force behind the development and use of customized healthcare is artificial intelligence (AI). This essay examines the use of AI in personalized healthcare and how it can affect several facets of healthcare provision. The study starts out by talking about how AI is being used in diagnostics, emphasizing how machine learning algorithms and the examination of various datasets can improve diagnostic accuracy. It goes into more detail about how AI can be used to create tailored treatment plans, utilizing patient-specific data and predictive analytics to maximize therapeutic results and reduce side effects. Examined are the ethical issues surrounding AI in customized healthcare, such as data privacy, algorithmic bias, and the value of responsibility and openness. The integration of AI into clinical practice is also covered in the study, along with prospects for improving decision-making, streamlining workflow, and overall healthcare delivery effectiveness.&#x0D; &#x0D;"
Enhanced Biased Weights AdaBoost Algorithm for Diabetes Detection on Imbalanced Dataset,2023,Afsana Ansari; Bhagyashri Kadam; Sunita Barve; Diptee Chikmurge,,1,W4388938041,10.1109/icccnt56998.2023.10308086,https://openalex.org/W4388938041,,AdaBoost; Machine learning; Artificial intelligence; Decision tree; Computer science,article,False,"The paper proposes a new approach to improve multi-fault diabetes diagnosis using ensemble learning - biased weights with AdaBoost algorithm on unbalanced datasets. The suggested technique makes utilization three distinct using machine learning algorithms like the Support Vector Machine,, Logistic Regression and Decision Trees. These models are combined using the AdaBoost algorithm with or without bias weights. The performance of the approach is evaluated using an imbalanced diabetes dataset of 2000 samples, and the results are compared to various machine learning models, namely Naive Bayes, Among the methods employed are kNearest Neighbours and Support Vector Machine with radial basis function, Artificial Neural Network, and popular ensemble learning techniques such as XGBoost, Light Gradient Boosting Machine, and Random Forest. Diabetes diagnosis accuracy is significantly improved by the proposed method, according to the study's findings, particularly when using decision tree models and unbalanced datasets. The most elevated precision pace of 99% is accomplished utilizing AdaBoost with decision tree as weak learner, alongside unfairness loads of 0.7 and 0.4. The findings suggest that the proposed technique could be effective for identifying diabetes in clinical settings. It could also be used to create a decision support system for doctors to help them diagnose and treat diabetes."
FairGAT: Fairness-Aware Graph Attention Networks,2024,Öykü Deniz Köse; Yanning Shen,ACM Transactions on Knowledge Discovery from Data,8,W4391753841,10.1145/3645096,https://openalex.org/W4391753841,https://dl.acm.org/doi/pdf/10.1145/3645096,Computer science; Graph; Theoretical computer science,article,True,"Graphs can facilitate modeling various complex systems such as gene networks and power grids as well as analyzing the underlying relations within them. Learning over graphs has recently attracted increasing attention, particularly graph neural network (GNN)–based solutions, among which graph attention networks (GATs) have become one of the most widely utilized neural network structures for graph-based tasks. Although it is shown that the use of graph structures in learning results in the amplification of algorithmic bias, the influence of the attention design in GATs on algorithmic bias has not been investigated. Motivated by this, the present study first carries out a theoretical analysis in order to demonstrate the sources of algorithmic bias in GAT-based learning for node classification. Then, a novel algorithm, FairGAT, which leverages a fairness-aware attention design, is developed based on the theoretical findings. Experimental results on real-world networks demonstrate that FairGAT improves group fairness measures while also providing comparable utility to the fairness-aware baselines for node classification and link prediction."
Impact of Artificial Intelligence and Virtual Reality on Educational Inclusion: A Systematic Review of Technologies Supporting Students with Disabilities,2024,Angelos Chalkiadakis; Antonia Seremetaki; Athanasia Kanellou; Maria Kallishi; Anastasia Morfopoulou; Marina Moraitaki; Sofia Mastrokoukou,Education Sciences,36,W4404144786,10.3390/educsci14111223,https://openalex.org/W4404144786,https://doi.org/10.3390/educsci14111223,Inclusion (mineral); Virtual reality; Educational technology; Computer science; Assistive technology,review,True,"The emergence of Artificial Intelligence (AI) and Virtual Reality (VR) technologies offers transformative potential for the advancement of inclusive education, particularly for students with disabilities. This systematic review critically evaluates the current state of research to assess the impact of AI and VR on enhancing educational accessibility, personalisation and social inclusion in education. AI-driven adaptive systems can dynamically tailor learning experiences to individual needs, while VR offers immersive, multi-sensory environments that promote experiential learning. Despite these advances, the review also identifies significant challenges, including the high cost of implementation, technical barriers and limited teacher readiness, which hinder widespread adoption. Ethical concerns such as privacy and algorithmic bias are cited as key areas that need careful consideration. The findings underscore the urgent need for further empirical research to explore the long-term impact of these technologies and advocate for more equitable access to AI and VR tools in underserved educational settings. Ultimately, the review highlights the importance of integrating AI and VR as part of a broader strategy to foster genuinely inclusive learning environments that align with the goals of the Convention on the Rights of Persons with Disabilities (CRPD)."
"Fairness And Bias in Artificial Intelligence: A Brief Survey of Sources, Impacts, And Mitigation Strategies",2023,Emilio Ferrara,arXiv (Cornell University),72,W4366388571,10.48550/arxiv.2304.07683,https://openalex.org/W4366388571,https://arxiv.org/pdf/2304.07683,Generative grammar; Artificial intelligence; Generative model; Computer science; Perception,preprint,True,"The significant advancements in applying Artificial Intelligence (AI) to healthcare decision-making, medical diagnosis, and other domains have simultaneously raised concerns about the fairness and bias of AI systems. This is particularly critical in areas like healthcare, employment, criminal justice, credit scoring, and increasingly, in generative AI models (GenAI) that produce synthetic media. Such systems can lead to unfair outcomes and perpetuate existing inequalities, including generative biases that affect the representation of individuals in synthetic data. This survey paper offers a succinct, comprehensive overview of fairness and bias in AI, addressing their sources, impacts, and mitigation strategies. We review sources of bias, such as data, algorithm, and human decision biases - highlighting the emergent issue of generative AI bias where models may reproduce and amplify societal stereotypes. We assess the societal impact of biased AI systems, focusing on the perpetuation of inequalities and the reinforcement of harmful stereotypes, especially as generative AI becomes more prevalent in creating content that influences public perception. We explore various proposed mitigation strategies, discussing the ethical considerations of their implementation and emphasizing the need for interdisciplinary collaboration to ensure effectiveness. Through a systematic literature review spanning multiple academic disciplines, we present definitions of AI bias and its different types, including a detailed look at generative AI bias. We discuss the negative impacts of AI bias on individuals and society and provide an overview of current approaches to mitigate AI bias, including data pre-processing, model selection, and post-processing. We emphasize the unique challenges presented by generative AI models and the importance of strategies specifically tailored to address these."
Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices,2024,Xukang Wang; Ying Cheng Wu; Xueliang Ji; Hongpeng Fu,Frontiers in Artificial Intelligence,16,W4398174495,10.3389/frai.2024.1320277,https://openalex.org/W4398174495,https://www.frontiersin.org/articles/10.3389/frai.2024.1320277/pdf?isPublishedV2=False,Emphasis (telecommunications); Political science; Computer science; Telecommunications,article,True,"Introduction Algorithmic decision-making systems are widely used in various sectors, including criminal justice, employment, and education. While these systems are celebrated for their potential to enhance efficiency and objectivity, they also pose risks of perpetuating and amplifying societal biases and discrimination. This paper aims to provide an indepth analysis of the types of algorithmic discrimination, exploring both the challenges and potential solutions. Methods The methodology includes a systematic literature review, analysis of legal documents, and comparative case studies across different geographic regions and sectors. This multifaceted approach allows for a thorough exploration of the complexity of algorithmic bias and its regulation. Results We identify five primary types of algorithmic bias: bias by algorithmic agents, discrimination based on feature selection, proxy discrimination, disparate impact, and targeted advertising. The analysis of the U.S. legal and regulatory framework reveals a landscape of principled regulations, preventive controls, consequential liability, self-regulation, and heteronomy regulation. A comparative perspective is also provided by examining the status of algorithmic fairness in the EU, Canada, Australia, and Asia. Conclusion Real-world impacts are demonstrated through case studies focusing on criminal risk assessments and hiring algorithms, illustrating the tangible effects of algorithmic discrimination. The paper concludes with recommendations for interdisciplinary research, proactive policy development, public awareness, and ongoing monitoring to promote fairness and accountability in algorithmic decision-making. As the use of AI and automated systems expands globally, this work highlights the importance of developing comprehensive, adaptive approaches to combat algorithmic discrimination and ensure the socially responsible deployment of these powerful technologies."
Ethnic Bias in Prediction and Decision Making Algorithms in Precision Psychiatry: Challenges in a Shrinking World,2025,Rajeev Krishnadas,Journal of Psychosocial Rehabilitation and Mental Health,1,W4410026059,10.1007/s40737-025-00472-0,https://openalex.org/W4410026059,https://link.springer.com/content/pdf/10.1007/s40737-025-00472-0.pdf,Ethnic group; Resizing; Algorithm; Computer science; Machine learning,article,True,"Abstract This article examines ethnic bias in predictive algorithms and decision-making systems within precision psychiatry. While these models aim to provide individualised outcomes and improve healthcare, they often perpetuate historical biases present in training datasets. Factors such as historical disparities, poor access to care, and ethnic under-representation in data collection exacerbate these biases, leading to inequitable healthcare predictions and decisions that affect outcomes for ethnic minority groups. The article emphasizes the necessity of understanding the causal relationships underlying data patterns to mitigate some of these biases and enhance the effectiveness and fairness of machine learning applications in mental health care."
"What's fair is… fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in the integration of clinical machine learning",2023,Melissa D. McCradden; Oluwadara Odusi; Shalmali Joshi; Ismail Akrout; Kagiso Ndlovu; Ben Glocker; Gabriel Maicas; Xiaoxuan Liu; Mjaye Mazwi; Tee Garnett; Lauren Oakden‐Rayner; Myrtede Alfred; Irvine Sihlahla; Oswa Shafei; Anna Goldenberg,,20,W4380318724,10.1145/3593013.3594096,https://openalex.org/W4380318724,https://dl.acm.org/doi/pdf/10.1145/3593013.3594096,Operationalization; Context (archaeology); Engineering ethics; Health care; Economic Justice,article,True,"The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools."
Survey and Evaluation of Hypertension Machine Learning Research,2023,Clea du Toit; Tran Tran; Neha Deo; Sachin Aryal; Stefanie Lip; Robert Sykes; Ishan Manandhar; Aristeidis Sionakidis; Leah Stevenson; Harsha Pattnaik; Safaa Alsanosi; Maria Kassi; Ngọc Minh Lê; Maggie Rostron; Sarah Nichol; Alisha Aman; Faisal A. Nawaz; Dhruven Mehta; Ramakumar Tummala; Linsay McCallum; Sandeep Reddy; Shyam Visweswaran; Rahul Kashyap; Bina Joe; Sandosh Padmanabhan,Journal of the American Heart Association,20,W4367368062,10.1161/jaha.122.027896,https://openalex.org/W4367368062,https://www.ahajournals.org/doi/pdf/10.1161/JAHA.122.027896,Medicine; Blood pressure; Artificial intelligence; Big data; Machine learning,article,True,"Background Machine learning (ML) is pervasive in all fields of research, from automating tasks to complex decision‐making. However, applications in different specialities are variable and generally limited. Like other conditions, the number of studies employing ML in hypertension research is growing rapidly. In this study, we aimed to survey hypertension research using ML, evaluate the reporting quality, and identify barriers to ML's potential to transform hypertension care. Methods and Results The Harmonious Understanding of Machine Learning Analytics Network survey questionnaire was applied to 63 hypertension‐related ML research articles published between January 2019 and September 2021. The most common research topics were blood pressure prediction (38%), hypertension (22%), cardiovascular outcomes (6%), blood pressure variability (5%), treatment response (5%), and real‐time blood pressure estimation (5%). The reporting quality of the articles was variable. Only 46% of articles described the study population or derivation cohort. Most articles (81%) reported at least 1 performance measure, but only 40% presented any measures of calibration. Compliance with ethics, patient privacy, and data security regulations were mentioned in 30 (48%) of the articles. Only 14% used geographically or temporally distinct validation data sets. Algorithmic bias was not addressed in any of the articles, with only 6 of them acknowledging risk of bias. Conclusions Recent ML research on hypertension is limited to exploratory research and has significant shortcomings in reporting quality, model validation, and algorithmic bias. Our analysis identifies areas for improvement that will help pave the way for the realization of the potential of ML in hypertension and facilitate its adoption."
Navigating ethical considerations in software development and deployment in technological giants,2024,Daniel Ajiga; Patrick Azuka Okeleke; Samuel Olaoluwa Folorunsho; Chinedu Ezeigweneme,International Journal of Engineering Research Updates,14,W4401829668,10.53430/ijeru.2024.7.1.0033,https://openalex.org/W4401829668,,Transparency (behavior); Accountability; Software deployment; Technological change; Process (computing),article,False,"The rapid evolution of software development and deployment in technological giants has brought unprecedented advancements and efficiencies, reshaping industries and societies. However, this rapid growth also presents significant ethical considerations that developers and organizations must navigate to ensure responsible and sustainable technology. This review explores the key ethical issues inherent in the software development lifecycle within large technology companies, focusing on data privacy, algorithmic bias, transparency, accountability, and the broader societal impact. Data privacy remains a paramount concern, with technological giants often possessing vast amounts of sensitive user information. Ensuring the ethical handling, storage, and use of this data is crucial to maintaining user trust and complying with regulatory frameworks. Additionally, algorithmic bias poses a significant challenge, as biased algorithms can perpetuate and even exacerbate social inequalities. Addressing this issue requires concerted efforts in diverse representation during the development process and rigorous testing for bias. Transparency and accountability are also essential in ethical software development. Technological giants must be transparent about their data practices and the functioning of their algorithms, providing users and stakeholders with clear information about how decisions are made. Moreover, establishing accountability mechanisms is vital to address potential harms and ensure that developers and organizations are held responsible for their technological outputs. The societal impact of software deployed by technological giants cannot be overlooked. The widespread adoption of new technologies can have far-reaching effects on employment, mental health, and social dynamics. Thus, ethical considerations must extend beyond technical aspects to encompass the broader implications of technology on society. In conclusion, navigating ethical considerations in software development and deployment within technological giants requires a multifaceted approach. By prioritizing data privacy, addressing algorithmic bias, ensuring transparency and accountability, and considering the societal impact, these companies can develop and deploy software that is not only innovative but also ethically responsible. This review underscores the importance of integrating ethical frameworks into the technological development process to foster trust, fairness, and societal well-being."
Artificial Intelligence (AI) Ethics in Accounting,2024,Brandon Schweitze,Journal of Accounting Ethics & Public Policy,22,W4394860663,10.60154/jaepp.2024.v25n1p67,https://openalex.org/W4394860663,https://www.jaepp.org/index.php/jaepp/article/download/349/329/445,Transparency (behavior); Accountability; Safeguarding; Engineering ethics; Ethical decision,article,True,"The rapid advancement of artificial intelligence (AI) has revolutionized the accounting profession, automating tasks, identifying patterns, and improving accuracy. However, the increasing reliance on AI raises ethical concerns regarding privacy, bias, transparency, and accountability. This research paper delves into the ethical considerations of AI implementation in accounting practices.Thepaper begins by examining the potential benefits of AI in accounting, highlighting its ability to streamline operations, enhance efficiency, and reduce errors. However, it also acknowledges the ethical risks associated with AI, including data privacy breaches, biased decision-making, lack of transparency, and accountability issues.The paper proposes a framework for responsible AI implementation in accounting to address these ethical concerns. The framework emphasizes establishing clear ethical guidelines,ensuring data privacy and security, mitigating AI algorithms' bias, promoting AI decisionmaking transparency, and establishing accountability mechanisms.The paper further explores the role of accountants in addressing AI ethics. Accountants are responsible for upholding ethical standards and ensuring that AI systems are used responsibly and ethically. They must be aware of the ethical implications of AI and have the knowledge and skills to mitigate ethical risks.In conclusion, the paper emphasizes the need for a proactive approach to AI ethics in accounting. By establishing clear ethical guidelines, promoting responsible AI implementation, and empowering accountants with ethical knowledge and skills, the accounting profession can harness the potential of AI while upholding ethical principles and safeguarding public trust."
A Biased Random-Key Genetic Algorithm for the Home Care Routing and Scheduling Problem: Exploring the Algorithm’s Configuration Process,2023,Ana Raquel Aguiar; Tânia Rodrigues Pereira Ramos; Maria Isabel Gomes,Springer proceedings in mathematics & statistics,2,W4319454262,10.1007/978-3-031-20788-4_1,https://openalex.org/W4319454262,,Robustness (evolution); Key (lock); Computer science; Solver; Genetic algorithm,book-chapter,False,
Artificial Intelligence in Personalized Learning with a Focus on Current Developments and Future Prospects,2024,Chaira Mahmoud; Jan Tind Sørensen,Research and Advances in Education,30,W4402026005,10.56397/rae.2024.08.04,https://openalex.org/W4402026005,https://doi.org/10.56397/rae.2024.08.04,Focus (optics); Current (fluid); Computer science; Artificial intelligence; Data science,article,True,"This paper provides a comprehensive review of the role of Artificial Intelligence (AI) in personalized learning, exploring current developments and future prospects. The integration of AI into education has led to significant advancements in adaptive learning systems, intelligent tutoring systems, and learning analytics, all of which contribute to more customized and effective learning experiences. The paper examines the benefits of AI-powered personalized learning, including enhanced student engagement, improved learning outcomes, and scalability. It also addresses the challenges and ethical considerations associated with AI in education, such as data privacy, equity, algorithmic bias, and the evolving role of teachers. Looking ahead, the paper discusses the future prospects of AI in personalized learning, highlighting the potential for more advanced adaptive systems, AI-driven content creation, and the integration of immersive technologies like virtual and augmented reality. The paper concludes by emphasizing the need for ongoing innovation, collaboration, and ethical considerations to fully realize the potential of AI in creating a more personalized, inclusive, and effective educational landscape."
Distributed localization algorithm for wireless sensor networks under minuscule bias injection attacks,2024,Xinming Chen; Ya Wang; Lei Shi; Jinliang Shao,,1,W4392382798,10.1117/12.3014916,https://openalex.org/W4392382798,,Wireless sensor network; Computer science; Wireless; Algorithm; Distributed algorithm,article,False,"In this paper, the distributed localization problem of wireless sensor networks under minuscule bias injection attacks is studied. It is assumed that the attacker launches minuscule bias injection attacks on the distance measurement between sensors during the localization process of wireless sensor network, and the accumulated bias value has a great impact on the localization accuracy. To solve the above problems, a distributed localization algorithm based on adjacent detection and numerical fitting is proposed in this paper. The proposed algorithm realizes the detection of minuscule bias injection attacks by adjacent detection and improves the localization accuracy of the algorithm by numerical fitting. Finally, the correctness and superiority of the proposed distributed localization algorithm are verified by simulation experiment."
Targeting Machine Learning and Artificial Intelligence Algorithms in Health Care to Reduce Bias and Improve Population Health,2024,Thelma C. Hurd; Fay Cobb Payton; Darryl B. Hood,Milbank Quarterly,3,W4401417168,10.1111/1468-0009.12712,https://openalex.org/W4401417168,https://doi.org/10.1111/1468-0009.12712,Population health; Health care; Software deployment; Artificial intelligence; Health policy,article,True,"Policy Points Artificial intelligence (AI) is disruptively innovating health care and surpassing our ability to define its boundaries and roles in health care and regulate its application in legal and ethical ways. Significant progress has been made in governance in the United States and the European Union. It is incumbent on developers, end users, the public, providers, health care systems, and policymakers to collaboratively ensure that we adopt a national AI health strategy that realizes the Quintuple Aim; minimizes race‐based medicine; prioritizes transparency, equity, and algorithmic vigilance; and integrates the patient and community voices throughout all aspects of AI development and deployment."
Innovating responsibly: ethical considerations for AI in early childhood education,2025,Ilene R. Berson; Michael J. Berson; Wenwei Luo,AI Brain and Child,19,W4408132665,10.1007/s44436-025-00003-5,https://openalex.org/W4408132665,https://link.springer.com/content/pdf/10.1007/s44436-025-00003-5.pdf,Engineering ethics; Early childhood education; Psychology; Political science; Environmental ethics,article,True,"Abstract The rapid integration of artificial intelligence (AI) into early childhood education (ECE) presents transformative possibilities but raises urgent ethical challenges that demand immediate attention. This scoping review examines 42 studies to explore key ethical concerns in four interconnected areas: data privacy, impacts on child development, algorithmic bias, and regulatory frameworks. Findings reveal significant gaps in safeguarding children’s sensitive data, with inadequate protections against breaches, profiling, and misuse. Emotional AI tools, such as social robots and emotion-recognition technologies, offer novel learning opportunities but risk undermining relational learning and fostering overreliance, manipulation, or loss of autonomy. The lack of developmentally appropriate design in AI systems further exacerbates these risks, failing to align technological solutions with the unique needs of young learners. Algorithmic bias, driven by non-representative datasets, perpetuates systemic inequities, disproportionately affecting marginalized communities and eroding fairness. Regulatory frameworks are fragmented and inconsistent, often lacking provisions tailored to the vulnerabilities of children or mechanisms for global enforcement. To address these challenges, this study highlights the urgency of establishing global frameworks that prioritize transparency, data minimization, and cultural inclusivity. Engaging educators, parents, and children in participatory governance is essential to align AI design with developmental needs and uphold children’s rights. These findings underscore the need for immediate and sustained efforts to ensure that AI systems in ECE foster equitable and ethical learning environments, safeguarding the well-being of young learners while advancing innovation responsibly."
A roadmap to artificial intelligence (AI): Methods for designing and building AI ready data to promote fairness,2024,Farah Kidwai‐Khan; Rixin Wang; Melissa Skanderson; Cynthia Brandt; Samah Fodeh; Julie A. Womack,Journal of Biomedical Informatics,15,W4396834898,10.1016/j.jbi.2024.104654,https://openalex.org/W4396834898,https://www.ncbi.nlm.nih.gov/pmc/articles/11144439,Raw data; Computer science; Artificial intelligence; Machine learning; Big data,article,True,
Auditing Political Exposure Bias: Algorithmic Amplification on Twitter/X Approaching the 2024 U.S. Presidential Election,2024,Jinyi Ye; Luca Luceri; Emilio Ferrara,SSRN Electronic Journal,3,W4405359942,10.2139/ssrn.5018879,https://openalex.org/W4405359942,https://doi.org/10.2139/ssrn.5018879,Presidential election; Politics; Audit; Presidential system; Political science,preprint,True,
Metrics and Algorithms for Identifying and Mitigating Bias in AI Design: A Counterfactual Fairness Approach,2025,D. H. Moon; Seongjin Ahn,IEEE Access,2,W4409014323,10.1109/access.2025.3556082,https://openalex.org/W4409014323,https://doi.org/10.1109/access.2025.3556082,Counterfactual thinking; Computer science; Algorithm; Artificial intelligence; Machine learning,article,True,"The rapid advancements in artificial intelligence (AI) have revolutionized industries such as healthcare, finance, and education. However, these advancements have also intensified ethical concerns regarding bias, fairness, and accountability in AI systems. Traditional fairness evaluation methods primarily focus on dataset-level biases, overlooking biases arising from model decision-making processes. This study introduces a novel framework for identifying, evaluating, and mitigating biases in AI models using counterfactual fairness, a robust approach that simulates alternative outcomes to minimize discriminatory effects. The proposed methodology integrates fairness-aware data preprocessing, feature selection, and model optimization strategies, ensuring equitable treatment across demographic groups. To validate the framework, we conducted empirical experiments using random forest and eXtreme Gradient Boosting models on the xAPI-Edu-Data dataset. Our results demonstrate significant improvements in demographic parity and equal opportunity fairness metrics while maintaining high predictive performance. Furthermore, comparative analysis with existing bias mitigation techniques confirms that our approach effectively reduces bias propagation in AI decision-making processes. By incorporating counterfactual fairness into AI design, this study provides a scalable and adaptable solution for ensuring ethical AI deployments aligned with regulatory standards."
Algorithmic Fairness in Lesion Classification by Mitigating Class Imbalance and Skin Tone Bias,2024,Faizanuddin Ansari; Tapabrata Chakraborti; Swagatam Das,Lecture notes in computer science,2,W4403071871,10.1007/978-3-031-72378-0_35,https://openalex.org/W4403071871,,Computer science; Class (philosophy); Tone (literature); Artificial intelligence; Skin lesion,book-chapter,False,
Variable Step-Size Diffusion Bias-Compensated APV Algorithm Over Networks,2024,Fuyi Huang; Shuting Yang; Sheng Zhang; Haiqiang Chen; Pengwei Wen,IEEE Transactions on Signal and Information Processing over Networks,2,W4404238455,10.1109/tsipn.2024.3496255,https://openalex.org/W4404238455,,Algorithm; Variable (mathematics); Diffusion; Computer science; Statistical physics,article,False,
Study on the Impact of Artificial Intelligence on Student Learning Outcomes,2024,P. Sasikala; R. Ravichandran,Journal of Digital Learning and Education,21,W4401886736,10.52562/jdle.v4i2.1234,https://openalex.org/W4401886736,https://doi.org/10.52562/jdle.v4i2.1234,Mathematics education; Psychology; Artificial intelligence; Computer science,article,True,"This study explores the transformative potential of Artificial Intelligence (AI) in education by analyzing its impact on student learning outcomes. Through a comprehensive literature review, the research synthesizes current findings on the integration of AI in educational settings, examining both the benefits and challenges it presents. The study explores into AI's role in personalizing learning experiences, enhancing student engagement, and improving academic performance. Ethical considerations such as data privacy and algorithmic bias are also assessed. This research also identifies existing gaps in the literature and suggests avenues for future inquiry, contributing to a deeper understanding of how AI can be effectively and responsibly integrated into education to optimize student success."
When facial recognition does not ‘recognise’: erroneous identifications and resulting liabilities,2023,Vera Lúcia Raposo,AI & Society,16,W4319460931,10.1007/s00146-023-01634-z,https://openalex.org/W4319460931,https://link.springer.com/content/pdf/10.1007/s00146-023-01634-z.pdf,Computer science; Identification (biology); Perspective (graphical); Probabilistic logic; Artificial intelligence,article,True,"Abstract Facial recognition is an artificial intelligence-based technology that, like many other forms of artificial intelligence, suffers from an accuracy deficit. This paper focuses on one particular use of facial recognition, namely identification, both as authentication and as recognition. Despite technological advances, facial recognition technology can still produce erroneous identifications. This paper addresses algorithmic identification failures from an upstream perspective by identifying the main causes of misidentifications (in particular, the probabilistic character of this technology, its ‘black box’ nature and its algorithmic bias) and from a downstream perspective, highlighting the possible legal consequences of such failures in various scenarios (namely liability lawsuits). In addition to presenting the causes and effects of such errors, the paper also presents measures that can be deployed to reduce errors and avoid liabilities."
Navigating Ethical Complexities Through Epistemological Analysis of ChatGPT,2023,Siraprapa Chavanayarn,Bulletin of Science Technology & Society,13,W4388911567,10.1177/02704676231216355,https://openalex.org/W4388911567,,Epistemology; Comprehension; Trustworthiness; Ethical issues; Ethical theories,article,False,"This article undertakes an epistemological analysis to explore the ethical complexities of ChatGPT, an AI system. While ethical concerns regarding AI have received considerable attention, the epistemological dimension has been largely neglected. By integrating epistemology, the study aims to deepen our understanding of the ethical issues associated with ChatGPT. Four specific issues are examined: ChatGPT's role in testimony, its potential designation as an expert, the influence of user epistemic limitations and vices, and the impact of algorithmic bias on ChatGPT. The study's findings contribute to an inclusive comprehension of the ethical implications arising from the epistemological complexities of ChatGPT. They reveal the limitations of ChatGPT as a trustworthy source of testimony, attributable to its lack of genuine understanding. The blurring of boundaries between AI-generated information and authentic expertise is identified as a significant concern. Furthermore, the study underscores the necessity of addressing the epistemic limitations and biases of users to foster responsible decision-making and prevent the perpetuation of flawed knowledge. Finally, the ethical ramifications of algorithmic bias in ChatGPT are explored, emphasizing its impact on societal fairness and justice."
Criminal Justice and Artificial Intelligence: How Should we Assess the Performance of Sentencing Algorithms?,2024,Jesper Ryberg,Philosophy & Technology,16,W4390795059,10.1007/s13347-024-00694-3,https://openalex.org/W4390795059,https://link.springer.com/content/pdf/10.1007/s13347-024-00694-3.pdf,Philosophy of technology; Criminal justice; Algorithm; Economic Justice; Criminology,article,True,"Abstract Artificial intelligence is increasingly permeating many types of high-stake societal decision-making such as the work at the criminal courts. Various types of algorithmic tools have already been introduced into sentencing. This article concerns the use of algorithms designed to deliver sentence recommendations. More precisely, it is considered how one should determine whether one type of sentencing algorithm (e.g., a model based on machine learning) would be ethically preferable to another type of sentencing algorithm (e.g., a model based on old-fashioned programming). Whether the implementation of sentencing algorithms is ethically desirable obviously depends upon various questions. For instance, some of the traditional issues that have received considerable attention are algorithmic biases and lack of transparency. However, the purpose of this article is to direct attention to a further challenge that has not yet been considered in the discussion of sentencing algorithms. That is, even if is assumed that the traditional challenges concerning biases, transparency, and cost-efficiency have all been solved or proven insubstantial, there will be a further serious challenge associated with the comparison of sentencing algorithms; namely, that we do not yet possess an ethically plausible and applicable criterion for assessing how well sentencing algorithms are performing."
"AM–GM Algorithm for Evaluating, Analyzing, and Correcting the Spatial Scaling Bias of the Leaf Area Index",2023,Jingyu Zhang; Rui Sun; Zhiqiang Xiao; Liang Zhao; Donghui Xie,Remote Sensing,1,W4380633039,10.3390/rs15123068,https://openalex.org/W4380633039,https://www.mdpi.com/2072-4292/15/12/3068/pdf?version=1686566171,Scaling; Leaf area index; Logarithm; Algorithm; Scale (ratio),article,True,"The leaf area index (LAI) is a crucial variable in climate, ecological, and land surface modeling. However, the estimation of the LAI from coarse-resolution remote sensing data can be affected by the spatial scaling bias, which arises from the nonlinearity of retrieval models and the heterogeneity of the land surface. This study provides an algorithm named Arithmetic Mean and Geometric Mean (AM–GM) to correct the spatial scaling bias. It is established based on negative logarithmic functions and avoids second-order stationarity. In this algorithm, relationships are derived between the scaling bias of LAI and the arithmetic and geometric means of directional gap probability for two commonly used remote sensing models, the Beer–Lambert law and a semi-empirical transfer function, respectively. According to the AM–GM algorithm, the expression representing the model nonlinearity is derived and utilized for the analysis of LAI scaling bias. Furthermore, the AM–GM algorithm is simplified by a linear relationship, which is constructed between two quantities related to the directional gap probability between two specific resolutions. Two scenes simulated by the LargE-Scale remote sensing data and image Simulation framework (LESS) model and three sites are used to evaluate the proposed algorithm and analyze the scaling bias of LAI. The validation results show that the AM–GM algorithm provides accurate correction of LAI scaling bias. The analyses based on the AM–GM algorithm demonstrate that the scaling bias of LAI increases with the increase in the LAI value, with stronger surface heterogeneity and coarser spatial resolution. The validation results of the simplified AM–GM algorithm demonstrate that at the Sud-Ouest site, the absolute value of the bias for the estimated LAI decreases from 0.10, 0.22, 0.29, and 0.31 to 0.04, 0.01, 0.04, and 0.05 at 200 m, 500 m, 1000 m, and 1500 m resolutions, respectively. In conclusion, the proposed algorithm is effective in the analysis and correction of the scaling bias for coarse-resolution LAI."
Addressing Algorithmic Bias,2025,Abhishek Benedict Kumar; Karun Sanjaya,Advances in computational intelligence and robotics book series,0,W4414697660,10.4018/979-8-3373-2387-9.ch011,https://openalex.org/W4414697660,,,book-chapter,False,"As artificial intelligence (AI) systems increasingly make decisions in areas such as justice, health, and finance, issues related to algorithmic biases have risen to prominence when discussing equity and inclusion. This chapter investigates how prejudices are built into data and model specifications, and how they can contribute to further entrenching social disparities. From a legal and ethical perspective, it examines the shortcoming that current anti-discrimination laws face when confronted with AI-generated harm. The chapter provides examples of actual cases, such as COMPAS risk assessment tool, to illustrate the real-life implications of biased algorithms. It calls for holistic approaches that include legal remedies, algorithmic transparency, participatory governance, and anti-discrimination data practices. By integrating the development of AI into fair, accountable, and human rights respecting principles, the chapter emphasize the immediate necessity of ensuring that technological development is aligned with just and inclusive development."
"Generative Visual AI in News Organizations: Challenges, Opportunities, Perceptions, and Policies",2024,T.J. Thomson; Ryan J. Thomas; Phoebe Matich,Digital Journalism,27,W4394571198,10.1080/21670811.2024.2331769,https://openalex.org/W4394571198,https://www.tandfonline.com/doi/pdf/10.1080/21670811.2024.2331769?needAccess=true,Perception; Generative grammar; Public relations; Political science; Knowledge management,article,True,"The use of AI-enabled text-to-image generators, such as Midjourney and DALL-E, raises profound questions about the purpose, meaning, and value of images generally, and the production, editing, and consumption of images in journalism specifically. This study explores how photo editors (or their equivalents) in seven countries perceive and/or use generative visual AI in their editorial operations and outlines the challenges and opportunities they see for the technology. It also identifies the extent to which these news organizations have policies governing how generative visual AI is used or, if not, the principles that they feel should inform their development. Participants identified mis/disinformation as the primary challenge of AI-generated images, also raising concerns about labor and copyright implications, the difficulty or impossibility of detecting AI-generated images, the potential for algorithmic bias, and the potential reputational risk of using AI-generated images. Conversely, participants saw potential for using AI for illustrations and brainstorming, while a minority saw it as an opportunity to increase efficiencies and cut costs."
Algorithm Biases and Values,2023,Domenico Talia,ACM eBooks,0,W4387807783,10.1145/3603178.3603186,https://openalex.org/W4387807783,,Citation; Computer science; Information retrieval; Operations research; World Wide Web,book-chapter,False,"chapter Share on Algorithm Biases and Values Author: Domenico Talia University of Calabria University of CalabriaView Profile Authors Info & Claims From Algorithms to Thinking Machines: The New Digital PowerOctober 2023https://doi.org/10.1145/3603178.3603186Published:20 October 2023Publication History 0citation0DownloadsMetricsTotal Citations0Total Downloads0Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access"
Leveraging AI in E-Learning: Personalized Learning and Adaptive Assessment through Cognitive Neuropsychology—A Systematic Analysis,2024,Constantinos Halkiopoulos; Evgenia Gkintoni,Electronics,168,W4402762189,10.3390/electronics13183762,https://openalex.org/W4402762189,https://doi.org/10.3390/electronics13183762,Neuropsychology; Computer science; Cognition; Personalized learning; Adaptive learning,article,True,"This paper reviews the literature on integrating AI in e-learning, from the viewpoint of cognitive neuropsychology, for Personalized Learning (PL) and Adaptive Assessment (AA). This review follows the PRISMA systematic review methodology and synthesizes the results of 85 studies that were selected from an initial pool of 818 records across several databases. The results indicate that AI can improve students’ performance, engagement, and motivation; at the same time, some challenges like bias and discrimination should be noted. The review covers the historic development of AI in education, its theoretical grounding, and its practical applications within PL and AA with high promise and ethical issues of AI-powered educational systems. Future directions are empirical validation of effectiveness and equity, development of algorithms that reduce bias, and exploration of ethical implications regarding data privacy. The review identifies the transformative potential of AI in developing personalized and adaptive learning (AL) environments, thus, it advocates continued development and exploration as a means to improve educational outcomes."
"Harnessing artificial intelligence in sepsis care: advances in early detection, personalized treatment, and real-time monitoring",2025,Fang Li; Shengguo Wang; Zhi Gao; Ma Qing; Shan L. Pan; Yingying Liu; Chengchen Hu,Frontiers in Medicine,26,W4406112882,10.3389/fmed.2024.1510792,https://openalex.org/W4406112882,https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2024.1510792/pdf,Sepsis; Intensive care medicine; Medicine; Computer science; Internal medicine,review,True,"Sepsis remains a leading cause of morbidity and mortality worldwide due to its rapid progression and heterogeneous nature. This review explores the potential of Artificial Intelligence (AI) to transform sepsis management, from early detection to personalized treatment and real-time monitoring. AI, particularly through machine learning (ML) techniques such as random forest models and deep learning algorithms, has shown promise in analyzing electronic health record (EHR) data to identify patterns that enable early sepsis detection. For instance, random forest models have demonstrated high accuracy in predicting sepsis onset in intensive care unit (ICU) patients, while deep learning approaches have been applied to recognize complications such as sepsis-associated acute respiratory distress syndrome (ARDS). Personalized treatment plans developed through AI algorithms predict patient-specific responses to therapies, optimizing therapeutic efficacy and minimizing adverse effects. AI-driven continuous monitoring systems, including wearable devices, provide real-time predictions of sepsis-related complications, enabling timely interventions. Beyond these advancements, AI enhances diagnostic accuracy, predicts long-term outcomes, and supports dynamic risk assessment in clinical settings. However, ethical challenges, including data privacy concerns and algorithmic biases, must be addressed to ensure fair and effective implementation. The significance of this review lies in addressing the current limitations in sepsis management and highlighting how AI can overcome these hurdles. By leveraging AI, healthcare providers can significantly enhance diagnostic accuracy, optimize treatment protocols, and improve overall patient outcomes. Future research should focus on refining AI algorithms with diverse datasets, integrating emerging technologies, and fostering interdisciplinary collaboration to address these challenges and realize AI’s transformative potential in sepsis care."
Robustly Improving Bandit Algorithms with Confounded and Selection Biased Offline Data: A Causal Approach,2024,Wen Huang; Xintao Wu,Proceedings of the AAAI Conference on Artificial Intelligence,2,W4393160650,10.1609/aaai.v38i18.30027,https://openalex.org/W4393160650,https://ojs.aaai.org/index.php/AAAI/article/download/30027/31806,Selection (genetic algorithm); Computer science; Algorithm; Artificial intelligence; Machine learning,article,True,"This paper studies bandit problems where an agent has access to offline data that might be utilized to potentially improve the estimation of each arm’s reward distribution. A major obstacle in this setting is the existence of compound biases from the observational data. Ignoring these biases and blindly fitting a model with the biased data could even negatively affect the online learning phase. In this work, we formulate this problem from a causal perspective. First, we categorize the biases into confounding bias and selection bias based on the causal structure they imply. Next, we extract the causal bound for each arm that is robust towards compound biases from biased observational data. The derived bounds contain the ground truth mean reward and can effectively guide the bandit agent to learn a nearly-optimal decision policy. We also conduct regret analysis in both contextual and non-contextual bandit settings and show that prior causal bounds could help consistently reduce the asymptotic regret."
Artificial intelligence bias in the prediction and detection of cardiovascular disease,2024,Ariana Mihan; Ambarish Pandey; Harriette G.C. Van Spall,npj Cardiovascular Health,13,W4404600835,10.1038/s44325-024-00031-9,https://openalex.org/W4404600835,https://doi.org/10.1038/s44325-024-00031-9,Socioeconomic status; Disease; Race (biology); Health equity; Perspective (graphical),article,True,"Abstract AI algorithms can identify those at risk of cardiovascular disease (CVD), allowing for early intervention to change the trajectory of disease. However, AI bias can arise from any step in the development, validation, and evaluation of algorithms. Biased algorithms can perform poorly in historically marginalized groups, amplifying healthcare inequities on the basis of age, sex or gender, race or ethnicity, and socioeconomic status. In this perspective, we discuss the sources and consequences of AI bias in CVD prediction or detection. We present an AI health equity framework and review bias mitigation strategies that can be adopted during the AI lifecycle."
"Harnessing the power of synthetic data in healthcare: innovation, application, and privacy",2023,Mauro Giuffrè; Dennis Shung,npj Digital Medicine,240,W4387457231,10.1038/s41746-023-00927-3,https://openalex.org/W4387457231,https://www.nature.com/articles/s41746-023-00927-3.pdf,Big data; Distrust; Accountability; Context (archaeology); Health care,review,True,
A Multi-objective Biased Random-Key Genetic Algorithm for the Siting of Emergency Vehicles,2023,Francesca Da Ros; Luca Di Gaspero; David La Barbera; Vincenzo Della Mea; Kevin Roitero; Laura Deroma; Sabrina Licata; Francesca Valent,Lecture notes in computer science,1,W4321504914,10.1007/978-3-031-26504-4_32,https://openalex.org/W4321504914,,Key (lock); Computer science; Genetic algorithm; Percentile; Event (particle physics),book-chapter,False,
Algorithmic Trading and AI: A Review of Strategies and Market Impact,2024,Wilhelmina Afua Addy; Adeola Olusola Ajayi-Nifise; Binaebi Gloria Bello; Sunday Tubokirifuruar Tula; Olubusola Odeyemi; Titilola Falaiye,World Journal of Advanced Engineering Technology and Sciences,37,W4391914348,10.30574/wjaets.2024.11.1.0054,https://openalex.org/W4391914348,,Algorithmic trading; Computer science; Financial economics; Economics,review,False,"This review explores the dynamic intersection of algorithmic trading and artificial intelligence (AI) within financial markets. It delves into the evolution, strategies, and broader market impact of algorithmic trading fueled by AI technologies. Examining the symbiotic relationship between advanced algorithms and AI, the review navigates through the various strategies employed, shedding light on their implications for market efficiency, liquidity, and overall stability. From high-frequency trading to machine learning-driven predictive analytics, this review unveils the multifaceted landscape of algorithmic trading in the era of AI, presenting both opportunities and challenges for financial markets. The review begins by tracing the historical development of algorithmic trading, emphasizing the paradigm shift with the integration of AI. From traditional programmatic trading to the emergence of sophisticated algorithms driven by machine learning and deep learning, the evolution sets the stage for a comprehensive understanding of the subject. An in-depth analysis of diverse algorithmic trading strategies unfolds, covering areas such as trend following, statistical arbitrage, market making, and sentiment analysis. The incorporation of AI introduces adaptive learning capabilities, enabling algorithms to evolve and optimize strategies based on real-time market conditions. Exploring the impact of algorithmic trading on financial markets, the review examines how AI-driven strategies contribute to market efficiency, liquidity provision, and price discovery. It dissects the implications for traditional market structures, regulatory considerations, and the potential risks associated with algorithmic dominance. Acknowledging the transformative power of algorithmic trading with AI, the review critically assesses the challenges and ethical considerations. From algorithmic bias to systemic risks, the review delves into the darker corners of this technological advancement, prompting a reflection on the need for responsible and transparent practices. The review concludes by peering into the future trajectory of algorithmic trading fueled by AI. Anticipated innovations, regulatory responses, and the evolving landscape of financial markets are discussed, offering insights into the ongoing transformation and potential disruptions in the realm of algorithmic trading. In essence, this review provides a nuanced perspective on the intricate relationship between algorithmic trading and AI, offering a comprehensive understanding of their strategies and the transformative impact on financial markets."
Exploring Algorithmic Bias in Cosmopolis,2025,Sudil Panta,NPRC Journal of Multidisciplinary Research.,0,W4415728678,10.3126/nprcjmr.v2i9.85172,https://openalex.org/W4415728678,https://www.nepjol.info/index.php/nprcjmr/article/download/85172/64855,,article,True,"Background: Cosmopolis by Don DeLillo illustrates algorithmic bias through the protagonist’s overdependence on the data-driven financial system. Within the scope of Cosmopolis, DeLillo sketches the world as an information-driven environment, where systems of the algorithm dictate financial and social realities. This paper investigates the representation of algorithmic bias—a critical issue for contemporary AI ethics—with respect to the novel. Methods: This paper explores an algorithmic bias through critical posthumanism, political ecology, and Jean Baudrillard’s hyperreality. It describes the device through which the character Eric Packer detaches himself from the material world and the marginalization of the character Benno Levin. Results &amp; Conclusion: The novel made clear that algorithmic bias is not a technical issue but a systemic flaw; rooted in abstract data, which mostly supersedes human reality. Packer's downfall thus reveals the hubris underlying the inherent limits of data-driven prediction, while Levin's situation points to the human cost of concerned systems. Cosmopolis is ultimately a kind of parable or cautionary tale about the urgency of transparency, ethical accountability, and the reinstatement of human agency in a world overwhelmingly driven by machines. Novelty: This study explores the algorithmic bias through the study of major protagonists like Eric Packer and Benno Levin. It investigates the structural issue of algorithmic bias by using critical posthumanism, Baudrillard’s hyperreality, and political ecology. This paper considers Cosmopolis as a foresighted critique of data-driven culture, bringing to light the ""black box"" dilemma and the human consequences of algorithmic marginalization before these topics were discussed in the general ethical community."
Can artificial intelligence’s limitations drive innovative work behaviour?,2023,Araz Zirar,Review of Managerial Science,26,W4319660222,10.1007/s11846-023-00621-4,https://openalex.org/W4319660222,https://link.springer.com/content/pdf/10.1007/s11846-023-00621-4.pdf,Transformational leadership; Thematic analysis; Perspective (graphical); Psychology; Work (physics),article,True,
Tutorials at The Web Conference 2023,2023,Valeria Fionda; Olaf Hartig; Reyhaneh Abdolazimi; Sihem Amer-Yahia; Hongzhi Chen; Xiao Chen; Peng Cui; Jeff Dalton; Xin Luna Dong; Lisette Espín-Noboa; Wenqi Fan; Manuela Fritz; Quan Gan; Jingtong Gao; Xiaojie Guo; Torsten Hahmann; Jiawei Han; Soyeon Caren Han; Estevam Hruschka; Liang Hu; Jiaxin Huang; Utkarshani Jaimini; Olivier Jeunen; Yushan Jiang; Fariba Karimi; George Karypis; Krishnaram Kenthapadi; Himabindu Lakkaraju; Hady W. Lauw; Thai Le; Trung-Hoang Le; Dongwon Lee; Geon Lee; Liat Levontin; Cheng–Te Li; Haoyang Li; Ying Li; Jay Chiehen Liao; Qidong Liu; Usha Lokala; Ben London; Siqu Long; Hande Küçük Mcginty; Meng Yu; Seungwhan Moon; Usman Naseem; Pradeep Natarajan; Behrooz Omidvar-Tehrani; Zijie Pan; Devesh Parekh; Jian Pei; Tiago P. Peixoto; Steven Pemberton; Josiah Poon; Filip Radlinski; Federico Rossetto; Kaushik Roy; Aghiles Salah; Mehrnoosh Sameki; Amit Sheth; Cogan Shimizu; Kijung Shin; Dongjin Song; Julia Stoyanovich; Dacheng Tao; Johanne R. Trippas; Quoc Truong; Yu-Che Tsai; Adaku Uchendu; Bram van den Akker; Lin Wang; Minjie Wang; Shoujin Wang; Xin Wang; Ingmar Weber; Henry Weld; Lingfei Wu; Da Xu; Yifan Ethan Xu; Shuyuan Xu; Bo Yang; Ke Yang; Elad Yom‐Tov; Jaemin Yoo; Zhou Yu; Reza Zafarani; Hamed Zamani; Meike Zehlike; Qi Zhang; Xikun Zhang; Yongfeng Zhang; Yu Zhang; Zheng Zhang; Liang Zhao; Xiangyu Zhao; Wenwu Zhu,,23,W4367310414,10.1145/3543873.3587713,https://openalex.org/W4367310414,https://dl.acm.org/doi/pdf/10.1145/3543873.3587713,Computer science; World Wide Web,preprint,True,"Social networks have been widely studied over the last century from multiple\ndisciplines to understand societal issues such as inequality in employment\nrates, managerial performance, and epidemic spread. Today, these and many more\nissues can be studied at global scale thanks to the digital footprints that we\ngenerate when browsing the Web or using social media platforms. Unfortunately,\nscientists often struggle to access to such data primarily because it is\nproprietary, and even when it is shared with privacy guarantees, such data is\neither no representative or too big. In this tutorial, we will discuss recent\nadvances and future directions in network modeling. In particular, we focus on\nhow to exploit synthetic networks to study real-world problems such as data\nprivacy, spreading dynamics, algorithmic bias, and ranking inequalities. We\nstart by reviewing different types of generative models for social networks\nincluding node-attributed and scale-free networks. Then, we showcase how to\nperform a network selection analysis to characterize the mechanisms of edge\nformation of any given real-world network.\n"
Considering How Machine‐Learning Algorithms (Re)produce Social Biases in Generated Faces,2024,Matthew Gusdorff; Alvin Grissom; Jeová Farias Sales Rocha Neto; Yupeng Lin; Ryan Trotter; Ryan F. Lei,Social and Personality Psychology Compass,1,W4404059775,10.1111/spc3.70021,https://openalex.org/W4404059775,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/spc3.70021,Adversarial system; Generative grammar; Computer science; Face (sociological concept); Class (philosophy),article,True,"ABSTRACT Advances in computer science–specifically in the development and use of generative machine learning–have provided powerful new tools for psychologists to create synthetic human faces as stimuli, which ultimately provide high‐quality photorealistic face images that have many advantages, including reducing typical ethical and privacy concerns and generating face images from minoritized communities that are typically underrepresented in existing face databases. However, there are a number of ways that using machine learning‐based face generation and manipulation software can introduce bias into the research process, thus threatening the validity of studies. The present article provides a summary of how one class of recently popular algorithms for generating faces–generative adversarial networks (GANs)—works, how we control GANs, and where biases (with a particular focus on racial biases) emerge throughout these processes. We discuss recommendations for mitigating these biases, as well as how these concepts manifest in similar modern text‐to‐image algorithms."
The Impact of Algorithmic Bias on Consumers,2024,Xin Chen,Frontiers in Business Economics and Management,0,W4395957389,10.54097/7m8yka94,https://openalex.org/W4395957389,https://doi.org/10.54097/7m8yka94,Computer science,article,True,"Algorithmic bias has aroused people's attention to the ethical problems of intelligent services, and directly affects consumers' willingness to use intelligent services. The purpose of this study is to explore the effect of algorithmic bias on consumers' willingness to use intelligent services, which has positive significance for the future design of intelligent services."
Balancing Data Privacy and Compliance in Blockchain-Based Financial Systems,2024,Sunday Abayomi Joseph,Journal of Engineering Research and Reports,21,W4402331609,10.9734/jerr/2024/v26i91271,https://openalex.org/W4402331609,https://doi.org/10.9734/jerr/2024/v26i91271,Blockchain; Compliance (psychology); Business; Computer security; Internet privacy,article,True,"This study explores the balance between data privacy and regulatory compliance in blockchain-based financial systems, focusing on privacy-enhancing technologies (PETs) such as Zero-Knowledge Proofs (ZKPs) and multiparty computations (MPCs). Through a comprehensive methodology combining literature review, comparative analysis, and empirical testing on the Ethereum test network, the research reveals significant trade-offs. Implementing ZKPs increased transaction times from 5 seconds to 12 seconds and gas fees from 0.02 ETH to 0.05 ETH, while computational load rose by 60%, highlighting the impact on scalability and efficiency. Chi-Square tests and regression analysis uncovered notable algorithmic biases, with low-value accounts experiencing 15% fewer transaction approvals and small mining pools receiving 20% fewer rewards than larger counterparts. Additionally, MPCs, while offering robust privacy, increased communication overhead by 35%, posing scalability challenges. The study recommends adopting a tiered privacy approach, implementing basic privacy measures for low-sensitivity transactions, and advanced technologies like ZKPs for high-sensitivity transactions while optimizing ZKPs to reduce their computational burden and enhance transaction speeds, and integrating artificial intelligence to detect and mitigate algorithmic biases in blockchain systems. Future research should also explore hybrid privacy solutions that combine the strengths of different PETs, such as ZKPs and MPCs, to achieve both robust privacy and high efficiency. Furthermore, investigating quantum-resistant cryptographic methods is crucial to safeguarding blockchain systems against emerging threats. These insights provide valuable guidance for financial institutions, blockchain developers, and policymakers, promoting the development of blockchain-based financial systems that optimize data privacy while maintaining system performance and regulatory compliance."
Improving social media use for disaster resilience: challenges and strategies,2023,Nina Lam; Michelle A. Meyer; Margaret Reams; Seungwon Yang; Kisung Lee; Lei Zou; Volodymyr Mihunov; Kejin Wang; Ryan Kirby; Heng Cai,International Journal of Digital Earth,32,W4385766542,10.1080/17538947.2023.2239768,https://openalex.org/W4385766542,https://www.tandfonline.com/doi/pdf/10.1080/17538947.2023.2239768?needAccess=true&role=button,Social media; Resilience (materials science); Emergency management; Data science; Big data,article,True,"This paper develops a social media-disaster resilience analysis framework by categorizing types of social media use and their challenges to better understand and assess its role in disaster resilience research and management. The framework is derived primarily from several case studies of Twitter use in three hurricane events in the United States – Hurricanes Isaac, Sandy, and Harvey. The paper first outlines four major contributions of social media data for disaster resilience research and management, which include serving as an effective communication platform, providing ground truth information for emergency response and rescue operations, providing information on people’s sentiments, and allowing predictive modeling. However, there are four key challenges to its uses, which include easy spreading of false information, social and geographical disparities of Twitter use, technical issues on processing and analyzing big and noisy data, especially on improving the locational accuracy of the tweets, and algorithm bias in AI and other types of modeling. Then, the paper proposes twenty strategies that the four sectors of the social media community – organizations, individuals, social media companies, and researchers – could take to improve social media use to increase disaster resilience."
A LiDAR-Assisted Reference Feedforward Bias Control Algorithm for Wind Turbines in the Transition Region,2023,Yanqin Huang; Guorong Zhu; Xiangtian Deng; Jianghua Lu; Chengzhen Jia; Hua Geng,,1,W4391216602,10.1109/peas58692.2023.10395638,https://openalex.org/W4391216602,,Control theory (sociology); Wind speed; Feed forward; Lidar; Aerodynamics,article,False,"The traditional reference bias control (RBC) algorithm has the problem that the rotor speed cannot be effectively controlled in the case of sudden increase of aerodynamic power, for which is caused by the delay of the reference signal. In this paper, the wind speed is obtained by LiDAR, and the accurate pitch angle command is obtained by using the inverse system method, which is used for the feedforward control of the reference bias signal. In the case of large changes in wind speed, the generator speed is well suppressed. Compared with the traditional RBC algorithm, this method accelerates the switching speed of torque control, and then improves the dynamic characteristics of the rotor speed in the transition region."
Toward a More Ethical Future of Artificial Intelligence and Data Science,2024,Wasswa Shafik,Advances in computational intelligence and robotics book series,24,W4392432224,10.4018/979-8-3693-2964-1.ch022,https://openalex.org/W4392432224,,Transparency (behavior); Engineering ethics; Equity (law); Ethical issues; Political science,book-chapter,False,"Examining the ethical aspects of artificial intelligence (AI) and data science (DS) recognizes their impressive progress in innovation while emphasizing the pressing necessity to tackle intricate ethical dilemmas. The chapter provides a detailed framework for navigating the changing environment, beginning with an examination of the increasing ethical challenges. The study highlights transparency, fairness, and responsibility as crucial for cultivating confidence in AI systems. The chapter emphasizes the urgent requirement to address problems such as algorithmic bias and privacy breaches with strong mitigation techniques. Furthermore, it promotes flexible policies that strike a balance between innovation and ethical safeguards. The examination of societal effects, particularly on various socioeconomic groups, economies, and cultures, is conducted thoroughly, with a focus on equity and the protection of individual rights. Finally, to proactively tackle future ethical challenges in technology, it is advisable to employ proactive solutions such as implementing AI ethics by design."
Social bias in artificial intelligence algorithms designed to improve cardiovascular risk assessment relative to the Framingham Risk Score: a protocol for a systematic review,2023,Ivneet Garcha; Susan P. Phillips,BMJ Open,2,W4378908275,10.1136/bmjopen-2022-067638,https://openalex.org/W4378908275,https://bmjopen.bmj.com/content/bmjopen/13/5/e067638.full.pdf,Medicine; Framingham Risk Score; Risk assessment; MEDLINE; Coronary artery disease,review,True,"Introduction Cardiovascular disease (CVD) prevention relies on timely identification of and intervention for individuals at risk. Risk assessment models such as the Framingham Risk Score (FRS) have been shown to over-estimate or under-estimate risk in certain groups, such as socioeconomically disadvantaged populations. Artificial intelligence (AI) and machine learning (ML) could be used to address such equity gaps to improve risk assessment; however, critical appraisal is warranted before ML-informed clinical decision-making is implemented. Methods and analysis This study will employ an equity-lens to identify sources of bias (ie, race/ethnicity, gender and social stratum) in ML algorithms designed to improve CVD risk assessment relative to the FRS. A comprehensive literature search will be completed using MEDLINE, Embase and IEEE to answer the research question: do AI algorithms that are designed for the estimation of CVD risk and that compare performance with the FRS address the sources of bias inherent in the FRS? No study date filters will be imposed on the search, but English language filters will be applied. Studies describing a specific algorithm or ML approach that provided a risk assessment output for coronary artery disease, heart failure, cardiac arrhythmias (ie, atrial fibrillation), stroke or a global CVD risk score, and that compared performance with the FRS are eligible for inclusion. Papers describing algorithms for the diagnosis rather than the prevention of CVD will be excluded. A structured narrative review analysis of included studies will be completed. Ethics and dissemination Ethics approval was not required. Ethics exemption was formally received from the General Research Ethics Board at Queen’s University. The completed systematic review will be submitted to a peer-reviewed journal and parts of the work will be presented at relevant conferences."
"Discrimination, artificial intelligence, and algorithmic decision-making",2025,Frederik Zuiderveen Borgesius,arXiv (Cornell University),156,W2992233710,10.48550/arxiv.2510.13465,https://openalex.org/W2992233710,https://arxiv.org/pdf/2510.13465,Artificial intelligence; Applications of artificial intelligence; Computer science,article,True,"Artificial intelligence (AI) has a huge impact on our personal lives and also on our democratic society as a whole. While AI offers vast opportunities for the benefit of people, its potential to embed and perpetuate bias and discrimination remains one of the most pressing challenges deriving from its increasing use. This new study, which was prepared by Prof. Frederik Zuiderveen Borgesius for the Anti-discrimination Department of the Council of Europe, elaborates on the risks of discrimination caused by algorithmic decision-making and other types of artificial intelligence (AI)."
Extreme gradient and boosting algorithm for improved bias-correction and downscaling of CMIP6 GCM data across indian river basin,2025,Chandni Thakur; Venkatesh Budamala; K. S. Kasiviswanathan; Claudia Teutschbein; Bankaru‐Swamy Soundharajan,Journal of Hydrology Regional Studies,2,W4410084813,10.1016/j.ejrh.2025.102443,https://openalex.org/W4410084813,https://doi.org/10.1016/j.ejrh.2025.102443,Downscaling; GCM transcription factors; Climatology; Structural basin; Environmental science,article,True,
Combining Human-in-the-Loop Systems and AI Fairness Toolkits to Reduce Age Bias in AI Job Hiring Algorithms,2024,Christopher G. Harris,,3,W4394713269,10.1109/bigcomp60711.2024.00019,https://openalex.org/W4394713269,,Computer science; Human-in-the-loop; Loop (graph theory); Artificial intelligence; Algorithm,article,False,"As artificial intelligence (AI) systems become more sophisticated, they are increasingly integrated into high-stakes decision-making processes, such as hiring, fraud detection, loan approvals, and medical diagnoses. However, this growing reliance on AI raises concerns about the potential for these systems to perpetuate and amplify societal biases. Researchers have developed two main approaches to bias mitigation in AI to address this issue: human-in-the-loop (HITL) systems and AI fairness toolkits. HITL systems involve human reviewers actively participating in the AI decision-making process, while AI fairness toolkits are software tools that can identify and mitigate bias. HITL systems are particularly effective in addressing biases tied to specific domains, while AI fairness toolkits can be useful in identifying and addressing bias proactively. This paper examines different combinations of HITL systems and AI fairness toolkits, conducts an experiment to evaluate biases in hiring decisions using each, and provides recommendations for organizations considering implementing one or both approaches."
Should ChatGPT Be Biased?&amp;nbsp;Challenges and Risks of Bias in Large Language Models,2023,Emilio Ferrara,SSRN Electronic Journal,103,W4388687124,10.2139/ssrn.4627814,https://openalex.org/W4388687124,https://doi.org/10.2139/ssrn.4627814,Psychology; Chemistry,preprint,True,
An Improved Doppler-Aided Smoothing Code Algorithm for Bds-2/Bds-3 Un-geo Satellites in Consideration of Satellite Code Bias,2023,Gao Xiao; Zongfang Ma; Luxiao Jia; Lin Pan,Preprints.org,2,W4379387128,10.20944/preprints202306.0160.v1,https://openalex.org/W4379387128,https://www.preprints.org/manuscript/202306.0160/v1/download,Algorithm; Smoothing; GNSS applications; Code (set theory); Computer science,preprint,True,"The extensive use of carrier-aided smoothing code (CSC) filter has led to reduce the noise level of raw code measurements in GNSS positioning and navigation applications. However, the existing CSC technique is sensitive to the changes of the integer ambiguity and then the smoothing proce-dure needs to be restarted in the presence of cycle-slips. As the Doppler shift is instantaneous ob-servation and immune to cycle-slips, Doppler-aided smoothing code (DSC) algorithm would be more promising in challenged environment. Based on the Hatch-filter, an optimal DSC approach is proposed with the principle of minimum variance. Meanwhile, to inhibit the effect of integral cumulative error of Doppler, a balance factor is adopted to adjust the contributions of raw code and DSC. The noise level of code observable is not only affected by thermal noise, but also limited by systematic bias. Satellite code bias (SCB) has been identified in the raw code observable on each frequency for each BDS-2 satellite. By minimizing the sum of absolute value of residuals, polyno-mial segment fitting algorithm as a function of elevation-angles is applied to establish the SCB cor-rection model based on epoch-differenced Multipath (MP) deviations. Finally, numerical experi-ments demonstrate the validity and efficiency of the refined DSC filter with SCB corrections on each available frequency for BDS-2 un-GEO satellites."
How Much Does Racial Bias Affect Mortgage Lending? Evidence from Human and Algorithmic Credit Decisions,2024,Neil Bhutta; Aurel Hizmo; Daniel Ringo,Working paper,3,W4392954719,10.21799/frbp.wp.2024.09,https://openalex.org/W4392954719,https://doi.org/10.21799/frbp.wp.2024.09,Affect (linguistics); Business; Psychology; Communication,report,True,
Participation and Division of Labor in User-Driven Algorithm Audits: How Do Everyday Users Work together to Surface Algorithmic Harms?,2023,Rushi Li; Sara Kingsley; Chelsea Fan; Proteeti Sinha; Nora Wai; J. Lee; Hong Shen; Motahhare Eslami; Jason Hong,,15,W4362679204,10.1145/3544548.3582074,https://openalex.org/W4362679204,https://dl.acm.org/doi/pdf/10.1145/3544548.3582074,Audit; Computer science; Work (physics); Division (mathematics); Human–computer interaction,preprint,True,"Recent years have witnessed an interesting phenomenon in which users come together to interrogate potentially harmful algorithmic behaviors they encounter in their everyday lives. Researchers have started to develop theoretical and empirical understandings of these user driven audits, with a hope to harness the power of users in detecting harmful machine behaviors. However, little is known about user participation and their division of labor in these audits, which are essential to support these collective efforts in the future. Through collecting and analyzing 17,984 tweets from four recent cases of user driven audits, we shed light on patterns of user participation and engagement, especially with the top contributors in each case. We also identified the various roles user generated content played in these audits, including hypothesizing, data collection, amplification, contextualization, and escalation. We discuss implications for designing tools to support user driven audits and users who labor to raise awareness of algorithm bias."
Unbiased recursive least squares identification methods for a class of nonlinear systems with irregularly missing data,2023,Wenxuan Liu; Meihang Li,International Journal of Adaptive Control and Signal Processing,75,W4379615208,10.1002/acs.3637,https://openalex.org/W4379615208,,Missing data; Bilinear interpolation; Least-squares function approximation; Algorithm; Nonlinear system,article,False,"Summary Missing data often occur in industrial processes. In order to solve this problem, an auxiliary model and a particle filter are adopted to estimate the missing outputs, and two unbiased parameter estimation methods are developed for a class of nonlinear systems (e.g., bilinear systems) with irregularly missing data. Firstly, an auxiliary model is constructed to estimate the unknown output, and an auxiliary model‐based multi‐innovation recursive least squares algorithm is presented by expanding the scalar innovation to an innovation vector. Secondly, according to the bias compensation principle, an auxiliary model‐based bias compensation multi‐innovation recursive least squares algorithm is proposed to compensate the bias caused by the colored noise. Thirdly, for further improving the parameter estimation accuracy, the unknown true output is estimated by a particle filter, and a particle filtering‐based bias compensation multi‐innovation recursive least squares algorithm is developed. Finally, a numerical example is selected to validate the effectiveness of the proposed algorithms. The simulation results indicate that the proposed algorithms have good performance in identifying bilinear systems with irregularly missing data."
Impact of Structural Bias on the Sine Cosine Algorithm: A Theoretical Investigation Using the Signature Test,2024,Kanchan Rajwar; Kusum Deep; M. Mathirajan,Communications in computer and information science,2,W4402543161,10.1007/978-3-031-69257-4_10,https://openalex.org/W4402543161,,Sine; Signature (topology); Trigonometric functions; Algorithm; Computer science,book-chapter,False,
Bias-Field Digitized Counterdiabatic Quantum Algorithm for Higher-Order Binary Optimization,2024,Sebastián V. Romero; Anne-Maria Visuri; Alejandro Gomez Cadavid; E. Solano; Narendra N. Hegade,arXiv (Cornell University),1,W4403593445,10.48550/arxiv.2409.04477,https://openalex.org/W4403593445,https://arxiv.org/pdf/2409.04477,Binary number; Algorithm; Order (exchange); Field (mathematics); Quantum,preprint,True,"Combinatorial optimization plays a crucial role in many industrial applications. While classical computing often struggles with complex instances, quantum optimization emerges as a promising alternative. Here, we present an enhanced bias-field digitized counterdiabatic quantum optimization (BF-DCQO) algorithm to address higher-order unconstrained binary optimization (HUBO). We apply BF-DCQO to a HUBO problem featuring three-local terms in the Ising spin-glass model, validated experimentally using 156 qubits on an IBM quantum processor. In the studied instances, our results outperform standard methods such as the quantum approximate optimization algorithm, quantum annealing, simulated annealing, and Tabu search. Furthermore, we provide numerical evidence of the feasibility of a similar HUBO problem on a 433-qubit Osprey-like quantum processor. Finally, we solve denser instances of the MAX 3-SAT problem in an IonQ emulator. Our results show that BF-DCQO offers an effective path for solving large-scale HUBO problems on current and near-term quantum processors."
Algorithmic inclusion: Shaping the predictive algorithms of artificial intelligence in hiring,2023,Elisabeth Kelan,Human Resource Management Journal,60,W4366989571,10.1111/1748-8583.12511,https://openalex.org/W4366989571,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/1748-8583.12511,Inclusion (mineral); Replicate; Audit; Computer science; Selection (genetic algorithm),article,True,"Abstract Despite frequent claims that increased use of artificial intelligence (AI) in hiring will reduce the human bias that has long plagued recruitment and selection, AI may equally replicate and amplify such bias and embed it in technology. This article explores exclusion and inclusion in AI‐supported hiring, focusing on three interrelated areas: data, design and decisions. It is suggested that in terms of data, organisational fit, categorisations and intersectionality require consideration in relation to exclusion. As various stakeholders collaborate to create AI, it is essential to explore which groups are dominant and how subjective assessments are encoded in technology. Although AI‐supported hiring should enhance recruitment decisions, evidence is lacking on how humans and machines interact in decision‐making, and how algorithms can be audited and regulated effectively for inclusion. This article recommends areas for interrogation through further research, and contributes to understanding how algorithmic inclusion can be achieved in AI‐supported hiring."
Mitigating Racial Bias in Health Care Algorithms: Improving Fairness in Access to Supportive Housing,2024,Krista R. Noam; Timothy Schmutte; Christopher Bory; Robert W. Plant,Psychiatric Services,1,W4400132939,10.1176/appi.ps.20230359,https://openalex.org/W4400132939,,Supportive housing; Psychology; Computer science; Medicine; Psychiatry,article,False,"Algorithms for guiding health care decisions have come under increasing scrutiny for being unfair to certain racial and ethnic groups. The authors describe their multistep process, using data from 3,465 individuals, to reduce racial and ethnic bias in an algorithm developed to identify state Medicaid beneficiaries experiencing homelessness and chronic health needs who were eligible for coordinated health care and housing supports. Through an iterative process of adjusting inputs, reviewing outputs with diverse stakeholders, and performing quality assurance, the authors developed an algorithm that achieved racial and ethnic parity in the selection of eligible Medicaid beneficiaries."
Adaptive Multi-Radar Anti-Bias Track Association Algorithm Based on Reference Topology Features,2025,Shaoming Wei; Xuan Zhou; Jun Wang; Pang Rui; Xueqing Li; Qiang Liu,Remote Sensing,1,W4410818401,10.3390/rs17111876,https://openalex.org/W4410818401,https://www.mdpi.com/2072-4292/17/11/1876/pdf?version=1748503309,Computer science; Track (disk drive); Association (psychology); Radar; Algorithm,article,True,"Accurate track association is essential for multi-radar fusion, since incorrect associations may result in significant errors in integrated information. To address the track association problem in multi-radar systems, particularly the challenges posed by offset bias, this paper proposes an adaptive multi-radar anti-bias track association algorithm based on reference topological features (RETs) that achieves accurate association despite offset bias and radar missed detections. The multi-radar adaptive RET algorithm employs the Optimal Sub-Pattern Assignment (OSPA) metric, which is corrected for offset bias, to measure the distance among RETs, thus generating an association cost matrix. The obtained distances among RETs follow a chi-squared distribution, thereby replacing the manually adjusted association threshold with an adaptive association threshold, enhancing robustness against offset bias and measurement noise. Subsequently, the multi-dimensional association cost matrix is filtered using threshold filtering to reduce erroneous associations caused by radar missed detections. Finally, the Lagrangian relaxation algorithm is applied to assign the association cost matrix and determine the final track association. The simulation results demonstrate that the multi-radar adaptive RET algorithm achieves accurate association results and exhibits considerable adaptability to radar offset bias and random noise errors."
Auditing Political Exposure Bias: Algorithmic Amplification on Twitter/X During the 2024 U.S. Presidential Election,2024,Jinyi Ye; Luca Luceri; Emilio Ferrara,arXiv (Cornell University),1,W4404390517,10.48550/arxiv.2411.01852,https://openalex.org/W4404390517,https://arxiv.org/pdf/2411.01852,Presidential election; Audit; Presidential system; Politics; Political science,preprint,True,"Approximately 50% of tweets in X's user timelines are personalized recommendations from accounts they do not follow. This raises a critical question: What political content are users exposed to beyond their established networks, and what implications does this have for democratic discourse online? In this paper, we present a six-week audit of X's algorithmic content recommendations during the 2024 U.S. Presidential Election by deploying 120 sock-puppet monitoring accounts to capture tweets from their personalized ""For You"" timelines. Our objective is to quantify out-of-network content exposure for right- and left-leaning user profiles and assess any potential inequalities and biases in political exposure. Our findings indicate that X's algorithm skews exposure toward a few high-popularity accounts across all users, with right-leaning users experiencing the highest level of exposure inequality. Both left- and right-leaning users encounter amplified exposure to accounts aligned with their own political views and reduced exposure to opposing viewpoints. Additionally, we observe that new accounts experience a right-leaning bias in exposure within their default timelines. Our work contributes to understanding how content recommendation systems may induce and reinforce biases while exacerbating vulnerabilities among politically polarized user groups. We underscore the importance of transparency-aware algorithms in addressing critical issues such as safeguarding election integrity and fostering a more informed digital public sphere."
Algorithmic Bias,2025,,The MIT Press eBooks,0,W4410988878,10.7551/mitpress/15834.003.0015,https://openalex.org/W4410988878,,Computer science,book-chapter,False,
Multitask Bias-Compensation DLMS Algorithm for Link Noise,2024,Yifan Wang; Pengwei Wen; Xinxin Xu; Jiaxin Chen; Shuyuan Zhang; Menghan Yang,,1,W4402744179,10.1109/icsip61881.2024.10671557,https://openalex.org/W4402744179,,Computer science; Link (geometry); Noise (video); Algorithm; Compensation (psychology),article,False,
"AI-powered revolution in plant sciences: advancements, applications, and challenges for sustainable agriculture and food security",2024,Deependra Kumar Gupta; Anselmo Pagani; Paolo Zamboni; Ajay Kumar Singh,Exploration of Foods and Foodomics,23,W4401385722,10.37349/eff.2024.00045,https://openalex.org/W4401385722,https://www.explorationpub.com/uploads/Article/A101045/101045.pdf,Food security; Sustainable agriculture; Agriculture; Interpretability; Robustness (evolution),article,True,"Artificial intelligence (AI) is revolutionizing plant sciences by enabling precise plant species identification, early disease diagnosis, crop yield prediction, and precision agriculture optimization. AI uses machine learning and image recognition to aid ecological research and biodiversity conservation. It plays a crucial role in plant breeding by accelerating the development of resilient, high-yielding crops with desirable traits. AI models using climate and soil data contribute to sustainable agriculture and food security. In plant phenotyping, AI automates the measurement and analysis of plant characteristics, enhancing our understanding of plant growth. Ongoing research aims to improve AI models’ robustness and interpretability while addressing data privacy and algorithmic biases. Interdisciplinary collaboration is essential to fully harness AI’s potential in plant sciences for a sustainable, food-secure future."
Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm,2024,Weiran Wang; Zelin Wu; Diamantino Caseiro; Tsendsuren Munkhdalai; Khe Chai Sim; Pat Rondon; Golan Pundak; G. Hugh Song; Rohit Prabhavalkar; Zhong Meng; Ding Zhao; Tara N. Sainath; Yanzhang He; Pedro Moreno Mengibar,,1,W4402112258,10.21437/interspeech.2024-1349,https://openalex.org/W4402112258,,Computer science; Matching (statistics); Algorithm; Artificial intelligence; Mathematics,article,False,
Measuring Bias in Job Recommender Systems: Auditing the Algorithms,2024,Shuo Zhang; Peter Kühn,,1,W4402121071,10.3386/w32889,https://openalex.org/W4402121071,,Recommender system; Audit; Computer science; Algorithm; Information retrieval,report,False,
Measuring Bias in Job Recommender Systems: Auditing the Algorithms,2024,Shuo Zhang; Peter Kuhn,SSRN Electronic Journal,1,W4401981012,10.2139/ssrn.4939157,https://openalex.org/W4401981012,http://doi.org/10.2139/ssrn.4939157,Recommender system; Audit; Computer science; Algorithm; Information retrieval,article,True,
AI in Education,2024,Ashok Singh Gaur; Hari Om Sharan; Rajeev Kumar,Advances in computational intelligence and robotics book series,20,W4392432031,10.4018/979-8-3693-2964-1.ch003,https://openalex.org/W4392432031,,Computer science; Geology,book-chapter,False,"As artificial intelligence (AI) continues to advance, its integration into the field of education presents both promising opportunities and ethical challenges. This chapter explores the multifaceted landscape of AI in education, examining the ethical considerations associated with its implementation. The opportunities encompass personalized learning experiences, adaptive assessment tools, and efficient administrative processes. However, ethical concerns arise regarding data privacy, algorithmic bias, accountability, and the potential exacerbation of educational inequalities. Artificial intelligence is a field of study that combines the applications of machine learning, algorithm production, and natural language processing. Applications of AI transform the tools of education. AI has a variety of educational applications, such as personalized learning platforms to promote students' learning, automated assessment systems to aid teachers, and facial recognition systems to generate insights about learners' behaviors."
Bias Detection and Correction Methods for Machine Learning Algorithms,2024,Genny Dimitrakopoulou; Nikolaos C. Kapsalis; George Kokkinis,,1,W4402288768,10.1109/eeite61750.2024.10654404,https://openalex.org/W4402288768,,Computer science; Artificial intelligence; Algorithm; Machine learning,article,False,
Auditing Political Exposure Bias: Algorithmic Amplification on Twitter/X During the 2024 U.S. Presidential Election,2025,Jinyi Ye; Luca Luceri; Emilio Ferrara,,2,W4411542040,10.1145/3715275.3732159,https://openalex.org/W4411542040,https://dl.acm.org/doi/pdf/10.1145/3715275.3732159,Presidential election; Audit; Politics; Presidential system; Political science,article,True,
Emotion-aware music tower blocks (EmoMTB ): an intelligent audiovisual interface for music discovery and recommendation,2023,Alessandro B. Melchiorre; David Penz; Christian Ganhör; Oleg Lesota; Vasco Fragoso; Florian Fritzl; Emilia Parada‐Cabaleiro; Franz Schubert; Markus Schedl,International Journal of Multimedia Information Retrieval,31,W4379230707,10.1007/s13735-023-00275-8,https://openalex.org/W4379230707,https://link.springer.com/content/pdf/10.1007/s13735-023-00275-8.pdf,Computer science; Recommender system; Active listening; Popularity; Set (abstract data type),article,True,"Abstract Music listening has experienced a sharp increase during the last decade thanks to music streaming and recommendation services. While they offer text-based search functionality and provide recommendation lists of remarkable utility, their typical mode of interaction is unidimensional, i.e., they provide lists of consecutive tracks, which are commonly inspected in sequential order by the user. The user experience with such systems is heavily affected by cognition biases (e.g., position bias, human tendency to pay more attention to first positions of ordered lists) as well as algorithmic biases (e.g., popularity bias, the tendency of recommender systems to overrepresent popular items). This may cause dissatisfaction among the users by disabling them to find novel music to enjoy. In light of such systems and biases, we propose an intelligent audiovisual music exploration system named EmoMTB . It allows the user to browse the entirety of a given collection in a free nonlinear fashion. The navigation is assisted by a set of personalized emotion-aware recommendations, which serve as starting points for the exploration experience. EmoMTB adopts the metaphor of a city, in which each track (visualized as a colored cube) represents one floor of a building. Highly similar tracks are located in the same building; moderately similar ones form neighborhoods that mostly correspond to genres. Tracks situated between distinct neighborhoods create a gradual transition between genres. Users can navigate this music city using their smartphones as control devices. They can explore districts of well-known music or decide to leave their comfort zone. In addition, EmoMTB integrates an emotion-aware music recommendation system that re-ranks the list of suggested starting points for exploration according to the user’s self-identified emotion or the collective emotion expressed in EmoMTB ’s Twitter channel. Evaluation of EmoMTB has been carried out in a threefold way: by quantifying the homogeneity of the clustering underlying the construction of the city, by measuring the accuracy of the emotion predictor, and by carrying out a web-based survey composed of open questions to obtain qualitative feedback from users."
"A Practical Introduction to Generative AI, Synthetic Media, and the Messages Found in the Latest Medium",2023,Jon M. Garon,SSRN Electronic Journal,34,W4328024808,10.2139/ssrn.4388437,https://openalex.org/W4328024808,https://doi.org/10.2139/ssrn.4388437,Generative grammar; Computer science; Data science; Artificial intelligence,article,True,
Ethical Implication of Artificial Intelligence (AI) Adoption in Financial Decision Making,2024,Omoshola S. Owolabi; Prince C. Uche; Nathaniel T. Adeniken; Christopher Ihejirika; Riyad Bin Islam; Bishal Jung Thapa Chhetri,Computer and Information Science,32,W4396498564,10.5539/cis.v17n1p49,https://openalex.org/W4396498564,https://ccsenet.org/journal/index.php/cis/article/download/0/0/50148/54269,Computer science; Artificial intelligence; Ethical decision; Engineering ethics; Engineering,article,True,"The integration of artificial intelligence (AI) into the financial sector has raised ethical concerns that need to be addressed. This paper analyzes the ethical implications of using AI in financial decision-making and emphasizes the importance of an ethical framework to ensure its fair and trustworthy deployment. The study explores various ethical considerations, including the need to address algorithmic bias, promote transparency and explainability in AI systems, and adhere to regulations that protect equity, accountability, and public trust. By synthesizing research and empirical evidence, the paper highlights the complex relationship between AI innovation and ethical integrity in finance. To tackle this issue, the paper proposes a comprehensive and actionable ethical framework that advocates for clear guidelines, governance structures, regular audits, and collaboration among stakeholders. This framework aims to maximize the potential of AI while minimizing negative impacts and unintended consequences. The study serves as a valuable resource for policymakers, industry professionals, researchers, and other stakeholders, facilitating informed discussions, evidence-based decision-making, and the development of best practices for responsible AI integration in the financial sector. The ultimate goal is to ensure fairness, transparency, and accountability while reaping the benefits of AI for both the financial sector and society."
Exploring the Potential of Chatbots in Critical Care Nephrology,2023,Supawadee Suppadungsuk; Charat Thongprayoon; Jing Miao; Pajaree Krisanapan; Fawad Qureshi; Kianoush Kashani; Wisit Cheungpasitporn,Medicines,34,W4387843512,10.3390/medicines10100058,https://openalex.org/W4387843512,https://www.mdpi.com/2305-6320/10/10/58/pdf?version=1697820232,Health care; Nephrology; Chatbot; Medicine; Multidisciplinary approach,review,True,"The exponential growth of artificial intelligence (AI) has allowed for its integration into multiple sectors, including, notably, healthcare. Chatbots have emerged as a pivotal resource for improving patient outcomes and assisting healthcare practitioners through various AI-based technologies. In critical care, kidney-related conditions play a significant role in determining patient outcomes. This article examines the potential for integrating chatbots into the workflows of critical care nephrology to optimize patient care. We detail their specific applications in critical care nephrology, such as managing acute kidney injury, alert systems, and continuous renal replacement therapy (CRRT); facilitating discussions around palliative care; and bolstering collaboration within a multidisciplinary team. Chatbots have the potential to augment real-time data availability, evaluate renal health, identify potential risk factors, build predictive models, and monitor patient progress. Moreover, they provide a platform for enhancing communication and education for both patients and healthcare providers, paving the way for enriched knowledge and honed professional skills. However, it is vital to recognize the inherent challenges and limitations when using chatbots in this domain. Here, we provide an in-depth exploration of the concerns tied to chatbots’ accuracy, dependability, data protection and security, transparency, potential algorithmic biases, and ethical implications in critical care nephrology. While human discernment and intervention are indispensable, especially in complex medical scenarios or intricate situations, the sustained advancements in AI signal that the integration of precision-engineered chatbot algorithms within critical care nephrology has considerable potential to elevate patient care and pivotal outcome metrics in the future."
Legal accountability and ethical considerations of AI in financial services,2024,Ngozi Samuel Uzougbo; Chinonso Gladys Ikegwu; Adefolake Olachi Adewusi,GSC Advanced Research and Reviews,50,W4396845448,10.30574/gscarr.2024.19.2.0171,https://openalex.org/W4396845448,https://gsconlinepress.com/journals/gscarr/sites/default/files/GSCARR-2024-0171.pdf,Accountability; Business; Financial services; Accounting; Engineering ethics,article,True,"Artificial Intelligence (AI) is revolutionizing the financial services industry, offering unparalleled opportunities for efficiency, innovation, and personalized services. However, along with its benefits, AI in financial services raises significant legal and ethical concerns. This paper explores the legal accountability and ethical considerations surrounding the use of AI in financial services, aiming to provide insights into how these challenges can be addressed. The legal accountability of AI in financial services revolves around the allocation of responsibility for AI-related decisions and actions. As AI systems become more autonomous, questions arise about who should be held liable for AI errors, misconduct, or regulatory violations. This paper examines the existing legal frameworks, such as data protection laws, consumer protection regulations, and liability laws, and assesses their adequacy in addressing AI-related issues. Ethical considerations in AI implementation in financial services are paramount, as AI systems can impact individuals' financial well-being and access to services. Issues such as algorithmic bias, transparency, and fairness are critical in ensuring ethical AI practices. This paper discusses the importance of ethical guidelines and frameworks for AI development and deployment in financial services, emphasizing the need for transparency, accountability, and fairness. The paper also examines the role of regulatory bodies and industry standards in addressing legal and ethical challenges associated with AI in financial services. It proposes recommendations for policymakers, regulators, and industry stakeholders to promote responsible AI practices, including the development of clear guidelines, enhanced transparency measures, and mechanisms for accountability. Overall, this paper highlights the complex interplay between AI, legal accountability, and ethical considerations in the financial services industry. By addressing these challenges, stakeholders can harness the full potential of AI while ensuring that it is deployed in a responsible and ethical manner, benefiting both businesses and consumers."
Expanding on the Frames: Making a Case for Algorithmic Literacy,2023,Susan Gardner Archambault,Communications in Information Literacy,11,W4389729208,10.15760/comminfolit.2023.17.2.11,https://openalex.org/W4389729208,https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1656&context=comminfolit,Information literacy; Computer science; Frame (networking); Dominance (genetics); Literacy,article,True,"Traditional information literacy skills (e.g., effectively finding and evaluating information) need to be updated due to the rapidly changing information ecosystem and the growing dominance of online platforms that use algorithms to control and shape information. This article proposes additions to the current ACRL Framework for Information Literacy for Higher Education that relate to algorithmic literacy. The “Authority Is Constructed and Contextual” frame can be applied to recognizing the need to question algorithmic authority (including algorithmic bias), the ""Information Has Value” frame can be used to acknowledge online platforms’ use of proprietary algorithms allowing third parties to access personal data, and the “Searching as Strategic Exploration” frame can draw attention to search results in online platforms are mediated through algorithms. Classroom activities to teach the new knowledge practices and dispositions are also included."
Algorithmic Bias and the New Chicago School,2025,J. Hannah Lee,arXiv (Cornell University),0,W4407122907,10.48550/arxiv.2502.00014,https://openalex.org/W4407122907,https://arxiv.org/pdf/2502.00014,Computer science; Mathematics education; Psychology,preprint,True,"AI systems are increasingly deployed in both public and private sectors to independently make complicated decisions with far-reaching impact on individuals and the society. However, many AI algorithms are biased in the collection or processing of data, resulting in prejudiced decisions based on demographic features. Algorithmic biases occur because of the training data fed into the AI system or the design of algorithmic models. While most legal scholars propose a direct-regulation approach associated with the right of explanation or transparency obligation, this article provides a different picture regarding how indirect regulation can be used to regulate algorithmic bias based on the New Chicago School framework developed by Lawrence Lessig. This article concludes that an effective regulatory approach toward algorithmic bias will be the right mixture of direct and indirect regulations through architecture, norms, market, and the law."
Algorithmic Biases in Computational Antitrust,2023,Vydhrithi Reddy Peesari,SSRN Electronic Journal,0,W4382201232,10.2139/ssrn.4468341,https://openalex.org/W4382201232,https://doi.org/10.2139/ssrn.4468341,Economics; Computer science; Mathematical economics; Econometrics; Law and economics,article,True,
A Guide to Cross-Validation for Artificial Intelligence in Medical Imaging,2023,Tyler Bradshaw; Zachary Huemann; Junjie Hu; Arman Rahmim,Radiology Artificial Intelligence,120,W4377989430,10.1148/ryai.220232,https://openalex.org/W4377989430,https://pmc.ncbi.nlm.nih.gov/articles/PMC10388213/pdf/ryai.220232.pdf,Hyperparameter; Convolutional neural network; Artificial intelligence; Machine learning; Field (mathematics),review,True,"Artificial intelligence (AI) is being increasingly used to automate and improve technologies within the field of medical imaging. A critical step in the development of an AI algorithm is estimating its prediction error through cross-validation (CV). The use of CV can help prevent overoptimism in AI algorithms and can mitigate certain biases associated with hyperparameter tuning and algorithm selection. This article introduces the principles of CV and provides a practical guide on the use of CV for AI algorithm development in medical imaging. Different CV techniques are described, as well as their advantages and disadvantages under different scenarios. Common pitfalls in prediction error estimation and guidance on how to avoid them are also discussed. <b>Keywords:</b> Education, Research Design, Technical Aspects, Statistics, Supervised Learning, Convolutional Neural Network (CNN) <i>Supplemental material is available for this article</i>. © RSNA, 2023."
Artificial intelligence in positive mental health: a narrative review,2024,Anoushka Thakkar; Ankita Gupta; Avinash De Sousa,Frontiers in Digital Health,126,W4392922283,10.3389/fdgth.2024.1280235,https://openalex.org/W4392922283,https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2024.1280235/pdf,Mental health; Psychology; Schizophrenia (object-oriented programming); Intervention (counseling); Autism,review,True,"The paper reviews the entire spectrum of Artificial Intelligence (AI) in mental health and its positive role in mental health. AI has a huge number of promises to offer mental health care and this paper looks at multiple facets of the same. The paper first defines AI and its scope in the area of mental health. It then looks at various facets of AI like machine learning, supervised machine learning and unsupervised machine learning and other facets of AI. The role of AI in various psychiatric disorders like neurodegenerative disorders, intellectual disability and seizures are discussed along with the role of AI in awareness, diagnosis and intervention in mental health disorders. The role of AI in positive emotional regulation and its impact in schizophrenia, autism spectrum disorders and mood disorders is also highlighted. The article also discusses the limitations of AI based approaches and the need for AI based approaches in mental health to be culturally aware, with structured flexible algorithms and an awareness of biases that can arise in AI. The ethical issues that may arise with the use of AI in mental health are also visited."
The Impact of Artificial Intelligence on Social Media Content,2024,Elsir Ali Saad Mohamed; Murtada Elbashir Osman; Badur Algasim Mohamed,Journal of Social Sciences,19,W4395680208,10.3844/jssp.2024.12.16,https://openalex.org/W4395680208,https://thescipub.com/pdf/jssp.2024.12.16.pdf,Content (measure theory); Social media; Psychology; Media content; Social psychology,article,True,"&lt;p&gt;Artificial Intelligence (AI) has become a powerful tool for creating and managing social media content. Social media platforms have integrated AI technology into their algorithms to optimize the user experience. However, the impact of AI on social media is a topic of debate. This research aims to explore the effects of AI and its implications for content creators and consumers. It is recommended that social media platforms ensure that the use of AI is transparent and ethical to maintain user trust. The impact of AI on social media content is significant and multifaceted, enabling personalized content recommendations, automated content generation, and real-time content analysis. However, there are also concerns about algorithmic bias and the potential for job displacement. As AI technology continues to evolve, it is essential to ensure that ethical considerations and social responsibility are prioritized in the development and use of AI in social media marketing. The impact of AI on social media content is a complex issue, with both positive and negative effects. While AI algorithms can enhance the user experience by providing personalized content, there are concerns that they may also lead to the spread of misinformation and the creation of filter bubbles. To mitigate these potential negative effects, it is important to promote transparency, media literacy, and human moderation, in order to ensure that social media content is accurate, diverse, and informative.&lt;/p&gt;"
Personalized cancer vaccine design using AI-powered technologies,2024,Anant Kumar; Shriniket Dixit; Kathiravan Srinivasan; M. Dinakaran; P. M. Durai Raj Vincent,Frontiers in Immunology,35,W4404196756,10.3389/fimmu.2024.1357217,https://openalex.org/W4404196756,https://doi.org/10.3389/fimmu.2024.1357217,Cancer; Cancer vaccine; Cancer immunotherapy; Medicine; Precision medicine,review,True,"Immunotherapy has ushered in a new era of cancer treatment, yet cancer remains a leading cause of global mortality. Among various therapeutic strategies, cancer vaccines have shown promise by activating the immune system to specifically target cancer cells. While current cancer vaccines are primarily prophylactic, advancements in targeting tumor-associated antigens (TAAs) and neoantigens have paved the way for therapeutic vaccines. The integration of artificial intelligence (AI) into cancer vaccine development is revolutionizing the field by enhancing various aspect of design and delivery. This review explores how AI facilitates precise epitope design, optimizes mRNA and DNA vaccine instructions, and enables personalized vaccine strategies by predicting patient responses. By utilizing AI technologies, researchers can navigate complex biological datasets and uncover novel therapeutic targets, thereby improving the precision and efficacy of cancer vaccines. Despite the promise of AI-powered cancer vaccines, significant challenges remain, such as tumor heterogeneity and genetic variability, which can limit the effectiveness of neoantigen prediction. Moreover, ethical and regulatory concerns surrounding data privacy and algorithmic bias must be addressed to ensure responsible AI deployment. The future of cancer vaccine development lies in the seamless integration of AI to create personalized immunotherapies that offer targeted and effective cancer treatments. This review underscores the importance of interdisciplinary collaboration and innovation in overcoming these challenges and advancing cancer vaccine development."
Challenging the appearance of machine intelligence: Cognitive bias in LLMs and Best Practices for Adoption,2023,Alaina N. Talboy; Elizabeth Fuller,arXiv (Cornell University),13,W4362655418,10.48550/arxiv.2304.01358,https://openalex.org/W4362655418,https://arxiv.org/pdf/2304.01358,Cognition; Set (abstract data type); Cognitive bias; Ethnic group; Psychology,preprint,True,"Assessments of algorithmic bias in large language models (LLMs) are generally catered to uncovering systemic discrimination based on protected characteristics such as sex and ethnicity. However, there are over 180 documented cognitive biases that pervade human reasoning and decision making that are routinely ignored when discussing the ethical complexities of AI. We demonstrate the presence of these cognitive biases in LLMs and discuss the implications of using biased reasoning under the guise of expertise. We call for stronger education, risk management, and continued research as widespread adoption of this technology increases. Finally, we close with a set of best practices for when and how to employ this technology as widespread adoption continues to grow."
"<scp>ChatGPT</scp>and consumers: Benefits, Pitfalls and Future Research Agenda",2023,Justin Paul; Akiko Ueno; Charles Dennis,International Journal of Consumer Studies,271,W4360945746,10.1111/ijcs.12928,https://openalex.org/W4360945746,,Novelty; Misinformation; Personalization; Marketing; Context (archaeology),article,False,"Abstract The need of the hour is to encourage research on topics with newness and novelty. In this context, this article discusses multidimensional benefits and potential pitfalls of using artificial intelligence‐based Chat Generative Pre‐trained Transformer (ChatGPT), and provides numerous ideas for future research in consumer studies and marketing in the context of ChatGPT. ChatGPT provides algorithm‐generated conversational responses to text‐based prompts. Since its launch in the late 2022, ChatGPT has generated significant debate surrounding its hallmarks, benefits and potential pitfalls. On the one hand, ChatGPT can offer enhanced consumer engagement, improved customer service, personalization and shopping, social interaction and communication practice, cost‐effectiveness, insights into consumer behaviour and improved marketing campaigns. On the other hand, potential pitfalls include concerns about consumer well‐being, bias, misinformation, lack of context, privacy concerns, ethical considerations and security. The article concludes by outlining a potential future research agenda in the area of ChatGPT and consumer studies. Overall, this article provides valuable insights into the benefits and challenges associated with ChatGPT, shedding light on its potential applications and the need for further research."
A Biased Random-Key Genetic Algorithm with Variable Mutants to Solve a Vehicle Routing Problem,2024,Paola Festa; Francesca Guerriero; Maurício G. C. Resende; Edoardo Scalzo,SSRN Electronic Journal,1,W4396884073,10.2139/ssrn.4828115,https://openalex.org/W4396884073,https://doi.org/10.2139/ssrn.4828115,Key (lock); Variable (mathematics); Computer science; Genetic algorithm; Vehicle routing problem,preprint,True,
Explainable spatially explicit geospatial artificial intelligence in urban analytics,2023,Pengyuan Liu; Yan Zhang; Filip Biljecki,Environment and Planning B Urban Analytics and City Science,39,W4387187512,10.1177/23998083231204689,https://openalex.org/W4387187512,,Geospatial analysis; Interpretability; Computer science; Analytics; Graph,article,False,"Geospatial artificial intelligence (GeoAI) is proliferating in urban analytics, where graph neural networks (GNNs) have become one of the most popular methods in recent years. However, along with the success of GNNs, the black box nature of AI models has led to various concerns (e.g. algorithmic bias and model misuse) regarding their adoption in urban analytics, particularly when studying socio-economics where high transparency is a crucial component of social justice. Therefore, the desire for increased model explainability and interpretability has attracted increasing research interest. This article proposes an explainable spatially explicit GeoAI-based analytical method that combines a graph convolutional network (GCN) and a graph-based explainable AI (XAI) method, called GNNExplainer. Here, we showcase the ability of our proposed method in two studies within urban analytics: traffic volume prediction and population estimation in the tasks of a node classification and a graph classification, respectively. For these tasks, we used Street View Imagery (SVI), a trending data source in urban analytics. We extracted semantic information from the images and assigned them as features of urban roads. The GCN first provided reasonable predictions related to these tasks by encoding roads as nodes and their connectivities and networks as graphs. The GNNExplainer then offered insights into how certain predictions are made. Through such a process, practical insights and conclusions can be derived from the urban phenomena studied here. In this paper we also set out a path for developing XAI in future urban studies."
Challenging Cognitive Load Theory: The Role of Educational Neuroscience and Artificial Intelligence in Redefining Learning Efficacy,2025,Evgenia Gkintoni; Hera Antonopoulou; Andrew Sortwell; Constantinos Halkiopoulos,Brain Sciences,70,W4407657137,10.3390/brainsci15020203,https://openalex.org/W4407657137,https://doi.org/10.3390/brainsci15020203,Computer science; Artificial intelligence; Scalability; Deep learning; Machine learning,review,True,"Background/Objectives: This systematic review integrates Cognitive Load Theory (CLT), Educational Neuroscience (EdNeuro), Artificial Intelligence (AI), and Machine Learning (ML) to examine their combined impact on optimizing learning environments. It explores how AI-driven adaptive learning systems, informed by neurophysiological insights, enhance personalized education for K-12 students and adult learners. This study emphasizes the role of Electroencephalography (EEG), Functional Near-Infrared Spectroscopy (fNIRS), and other neurophysiological tools in assessing cognitive states and guiding AI-powered interventions to refine instructional strategies dynamically. Methods: This study reviews n = 103 papers related to the integration of principles of CLT with AI and ML in educational settings. It evaluates the progress made in neuroadaptive learning technologies, especially the real-time management of cognitive load, personalized feedback systems, and the multimodal applications of AI. Besides that, this research examines key hurdles such as data privacy, ethical concerns, algorithmic bias, and scalability issues while pinpointing best practices for robust and effective implementation. Results: The results show that AI and ML significantly improve Learning Efficacy due to managing cognitive load automatically, providing personalized instruction, and adapting learning pathways dynamically based on real-time neurophysiological data. Deep Learning models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Support Vector Machines (SVMs) improve classification accuracy, making AI-powered adaptive learning systems more efficient and scalable. Multimodal approaches enhance system robustness by mitigating signal variability and noise-related limitations by combining EEG with fMRI, Electrocardiography (ECG), and Galvanic Skin Response (GSR). Despite these advances, practical implementation challenges remain, including ethical considerations, data security risks, and accessibility disparities across learner demographics. Conclusions: AI and ML are epitomes of redefinition potentials that solid ethical frameworks, inclusive design, and scalable methodologies must inform. Future studies will be necessary for refining pre-processing techniques, expanding the variety of datasets, and advancing multimodal neuroadaptive learning for developing high-accuracy, affordable, and ethically responsible AI-driven educational systems. The future of AI-enhanced education should be inclusive, equitable, and effective across various learning populations that would surmount technological limitations and ethical dilemmas."
A Survey on the Optimization of Artificial Neural Networks Using Swarm Intelligence Algorithms,2023,Bibi Aamirah Shafaa Emambocus; Muhammed Basheer Jasser; Angela Amphawan,IEEE Access,66,W4313477820,10.1109/access.2022.3233596,https://openalex.org/W4313477820,https://ieeexplore.ieee.org/ielx7/6287639/10005208/10004960.pdf,Computer science; Artificial neural network; Swarm intelligence; Artificial intelligence; Algorithm,article,True,"Artificial Neural Networks (ANNs) are becoming increasingly useful in numerous areas as they have a myriad of applications. Prior to using ANNs, the network structure needs to be determined and the ANN needs to be trained. The network structure is usually chosen based on trial and error. The training, which consists of finding the optimal connection weights and biases of the ANN, is usually done using gradient-descent algorithms. It has been found that swarm intelligence algorithms are favorable for both determining the network structure and for the training of ANNs. This is because they are able to determine the network structure in an intelligent way, and they are better at finding the most optimal connection weights and biases during the training as opposed to conventional algorithms. Recently, a number of swarm intelligence algorithms have been employed for optimizing different types of neural networks. However, there is no comprehensive survey on the swarm intelligence algorithms used for optimizing ANNs. In this paper, we present a review of the different types of ANNs optimized using swarm intelligence algorithms, the way the ANNs are optimized, the different swarm intelligence algorithms used, and the applications of the ANNs optimized by swarm intelligence algorithms."
Algorithm-mediated social learning in online social networks,2023,William J. Brady; Joshua Conrad Jackson; Björn Lindström; Molly J. Crockett,Trends in Cognitive Sciences,76,W4385546082,10.1016/j.tics.2023.06.008,https://openalex.org/W4385546082,,Social learning; Exploit; Social media; Computer science; Misinformation,review,False,
"Application of deep learning and machine learning models to improve healthcare in sub-Saharan Africa: Emerging opportunities, trends and implications",2023,Elliot Mbunge; John Batani,Telematics and Informatics Reports,48,W4386374624,10.1016/j.teler.2023.100097,https://openalex.org/W4386374624,https://doi.org/10.1016/j.teler.2023.100097,Health care; Transparency (behavior); Artificial intelligence; Deep learning; Psychological intervention,article,True,"Deep learning and machine learning techniques present unmatched opportunities to improve healthcare in sub-Saharan Africa (SSA). However, there is a paucity of literature on AI-based applications deployed to improve care in SSA, which makes it challenging to organise the research contributions in the present and to highlight obstacles and emerging research areas that need to be explored in the future. This study applied the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analysis) model to conduct a comprehensive review of deep learning and machine learning models deployed in SSA to improve access to care while exploring emerging opportunities, trends and implications for integrating AI-based models in SSA healthcare. This study reveals that AI models can analyse and derive inferences from massive health data for early detection, diagnosis, monitoring for chronic disorders, prediction of diseases, monitoring large-scale public health patterns and help limit exposure in contagious environments. AI can facilitate the development of targeted health interventions and improve patient outcomes in all stages of diagnosis, treatment, drug development and monitoring, personalised medicine, patient control and care. Integrating AI models with health applications can tremendously assist health professionals and policymakers in disease diagnosis and making informed decisions. AI algorithms bias, poor access to health data and formats, and lack of policies and frameworks supporting the integration of data-driven AI-based solutions into health systems hinder the integration of AI-based models into health systems. There is a need for transparency and ethical use of AI and crafting policies that support the use of AI in SSA health systems. Utilising AI-based models in healthcare can also assist researchers and healthcare workers to move towards smart care and better comprehend future research needs of AI in smart care."
A Novel Bias-Compensated Linear Constrained Least Mean Squares Algorithm Over Distributed Network,2023,Liru Wang; Lijuan Jia; Deshui Miao; Yinan Guo; Shunshoku Kanae,,1,W4386821599,10.23919/ccc58697.2023.10240407,https://openalex.org/W4386821599,,Robustness (evolution); Computer science; Algorithm; Variance (accounting); Interference (communication),article,False,"In this paper, we propose a Diffusion Bias-Compensated Constrained Least Mean Squares (D-BC-CLMS) algorithm based on the idea of distributed estimation for adaptive filtering in network containing input noises. To reduce the interference of input noises, we use a new cost function. The variance of the input noises is derived by a novel method that uses some reasonable assumptions without any prior knowledge. Then we combine the diffusion strategy with BC-CLMS to improve the performance of single agent and to obtain more robustness. Eventually, simulation results confirm the theory is correct and demonstrate the excellent performance of the novel algorithm by comparing it with other conventional algorithms."
Direct integration bias-compensated maximum correntropy criterion algorithm independent of measurement noise samples,2024,Qin Song; Jingen Ni,Information Sciences,1,W4396979063,10.1016/j.ins.2024.120740,https://openalex.org/W4396979063,,Algorithm; Noise (video); Computer science; Mathematics; Pattern recognition (psychology),article,False,
Convergence of artificial intelligence with social media: A bibliometric &amp; qualitative analysis,2024,Tahereh Saheb; Mouwafac Sidaoui; Bill Schmarzo,Telematics and Informatics Reports,36,W4396829571,10.1016/j.teler.2024.100146,https://openalex.org/W4396829571,https://doi.org/10.1016/j.teler.2024.100146,Social media; Computer science; Sentiment analysis; Data science; Thematic analysis,article,True,"The integration of artificial intelligence (AI) and social media has provided numerous benefits to businesses, including improved audience analysis and content optimization. However, AI has facilitated the spread of misinformation, emphasizing the importance of taking a balanced approach that considers both the technology's positive applications and its ethical risks. This paper looks at the intersection of AI and social media. The researchers use a mixed-method approach to analyze 1540 scholarly documents, combining bibliometric and systematic literature review techniques. The goal of this research is to identify the most important topics and trends, as well as potential business values and implications, in the AI Social Media domain. The first stage of the research involved a quantitative keyword co-occurrence analysis, which resulted in the identification of ten dominant themes. These include Conversational Agents &amp; User Experience, Human Emotion and Content Recommendation &amp; Moderation, Collective Intelligence in Emergency Management, Algorithmic Activism on social media, Deep Fakes and Fake News, Generative Artificial Intelligence, Algorithmic Bias in Content Moderation Systems, Deep Sentiment Analysis, Metaverse Technologies, and NLP &amp; Mental Health Detection. Each identified theme is then subjected to a qualitative thematic literature review, which provides a more in-depth, context-specific understanding of the associated findings. Because of this comprehensive approach, the study provides a broad overview of the current state of AI social media, shedding light on the potential applications and far-reaching implications of this interdisciplinary nexus. The study's findings have the potential to shape strategic decision-making, policy development, and future research directions in this rapidly changing field."
Moving Beyond the Stigma: Understanding and Overcoming the Resistance to the Acceptance and Adoption of Artificial Intelligence Chatbots,2023,Ismail Dergaa; Feten Fekih‐Romdhane; Jordan M. Glenn; Mohamed Saifeddin Fessi; Karim Chamari; Wissem Dhahbi; Makram Zghibi; Nicola Luigi Bragazzi; Mohamed Ben Aissa; Noomen Guelmami; Abdelfattah El Omri; Sarya Swed; Katja Weiss; Beat Knechtle; Helmi Ben Saad,New Asian Journal of Medicine,25,W4390901779,10.61838/kman.najm.1.2.4,https://openalex.org/W4390901779,https://nasianjmed.com/index.php/najm/article/download/12/21,Stigma (botany); Resistance (ecology); Psychology; Social psychology; Psychiatry,article,True,"Artificial intelligence chatbots may fundamentally transform academic research, automate mundane tasks, and enhance productivity. However, the integration of artificial intelligence chatbots (AIc) is impeded by a complex stigma deeply rooted in individuals’ misconceptions and apprehension, including concerns about academic integrity, job displacement, data privacy, and algorithmic bias. The aim of this study was to scrutinize the origins and impacts of the stigma associated with artificial intelligence chatbots within the realm of academic research and to propose strategies to mitigate such stigmas. This study draws parallels between the reception of artificial intelligence chatbots and previous transformative technologies, presenting case studies illustrating the spectrum of responses to the integration of artificial intelligence chatbots into academic research. This study identifies the need for a shift in mindset from perceiving artificial intelligence chatbots as threats to recognizing them as facilitators of efficiency and innovation. It also underscores the importance of understanding these models as tools that aid researchers but do not replace the need for human expertise and judgment. We further highlighted the role of education, transparency, regulation, and ethical guidelines in overcoming the stigma associated with artificial intelligence chatbots. Given how adaptable people are, the surrounding stigma will likely fade with time. We support a cooperative strategy with continuing education and discussion to maximize the benefits of artificial intelligence chatbots while minimizing their drawbacks, hopefully paving the way for their ethical and successful application in scholarly research."
The Promises and Pitfalls of Using Chat GPT for Self-Determined Learning in Higher Education: An Argumentative Review,2023,FX. Risang Baskara,Prosiding Seminar Nasional Fakultas Tarbiyah dan Ilmu Keguruan IAIM Sinjai,24,W4385781160,10.47435/sentikjar.v2i0.1825,https://openalex.org/W4385781160,https://journal.uiad.ac.id/index.php/SENTIKJAR/article/download/1825/858,Psychological intervention; Compromise; Psychology; Viewpoints; Self-regulated learning,article,True,"The potential of artificial intelligence language models, such as Chat GPT, to support self-determined learning in higher education has garnered increasing attention from educators, researchers, and policymakers. However, the promises and pitfalls of using Chat GPT for self-determined learning remain subject to debate and warrant further exploration. In this argumentative review, we examine the central questions and statements of the problem related to the use of Chat GPT for self-determined learning in higher education. We synthesise and critically evaluate the existing literature on the potential of Chat GPT to support self-directed and self-determined learning and highlight the main challenges and concerns associated with its use. According to our analysis, Chat GPT promises to improve self-determined learning by offering individualised feedback, resources, and assistance to learners that can foster their acquisition of knowledge and skills. However, using Chat GPT in self-determined learning also raises ethical and pragmatic concerns. These include issues about privacy, data security, and algorithmic bias, which could compromise the effectiveness and reliability of Chat GPT-based interventions. We posit that while the potential benefits of Chat GPT for self-determined learning are significant, they must be weighed against its potential drawbacks. As such, the design, implementation, and assessment of Chat GPT-based higher education interventions must be carefully considered. Our results indicate that the advancement of Chat GPT-based interventions for self-determined learning in higher education necessitates a nuanced and multidisciplinary approach that considers the viewpoints of educators, researchers, learners, and other interested stakeholders."
Real Estate Insights: Is the AI revolution a real estate boon or bane?,2023,Philip Seagraves,Journal of Property Investment and Finance,22,W4383105045,10.1108/jpif-05-2023-0045,https://openalex.org/W4383105045,,Real estate; Valuation (finance); Automation; Computer science; Artificial intelligence,article,False,"Purpose The paper aims to provide a comprehensive analysis of artificial intelligence’s (AI) transformative impact on the real estate industry. By examining various AI applications, from property recommendations to compliance automation, this study highlights potential benefits such as increased accuracy and efficiency. At the same time, this study critically discusses potential drawbacks, like privacy concerns and job displacement. The paper's goal is to offer valuable insights to industry professionals and policy makers, aiding strategic decision-making as AI continues to reshape the landscape of the real estate sector. Design/methodology/approach This paper employs an extensive literature review, combined with a qualitative analysis of case studies. Various AI applications in the real estate industry are examined, including machine learning for property recommendations and valuation, VR/AR property tours, AI automation for contract and regulatory compliance, and chatbots for customer service. The study also delves into the optimisation potential of AI in building management, lead generation, and risk assessment, whilst critically discussing potential challenges such as data privacy, algorithmic bias, and job displacement. The outcomes aim to inform strategic decisions for industry professionals and policy makers. Findings The study finds that AI has significant potential to revolutionise the real estate industry through enhanced accuracy in property valuation, efficient automation and immersive AR/VR experiences. AI-driven chatbots and optimisation in building management also hold promise. However, this study also uncovers potential challenges, including data privacy issues, algorithmic biases, and possible job displacement due to increased automation. The insights gleaned from this study underscore the importance of strategic decision-making in harnessing the benefits of AI while mitigating potential drawbacks in the real estate sector. Practical implications The paper's practical implications extend to industry professionals, policy makers, and technology developers. Professionals gain insights into how AI can enhance efficiency and accuracy in the real estate sector, guiding strategic decision-making. For policy makers, understanding potential challenges like data privacy and job displacement informs regulatory measures. Technology developers can also benefit from understanding the sector-specific applications and concerns raised. Additionally, highlighting the need for addressing algorithmic bias and privacy concerns in AI systems may foster better design practices. Therefore, the paper's findings could significantly shape the future trajectory of AI integration in real estate. Originality/value The paper provides original value by offering a comprehensive analysis of the transformative impact of AI in the real estate industry. Its multi-faceted examination of AI applications, coupled with a critical discussion on potential challenges, provides a balanced perspective. The paper's focus on informing strategic decisions for professionals and policy makers makes it a valuable resource. Moreover, by considering both benefits and drawbacks, this study contributes to the discourse on AI's broader societal implications. In the context of rapid technological change, such comprehensive studies are rare, adding to the paper's originality."
Automatic detection and classification of lung cancer CT scans based on deep learning and ebola optimization search algorithm,2023,Tehnan I. A. Mohamed; Olaide N. Oyelade; Absalom E. Ezugwu,PLoS ONE,92,W4385932710,10.1371/journal.pone.0285796,https://openalex.org/W4385932710,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0285796&type=printable,Lung cancer; Algorithm; Computer science; Cancer; Artificial intelligence,article,True,"Recently, research has shown an increased spread of non-communicable diseases such as cancer. Lung cancer diagnosis and detection has become one of the biggest obstacles in recent years. Early lung cancer diagnosis and detection would reliably promote safety and the survival of many lives globally. The precise classification of lung cancer using medical images will help physicians select suitable therapy to reduce cancer mortality. Much work has been carried out in lung cancer detection using CNN. However, lung cancer prediction still becomes difficult due to the multifaceted designs in the CT scan. Moreover, CNN models have challenges that affect their performance, including choosing the optimal architecture, selecting suitable model parameters, and picking the best values for weights and biases. To address the problem of selecting optimal weight and bias combination required for classification of lung cancer in CT images, this study proposes a hybrid metaheuristic and CNN algorithm. We first designed a CNN architecture and then computed the solution vector of the model. The resulting solution vector was passed to the Ebola optimization search algorithm (EOSA) to select the best combination of weights and bias to train the CNN model to handle the classification problem. After thoroughly training the EOSA-CNN hybrid model, we obtained the optimal configuration, which yielded good performance. Experimentation with the publicly accessible Iraq-Oncology Teaching Hospital / National Center for Cancer Diseases (IQ-OTH/NCCD) lung cancer dataset showed that the EOSA metaheuristic algorithm yielded a classification accuracy of 0.9321. Similarly, the performance comparisons of EOSA-CNN with other methods, namely, GA-CNN, LCBO-CNN, MVO-CNN, SBO-CNN, WOA-CNN, and the classical CNN, were also computed and presented. The result showed that EOSA-CNN achieved a specificity of 0.7941, 0.97951, 0.9328, and sensitivity of 0.9038, 0.13333, and 0.9071 for normal, benign, and malignant cases, respectively. This confirms that the hybrid algorithm provides a good solution for the classification of lung cancer."
A linear least squares algorithm for PolInSAR forest height inversion that considers the influence of introduced biases,2025,Dongfang Lin; Yibin Yao; Yongsheng Liu; Haiqiang Fu; Shaoning Li,Remote Sensing Letters,2,W4406122224,10.1080/2150704x.2024.2442112,https://openalex.org/W4406122224,,Inversion (geology); Algorithm; Computer science; Singular value decomposition; Remote sensing,article,False,
A BIASED RANDOM-KEY GENETIC ALGORITHM FOR THE 2D STRIP PACKING PROBLEM WITH ORDER AND STABILITY CONSTRAINTS,2024,Sonam Mandal; Thiago Alves de Queiroz; Flávio K. Miyazawa,Pesquisa Operacional,2,W4403838282,10.1590/0101-7438.2023.043.00284365,https://openalex.org/W4403838282,https://doi.org/10.1590/0101-7438.2023.043.00284365,Key (lock); Stability (learning theory); Packing problems; Mathematical optimization; Genetic algorithm,article,True,
An Improved Doppler-Aided Smoothing Code Algorithm for BeiDou-2/BeiDou-3 un-Geostationary Earth Orbit Satellites in Consideration of Satellite Code Bias,2023,Xiao Gao; Zongfang Ma; Luxiao Jia; Lin Pan,Remote Sensing,2,W4384558097,10.3390/rs15143549,https://openalex.org/W4384558097,https://www.mdpi.com/2072-4292/15/14/3549/pdf?version=1689557189,Algorithm; Pseudorange; Computer science; Smoothing; Doppler effect,article,True,"The extensive use of carrier-aided smoothing code (CSC) filters has led to a reduction in the noise level of raw code measurements in GNSS positioning and navigation applications. However, the existing CSC technique is sensitive to the changes in the integer ambiguity, and then the smoothing procedure needs to be restarted in the presence of cycle slips. As the Doppler shift is instantaneously observed and immune to cycle slips, the Doppler-aided smoothing code (DSC) algorithm would be more promising in a challenged environment. Based on the Hatch filter, an optimal DSC approach is proposed with the principle of minimum variance. Meanwhile, to inhibit the effect of the integral cumulative error of the Doppler, a balance factor is adopted to adjust the contributions of raw code and DSC. The noise level of code observable is not only affected by thermal noise, but also limited by systematic bias. Satellite code bias (SCB) was identified in the raw code observable on each frequency for each BDS-2 satellite. By minimizing the sum of the absolute value of residuals, the polynomial segment fitting algorithm as a function of elevation angles is applied to establish the SCB correction model based on epoch-differenced multipath (MP) deviations. Finally, different types of experiments demonstrate the validity and efficiency of the refined DSC filter with SCB corrections on each available frequency for BDS un-GEO satellites."
