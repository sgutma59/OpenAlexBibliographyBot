title,year,authors,venue,citation_count,openalex_id,doi,url,open_access_pdf,concepts,type,is_oa,abstract
Guiding Principles to Address the Impact of Algorithm Bias on Racial and Ethnic Disparities in Health and Health Care,2023,Marshall H. Chin; Nasim Afsarmanesh; Arlene S. Bierman; Christine Chang; Caleb J. Colón-Rodríguez; Prashila Dullabh; Deborah G. Duran; Malika Fair; Tina Hernandez‐Boussard; Maia Hightower; Anjali Jain; William B. Jordan; Stephen Konya; Roslyn Holliday Moore; Tamra Tyree Moore; Richard Rodriguez; Gauher Shaheen; Lynne Page Snyder; Mithuna Srinivasan; Craig A. Umscheid; Lucila Ohno‐Machado,JAMA Network Open,130,W4389794809,10.1001/jamanetworkopen.2023.45050,https://openalex.org/W4389794809,https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2812958/chin_2023_sc_230007_1702050468.82841.pdf,Health equity; Health care; Algorithm; Ethnic group; Health policy,article,True,"Importance Health care algorithms are used for diagnosis, treatment, prognosis, risk stratification, and allocation of resources. Bias in the development and use of algorithms can lead to worse outcomes for racial and ethnic minoritized groups and other historically marginalized populations such as individuals with lower income. Objective To provide a conceptual framework and guiding principles for mitigating and preventing bias in health care algorithms to promote health and health care equity. Evidence Review The Agency for Healthcare Research and Quality and the National Institute for Minority Health and Health Disparities convened a diverse panel of experts to review evidence, hear from stakeholders, and receive community feedback. Findings The panel developed a conceptual framework to apply guiding principles across an algorithm’s life cycle, centering health and health care equity for patients and communities as the goal, within the wider context of structural racism and discrimination. Multiple stakeholders can mitigate and prevent bias at each phase of the algorithm life cycle, including problem formulation (phase 1); data selection, assessment, and management (phase 2); algorithm development, training, and validation (phase 3); deployment and integration of algorithms in intended settings (phase 4); and algorithm monitoring, maintenance, updating, or deimplementation (phase 5). Five principles should guide these efforts: (1) promote health and health care equity during all phases of the health care algorithm life cycle; (2) ensure health care algorithms and their use are transparent and explainable; (3) authentically engage patients and communities during all phases of the health care algorithm life cycle and earn trustworthiness; (4) explicitly identify health care algorithmic fairness issues and trade-offs; and (5) establish accountability for equity and fairness in outcomes from health care algorithms. Conclusions and Relevance Multiple stakeholders must partner to create systems, processes, regulations, incentives, standards, and policies to mitigate and prevent algorithmic bias. Reforms should implement guiding principles that support promotion of health and health care equity in all phases of the algorithm life cycle as well as transparency and explainability, authentic community engagement and ethical partnerships, explicit identification of fairness issues and trade-offs, and accountability for equity and fairness."
An adversarial training framework for mitigating algorithmic biases in clinical machine learning,2023,Jenny Yang; Andrew A. S. Soltan; David W. Eyre; Yang Yang; David A. Clifton,npj Digital Medicine,86,W4361285150,10.1038/s41746-023-00805-y,https://openalex.org/W4361285150,https://www.nature.com/articles/s41746-023-00805-y.pdf,Adversarial system; Odds; Task (project management); Machine learning; Computer science,article,True,"Machine learning is becoming increasingly prominent in healthcare. Although its benefits are clear, growing attention is being given to how these tools may exacerbate existing biases and disparities. In this study, we introduce an adversarial training framework that is capable of mitigating biases that may have been acquired through data collection. We demonstrate this proposed framework on the real-world task of rapidly predicting COVID-19, and focus on mitigating site-specific (hospital) and demographic (ethnicity) biases. Using the statistical definition of equalized odds, we show that adversarial training improves outcome fairness, while still achieving clinically-effective screening performances (negative predictive values >0.98). We compare our method to previous benchmarks, and perform prospective and external validation across four independent hospital cohorts. Our method can be generalized to any outcomes, models, and definitions of fairness."
Bias in artificial intelligence algorithms and recommendations for mitigation,2023,Lama Nazer; Razan Zatarah; Shai Waldrip; Janny Xue Chen Ke; Mira Moukheiber; Ashish K. Khanna; Rachel S. Hicklen; Lama Moukheiber; Dana Moukheiber; Haobo Ma; Piyush Mathur,PLOS Digital Health,328,W4381716616,10.1371/journal.pdig.0000278,https://openalex.org/W4381716616,https://journals.plos.org/digitalhealth/article/file?id=10.1371/journal.pdig.0000278&type=printable,Health care; Computer science; Preprocessor; Artificial intelligence; Data science,review,True,"The adoption of artificial intelligence (AI) algorithms is rapidly increasing in healthcare. Such algorithms may be shaped by various factors such as social determinants of health that can influence health outcomes. While AI algorithms have been proposed as a tool to expand the reach of quality healthcare to underserved communities and improve health equity, recent literature has raised concerns about the propagation of biases and healthcare disparities through implementation of these algorithms. Thus, it is critical to understand the sources of bias inherent in AI-based algorithms. This review aims to highlight the potential sources of bias within each step of developing AI algorithms in healthcare, starting from framing the problem, data collection, preprocessing, development, and validation, as well as their full implementation. For each of these steps, we also discuss strategies to mitigate the bias and disparities. A checklist was developed with recommendations for reducing bias during the development and implementation stages. It is important for developers and users of AI-based algorithms to keep these important considerations in mind to advance health equity for all populations."
A Comprehensive Review of AI Techniques for Addressing Algorithmic Bias in Job Hiring,2024,Elham Albaroudi; Taha Mansouri; Ali Alameer,AI,42,W4391598857,10.3390/ai5010019,https://openalex.org/W4391598857,https://www.mdpi.com/2673-2688/5/1/19/pdf?version=1707285598,Computer science; Data science; Political science,review,True,"The study comprehensively reviews artificial intelligence (AI) techniques for addressing algorithmic bias in job hiring. More businesses are using AI in curriculum vitae (CV) screening. While the move improves efficiency in the recruitment process, it is vulnerable to biases, which have adverse effects on organizations and the broader society. This research aims to analyze case studies on AI hiring to demonstrate both successful implementations and instances of bias. It also seeks to evaluate the impact of algorithmic bias and the strategies to mitigate it. The basic design of the study entails undertaking a systematic review of existing literature and research studies that focus on artificial intelligence techniques employed to mitigate bias in hiring. The results demonstrate that the correction of the vector space and data augmentation are effective natural language processing (NLP) and deep learning techniques for mitigating algorithmic bias in hiring. The findings underscore the potential of artificial intelligence techniques in promoting fairness and diversity in the hiring process with the application of artificial intelligence techniques. The study contributes to human resource practice by enhancing hiring algorithms’ fairness. It recommends the need for collaboration between machines and humans to enhance the fairness of the hiring process. The results can help AI developers make algorithmic changes needed to enhance fairness in AI-driven tools. This will enable the development of ethical hiring tools, contributing to fairness in society."
"Risk and the future of AI: Algorithmic bias, data colonialism, and marginalization",2023,Anmol Arora; M. Barrett; Euisin Lee; Eivor Oborn; Karl Prince,Information and Organization,58,W4386131004,10.1016/j.infoandorg.2023.100478,https://openalex.org/W4386131004,,Colonialism; Data science; Computer science; History; Archaeology,article,False,
Addressing AI Algorithmic Bias in Health Care,2024,Raj M. Ratwani; Karey M. Sutton; Jessica E. Galarraga,JAMA,34,W4402223957,10.1001/jama.2024.13486,https://openalex.org/W4402223957,,Medicine; Health care; Applications of artificial intelligence; MEDLINE; Artificial intelligence,editorial,False,"This Viewpoint discusses the bias that exists in artificial intelligence (AI) algorithms used in health care despite recent federal rules to prohibit discriminatory outcomes from AI and recommends ways in which health care facilities, AI developers, and regulators could share responsibilities and actions to address bias."
Advancing algorithmic bias management capabilities in AI-driven marketing analytics research,2023,Shahriar Akter; Saida Sultana; Marcello M. Mariani; Samuel Fosso Wamba; Konstantina Spanaki; Yogesh K. Dwivedi,Industrial Marketing Management,34,W4386300838,10.1016/j.indmarman.2023.08.013,https://openalex.org/W4386300838,https://doi.org/10.1016/j.indmarman.2023.08.013,Customer lifetime value; Analytics; Equity (law); Marketing; Data science,article,True,"Algorithms in the age of artificial intelligence (AI) constantly transform customer behaviour, marketing programs, and marketing strategies in industrial markets. However, algorithms often fail to perform as expected due to various data, model, and market biases. Motivated by this challenge, this study presents a framework of algorithmic bias management capabilities for industrial markets that contribute to customer equity in terms of value, brand and relationship equity. Drawing on the dynamic capability theory, this study fills this gap by conducting a literature review, thematic analysis, and two rounds of surveys (n=200 analytics professionals and n=200 business customers) in the financial service industry in Australia. The findings show that algorithmic bias management capability consists of three primary dimensions (data, model, and deployment capabilities) and nine subdimensions. These findings have important implications for scholars and managers interested in developing algorithmic bias management capabilities to influence customer equity in industrial markets."
The Potential of Diverse Youth as Stakeholders in Identifying and Mitigating Algorithmic Bias for a Future of Fairer AI,2023,Jaemarie Solyst; Ellia Yang; Shixian Xie; Amy Ogan; Jessica Hammer; Motahhare Eslami,Proceedings of the ACM on Human-Computer Interaction,32,W4387344911,10.1145/3610213,https://openalex.org/W4387344911,https://dl.acm.org/doi/pdf/10.1145/3610213,Harm; Diversity (politics); Injustice; Inclusion (mineral); Psychology,article,True,"Youth regularly use technology driven by artificial intelligence (AI). However, it is increasingly well-known that AI can cause harm on small and large scales, especially for those underrepresented in tech fields. Recently, users have played active roles in surfacing and mitigating harm from algorithmic bias. Despite being frequent users of AI, youth have been under-explored as potential contributors and stakeholders to the future of AI. We consider three notions that may be at the root of youth facing barriers to playing an active role in responsible AI, which are youth (1) cannot understand the technical aspects of AI, (2) cannot understand the ethical issues around AI, and (3) need protection from serious topics related to bias and injustice. In this study, we worked with youth (N = 30) in first through twelfth grade and parents (N = 6) to explore how youth can be part of identifying algorithmic bias and designing future systems to address problematic technology behavior. We found that youth are capable of identifying and articulating algorithmic bias, often in great detail. Participants suggested different ways users could give feedback for AI that reflects their values of diversity and inclusion. Youth who may have less experience with computing or exposure to societal structures can be supported by peers or adults with more of this knowledge, leading to critical conversations about fairer AI. This work illustrates youths' insights, suggesting that they should be integrated in building a future of responsible AI."
Disambiguating Algorithmic Bias: From Neutrality to Justice,2023,Elizabeth Edenberg; Alexandra Wood,,19,W4386242313,10.1145/3600211.3604695,https://openalex.org/W4386242313,https://doi.org/10.1145/3600211.3604695,Neutrality; Injustice; Prejudice (legal term); Implicit bias; Economic Justice,article,True,"As algorithms have become ubiquitous in consequential domains, societal concerns about the potential for discriminatory outcomes have prompted urgent calls to address algorithmic bias. In response, a rich literature across computer science, law, and ethics is rapidly proliferating to advance approaches to designing fair algorithms. Yet computer scientists, legal scholars, and ethicists are often not speaking the same language when using the term 'bias.' Debates concerning whether society can or should tackle the problem of algorithmic bias are hampered by conflations of various understandings of bias, ranging from neutral deviations from a standard to morally problematic instances of injustice due to prejudice, discrimination, and disparate treatment. This terminological confusion impedes efforts to address clear cases of discrimination."
Algorithmic bias in artificial intelligence is a problem—And the root issue is power,2023,Rae Walker; Jess Dillard‐Wright; Favorite Iradukunda,Nursing Outlook,22,W4385788498,10.1016/j.outlook.2023.102023,https://openalex.org/W4385788498,http://www.nursingoutlook.org/article/S0029655423001288/pdf,Injustice; Health care; Power (physics); Root (linguistics); Curriculum,article,True,"Artificial intelligence (AI) in health care continues to expand at a rapid rate, impacting both nurses and communities we accompany in care.We argue algorithmic bias is but a symptom of a more systemic and longstanding problem: power imbalances related to the creation, development, and use of health care technologies.This commentary responds to Drs. O'Connor and Booth's 2022 article, ""Algorithmic bias in health care: Opportunities for nurses to improve equality in the age of artificial intelligence.""Nurses need not 'reinvent the wheel' when it comes to AI policy, curricula, or ethics. We can and should follow the lead of communities already working 'from the margins' who provide ample guidance.Its neither feasible nor just to expect individual nurses to counter systemic injustice in health care through individual actions, more technocentric curricula, or industry partnerships. We need disciplinary supports for collective action to renegotiate power for AI tech."
"AI Algorithmic Bias: Understanding its Causes, Ethical and Social Implications",2023,Lakshitha R Jain; Vineetha Menon,,21,W4389988494,10.1109/ictai59109.2023.00073,https://openalex.org/W4389988494,,Computer science; Artificial intelligence; Selection bias; Machine learning; Apprehension,article,False,"The escalating usage of artificial intelligence (AI) and machine learning algorithms across diverse fields has prompted apprehension regarding the propagation of algorithmic bias, which may exacerbate instances of discrimination and inequality. Algorithmic bias in AI and machine learning (ML) techniques manifests in real-world applications as a result of either insufficient data variation or augmentation availability in the AI/ML training data, or a flawed learning policy. This leads to the accidental propagation of AI bias as an unjust treatment of particular groups of individuals, owing to their race, gender [1], age, or other distinguishing attributes in practical applications. This paper offers a comprehensive analysis of algorithmic bias, encompassing its origins, ethical and social ramifications, and possible remediations. In addition, this paper introduces an innovative methodology for identifying and measuring algorithmic bias that integrates statistical analysis with input from users and domain specialists. This exposition examines distinct forms of algorithmic biases, such as selection bias, confirmation bias, and measurement bias, and examines underlying catalysts for algorithmic bias, encompassing data integrity concerns, decisions regarding algorithmic design, and institutional prejudgments. The adverse ramifications of algorithmic bias, including the perpetuation of social inequality and the impeding of societal advancement, are the focus of our examination. The present study seeks to make a contribution to the advancement of impartial [2] and equitable AI systems with the potential to foster societal progress and benefit individuals across diverse demographics by identifying the sources and repercussions of algorithmic bias and recommending efficacious interventions."
Data’s Impact on Algorithmic Bias,2023,Donghee Shin; Emily Y. Shin,Computer,12,W4377818787,10.1109/mc.2023.3262909,https://openalex.org/W4377818787,https://ieeexplore.ieee.org/ielx7/2/10132019/10132055.pdf,Computer science; Column (typography); Artificial intelligence; Machine learning; Algorithm,article,True,"Algorithmic bias refers to systematic and structured errors in an artificial intelligence system that generate unfair results and inequalities. This column discusses how bias in algorithms appears, amplifies over time, and shapes people's thinking, potentially leading to discrimination."
Tackling algorithmic bias and promoting transparency in health datasets: the STANDING Together consensus recommendations,2024,Joseph Alderman; Joanne Palmer; Elinor Laws; Melissa D. McCradden; Johan Ordish; Marzyeh Ghassemi; Stephen Pfohl; Negar Rostamzadeh; Heather Cole-Lewis; Ben Glocker; Melanie Calvert; Tom Pollard; J. M. Gill; Jacqui Gath; Ade Adebajo; Jude Beng; Cheuk Wing Leung; Stephanie Kuku; Lesley-Anne Farmer; Rubeta Matin; Bilal A. Mateen; Francis McKay; Katherine Heller; Alan Karthikesalingam; Darren Treanor; Maxine Mackintosh; Lauren Oakden‐Rayner; Russell J. Pearson; Arjun K. Manrai; Puja Myles; Judit Kumuthini; Zoher Kapacee; Neil J. Sebire; Lama Nazer; Jarrel Seah; Ashley Akbari; Lewis E. Berman; Judy Wawira Gichoya; Lorenzo Righetto; D. V. K. Samuel; William Wasswa; Maria Charalambides; Anmol Arora; Sameer Pujari; Charlotte Summers; Elizabeth Sapey; S P Wilkinson; Vishal Thakker; Alastair K. Denniston; Xiaoxuan Liu,The Lancet Digital Health,36,W4405534231,10.1016/s2589-7500(24)00224-3,https://openalex.org/W4405534231,https://doi.org/10.1016/s2589-7500(24)00224-3,Transparency (behavior); Data science; Computer science; Political science; Computer security,review,True,
"Practical, epistemic and normative implications of algorithmic bias in healthcare artificial intelligence: a qualitative study of multidisciplinary expert perspectives",2023,Yves Saint James Aquino; Stacy M. Carter; Nehmat Houssami; Annette Braunack‐Mayer; Khin Than Win; Chris Degeling; Lei Wang; Wendy Rogers,Journal of Medical Ethics,37,W4321610264,10.1136/jme-2022-108850,https://openalex.org/W4321610264,https://jme.bmj.com/content/medethics/early/2023/02/22/jme-2022-108850.full.pdf,Health care; Normative; Inclusion (mineral); Multidisciplinary approach; Psychology,article,True,"There is a growing concern about artificial intelligence (AI) applications in healthcare that can disadvantage already under-represented and marginalised groups (eg, based on gender or race)."
Auditing Work: Exploring the New York City algorithmic bias audit regime,2024,Lara Groves; Jacob Metcalf; Alayna Kennedy; Briana Vecchione; Andrew Strait,"2022 ACM Conference on Fairness, Accountability, and Transparency",22,W4399362708,10.1145/3630106.3658959,https://openalex.org/W4399362708,https://dl.acm.org/doi/pdf/10.1145/3630106.3658959,Audit; Work (physics); Computer science; Accounting; Business,article,True,"In July 2023, New York City (NYC) implemented the first attempt to create an algorithm auditing regime for commercial machine-learning systems. Local Law 144 (LL 144), requires NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to be subject to annual bias audits by an independent auditor. In this paper, we analyse what lessons can be learned from LL 144 for other national attempts to create algorithm auditing regimes. Using qualitative interviews with 17 experts and practitioners working within the regime, we find LL 144 has failed to create an effective auditing regime: the law fails to clearly define key aspects like AEDTs and what constitutes an independent auditor, leaving auditors, vendors who create AEDTs, and companies using AEDTs to define the law's practical implementation in ways that failed to protect job applicants. Several factors contribute to this: first, the law was premised on a faulty transparency-driven theory of change that fails to stop biased AEDTs from being used by employers. Second, industry lobbying led to the definition of what constitutes an AEDT being narrowed to the point where most companies considered their tools exempt. Third, we find auditors face enormous practical and cultural challenges gaining access to data from employers and vendors building these tools. Fourth, we find wide disagreement over what constitutes a legitimate auditor and identify four different kinds of 'auditor roles' that serve different functions and offer different kinds of services. We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes that use clearer definitions and metrics and more accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, and can provide guidance for policymakers seeking to create similar regimes."
Inside the Black Box: Detecting and Mitigating Algorithmic Bias Across Racialized Groups in College Student-Success Prediction,2024,Denisa Gándara; Hadis Anahideh; Matthew P. Ison; Lorenzo Picchiarini,AERA Open,21,W4400485150,10.1177/23328584241258741,https://openalex.org/W4400485150,https://journals.sagepub.com/doi/pdf/10.1177/23328584241258741,Racial bias; Implicit bias; Racism; Psychological intervention; Higher education,article,True,"Colleges and universities are increasingly turning to algorithms that predict college-student success to inform various decisions, including those related to admissions, budgeting, and student-success interventions. Because predictive algorithms rely on historical data, they capture societal injustices, including racism. In this study, we examine how the accuracy of college student success predictions differs between racialized groups, signaling algorithmic bias. We also evaluate the utility of leading bias-mitigating techniques in addressing this bias. Using nationally representative data from the Education Longitudinal Study of 2002 and various machine learning modeling approaches, we demonstrate how models incorporating commonly used features to predict college-student success are less accurate when predicting success for racially minoritized students. Common approaches to mitigating algorithmic bias are generally ineffective at eliminating disparities in prediction outcomes and accuracy between racialized groups."
Are algorithms biased in education? Exploring racial bias in predicting community college student success,2024,Kelli Bird; Benjamin Castleman; Yifeng Song,Journal of Policy Analysis and Management,13,W4391402561,10.1002/pam.22569,https://openalex.org/W4391402561,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/pam.22569,Racial bias; Community college; Psychology; Computer science; Implicit bias,article,True,"Abstract Predictive analytics are increasingly pervasive in higher education. However, algorithmic bias has the potential to reinforce racial inequities in postsecondary success. We provide a comprehensive and translational investigation of algorithmic bias in two separate prediction models—one predicting course completion, the second predicting degree completion. We show that if either model were used to target additional supports for “at‐risk” students, then the algorithmic bias would lead to fewer marginal Black students receiving these resources. We also find the magnitude of algorithmic bias varies within the distribution of predicted success. With the degree completion model, the amount of bias is over 5 times higher when we define at‐risk using the bottom decile than when we focus on students in the bottom half of predicted scores; in the course completion model, the reverse is true. These divergent patterns emphasize the contextual nature of algorithmic bias and attempts to mitigate it. Our results moreover suggest that algorithmic bias is due in part to currently‐available administrative data being relatively less useful at predicting Black student success, particularly for new students; this suggests that additional data collection efforts have the potential to mitigate bias."
Misinformation and Algorithmic Bias,2024,Donghee Shin,,7,W4393191699,10.1007/978-3-031-52569-8_2,https://openalex.org/W4393191699,,Misinformation; Internet privacy; Computer science; Computer security; Psychology,book-chapter,False,"What happens if the data fed to AI are biased? What happens if the response of a chatbot spreads misinformation? Unlike many people hope, AI is as biased as humans are. Bias can originate from various venues, including but not limited to the design and unintended or unanticipated use of the algorithm or algorithmic decisions about the way data are coded, framed, filtered, or analyzed to train machine learning. Algorithmic bias has been widely seen in advertising, content recommendations, and search engine results. Algorithmic prejudice has been found in cases ranging from political campaign outcomes to the proliferation of fake news and misinformation. It has also surfaced in health care, education, and public service, aggravating existing societal, socioeconomic, and political biases. These algorithm-induced biases can exert negative impacts on a range of social interactions, ranging from unintended privacy infringements to solidifying societal biases of gender, race, ethnicity, and culture. The significance of the data used in training algorithms should not be underestimated. Humans should play a part in the datafication of algorithms, as preventing the spread of misinformation is difficult by technology alone, especially considering the rate at which information can spread online."
Navigating algorithm bias in AI: ensuring fairness and trust in Africa,2024,Notice Pasipamire; Abton Muroyiwa,Frontiers in Research Metrics and Analytics,12,W4403745416,10.3389/frma.2024.1486600,https://openalex.org/W4403745416,https://doi.org/10.3389/frma.2024.1486600,Transparency (behavior); Compromise; Perspective (graphical); Sustainable development; Public relations,article,True,"This article presents a perspective on the impact of algorithmic bias on information fairness and trust in artificial intelligence (AI) systems within the African context. The author's personal experiences and observations, combined with relevant literature, formed the basis of this article. The authors demonstrate why algorithm bias poses a substantial challenge in Africa, particularly regarding fairness and the integrity of AI applications. This perspective underscores the urgent need to address biases that compromise the fairness of information dissemination and undermine public trust. The authors advocate for the implementation of strategies that promote inclusivity, enhance cultural sensitivity, and actively engage local communities in the development of AI systems. By prioritizing ethical practices and transparency, stakeholders can mitigate the risks associated with bias, thereby fostering trust and ensuring equitable access to technology. Additionally, the article explores the potential consequences of inaction, including exacerbated social disparities, diminished confidence in public institutions, and economic stagnation. Ultimately, this work argues for a collaborative approach to AI that positions Africa as a leader in responsible development, ensuring that technology serves as a catalyst for sustainable development and social justice."
Investigating algorithmic bias in student progress monitoring,2024,Jamiu Adekunle Idowu; Adriano Koshiyama; Philip Treleaven,Computers and Education Artificial Intelligence,11,W4400767613,10.1016/j.caeai.2024.100267,https://openalex.org/W4400767613,https://doi.org/10.1016/j.caeai.2024.100267,Computer science; Learning analytics; Grading (engineering); Analytics; Data science,article,True,"This research investigates bias in AI algorithms used for monitoring student progress, specifically focusing on bias related to age, disability, and gender. The study is motivated by incidents such as the UK A-level grading controversy, which demonstrated the real-world implications of biased algorithms. Using the Open University Learning Analytics Dataset, the research evaluates fairness with metrics like ABROCA, Average Odds Difference, and Equality of Opportunity Difference. The analysis is structured into three experiments. The first experiment examines fairness as an attribute of the data sources and reveals that institutional data is the primary contributor to model discrimination, followed by Virtual Learning Environment data, while assessment data is the least biased. In the second experiment, the research introduces the Optimal Time Index, which pinpoints Day 60 of an average 255-day course as the optimal time for predicting student outcomes, balancing timely interventions, model accuracy, and efficient resource allocation. The third experiment implements bias mitigation strategies throughout the model's life cycle, achieving fairness without compromising accuracy. Finally, this study introduces the Student Progress Card, designed to provide actionable personalized feedback for each student."
Investigating Algorithmic Bias on Bayesian Knowledge Tracing and Carelessness Detectors,2024,Andres Felipe Zambrano; Jiayi Zhang; Ryan S. Baker,,10,W4392445451,10.1145/3636555.3636890,https://openalex.org/W4392445451,,Carelessness; Computer science; Demographics; Debiasing; Bayesian probability,article,False,"In today's data-driven educational technologies, algorithms have a pivotal impact on student experiences and outcomes. Therefore, it is critical to take steps to minimize biases, to avoid perpetuating or exacerbating inequalities. In this paper, we investigate the degree to which algorithmic biases are present in two learning analytics models: knowledge estimates based on Bayesian Knowledge Tracing (BKT) and carelessness detectors. Using data from a learning platform used across the United States at scale, we explore algorithmic bias following three different approaches: 1) analyzing the performance of the models on every demographic group in the sample, 2) comparing performance across intersectional groups of these demographics, and 3) investigating whether the models trained using specific groups can be transferred to demographics that were not observed during the training process. Our experimental results show that the performance of these models is close to equal across all the demographic and intersectional groups. These findings establish the feasibility of validating educational algorithms for intersectional groups and indicate that these algorithms can be fairly used for diverse students at scale."
Artificial Intelligence and Algorithmic Bias,2023,Natasha H. Williams,The International Library of Bioethics,8,W4390476661,10.1007/978-3-031-48262-5_1,https://openalex.org/W4390476661,,Section (typography); Computer science; Artificial intelligence; Applications of artificial intelligence; Health care,book-chapter,False,
Enhancing children’s understanding of algorithmic biases in and with text-to-image generative AI,2024,Henriikka Vartiainen; Juho Kahila; Matti Tedre; Sonsoles López‐Pernas; Nicolas Pope,New Media & Society,8,W4397034407,10.1177/14614448241252820,https://openalex.org/W4397034407,https://journals.sagepub.com/doi/pdf/10.1177/14614448241252820,Generative grammar; Image (mathematics); Computer science; Generative model; Psychology,article,True,"Despite the growing concerns surrounding algorithmic biases in generative AI (artificial intelligence), there is a noticeable lack of research on how to facilitate children and young people’s awareness and understanding of them. This study aimed to address this gap by conducting hands-on workshops with fourth- and seventh-grade students in Finland, and by focusing on students’ ( N = 209) evolving explanations of the potential causes of algorithmic biases within text-to-image generative models. Statistically significant progress in children’s data-driven explanations was observed on a written reasoning test, which was administered prior to and after the intervention, as well as in their responses to the worksheets they filled out during a lesson that focused on algorithmic biases. The article concludes with a discussion on the development and facilitation of children’s understanding of algorithmic biases."
Automating Automaticity: How the Context of Human Choice Affects the Extent of Algorithmic Bias,2023,Amanda Agan; Diag Davenport; J. Ludwig; Sendhil Mullainathan,,11,W4321366289,10.3386/w30981,https://openalex.org/W4321366289,https://doi.org/10.3386/w30981,Automaticity; Context (archaeology); Computer science; Cognitive psychology; Psychology,report,True,"Consumer choices are increasingly mediated by algorithms, which use data on those past choices to infer consumer preferences and then curate future choice sets.Behavioral economics suggests one reason these algorithms so often fail: choices can systematically deviate from preferences.For example, research shows that prejudice can arise not just from preferences and beliefs, but also from the context in which people choose.When people behave automatically, biases creep in; snap decisions are typically more prejudiced than slow, deliberate ones, and can lead to behaviors that users themselves do not consciously want or intend.As a result, algorithms trained on automatic behaviors can misunderstand the prejudice of users: the more automatic the behavior, the greater the error.We empirically test these ideas in a lab experiment, and find that more automatic behavior does indeed seem to lead to more biased algorithms.We then explore the large-scale consequences of this idea by carrying out algorithmic audits of Facebook in its two biggest markets, the US and India, focusing on two algorithms that differ in how users engage with them: News Feed (people interact with friends' posts fairly automatically) and People You May Know (people choose friends fairly deliberately).We find significant out-group bias in the News Feed algorithm (e.g., whites are less likely to be shown Black friends' posts, and Muslims less likely to be shown Hindu friends' posts), but no detectable bias in the PYMK algorithm.Together, these results suggest a need to rethink how large-scale algorithms use data on human behavior, especially in online contexts where so much of the measured behavior might be quite automatic."
Automating Automaticity: How the Context of Human Choice Affects the Extent of Algorithmic Bias,2023,Amanda Agan; Diag Davenport; Jens Ludwig; Sendhil Mullainathan,SSRN Electronic Journal,11,W4321379767,10.2139/ssrn.4364729,https://openalex.org/W4321379767,,Automaticity; Context (archaeology); Computer science; Cognitive psychology; Psychology,article,False,"Consumer choices are increasingly mediated by algorithms, which use data on those past choices to infer consumer preferences and then curate future choice sets. Behavioral economics suggests one reason these algorithms so often fail: choices can systematically deviate from preferences. For example, research shows that prejudice can arise not just from preferences and beliefs, but also from the context in which people choose. When people behave automatically, biases creep in; snap decisions are typically more prejudiced than slow, deliberate ones, and can lead to behaviors that users themselves do not consciously want or intend. As a result, algorithms trained on automatic behaviors can misunderstand the prejudice of users: the more automatic the behavior, the greater the error. We empirically test these ideas in a lab experiment, and find that more automatic behavior does indeed seem to lead to more biased algorithms. We then explore the large-scale consequences of this idea by carrying out algorithmic audits of Facebook in its two biggest markets, the US and India, focusing on two algorithms that differ in how users engage with them: News Feed (people interact with friends' posts fairly automatically) and People You May Know (people choose friends fairly deliberately). We find significant out-group bias in the News Feed algorithm (e.g., whites are less likely to be shown Black friends' posts, and Muslims less likely to be shown Hindu friends' posts), but no detectable bias in the PYMK algorithm. Together, these results suggest a need to rethink how large-scale algorithms use data on human behavior, especially in online contexts where so much of the measured behavior might be quite automatic.Institutional subscribers to the NBER working paper series, and residents of developing countries may download this paper without additional charge at www.nber.org."
Mitigating Algorithmic Bias in AI-Driven Cardiovascular Imaging for Fairer Diagnostics,2024,Md Abu Sufian; Lujain Alsadder; Wahiba Hamzi; Sadia Zaman; A. S. M. Sharifuzzaman Sagar; Boumediene Hamzi,Diagnostics,8,W4404756939,10.3390/diagnostics14232675,https://openalex.org/W4404756939,https://doi.org/10.3390/diagnostics14232675,Computer science; Artificial intelligence; Data science,article,True,": The research addresses algorithmic bias in deep learning models for cardiovascular risk prediction, focusing on fairness across demographic and socioeconomic groups to mitigate health disparities. It integrates fairness-aware algorithms, susceptible carrier-infected-recovered (SCIR) models, and interpretability frameworks to combine fairness with actionable AI insights supported by robust segmentation and classification metrics."
Algorithmic bias in educational systems: Examining the impact of AI-driven decision making in modern education,2025,Obed Boateng; Bright Boateng,World Journal of Advanced Research and Reviews,10,W4407040219,10.30574/wjarr.2025.25.1.0253,https://openalex.org/W4407040219,,Computer science; Mathematics education; Psychology; Data science; Artificial intelligence,article,False,"The increasing integration of artificial intelligence and algorithmic systems in educational settings has raised critical concerns about their impact on educational equity. This paper examines the manifestation and implications of algorithmic bias across various educational domains, including admissions processes, assessment systems, and learning management platforms. Through analysis of current research and studies, we investigate how these biases can perpetuate or exacerbate existing educational disparities, particularly affecting students from marginalized communities. The study reveals that algorithmic bias in education operates through multiple channels, from data collection and algorithm design to implementation practices and institutional policies. Our findings indicate that biased algorithms can significantly impact students' educational trajectories, creating new forms of systemic barriers in education. We propose a comprehensive framework for addressing these challenges, combining technical solutions with policy reforms and institutional guidelines. This research contributes to the growing discourse on ethical AI in education and provides practical strategies for creating more equitable educational systems in an increasingly digitized world."
Algorithmic Bias: A Challenge for Ethical Artificial Intelligence (AI),2023,Divya Dwivedi,,6,W4390444195,10.1007/978-981-99-8834-1_5,https://openalex.org/W4390444195,,Exploit; Computer science; Artificial intelligence; Ethical issues; Cognition,book-chapter,False,"Artificial Intelligence (AI) has become an important aspect of our lives as it has humongous potential to support humans in various domains by sharing the cognitive load. However, the ethical side of AI poses serious challenges. There have been several cases when AI algorithms are declared as unfair, inscrutable, harmful, i.e., biased. Therefore, it becomes important to understand—What kinds of algorithmic biases exist and how do they occur? What are their sources? How can they be identified and corrected to make them more ethical? What are the optimum ways to exploit them? This chapter offers a thematic review of 'Algorithmic bias' by exploring the recent literature (2016–2022) to find the answers to the above questions."
Algorithmic bias and research integrity; the role of nonhuman authors in shaping scientific knowledge with respect to artificial intelligence: a perspective,2023,Malik Olatunde Oduoye; Binish Javed; Nikhil Gupta; Che Mbali Valentina Sih,International Journal of Surgery,15,W4380714834,10.1097/js9.0000000000000552,https://openalex.org/W4380714834,https://doi.org/10.1097/js9.0000000000000552,Sophistication; Prejudice (legal term); Perspective (graphical); Transparency (behavior); Computer science,article,True,"Artificial intelligence technologies were developed to assist authors in bettering the organization and caliber of their published papers, which are both growing in quantity and sophistication. Even though the usage of artificial intelligence tools in particular ChatGPT's natural language processing systems has been shown to be beneficial in research, there are still concerns about accuracy, responsibility, and transparency when it comes to the norms regarding authorship credit and contributions. Genomic algorithms quickly examine large amounts of genetic data to identify potential disease-causing mutations. By analyzing millions of medications for potential therapeutic benefits, they can quickly and relatively economically find novel approaches to treatment. Researchers from several fields can collaborate on difficult tasks with the assistance of nonhuman writers, promoting interdisciplinary research. Sadly, there are a number of significant disadvantages associated with employing nonhuman authors, including the potential for algorithmic prejudice. Biased data may be reinforced by the algorithm since machine learning algorithms can only be as objective as the data they are trained on. It is overdue that scholars bring forth basic moral concerns in the fight against algorithmic prejudice. Overall, even if the use of nonhuman authors has the potential to significantly improve scientific research, it is crucial for scientists to be aware of these drawbacks and take precautions to avoid bias and limits. To provide accurate and objective results, algorithms must be carefully designed and implemented, and researchers need to be mindful of the larger ethical ramifications of their usage."
EXplainable Artificial Intelligence (XAI) for facilitating recognition of algorithmic bias: An experiment from imposed users’ perspectives,2024,Ching‐Hua Chuan; Ruoyu Sun; Shiyun Tian; Wan‐Hsiu Sunny Tsai,Telematics and Informatics,10,W4396793783,10.1016/j.tele.2024.102135,https://openalex.org/W4396793783,https://doi.org/10.1016/j.tele.2024.102135,Raising (metalworking); Computer science; Perception; Artificial intelligence; Cognitive psychology,article,True,"This study explored the potential of eXplainable Artificial Intelligence (XAI) in raising user awareness of algorithmic bias. This study examined the popular ""explanation by example"" XAI approach, where users receive explanatory examples resembling their input. As this XAI approach allows users to gauge the congruence between these examples and their circumstances, perceived incongruence then evokes perceptions of unfairness and exclusion, prompting users not to put blind trust in the system and raising awareness of algorithmic bias stemming from non-inclusive datasets. The results further highlight the moderating role of users' prior experience with discrimination."
Measuring algorithmic bias to analyze the reliability of AI tools that predict depression risk using smartphone sensed-behavioral data,2024,Daniel A. Adler; Caitlin A. Stamatis; Jonah Meyerhoff; David C. Mohr; Fei Wang; Gabriel J. Aranovich; Srijan Sen; Tanzeem Choudhury,npj Mental Health Research,12,W4395010721,10.1038/s44184-024-00057-y,https://openalex.org/W4395010721,https://www.nature.com/articles/s44184-024-00057-y.pdf,Generalizability theory; Depression (economics); Reliability (semiconductor); Mental health; Computer science,article,True,"Abstract AI tools intend to transform mental healthcare by providing remote estimates of depression risk using behavioral data collected by sensors embedded in smartphones. While these tools accurately predict elevated depression symptoms in small, homogenous populations, recent studies show that these tools are less accurate in larger, more diverse populations. In this work, we show that accuracy is reduced because sensed-behaviors are unreliable predictors of depression across individuals: sensed-behaviors that predict depression risk are inconsistent across demographic and socioeconomic subgroups. We first identified subgroups where a developed AI tool underperformed by measuring algorithmic bias, where subgroups with depression were incorrectly predicted to be at lower risk than healthier subgroups. We then found inconsistencies between sensed-behaviors predictive of depression across these subgroups. Our findings suggest that researchers developing AI tools predicting mental health from sensed-behaviors should think critically about the generalizability of these tools, and consider tailored solutions for targeted populations."
Reflecting on Algorithmic Bias With Design Fiction: The MiniCoDe Workshops,2024,Tommaso Turchi; Alessio Malizia; Simone Borsci,IEEE Intelligent Systems,5,W4390806593,10.1109/mis.2024.3352977,https://openalex.org/W4390806593,https://ieeexplore.ieee.org/ielx7/9670/5196652/10388369.pdf,Transparency (behavior); Computer science; Accountability; Economic Justice; Implementation,article,True,"In an increasingly complex everyday life, algorithms – often learnt from data, i.e. machine learning (ML) – are used to make or assist operational decisions. However, developers and designers usually are not entirely aware of how to reflect on social justice while designing ML algorithms and applications. Algorithmic social justice – i.e., designing algorithms including fairness, transparency, and accountability – aims at helping expose, counterbalance, and remedy bias and exclusion in future ML-based decision-making applications. How might we entice people to engage in more reflective practices that examine the ethical consequences of ML algorithmic bias in society? We developed and tested a Design Fiction-driven methodology to enable multi-disciplinary teams to perform intense, workshop-like gatherings to let emerge potential ethical issues and mitigate bias through a series of guided steps. With this contribution, we present an original and innovative use of Design Fiction as a method to reduce algorithmic bias in co-design activities."
Technology assisted research assessment: algorithmic bias and transparency issues,2023,Mike Thelwall; Kayvan Kousha,Aslib Journal of Information Management,6,W4387204488,10.1108/ajim-04-2023-0119,https://openalex.org/W4387204488,https://eprints.whiterose.ac.uk/203219/1/Artificial_intelligence_technologies_to_support_research_assessment-transparency%20bias_preprint.pdf,Transparency (behavior); Scope (computer science); Originality; Computer science; Data science,article,True,"Purpose Technology is sometimes used to support assessments of academic research in the form of automatically generated bibliometrics for reviewers to consult during their evaluations or by replacing some or all human judgements. With artificial intelligence (AI), there is increasing scope to use technology to assist research assessment processes in new ways. Since transparency and fairness are widely considered important for research assessment and AI introduces new issues, this review investigates their implications. Design/methodology/approach This article reviews and briefly summarises transparency and fairness concerns in general terms and through the issues that they raise for various types of Technology Assisted Research Assessment (TARA). Findings Whilst TARA can have varying levels of problems with both transparency and bias, in most contexts it is unclear whether it worsens the transparency and bias problems that are inherent in peer review. Originality/value This is the first analysis that focuses on algorithmic bias and transparency issues for technology assisted research assessment."
"Algorithmic bias, data ethics, and governance: Ensuring fairness, transparency and compliance in AI-powered business analytics applications",2025,Julien Kiesse Bahangulu; Louis Owusu-Berko,World Journal of Advanced Research and Reviews,9,W4407857156,10.30574/wjarr.2025.25.2.0571,https://openalex.org/W4407857156,,Transparency (behavior); Compliance (psychology); Corporate governance; Analytics; Business ethics,article,False,"The widespread adoption of AI-powered business analytics applications has revolutionized decision-making, yet it has also introduced significant challenges related to algorithmic bias, data ethics, and governance. As organizations increasingly rely on machine learning and big data analytics for customer profiling, credit scoring, hiring decisions, and predictive analytics, concerns about fairness, transparency, and compliance have intensified. Algorithmic biases—often stemming from biased training data, flawed model assumptions, and insufficient diversity in datasets—can result in discriminatory outcomes, reinforcing societal inequalities and reputational risks for businesses. To address these concerns, robust data ethics frameworks must be integrated into AI governance strategies. Ethical AI principles emphasize accountability, explainability, and bias mitigation techniques, ensuring that decision-making algorithms are transparent and justifiable. Organizations must implement bias detection methods, fairness-aware machine learning models, and continuous audits to minimize unintended consequences. Additionally, regulatory frameworks such as GDPR, CCPA, and AI-specific compliance laws necessitate stringent governance practices to protect consumer rights and data privacy. Beyond compliance, fostering public trust in AI-powered analytics requires organizations to adopt ethical data stewardship, ensuring that AI models align with corporate social responsibility (CSR) initiatives and stakeholder expectations. The intersection of data ethics, algorithmic accountability, and regulatory compliance presents both challenges and opportunities for businesses seeking to leverage AI responsibly. This paper examines key strategies for mitigating algorithmic bias, establishing ethical AI governance models, and ensuring fairness in data-driven business applications, providing a roadmap for organizations to enhance transparency, compliance, and equitable AI adoption."
Trapped in the search box: An examination of algorithmic bias in search engine autocomplete predictions,2023,Cong Lin; Yuxin Gao; Na Ta; Kaiyu Li; Hongyao Fu,Telematics and Informatics,10,W4388010320,10.1016/j.tele.2023.102068,https://openalex.org/W4388010320,,Computer science; Search engine; Disadvantaged; Black box; Debiasing,article,False,
Algorithmic bias: Social science research integration through the 3-D Dependable AI Framework,2024,Kalinda Ukanwa,Current Opinion in Psychology,7,W4400216206,10.1016/j.copsyc.2024.101836,https://openalex.org/W4400216206,https://doi.org/10.1016/j.copsyc.2024.101836,Psychology; Psychological research; Through-the-lens metering; Data science; Cognitive science,review,True,"Algorithmic bias has emerged as a critical challenge in the age of responsible production of artificial intelligence (AI). This paper reviews recent research on algorithmic bias and proposes increased engagement of psychological and social science research to understand antecedents and consequences of algorithmic bias. Through the lens of the 3-D Dependable AI Framework, this article explores how social science disciplines, such as psychology, can contribute to identifying and mitigating bias at the Design, Develop, and Deploy stages of the AI life cycle. Finally, we propose future research directions to further address the complexities of algorithmic bias and its societal implications."
Mitigating Algorithmic Bias with Limited Annotations,2023,Guanchu Wang; Mengnan Du; Ninghao Liu; Na Zou; Xia Hu,Lecture notes in computer science,5,W4386804585,10.1007/978-3-031-43415-0_15,https://openalex.org/W4386804585,,Computer science; Annotation; Benchmark (surveying); Bounding overwatch; Baseline (sea),book-chapter,False,
Algorithmic bias and racial inequality: a critical review,2024,Maximilian Kasy,Oxford Review of Economic Policy,4,W4404610377,10.1093/oxrep/grae031,https://openalex.org/W4404610377,https://doi.org/10.1093/oxrep/grae031,Inequality; Economics; Neoclassical economics; Mathematical economics; Econometrics,review,True,"Abstract Most definitions of algorithmic bias and fairness encode decision-maker interests, such as profits, rather than the interests of disadvantaged groups (e.g. racial minorities): bias is defined as a deviation from profit maximization. Future research should instead focus on the causal effect of automated decisions on the distribution of welfare, both across and within groups. The literature emphasizes some apparent contradictions between different notions of fairness, and between fairness and profits. These contradictions vanish, however, when profits are maximized. Existing work involves conceptual slippages between statistical notions of bias and misclassification errors, economic notions of profit, and normative notions of bias and fairness. Notions of bias nonetheless carry some interest within the welfare paradigm that I advocate for, if we understand bias and discrimination as mechanisms and potential points of intervention."
"ABCs: Differentiating Algorithmic Bias, Automation Bias, and Automation Complacency",2023,Amanda Potasznik,,4,W4381893935,10.1109/ethics57328.2023.10155094,https://openalex.org/W4381893935,,Terminology; Conflation; Automation; Documentation; Meaning (existential),article,False,"Algorithmic bias, automation bias, and automation complacency have been identified as culprits of a variety of human-computer interaction missteps, ethical offenses, and societal harms. However, these three terms are often mistaken and conflated. Students and professionals alike may have difficulty differentiating between the terms and fully understanding the impact of such psychological phenomena on their work and research. A review of relevant literature is conducted in order to establish an overview of historical documentation and analysis of the underlying themes; definitions and examples of each concept are then synthesized in order to provide a holistic understanding of the meaning behind this set of terminology."
Awareness of Racial and Ethnic Bias and Potential Solutions to Address Bias With Use of Health Care Algorithms,2023,Anjali Jain; Jasmin R. Brooks; Cleothia C. Alford; Christine Chang; Nora Mueller; Craig A. Umscheid; Arlene S. Bierman,JAMA Health Forum,51,W4379094721,10.1001/jamahealthforum.2023.1197,https://openalex.org/W4379094721,https://jamanetwork.com/journals/jama-health-forum/articlepdf/2805595/jain_2023_oi_230028_1685467552.69689.pdf,Ethnic group; Health care; Algorithm; Affect (linguistics); Health information technology,article,True,"Importance Algorithms are commonly incorporated into health care decision tools used by health systems and payers and thus affect quality of care, access, and health outcomes. Some algorithms include a patient’s race or ethnicity among their inputs and can lead clinicians and decision-makers to make choices that vary by race and potentially affect inequities. Objective To inform an evidence review on the use of race- and ethnicity-based algorithms in health care by gathering public and stakeholder perspectives about the repercussions of and efforts to address algorithm-related bias. Design, Setting, and Participants Qualitative methods were used to analyze responses. Responses were initially open coded and then consolidated to create a codebook, with themes and subthemes identified and finalized by consensus. This qualitative study was conducted from May 4, 2021, through December 7, 2022. Forty-two organization representatives (eg, clinical professional societies, universities, government agencies, payers, and health technology organizations) and individuals responded to the request for information. Main Outcomes and Measures Identification of algorithms with the potential for race- and ethnicity-based biases and qualitative themes. Results Forty-two respondents identified 18 algorithms currently in use with the potential for bias, including, for example, the Simple Calculated Osteoporosis Risk Estimation risk prediction tool and the risk calculator for vaginal birth after cesarean section. The 7 qualitative themes, with 31 subthemes, included the following: (1) algorithms are in widespread use and have significant repercussions, (2) bias can result from algorithms whether or not they explicitly include race, (3) clinicians and patients are often unaware of the use of algorithms and potential for bias, (4) race is a social construct used as a proxy for clinical variables, (5) there is a lack of standardization in how race and social determinants of health are collected and defined, (6) bias can be introduced at all stages of algorithm development, and (7) algorithms should be discussed as part of shared decision-making between the patient and clinician. Conclusions and Relevance This qualitative study found that participants perceived widespread and increasing use of algorithms in health care and lack of oversight, potentially exacerbating racial and ethnic inequities. Increasing awareness for clinicians and patients and standardized, transparent approaches for algorithm development and implementation may be needed to address racial and ethnic biases related to algorithms."
Detection and Mitigation of Algorithmic Bias via Predictive Parity,2023,Cyrus DiCiccio; Brian Hsu; YinYin Yu; Preetam Nandy; Kinjal Basu,"2022 ACM Conference on Fairness, Accountability, and Transparency",5,W4379089795,10.1145/3593013.3594117,https://openalex.org/W4379089795,https://doi.org/10.1145/3593013.3594117,Computer science; Parametric statistics; Parity (physics); Calibration; Transformation (genetics),article,True,"Predictive parity (PP), also known as sufficiency, is a core definition of algorithmic fairness essentially stating that model outputs must have the same interpretation of expected outcomes regardless of group. Testing and satisfying PP is especially important in many settings where model scores are interpreted by humans or directly provide access to opportunity, such as healthcare or banking. Solutions for PP violations have primarily been studied through the lens of model calibration. However, we find that existing calibration-based tests and mitigation methods are designed for independent data, which is often not assumable in large-scale applications such as social media or medical testing. In this work, we address this issue by developing a statistically rigorous non-parametric regression based test for PP with dependent observations. We then apply our test to illustrate that PP testing can significantly vary under the two assumptions. Lastly, we provide a mitigation solution to provide a minimally-biased post-processing transformation function to achieve PP."
Algorithmic bias: the state of the situation and policy recommendations,2023,OECD,Digital education outlook,3,W4400709265,10.1787/09e55ac4-en,https://openalex.org/W4400709265,,State (computer science); Computer science; Race (biology); Gender bias; Data science,book-chapter,False,"This chapter discusses the current state of the evidence on algorithmic bias in education. After defining algorithmic bias and its possible origins, it reviews the existing international evidence about algorithmic bias in education, which has focused on gender and race, but has also involved some other demographic categories. The chapter concludes with a few recommendations, notably to ensure that privacy requirements do not prevent researchers and developers from identifying bias, so that it can be addressed."
Hierarchical Dependencies in Classroom Settings Influence Algorithmic Bias Metrics,2024,Clara Belitz; HaeJin Lee; Nidhi Nasiar; Stephen Fancsali; STEVE RITTER; Husni Almoubayyed; Ryan S. Baker; Jaclyn Ocumpaugh; Nigel Bosch,,4,W4392445400,10.1145/3636555.3636869,https://openalex.org/W4392445400,https://dl.acm.org/doi/pdf/10.1145/3636555.3636869,Computer science; Multilevel model; Machine learning; Artificial intelligence; Contrast (vision),article,True,"Measuring algorithmic bias in machine learning has historically focused on statistical inequalities pertaining to specific groups. However, the most common metrics (i.e., those focused on individual- or group-conditioned error rates) are not currently well-suited to educational settings because they assume that each individual observation is independent from the others. This is not statistically appropriate when studying certain common educational outcomes, because such metrics cannot account for the relationship between students in classrooms or multiple observations per student across an academic year. In this paper, we present novel adaptations of algorithmic bias measurements for regression for both independent and nested data structures. Using hierarchical linear models, we rigorously measure algorithmic bias in a machine learning model of the relationship between student engagement in an intelligent tutoring system and year-end standardized test scores. We conclude that classroom-level influences had a small but significant effect on models. Examining significance with hierarchical linear models helps determine which inequalities in educational settings might be explained by small sample sizes rather than systematic differences."
"Evidence of What, for Whom? The Socially Contested Role of Algorithmic Bias in a Predictive Policing Tool",2024,Marta Ziosi; Dasha Pruss,"2022 ACM Conference on Fairness, Accountability, and Transparency",5,W4399365130,10.1145/3630106.3658991,https://openalex.org/W4399365130,https://dl.acm.org/doi/pdf/10.1145/3630106.3658991,Cognitive reframing; Criminal justice; Psychological intervention; Context (archaeology); Politics,article,True,"This paper presents a critical, qualitative study of the social role of algorithmic bias in the context of the Chicago crime prediction algorithm, a predictive policing tool that forecasts when and where in the city crime is most likely to occur. Through interviews with 18 Chicago-area community organizations, academic researchers, and public sector actors, we show that stakeholders from different groups articulate diverse problem diagnoses of the tool's algorithmic bias, strategically using it as evidence to advance criminal justice interventions that align with stakeholders' positionality and political ends. Drawing inspiration from Catherine D'Ignazio's taxonomy of ""refusing and using"" data, we find that stakeholders use evidence of algorithmic bias to reform the policies around police patrol allocation; reject algorithm-based policing interventions; reframe crime as a structural rather than interpersonal problem; reveal data on authority figures in an effort to subvert their power; repair and heal families and communities; and, in the case of more powerful actors, to reaffirm their own authority or existing power structures. We identify the implicit assumptions and scope of these varied uses of algorithmic bias as evidence, showing that they require different (and sometimes conflicting) values about policing and AI. This divergence reflects long-standing tensions in the criminal justice reform landscape between the values of liberation and healing often centered by system-impacted communities and the values of surveillance and deterrence often instantiated in data-driven reform measures. We advocate for centering the interests and experiential knowledge of communities impacted by incarceration to ensure that evidence of algorithmic bias can serve as a device to challenge the status quo."
Algorithmic Bias in News: Can Machine Learning Be Part of the Solution?,2023,Haffaz Aladeen; Nadal Burgers; Tamir Imaad,Authorea (Authorea),6,W4322724920,10.22541/au.167770658.88067473/v1,https://openalex.org/W4322724920,https://doi.org/10.22541/au.167770658.88067473/v1,Computer science; Artificial intelligence; Machine learning; Data science,preprint,True,
"Whither bias goes, I will go: An integrative, systematic review of algorithmic bias mitigation.",2024,Louis Hickman; Christopher Huynh; Jessica Gass; Brandon M. Booth; Jason Kuruzovich; Louis Tay,Journal of Applied Psychology,4,W4404355635,10.1037/apl0001255,https://openalex.org/W4404355635,https://doi.org/10.1037/apl0001255,Selection bias; Psychology; Selection (genetic algorithm); Computer science; Management science,review,True,"Machine learning (ML) models are increasingly used for personnel assessment and selection (e.g., resume screeners, automatically scored interviews). However, concerns have been raised throughout society that ML assessments may be biased and perpetuate or exacerbate inequality. Although organizational researchers have begun investigating ML assessments from traditional psychometric and legal perspectives, there is a need to understand, clarify, and integrate fairness operationalizations and algorithmic bias mitigation methods from the computer science, data science, and organizational research literatures. We present a four-stage model of developing ML assessments and applying bias mitigation methods, including 1) generating the training data, 2) training the model, 3) testing the model, and 4) deploying the model. When introducing the four-stage model, we describe potential sources of bias and unfairness at each stage. Then, we systematically review definitions and operationalizations of algorithmic bias, legal requirements governing personnel selection from the United States and Europe, and research on algorithmic bias mitigation across multiple domains and integrate these findings into our framework. Our review provides insights for both research and practice by elucidating possible mechanisms of algorithmic bias while identifying which bias mitigation methods are legal and effective. This integrative framework also reveals gaps in the knowledge of algorithmic bias mitigation that should be addressed by future collaborative research between organizational researchers, computer scientists, and data scientists. We provide recommendations for developing and deploying ML assessments, as well as recommendations for future research into algorithmic bias and fairness."
Putting algorithmic bias on top of the agenda in the discussions on autonomous weapons systems,2024,Ishmael Bhila,Digital War,4,W4399209593,10.1057/s42984-024-00094-z,https://openalex.org/W4399209593,https://link.springer.com/content/pdf/10.1057/s42984-024-00094-z.pdf,Computer science; Political science; Computer security; Epistemology; Philosophy,article,True,"Abstract Biases in artificial intelligence have been flagged in academic and policy literature for years. Autonomous weapons systems—defined as weapons that use sensors and algorithms to select, track, target, and engage targets without human intervention—have the potential to mirror systems of societal inequality which reproduce algorithmic bias. This article argues that the problem of engrained algorithmic bias poses a greater challenge to autonomous weapons systems developers than most other risks discussed in the Group of Governmental Experts on Lethal Autonomous Weapons Systems (GGE on LAWS), which should be reflected in the outcome documents of these discussions. This is mainly because it takes longer to rectify a discriminatory algorithm than it does to issue an apology for a mistake that occurs occasionally. Highly militarised states have controlled both the discussions and their outcomes, which have focused on issues that are pertinent to them while ignoring what is existential for the rest of the world. Various calls from civil society, researchers, and smaller states for a legally binding instrument to regulate the development and use of autonomous weapons systems have always included the call for recognising algorithmic bias in autonomous weapons, which has not been reflected in discussion outcomes. This paper argues that any ethical framework developed for the regulation of autonomous weapons systems should, in detail, ensure that the development and use of autonomous weapons systems do not prejudice against vulnerable sections of (global) society."
Postcolonial Differentials in Algorithmic Bias: Challenging Digital Neo-Colonialism in Africa,2023,Sunita Menon,SCRIPTed A Journal of Law Technology & Society,4,W4386867404,10.2218/scrip.20.2.2023.8980,https://openalex.org/W4386867404,http://journals.ed.ac.uk/script-ed/article/download/8980/11926,Colonialism; Cognitive reframing; Parallels; Neocolonialism; Neutrality,article,True,"As digital technologies become the dominant driver of the global economy, Africa finds itself once again faced with the prospect of developmental stagnation. In an increasingly technological age, parallels to the colonial era can be made, particularly in reference to the detrimental impact on the African economy and the continent’s developmental trajectory. AI, which drives these technologies, is informed by algorithms. The biases inherent in these algorithms lead to digital discrimination. This discrimination has resulted in a new form of colonialism, referred to as digital neocolonialism, which denotes the exclusionary barrier that has been created by algorithms. This work challenges algorithmic bias through the application of postcolonial theory, which calls for a dismantling of colonial imposition by reimagining and reframing the concept of the ‘other’. The gaps in current AI systems, and the power imbalances created, are interrogated through an analysis of bias and its impact. Through a postcolonial lens, a call is made for more inclusive AI systems, and datasets that challenges the assumed neutrality of algorithms."
"On the Algorithmic Bias of Aligning Large Language Models with RLHF:
  Preference Collapse and Matching Regularization",2024,Jiancong Xiao; Ziniu Li; Xingyu Xie; Emily Getzen; Cong Fang; Long Qi; Weijie Su,arXiv (Cornell University),5,W4399115638,10.48550/arxiv.2405.16455,https://openalex.org/W4399115638,http://arxiv.org/pdf/2405.16455,Regularization (linguistics); Matching (statistics); Preference; Econometrics; Computer science,preprint,True,"Accurately aligning large language models (LLMs) with human preferences is crucial for informing fair, economically sound, and statistically efficient decision-making processes. However, we argue that reinforcement learning from human feedback (RLHF) -- the predominant approach for aligning LLMs with human preferences through a reward model -- suffers from an inherent algorithmic bias due to its Kullback--Leibler-based regularization in optimization. In extreme cases, this bias could lead to a phenomenon we term preference collapse, where minority preferences are virtually disregarded. To mitigate this algorithmic bias, we introduce preference matching (PM) RLHF, a novel approach that provably aligns LLMs with the preference distribution of the reward model under the Bradley--Terry--Luce/Plackett--Luce model. Central to our approach is a PM regularizer that takes the form of the negative logarithm of the LLM's policy probability distribution over responses, which helps the LLM balance response diversification and reward maximization. Notably, we obtain this regularizer by solving an ordinary differential equation that is necessary for the PM property. For practical implementation, we introduce a conditional variant of PM RLHF that is tailored to natural language generation. Finally, we empirically validate the effectiveness of conditional PM RLHF through experiments on the OPT-1.3B and Llama-2-7B models, demonstrating a 29% to 41% improvement in alignment with human preferences, as measured by a certain metric, compared to standard RLHF."
"What Goes In, Must Come Out: Generative Artificial Intelligence Does Not Present Algorithmic Bias Across Race and Gender in Medical Residency Specialties",2024,Lin Shu; Saket Pandit; Tara Tritsch; Arkene Levy; Mohammadali M. Shoja,Cureus,8,W4391930049,10.7759/cureus.54448,https://openalex.org/W4391930049,https://assets.cureus.com/uploads/original_article/pdf/215498/20240219-2672-fzq8af.pdf,Specialty; Transformative learning; Race (biology); Diversity (politics); Generative grammar,article,True,"Objective Artificial Intelligence (AI) has made significant inroads into various domains, including medicine, raising concerns about algorithmic bias. This study investigates the presence of biases in generative AI programs, with a specific focus on gender and racial representations across 19 medical residency specialties. Methodology This comparative study utilized DALL-E2 to generate faces representing 19 distinct residency training specialties, as identified by the Association of American Medical Colleges (AAMC), which were then compared to the AAMC's residency specialty breakdown with respect to race and gender. Results Our findings reveal an alignment between OpenAI's DALL-E2's predictions and the current demographic landscape of medical residents, suggesting an absence of algorithmic bias in this AI model. Conclusion This revelation gives rise to important ethical considerations. While AI excels at pattern recognition, it inherits and mirrors the biases present in its training data. To combat AI bias, addressing real-world disparities is imperative. Initiatives to promote inclusivity and diversity within medicine are commendable and contribute to reshaping medical education. This study underscores the need for ongoing efforts to dismantle barriers and foster inclusivity in historically male-dominated medical fields, particularly for underrepresented populations. Ultimately, our findings underscore the crucial role of real-world data quality in mitigating AI bias. As AI continues to shape healthcare and education, the pursuit of equitable, unbiased AI applications should remain at the forefront of these transformative endeavors."
(Some) algorithmic bias as institutional bias,2023,Camila Hernandez Flowerman,Ethics and Information Technology,3,W4328108935,10.1007/s10676-023-09698-7,https://openalex.org/W4328108935,,Computer science; Positive economics; Mathematical economics; Econometrics; Epistemology,article,False,
Artificial intelligence and algorithmic bias? Field tests on social network with teens,2024,Grazia Cecere; Clara Jean; Fabrice Le Guel; Matthieu Manant,Technological Forecasting and Social Change,6,W4391074079,10.1016/j.techfore.2023.123204,https://openalex.org/W4391074079,https://www.sciencedirect.com/science/article/am/pii/S0040162523008892,Gender bias; Categorization; Visibility; Computer science; Field (mathematics),article,True,
Prevention of Bias and Discrimination in Clinical Practice Algorithms,2023,Carmel Shachar; Sara Gerke,JAMA,44,W4313545847,10.1001/jama.2022.23867,https://openalex.org/W4313545847,,Medicine; Liability; Health care; Algorithm; MEDLINE,article,False,This Viewpoint discusses a proposed DHHS rule to address discrimination in clinical algorithms and the need for additional considerations to ensure the burden of liability for biased algorithms is not disproportionately placed on health care professionals.
Social media and volunteer rescue requests prediction with random forest and algorithm bias detection: a case of Hurricane Harvey,2023,Volodymyr Mihunov; Kejin Wang; Zheye Wang; Nina Lam; Mingxuan Sun,Environmental Research Communications,8,W4380537468,10.1088/2515-7620/acde35,https://openalex.org/W4380537468,https://iopscience.iop.org/article/10.1088/2515-7620/acde35/pdf,Random forest; Flood myth; Stalking; Index (typography); Computer science,article,True,"Abstract AI fairness is tasked with evaluating and mitigating bias in algorithms that may discriminate towards protected groups. This paper examines if bias exists in AI algorithms used in disaster management and in what manner. We consider the 2017 Hurricane Harvey when flood victims in Houston resorted to social media to request for rescue. We evaluate a Random Forest regression model trained to predict Twitter rescue request rates from social-environmental data using three fairness criteria (independence, separation, and sufficiency). The Social Vulnerability Index (SVI), its four sub-indices, and four variables representing digital divide were considered sensitive attributes. The Random Forest regression model extracted seven significant predictors of rescue request rates, and from high to low importance they were percent of renter occupied housing units, percent of roads in flood zone, percent of flood zone area, percent of wetland cover, percent of herbaceous, forested and shrub cover, mean elevation, and percent of households with no computer or device. Partial Dependence plots of rescue request rates against each of the seven predictors show the non-linear nature of their relationships. Results of the fairness evaluation of the Random Forest model using the three criteria show no obvious biases for the nine sensitive attributes, except that a minor imperfect sufficiency was found with the SVI Housing and Transportation sub-index. Future AI modeling in disaster research could apply the same methodology used in this paper to evaluate fairness and help reduce unfair resource allocation and other social and geographical disparities."
Artificial Intelligence in Criminalistics and Forensic Examination: Issues of Legal Personality and Algorithmic Bias,2023,А. В. Кокин; Yu. D. Denisov,Theory and Practice of Forensic Science,4,W4385650910,10.30764/1819-2785-2023-2-30-37,https://openalex.org/W4385650910,https://www.tipse.ru/jour/article/download/775/672,Computer science; Artificial intelligence; Personality; Human intelligence; Identification (biology),article,True,"Active development and implementation of artificial intelligence technologies (AI) in various spheres of human activity have started the processes of qualitative change in public relations. This fact necessitates the development of legal and technical standards to regulate AI technologies. In this regard, the most controversial issue is the recognition of AI personality. The analysis of various opinions on the matter shows the lack of a consolidated approach in the existing legal doctrine. Creating the legal status for AI systems would provide for several options depending on its type and purpose – from technical means to the status of an “electronic personality” and recognition as a full-fledged subject of law. Considering the specifics of criminalistics and forensic examination, it is better to position AI systems as technical means. Machine learning is considered a form of AI. It is the use of mathematical data models that enables computer training through specialized algorithms and training data. Algorithms can create or reproduce distortions and inaccuracies unintentionally embedded in the training data, which causes the manifestation of algorithmic bias. To eliminate bias of algorithms it is necessary to pay attention to the quality of training data. The author has developed special methods to prepare such data, which are presented in this article in relation to ballistic identification systems. Also, one of the elements of system technical solutions to the problem of bias of AI algorithms is the development of standards for minimizing unjustified bias in algorithmic solutions."
Understanding algorithm bias in artificial intelligence-enabled ERP software customization,2023,S. Parthasarathy; S. Padmapriya,Journal of Ethics in Entrepreneurship and Technology,3,W4381376758,10.1108/jeet-04-2023-0006,https://openalex.org/W4381376758,https://www.emerald.com/insight/content/doi/10.1108/JEET-04-2023-0006/full/pdf?title=understanding-algorithm-bias-in-artificial-intelligence-enabled-erp-software-customization,Personalization; Computer science; Enterprise resource planning; Software; Artificial intelligence,article,True,"Purpose Algorithm bias refers to repetitive computer program errors that give some users more weight than others. The aim of this article is to provide a deeper insight of algorithm bias in AI-enabled ERP software customization. Although algorithmic bias in machine learning models has uneven, unfair and unjust impacts, research on it is mostly anecdotal and scattered. Design/methodology/approach As guided by the previous research (Akter et al. , 2022), this study presents the possible design bias (model, data and method) one may experience with enterprise resource planning (ERP) software customization algorithm. This study then presents the artificial intelligence (AI) version of ERP customization algorithm using k-nearest neighbours algorithm. Findings This study illustrates the possible bias when the prioritized requirements customization estimation (PRCE) algorithm available in the ERP literature is executed without any AI. Then, the authors present their newly developed AI version of the PRCE algorithm that uses ML techniques. The authors then discuss its adjoining algorithmic bias with an illustration. Further, the authors also draw a roadmap for managing algorithmic bias during ERP customization in practice. Originality/value To the best of the authors’ knowledge, no prior research has attempted to understand the algorithmic bias that occurs during the execution of the ERP customization algorithm (with or without AI)."
"Cognitively Biased Users Interacting with Algorithmically Biased Results
  in Whole-Session Search on Debated Topics",2024,Ben Wang; Jiqun Liu,arXiv (Cornell University),4,W4393284545,10.1145/3664190.3672520,https://openalex.org/W4393284545,https://arxiv.org/pdf/2403.17286,Session (web analytics); Crowdsourcing; Openness to experience; Confirmation bias; Computer science,preprint,True,"When interacting with information retrieval (IR) systems, users, affected by confirmation biases, tend to select search results that confirm their existing beliefs on socially significant contentious issues. To understand the judgments and attitude changes of users searching online, our study examined how cognitively biased users interact with algorithmically biased search engine result pages (SERPs). We designed three-query search sessions on debated topics under various bias conditions. We recruited 1,321 crowdsourcing participants and explored their attitude changes, search interactions, and the effects of confirmation bias. Three key findings emerged: 1) most attitude changes occur in the initial query of a search session; 2) Confirmation bias and result presentation on SERPs affect the number and depth of clicks in the current query and perceived familiarity with clicked results in subsequent queries; 3) The bias position also affects attitude changes of users with lower perceived openness to conflicting opinions. Our study goes beyond traditional simulation-based evaluation settings and simulated rational users, sheds light on the mixed effects of human biases and algorithmic biases in information retrieval tasks on debated topics, and can inform the design of bias-aware user models, human-centered bias mitigation techniques, and socially responsible intelligent IR systems."
Tackling Algorithmic Bias and Promoting Transparency in Health Datasets: The STANDING Together Consensus Recommendations,2024,Joseph Alderman; Joanne Palmer; Elinor Laws; Melissa D. McCradden; Johan Ordish; Marzyeh Ghassemi; Stephen Pfohl; Negar Rostamzadeh; Heather Cole-Lewis; Ben Glocker; Melanie Calvert; Tom Pollard; Jaspret Gill; Jacqui Gath; Ade Adebajo; Jude Beng; Cheuk Wing Leung; Stephanie Kuku; L. J. Farmer; Rubeta Matin; Bilal A. Mateen; Francis McKay; Katherine Heller; Alan Karthikesalingam; Darren Treanor; Maxine Mackintosh; Lauren Oakden‐Rayner; Russell Pearson; Arjun K. Manrai; Puja Myles; Judit Kumuthini; Zoher Kapacee; Neil J. Sebire; Lama Nazer; Jarrel Seah; Ashley Akbari; Lewis E. Berman; Judy Wawira Gichoya; Lorenzo Righetto; Diana Samuel; William Wasswa; Maria Charalambides; Anmol Arora; Sameer Pujari; Charlotte Summers; Elizabeth Sapey; Stephen Wilkinson; Vishal Thakker; Alastair K. Denniston; Xiaoxuan Liu,NEJM AI,5,W4405533437,10.1056/aip2401088,https://openalex.org/W4405533437,,Transparency (behavior); Data science; Computer science; Internet privacy; Computer security,article,False,"Without careful dissection of the ways in which biases can be encoded into artificial intelligence (AI) health technologies, there is a risk of perpetuating existing health inequalities at scale. One major source of bias is the data that underpins such technologies. The STANDING Together recommendations aim to encourage transparency regarding limitations of health datasets and proactive evaluation of their effect across population groups. Draft recommendation items were informed by a systematic review and stakeholder survey. The recommendations were developed using a Delphi approach, supplemented by a public consultation and international interview study. Overall, more than 350 representatives from 58 countries provided input into this initiative. 194 Delphi participants from 25 countries voted and provided comments on 32 candidate items across three electronic survey rounds and one in-person consensus meeting. The 29 STANDING Together consensus recommendations are presented here in two parts. Recommendations for Documentation of Health Datasets provide guidance for dataset curators to enable transparency around data composition and limitations. Recommendations for Use of Health Datasets aim to enable identification and mitigation of algorithmic biases that might exacerbate health inequalities. These recommendations are intended to prompt proactive inquiry rather than acting as a checklist. We hope to raise awareness that no dataset is free of limitations, so transparent communication of data limitations should be perceived as valuable, and absence of this information as a limitation. We hope that adoption of the STANDING Together recommendations by stakeholders across the AI health technology lifecycle will enable everyone in society to benefit from technologies which are safe and effective. (Funded by The NHS AI Lab and The Health Foundation, and supported by the National Institute for Health and Care Research [NIHR].)"
Algorithmic Bias in Criminal Risk Assessment: The Consequences of Racial Differences in Arrest as a Measure of Crime,2024,Roland Neil; Michael Zanger-Tishler,Annual Review of Criminology,4,W4402250311,10.1146/annurev-criminol-022422-125019,https://openalex.org/W4402250311,,Measure (data warehouse); Criminology; Psychology; Actuarial science; Computer science,article,False,"There is great concern about algorithmic racial bias in the risk assessment instruments (RAIs) used in the criminal legal system. When testing for algorithmic bias, most research effectively uses arrest data as an unbiased measure of criminal offending, which collides with longstanding concerns that arrest is a biased proxy of offending. Given the centrality of arrest data in RAIs, racial differences in how arrest proxies offending may be a key pathway through which RAIs become biased. In this review, we evaluate the extensive body of research on racial differences in arrest as a measure of crime. Furthermore, we detail several ways that racial bias in arrest records could create algorithmic bias, although little research has attempted to measure the degree of algorithmic bias generated by using racially biased arrest records. We provide a roadmap to assist future research in understanding the impact of biased arrest records on RAIs."
Standards for the Control of Algorithmic Bias,2023,Natalie Heisler; Maura R. Grossman,,2,W4381735403,10.1201/b23364,https://openalex.org/W4381735403,,Computer science; Control (management); Artificial intelligence,book,False,"Governments around the world use machine learning in automated decision-making systems for a broad range of functions. However, algorithmic bias in machine learning can result in automated decisions that produce disparate impact and may compromise Charter guarantees of substantive equality. This book seeks to answer the question: what standards should be applied to machine learning to mitigate disparate impact in government use of automated decision-making? The regulatory landscape for automated decision-making, in Canada and across the world, is far from settled. Legislative and policy models are emerging, and the role of standards is evolving to support regulatory objectives. While acknowledging the contributions of leading standards development organizations, the authors argue that the rationale for standards must come from the law and that implementing such standards would help to reduce future complaints by, and would proactively enable human rights protections for, those subject to automated decision-making. The book presents a proposed standards framework for automated decision-making and provides recommendations for its implementation in the context of the government of Canada's Directive on Automated Decision-Making. As such, this book can assist public agencies around the world in developing and deploying automated decision-making systems equitably as well as being of interest to businesses that utilize automated decision-making processes."
The Sociodemographic Biases in Machine Learning Algorithms: A Biomedical Informatics Perspective,2024,Gillian Franklin; Rachel Stephens; Muhammad Piracha; Shmuel Tiosano; Frank LeHouillier; Ross Koppel; Peter L. Elkin,Life,27,W4398169659,10.3390/life14060652,https://openalex.org/W4398169659,https://www.mdpi.com/2075-1729/14/6/652/pdf?version=1716284761,Socioeconomic status; Machine learning; Health care; Computer science; Selection bias,article,True,"Artificial intelligence models represented in machine learning algorithms are promising tools for risk assessment used to guide clinical and other health care decisions. Machine learning algorithms, however, may house biases that propagate stereotypes, inequities, and discrimination that contribute to socioeconomic health care disparities. The biases include those related to some sociodemographic characteristics such as race, ethnicity, gender, age, insurance, and socioeconomic status from the use of erroneous electronic health record data. Additionally, there is concern that training data and algorithmic biases in large language models pose potential drawbacks. These biases affect the lives and livelihoods of a significant percentage of the population in the United States and globally. The social and economic consequences of the associated backlash cannot be underestimated. Here, we outline some of the sociodemographic, training data, and algorithmic biases that undermine sound health care risk assessment and medical decision-making that should be addressed in the health care system. We present a perspective and overview of these biases by gender, race, ethnicity, age, historically marginalized communities, algorithmic bias, biased evaluations, implicit bias, selection/sampling bias, socioeconomic status biases, biased data distributions, cultural biases and insurance status bias, conformation bias, information bias and anchoring biases and make recommendations to improve large language model training data, including de-biasing techniques such as counterfactual role-reversed sentences during knowledge distillation, fine-tuning, prefix attachment at training time, the use of toxicity classifiers, retrieval augmented generation and algorithmic modification to mitigate the biases moving forward."
Effects of Racial Bias in Pulse Oximetry on Children and How to Address Algorithmic Bias in Clinical Medicine,2023,Keyaria D. Gray; Hamsa L. Subramaniam; Erich Huang,JAMA Pediatrics,6,W4327893915,10.1001/jamapediatrics.2023.0077,https://openalex.org/W4327893915,,Medicine; Otorhinolaryngology; Family medicine; Neurology; MEDLINE,letter,False,"Our website uses cookies to enhance your experience. By continuing to use our site, or clicking ""Continue,"" you are agreeing to our Cookie Policy | Continue JAMA Pediatrics HomeNew OnlineCurrent IssueFor Authors Podcast Journals JAMA JAMA Network Open JAMA Cardiology JAMA Dermatology JAMA Health Forum JAMA Internal Medicine JAMA Neurology JAMA Oncology JAMA Ophthalmology JAMA Otolaryngology–Head & Neck Surgery JAMA Pediatrics JAMA Psychiatry JAMA Surgery Archives of Neurology & Psychiatry (1919-1959) JN Learning / CMESubscribeJobsInstitutions / LibrariansReprints & Permissions Terms of Use | Privacy Policy | Accessibility Statement 2023 American Medical Association. All Rights Reserved Search All JAMA JAMA Network Open JAMA Cardiology JAMA Dermatology JAMA Forum Archive JAMA Health Forum JAMA Internal Medicine JAMA Neurology JAMA Oncology JAMA Ophthalmology JAMA Otolaryngology–Head & Neck Surgery JAMA Pediatrics JAMA Psychiatry JAMA Surgery Archives of Neurology & Psychiatry Input Search Term Sign In Individual Sign In Sign inCreate an Account Access through your institution Sign In Purchase Options: Buy this article Rent this article Subscribe to the JAMA Pediatrics journal"
Encoding normative ethics: On algorithmic bias and disability,2023,Ian Moura,First Monday,2,W4317038352,10.5210/fm.v28i1.12905,https://openalex.org/W4317038352,https://firstmonday.org/ojs/index.php/fm/article/download/12905/10789,Ableism; Harm; Normative; Context (archaeology); Psychological intervention,article,True,"Computer-based algorithms have the potential to encode and exacerbate ableism and may contribute to disparate outcomes for disabled people. The threat of algorithmic bias to people with disabilities is inseparable from the longstanding role of technology as a normalizing agent, and from questions of how society defines shared values, quantifies ethics, conceptualizes and measures risk, and strives to allocate limited resources. This article situates algorithmic bias amidst the larger context of normalization, draws on social and critical theories that can be used to better understand both ableism and algorithmic bias as they operate in the United States, and proposes concrete steps to mitigate harm to the disability community as a result of algorithmic adoption. Examination of two cases — the allocation of lifesaving medical interventions during the COVID-19 pandemic and approaches to autism diagnosis and intervention — demonstrate instances of the mismatch between disabled people’s lived experiences and the goals and understandings advanced by nondisabled people. These examples highlight the ways particular ethical norms can become part of technological systems, and the harm that can ripple outward from misalignment of formal ethics and community values."
Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation,2023,Hao Liang; Pietro Perona; Guha Balakrishnan,2021 IEEE/CVF International Conference on Computer Vision (ICCV),6,W4390872687,10.1109/iccv51070.2023.00459,https://openalex.org/W4390872687,https://arxiv.org/pdf/2308.05441,Benchmarking; Computer science; Artificial intelligence; Facial recognition system; Face (sociological concept),article,True,"We propose an experimental method for measuring bias in face recognition systems. Existing methods to measure bias depend on benchmark datasets that are collected in the wild and annotated for protected (e.g., race, gender) and unprotected (e.g., pose, lighting) attributes. Such observational datasets only permit correlational conclusions, e.g., ""Algorithm A's accuracy is different on female and male faces in dataset X."". By contrast, experimental methods manipulate attributes individually and thus permit causal conclusions, e.g., ""Algorithm A's accuracy is affected by gender and skin color.""Our method is based on generating synthetic faces using a neural face generator, where each attribute of interest is modified independently while leaving all other attributes constant. Human observers crucially provide the ground truth on perceptual identity similarity between synthetic image pairs. We validate our method quantitatively by evaluating race and gender biases of three research-grade face recognition models. Our synthetic pipeline reveals that for these algorithms, accuracy is lower for Black and East Asian population subgroups. Our method can also quantify how perceptual changes in attributes affect face identity distances reported by these models. Our large synthetic dataset, consisting of 48,000 synthetic face image pairs (10,200 unique synthetic faces) and 555,000 human annotations (individual attributes and pairwise identity comparisons) is available to researchers in this important area."
Identifying and Mitigating Algorithmic Bias in Student Emotional Analysis,2024,T. S. Ashwin; Gautam Biswas,Lecture notes in computer science,4,W4400210740,10.1007/978-3-031-64299-9_7,https://openalex.org/W4400210740,,Computer science; Artificial intelligence,book-chapter,False,
The formal rationality of artificial intelligence-based algorithms and the problem of bias,2023,Rohit Nishant; Dirk Schneckenberg; M. N. Ravishankar,Journal of Information Technology,45,W4378672952,10.1177/02683962231176842,https://openalex.org/W4378672952,https://journals.sagepub.com/doi/pdf/10.1177/02683962231176842,Rationality; Bounded rationality; Computer science; Artificial intelligence; Context (archaeology),article,True,"This paper presents a new perspective on the problem of bias in artificial intelligence (AI)-driven decision-making by examining the fundamental difference between AI and human rationality in making sense of data. Current research has focused primarily on software engineers’ bounded rationality and bias in the data fed to algorithms but has neglected the crucial role of algorithmic rationality in producing bias. Using a Weberian distinction between formal and substantive rationality, we inquire why AI-based algorithms lack the ability to display common sense in data interpretation, leading to flawed decisions. We first conduct a rigorous text analysis to uncover and exemplify contextual nuances within the sampled data. We then combine unsupervised and supervised learning, revealing that algorithmic decision-making characterizes and judges data categories mechanically as it operates through the formal rationality of mathematical optimization procedures. Next, using an AI tool, we demonstrate how formal rationality embedded in AI-based algorithms limits its capacity to perform adequately in complex contexts, thus leading to bias and poor decisions. Finally, we delineate the boundary conditions and limitations of leveraging formal rationality to automatize algorithmic decision-making. Our study provides a deeper understanding of the rationality-based causes of AI’s role in bias and poor decisions, even when data is generated in a largely bias-free context."
Algorithmic Bias in De-Identification Tools,2023,Paul M. Heider,2022 IEEE 10th International Conference on Healthcare Informatics (ICHI),2,W4389543635,10.1109/ichi57859.2023.00129,https://openalex.org/W4389543635,,Identification (biology); Race (biology); Computer science; Racial bias; Parity (physics),article,False,"We present a series of experiments designed to quantify the algorithmic bias inherent in six off-the-shelf de-identification systems. We used false negative rate (FNR) and true positive rate (TPR) parity measures in addition to F <inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</inf> -score to evaluate the systems across two environments. In the first condition, we inferred gender and race/ethnicity labels for a pre-existing de-identification corpus and analyzed performance on disaggregated subgroups. In the second condition, we controlled the dominant race/ethnicity bias for names used as realistic surrogates for resynthesizing a different pre-existing de-identification corpus. In both conditions, we found cases of strong bias and near-zero bias."
Echo Chambers and Algorithmic Bias: The Homogenization of Online Culture in a Smart Society,2024,Salsa Della Guitara Putri; Eko Priyo Purnomo; Tiara Khairunissa,SHS Web of Conferences,3,W4404386672,10.1051/shsconf/202420205001,https://openalex.org/W4404386672,https://doi.org/10.1051/shsconf/202420205001,Homogenization (climate); Echo (communications protocol); Computer science; Materials science; Art,article,True,"The rise of smart societies, characterized by extensive use of technology and data-driven algorithms, promises to improve our lives. However, this very technology presents a potential threat to the richness and diversity of online culture. This thesis explores the phenomenon of echo chambers and algorithmic bias, examining how they contribute to the homogenization of online experiences. Social media algorithms personalize content feeds, presenting users with information that reinforces their existing beliefs. This creates echo chambers, where users are isolated from diverse viewpoints. Algorithmic bias, stemming from the data used to train these algorithms, can further exacerbate this issue. The main data in this study were sourced from previous studies (secondary data) which focused on research related homogenizing on online culture. The thesis investigates the impact of echo chambers and algorithmic bias on online culture within smart societies. It explores how these factors limit exposure to a variety of ideas and perspectives, potentially leading to a homogenized online experience. By examining the interplay between echo chambers, algorithmic bias, and the homogenization of online culture in smart societies, this thesis aims to contribute to a more nuanced understanding of the impact of technology on our online experiences."
Can Algorithm Knowledge Stop Women from Being Targeted by Algorithm Bias? The New Digital Divide on Weibo,2023,Yang Zhang; Huashan Chen,Journal of Broadcasting & Electronic Media,4,W4380224377,10.1080/08838151.2023.2218955,https://openalex.org/W4380224377,,Perspective (graphical); Computer science; Algorithm; Gender bias; Artificial intelligence,article,False,"Algorithm knowledge of users plays a crucial role in avoiding them from algorithm bias in recommendation systems. Gender of users has been found to correlate with algorithm bias, but also leaving behind a question of whether this relationship can be described by algorithm knowledge. By using Weibo as an example system, we clarify the aforementioned question from a digital divide theory perspective. We combine a traditional method (questionnaire) with a deep learning computational method to explain algorithm bias in two sequential studies. Our findings suggest that algorithm knowledge solely works for men while fails to protect women. Who users follow helps determine what information they are exposed to on Weibo, and this renders female users’ algorithm knowledge useless. This work provides a valuable perspective on algorithm bias: we view algorithm bias as a new digital divide and contribute to the understanding of gender differences by applying the digital divide perspective. Methodologically, we contribute by integrating traditional and computational methods to explain algorithm bias from a folk theory perspective."
Generative AI in Writing Research Papers: A New Type of Algorithmic Bias and Uncertainty in Scholarly Work,2024,Rishab Jain; Aditya Jain,Lecture notes in networks and systems,7,W4401132252,10.1007/978-3-031-66329-1_42,https://openalex.org/W4401132252,,Generative grammar; Computer science; Work (physics); Artificial intelligence; Data science,book-chapter,False,
Ethical Considerations of Using ChatGPT in Health Care,2023,Changyu Wang; Siru Liu; Hao Yang; Guo Jiu-lin; Yuxuan Wu; Jialin Liu,Journal of Medical Internet Research,375,W4385242971,10.2196/48009,https://openalex.org/W4385242971,https://www.jmir.org/2023/1/e48009/PDF,Transparency (behavior); Harm; Health care; Compassion; Liability,article,True,"ChatGPT has promising applications in health care, but potential ethical issues need to be addressed proactively to prevent harm. ChatGPT presents potential ethical challenges from legal, humanistic, algorithmic, and informational perspectives. Legal ethics concerns arise from the unclear allocation of responsibility when patient harm occurs and from potential breaches of patient privacy due to data collection. Clear rules and legal boundaries are needed to properly allocate liability and protect users. Humanistic ethics concerns arise from the potential disruption of the physician-patient relationship, humanistic care, and issues of integrity. Overreliance on artificial intelligence (AI) can undermine compassion and erode trust. Transparency and disclosure of AI-generated content are critical to maintaining integrity. Algorithmic ethics raise concerns about algorithmic bias, responsibility, transparency and explainability, as well as validation and evaluation. Information ethics include data bias, validity, and effectiveness. Biased training data can lead to biased output, and overreliance on ChatGPT can reduce patient adherence and encourage self-diagnosis. Ensuring the accuracy, reliability, and validity of ChatGPT-generated content requires rigorous validation and ongoing updates based on clinical practice. To navigate the evolving ethical landscape of AI, AI in health care must adhere to the strictest ethical standards. Through comprehensive ethical guidelines, health care professionals can ensure the responsible use of ChatGPT, promote accurate and reliable information exchange, protect patient privacy, and empower patients to make informed decisions about their health care."
A systematic review of socio-technical gender bias in AI algorithms,2023,Paula Hall; Debbie Ellis,Online Information Review,43,W4327611086,10.1108/oir-08-2021-0452,https://openalex.org/W4327611086,,Identification (biology); Computer science; Originality; Process (computing); Systematic review,review,False,"Purpose Gender bias in artificial intelligence (AI) should be solved as a priority before AI algorithms become ubiquitous, perpetuating and accentuating the bias. While the problem has been identified as an established research and policy agenda, a cohesive review of existing research specifically addressing gender bias from a socio-technical viewpoint is lacking. Thus, the purpose of this study is to determine the social causes and consequences of, and proposed solutions to, gender bias in AI algorithms. Design/methodology/approach A comprehensive systematic review followed established protocols to ensure accurate and verifiable identification of suitable articles. The process revealed 177 articles in the socio-technical framework, with 64 articles selected for in-depth analysis. Findings Most previous research has focused on technical rather than social causes, consequences and solutions to AI bias. From a social perspective, gender bias in AI algorithms can be attributed equally to algorithmic design and training datasets. Social consequences are wide-ranging, with amplification of existing bias the most common at 28%. Social solutions were concentrated on algorithmic design, specifically improving diversity in AI development teams (30%), increasing awareness (23%), human-in-the-loop (23%) and integrating ethics into the design process (21%). Originality/value This systematic review is the first of its kind to focus on gender bias in AI algorithms from a social perspective within a socio-technical framework. Identification of key causes and consequences of bias and the breakdown of potential solutions provides direction for future research and policy within the growing field of AI ethics. Peer review The peer review history for this article is available at https://publons.com/publon/10.1108/OIR-08-2021-0452"
LUCID: Exposing Algorithmic Bias through Inverse Design,2023,Carmen Mazijn; Carina Prunkl; Andres Algaba; Jan Danckaert; Vincent Ginis,Proceedings of the AAAI Conference on Artificial Intelligence,3,W4382318028,10.1609/aaai.v37i12.26683,https://openalex.org/W4382318028,https://ojs.aaai.org/index.php/AAAI/article/download/26683/26455,Toolbox; Computer science; Set (abstract data type); Focus (optics); Outcome (game theory),article,True,"AI systems can create, propagate, support, and automate bias in decision-making processes. To mitigate biased decisions, we both need to understand the origin of the bias and define what it means for an algorithm to make fair decisions. Most group fairness notions assess a model's equality of outcome by computing statistical metrics on the outputs. We argue that these output metrics encounter intrinsic obstacles and present a complementary approach that aligns with the increasing focus on equality of treatment. By Locating Unfairness through Canonical Inverse Design (LUCID), we generate a canonical set that shows the desired inputs for a model given a preferred output. The canonical set reveals the model's internal logic and exposes potential unethical biases by repeatedly interrogating the decision-making process. We evaluate LUCID on the UCI Adult and COMPAS data sets and find that some biases detected by a canonical set differ from those of output metrics. The results show that by shifting the focus towards equality of treatment and looking into the algorithm's internal workings, the canonical sets are a valuable addition to the toolbox of algorithmic fairness evaluation."
"Communicating and combating algorithmic bias: effects of data diversity, labeler diversity, performance bias, and user feedback on AI trust",2024,Cheng Chen; S. Shyam Sundar,Human-Computer Interaction,7,W4403143618,10.1080/07370024.2024.2392494,https://openalex.org/W4403143618,,Diversity (politics); Computer science; Social psychology; Psychology; Political science,article,False,
Pre-processing Techniques to Mitigate Against Algorithmic Bias,2023,Maliheh Heidarpour Shahrezaei; Róisín Loughran; Kevin Mc Daid,,3,W4392980986,10.1109/aics60730.2023.10470759,https://openalex.org/W4392980986,,Computer science,article,False,"A significant portion of current AI research is focused on ensuring that model decisions are fair and free of bias. Such research should consider not merely the algorithm but also the datasets, metrics and approaches used. In this paper, we work on several pre-processing techniques to achieve fair results for classification tasks by assigning weights, sampling and changing class labels. We used two well-known classifiers, Logistic Regression and Decision Tree, performing experiments on a popular data set in the fairness domain. This research aims to compare the effects of different pre-processing techniques on the resulting confusion matrix elements and the derived fairness metrics. We found that the Massaging technique with the Logistic regression classifier resulted in the Disparate Impact value that was closest to one. While, for the Decision Tree classifier, Reweighting and Uniform Sampling performed better than Massaging for all of our fairness metrics and both sensitive attributes."
Evaluating Algorithmic Bias in 30-Day Hospital Readmission Models: Retrospective Analysis,2024,H. Echo Wang; Jonathan P. Weiner; Suchi Saria; Hadi Kharrazi,Journal of Medical Internet Research,3,W4392314714,10.2196/47125,https://openalex.org/W4392314714,https://doi.org/10.2196/47125,Retrospective cohort study; Medicine; Computer science; Statistics; Emergency medicine,article,True,"Background The adoption of predictive algorithms in health care comes with the potential for algorithmic bias, which could exacerbate existing disparities. Fairness metrics have been proposed to measure algorithmic bias, but their application to real-world tasks is limited. Objective This study aims to evaluate the algorithmic bias associated with the application of common 30-day hospital readmission models and assess the usefulness and interpretability of selected fairness metrics. Methods We used 10.6 million adult inpatient discharges from Maryland and Florida from 2016 to 2019 in this retrospective study. Models predicting 30-day hospital readmissions were evaluated: LACE Index, modified HOSPITAL score, and modified Centers for Medicare &amp; Medicaid Services (CMS) readmission measure, which were applied as-is (using existing coefficients) and retrained (recalibrated with 50% of the data). Predictive performances and bias measures were evaluated for all, between Black and White populations, and between low- and other-income groups. Bias measures included the parity of false negative rate (FNR), false positive rate (FPR), 0-1 loss, and generalized entropy index. Racial bias represented by FNR and FPR differences was stratified to explore shifts in algorithmic bias in different populations. Results The retrained CMS model demonstrated the best predictive performance (area under the curve: 0.74 in Maryland and 0.68-0.70 in Florida), and the modified HOSPITAL score demonstrated the best calibration (Brier score: 0.16-0.19 in Maryland and 0.19-0.21 in Florida). Calibration was better in White (compared to Black) populations and other-income (compared to low-income) groups, and the area under the curve was higher or similar in the Black (compared to White) populations. The retrained CMS and modified HOSPITAL score had the lowest racial and income bias in Maryland. In Florida, both of these models overall had the lowest income bias and the modified HOSPITAL score showed the lowest racial bias. In both states, the White and higher-income populations showed a higher FNR, while the Black and low-income populations resulted in a higher FPR and a higher 0-1 loss. When stratified by hospital and population composition, these models demonstrated heterogeneous algorithmic bias in different contexts and populations. Conclusions Caution must be taken when interpreting fairness measures’ face value. A higher FNR or FPR could potentially reflect missed opportunities or wasted resources, but these measures could also reflect health care use patterns and gaps in care. Simply relying on the statistical notions of bias could obscure or underplay the causes of health disparity. The imperfect health data, analytic frameworks, and the underlying health systems must be carefully considered. Fairness measures can serve as a useful routine assessment to detect disparate model performances but are insufficient to inform mechanisms or policy changes. However, such an assessment is an important first step toward data-driven improvement to address existing health disparities."
Algorithmic Bias of Social Media,2023,Daman Preet Singh,The Motley Undergraduate Journal,2,W4388980952,10.55016/ojs/muj.v1i2.77457,https://openalex.org/W4388980952,https://journalhosting.ucalgary.ca/index.php/muj/article/download/77457/57007,Popularity; Social media; Visibility; Internet privacy; Content (measure theory),article,True,"Social media apps like YouTube and Instagram came as platforms that allowed users to express themselves freely to their friends and families, but corporations changed social media down to its core. Due to the rising popularity of short video-based content on TikTok, platforms like Instagram introduced similar content to capitalize on the hype that TikTok created. In doing so, Instagram made changes to the content promotion algorithm to promote “Reels” over the other content options. Driven by profits the company stopped caring about their users, leading to backlash from the community. Creators on the platform started playing a visibility game (Cotter, 2019) to grow and be seen in user feeds, the “game” pushes them to make content they would not be making in the first place and following trends. In this paper I am looking at the case of a creator in the photography community affected by these changes in algorithm and analyzing the situation through a critical media theory framework. The study discusses the practices of the platform and the effects on the creator community while also looking at resistance from users. I also discuss a new potential alternative platform to Instagram for photographers, that markets itself as a platform built without an algorithm, for a community."
Decoding Algorithmic Bias,2024,Ozgur Aksoy,Advances in human resources management and organizational development book series,1,W4391765373,10.4018/979-8-3693-1766-2.ch013,https://openalex.org/W4391765373,,Decoding methods; Computer science; Algorithm,book-chapter,False,"Predictive algorithms are increasingly used to assist decision-making for efficiency gains. However, it is essential to acknowledge that algorithms can mirror systemic biases in their predictions in a way that favors certain groups over others, even if they are immune to cognitive biases. The notion of algorithms generating unfair predictions is referred to as “algorithmic bias.” Addressing cognitive biases in humans might not always be an effective solution to mitigate algorithmic bias. Therefore, it is essential to understand when and how quantitative technical mitigation methods can address this issue. This chapter explores the fundamental concepts of algorithmic bias, its sources, and technical mitigation strategies. In a world where humans and AI are intertwined, it is our responsibility to ensure a fair digital future. Addressing algorithmic bias is critical to achieving this goal."
Algorithmic fairness and bias mitigation for clinical machine learning with deep reinforcement learning,2023,Jenny Yang; Andrew A. S. Soltan; David W. Eyre; David A. Clifton,Nature Machine Intelligence,71,W4385416124,10.1038/s42256-023-00697-3,https://openalex.org/W4385416124,https://www.nature.com/articles/s42256-023-00697-3.pdf,Reinforcement learning; Computer science; Reinforcement; Artificial intelligence; Computer security,article,True,"As models based on machine learning continue to be developed for healthcare applications, greater effort is needed to ensure that these technologies do not reflect or exacerbate any unwanted or discriminatory biases that may be present in the data. Here we introduce a reinforcement learning framework capable of mitigating biases that may have been acquired during data collection. In particular, we evaluated our model for the task of rapidly predicting COVID-19 for patients presenting to hospital emergency departments and aimed to mitigate any site (hospital)-specific and ethnicity-based biases present in the data. Using a specialized reward function and training procedure, we show that our method achieves clinically effective screening performances, while significantly improving outcome fairness compared with current benchmarks and state-of-the-art machine learning methods. We performed external validation across three independent hospitals, and additionally tested our method on a patient intensive care unit discharge status task, demonstrating model generalizability."
Are algorithmic bias claims supported?,2023,Solomon Messing,Science,2,W4387128670,10.1126/science.adk1211,https://openalex.org/W4387128670,,Computer science,letter,False,
Auditing Work: Exploring the New York City algorithmic bias audit regime,2024,Lara Groves; Jacob Metcalf; Alayna Kennedy; Briana Vecchione; Andrew Strait,arXiv (Cornell University),4,W4391833200,10.48550/arxiv.2402.08101,https://openalex.org/W4391833200,https://arxiv.org/pdf/2402.08101,Audit; Work (physics); Accounting; Business; Engineering,preprint,True,"In July 2023, New York City (NYC) initiated the first algorithm auditing system for commercial machine-learning systems. Local Law 144 (LL 144) mandates NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to undergo annual bias audits conducted by an independent auditor. This paper examines lessons from LL 144 for other national algorithm auditing attempts. Through qualitative interviews with 16 experts and practitioners within the regime, we find that LL 144 has not effectively established an auditing regime. The law fails to clearly define key aspects, such as AEDTs and independent auditors, leading auditors, AEDT vendors, and companies using AEDTs to define the law's practical implementation in ways that failed to protect job applicants. Contributing factors include the law's flawed transparency-driven theory of change, industry lobbying narrowing the definition of AEDTs, practical and cultural challenges faced by auditors in accessing data, and wide disagreement over what constitutes a legitimate auditor, resulting in four distinct 'auditor roles.' We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes, emphasizing clearer definitions, metrics, and increased accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, providing guidance for policymakers seeking to create similar regimes."
Addressing Algorithmic Bias in AI‐Driven HRM Systems: Implications for Strategic HRM Effectiveness,2025,Ruwan Bandara; Kumar Biswas; Shahriar Akter; Sujana Shafique; Mahfuzur Rahman,Human Resource Management Journal,3,W4410793547,10.1111/1748-8583.12609,https://openalex.org/W4410793547,https://doi.org/10.1111/1748-8583.12609,Business; Human resource management; Industrial organization; Knowledge management; Operations management,article,True,"ABSTRACT AI and machine learning algorithms are revolutionising the modern workplace by transforming HR functions to deliver superior outcomes for both employees and organisations. However, research shows that these algorithms often fail to deliver optimal HR solutions, primarily due to inherent biases. Developing capabilities to overcome algorithmic biases is critical for firms, as these biases present significant challenges to fairness and inclusivity in HR decision‐making, ultimately impacting the effectiveness of HR practices. To address this challenge, our study, grounded in the dynamic capability perspective, presents a model to address algorithmic biases in people management and achieve superior strategic HR outcomes. To test our theoretical model, we collected survey data using a two‐wave, time‐lagged approach from HR professionals and employees working in firms within the Australian financial and insurance industries. The key findings reveal three critical dimensions of HR algorithmic bias management capability: data bias, model bias, and deployment bias management capabilities, which significantly influence AI‐enabled high‐performance HR practices and, in turn, positively impact strategic HRM effectiveness. Our novel findings on the dimensions of HR bias management capability contribute to advancing the dynamic capability view in HRM research. They also offer a comprehensive bias management framework that allows HR professionals to address the strategic, ethical, and operational challenges emerging from the use of AI‐augmented HR practices in the dynamic workplace, helping sustain a competitive advantage."
Mitigating Racial And Ethnic Bias And Advancing Health Equity In Clinical Algorithms: A Scoping Review,2023,Michael P. Cary; Anna Zink; Sijia Wei; Andrew Olson; Mengying Yan; Rashaud Senior; Sophia Bessias; Kais Gadhoumi; Genevieve Jean-Pierre; Dingyue Wang; Leila Ledbetter; Nicoleta Economou-Zavlanos; Ziad Obermeyer; Michael Pencina,Health Affairs,48,W4387245432,10.1377/hlthaff.2023.00553,https://openalex.org/W4387245432,https://doi.org/10.1377/hlthaff.2023.00553,Health care; Notice; Health equity; Equity (law); Medicine,review,True,"In August 2022 the Department of Health and Human Services (HHS) issued a notice of proposed rulemaking prohibiting covered entities, which include health care providers and health plans, from discriminating against individuals when using clinical algorithms in decision making. However, HHS did not provide specific guidelines on how covered entities should prevent discrimination. We conducted a scoping review of literature published during the period 2011–22 to identify health care applications, frameworks, reviews and perspectives, and assessment tools that identify and mitigate bias in clinical algorithms, with a specific focus on racial and ethnic bias. Our scoping review encompassed 109 articles comprising 45 empirical health care applications that included tools tested in health care settings, 16 frameworks, and 48 reviews and perspectives. We identified a wide range of technical, operational, and systemwide bias mitigation strategies for clinical algorithms, but there was no consensus in the literature on a single best practice that covered entities could employ to meet the HHS requirements. Future research should identify optimal bias mitigation methods for various scenarios, depending on factors such as patient population, clinical setting, algorithm design, and types of bias to be addressed."
Comprehending Algorithmic Bias and Strategies for Fostering Trust in Artificial Intelligence,2024,Sidhi Menon U; Theresa Siby; Natchimuthu Natchimuthu,Advances in web technologies and engineering book series,2,W4391644764,10.4018/979-8-3693-1762-4.ch014,https://openalex.org/W4391644764,,Prejudice (legal term); Selection bias; Computer science; Gender bias; Artificial intelligence,book-chapter,False,"Fairness is threatened by algorithm bias, systematic and unfair disparities in machine learning results. Amazon's AI-driven hiring tool favoured men. AI promised data-driven, impartial decision-making, but it has revealed sector-wide prejudice, perpetuating systematic imbalances. The algorithm's bias is data and design. Biassed historical data and feature selection and pre-processing can bias algorithms. Development is harmed by human biases. Algorithm prejudice impacts money, education, employment, and crime. Diverse and representative data collection, understanding complicated “black box” algorithms, and legal and ethical considerations are needed to address this bias. Despite these issues, algorithm bias elimination techniques are emerging. This chapter uses secondary data to study algorithm bias. Algorithm bias is defined, its origins, its prevalence in data, examples, and issues are discussed. The chapter also tackles bias reduction and elimination to make AI a more reliable and impartial decision-maker."
"Digital Ageism, Algorithmic Bias, and Feminist Critical Theory",2023,Rune Nyrup; Charlene H. Chu; Elena Falco,Oxford University Press eBooks,3,W4388932751,10.1093/oso/9780192889898.003.0018,https://openalex.org/W4388932751,https://doi.org/10.1093/oso/9780192889898.003.0018,Sociology; Inequality; Intersectionality; Perspective (graphical); Social inequality,book-chapter,True,"Abstract In this chapter, Nyrup, Chu, and Falco coin the term digital ageism to highlight the interplay between social inequality and tech development, aligning with feminist work that views society and technology as co-constitutive. The essay details the results of encoded ageism, including medical technologies that offer less accurate diagnoses on older populations, the unequal division of resources, and the perspective that younger people are inevitably better at using new technologies. Drawing on the work of Sally Haslanger and Iris Marion Young, they explore how technical limitations and the insufficient representation of older people in design and development teams are shaped by self-reinforcing structural inequality. This essay therefore offers a crucial intervention in the debate by identifying and tracking ageist harms and their intersections with disability, race, gender, and class-based injustices."
Algorithmic Bias and Data Injustice: Dark Side or Dark Matter?,2023,Aleksi Aaltonen; Francesco Gualdi; Mayur Joshi; Silvia Masiero; Monideepa Tarafdar; Marta Stelmaszak; Kari Koskinen,Academy of Management Proceedings,2,W4385224588,10.5465/amproc.2023.16682symposium,https://openalex.org/W4385224588,,Harm; Injustice; Great Rift; Framing (construction); Big data,article,False,"This panel brings together five contributions that, impinging on the notions of algorithmic bias and data injustice, explore both the dynamics producing data-induced harm and the manifestations of such harm on people. Ranging from data-based treatment of LGBTQ+ communities, to algorithmic bias in e-government and exclusion of recipients from datafied food security systems, the panel engages the debate on whether the notion of a ‘dark side’, widely applied to the adverse side effects of information systems, is appropriate to discuss data-induced unfairness. As an alternative framing, the panel introduces the notion of a ‘dark matter’ of datafied systems, where bias and injustice are designed into the technology. The panel aims at generating debate on unfairness with the view of imagining fairer data-based technologies, and thus contributing to building a future where a ‘force for good’ can effectively stem from datafication."
"Out of One, Many: Using Language Models to Simulate Human Samples",2023,Lisa P. Argyle; Ethan C. Busby; Nancy Fulda; Joshua R. Gubler; Christopher Rytting; David Wingate,Political Analysis,343,W4321455981,10.1017/pan.2023.2,https://openalex.org/W4321455981,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/035D7C8A55B237942FB6DBAD7CAA4E49/S1047198723000025a.pdf/div-class-title-out-of-one-many-using-language-models-to-simulate-human-samples-div.pdf,Variety (cybernetics); Fidelity; Computer science; Context (archaeology); Sociocultural evolution,article,True,"Abstract We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the “algorithmic bias” within one such tool—the GPT-3 language model—is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines."
Emerging algorithmic bias: fairness drift as the next dimension of model maintenance and sustainability,2025,Sharon E. Davis; Chad Dorn; Daniel Park; Michael E. Matheny,Journal of the American Medical Informatics Association,3,W4408405690,10.1093/jamia/ocaf039,https://openalex.org/W4408405690,https://doi.org/10.1093/jamia/ocaf039,Operationalization; Computer science; Metric (unit); Population; Performance metric,article,True,"Abstract Objectives While performance drift of clinical prediction models is well-documented, the potential for algorithmic biases to emerge post-deployment has had limited characterization. A better understanding of how temporal model performance may shift across subpopulations is required to incorporate fairness drift into model maintenance strategies. Materials and Methods We explore fairness drift in a national population over 11 years, with and without model maintenance aimed at sustaining population-level performance. We trained random forest models predicting 30-day post-surgical readmission, mortality, and pneumonia using 2013 data from US Department of Veterans Affairs facilities. We evaluated performance quarterly from 2014 to 2023 by self-reported race and sex. We estimated discrimination, calibration, and accuracy, and operationalized fairness using metric parity measured as the gap between disadvantaged and advantaged groups. Results Our cohort included 1 739 666 surgical cases. We observed fairness drift in both the original and temporally updated models. Model updating had a larger impact on overall performance than fairness gaps. During periods of stable fairness, updating models at the population level increased, decreased, or did not impact fairness gaps. During periods of fairness drift, updating models restored fairness in some cases and exacerbated fairness gaps in others. Discussion This exploratory study highlights that algorithmic fairness cannot be assured through one-time assessments during model development. Temporal changes in fairness may take multiple forms and interact with model updating strategies in unanticipated ways. Conclusion Equitable and sustainable clinical artificial intelligence deployments will require novel methods to monitor algorithmic fairness, detect emerging bias, and adopt model updates that promote fairness."
Pitfalls and Best Practices in Evaluation of AI Algorithmic Biases in Radiology,2025,Paul H. Yi; Preetham Bachina; Beepul Bharti; Sean P. Garin; Adway Kanhere; Pranav Kulkarni; David Li; Vishwa S. Parekh; Samantha M. Santomartino; Linda Moy; Jeremias Sulam,Radiology,2,W4410542464,10.1148/radiol.241674,https://openalex.org/W4410542464,,Medicine; MEDLINE; Medical physics; Data science; Artificial intelligence,review,False,"Evaluation of algorithmic biases, or artificial intelligence biases, is challenging in radiology due to incomplete reporting of demographic information in medical imaging datasets, variability in definitions of demographic categories, and inconsistent statistical definitions of bias."
The Algorithmic Bias in Recommendation Systems and Its Social Impact on User Behavior : Algorithmic Bias in Recommendation Systems,2024,,,2,W4405554134,10.70693/itphss.v1i1.204,https://openalex.org/W4405554134,,Recommender system; Computer science; Data science; Information retrieval,article,False,
"Large Language Models in Medical Education: Opportunities, Challenges, and Future Directions",2023,Alaa Abd‐Alrazaq; Rawan AlSaad; Dari Alhuwail; Arfan Ahmed; M Healy; Syed Latifi; Sarah Aziz; Rafat Damseh; Sadam Alabed Alrazak; Javaid I. Sheikh,JMIR Medical Education,391,W4376866715,10.2196/48291,https://openalex.org/W4376866715,https://mededu.jmir.org/2023/1/e48291/PDF,Engineering ethics; Curriculum; Misinformation; Competence (human resources); Paradigm shift,article,True,"The integration of large language models (LLMs), such as those in the Generative Pre-trained Transformers (GPT) series, into medical education has the potential to transform learning experiences for students and elevate their knowledge, skills, and competence. Drawing on a wealth of professional and academic experience, we propose that LLMs hold promise for revolutionizing medical curriculum development, teaching methodologies, personalized study plans and learning materials, student assessments, and more. However, we also critically examine the challenges that such integration might pose by addressing issues of algorithmic bias, overreliance, plagiarism, misinformation, inequity, privacy, and copyright concerns in medical education. As we navigate the shift from an information-driven educational paradigm to an artificial intelligence (AI)-driven educational paradigm, we argue that it is paramount to understand both the potential and the pitfalls of LLMs in medical education. This paper thus offers our perspective on the opportunities and challenges of using LLMs in this context. We believe that the insights gleaned from this analysis will serve as a foundation for future recommendations and best practices in the field, fostering the responsible and effective use of AI technologies in medical education."
On the consequences of AI bias: when moral values supersede algorithm bias,2024,Kwadwo Asante; David Sarpong; Derrick Boakye,Journal of Managerial Psychology,2,W4403732081,10.1108/jmp-05-2024-0379,https://openalex.org/W4403732081,https://www.emerald.com/insight/content/doi/10.1108/JMP-05-2024-0379/full/pdf?title=on-the-consequences-of-ai-bias-when-moral-values-supersede-algorithm-bias,Psychology; Social psychology; Response bias; Algorithm; Mathematics,article,True,"Purpose This study responded to calls to investigate the behavioural and social antecedents that produce a highly positive response to AI bias in a constrained region, which is characterised by a high share of people with minimal buying power, growing but untapped market opportunities and a high number of related businesses operating in an unregulated market. Design/methodology/approach Drawing on empirical data from 225 human resource managers from Ghana, data were sourced from senior human resource managers across industries such as banking, insurance, media, telecommunication, oil and gas and manufacturing. Data were analysed using a fussy set qualitative comparative analysis (fsQCA). Findings The results indicated that managers who regarded their response to AI bias as a personal moral duty felt a strong sense of guilt towards the unintended consequences of AI logic and reasoning. Therefore, managers who perceived the processes that guide AI algorithms' reasoning as discriminating showed a high propensity to address this prejudicial outcome. Practical implications As awareness of consequences has to go hand in hand with an ascription of responsibility; organisational heads have to build the capacity of their HR managers to recognise the importance of taking personal responsibility for artificial intelligence algorithm bias because, by failing to nurture the appropriate attitude to reinforce personal norm among managers, no immediate action will be taken. Originality/value By integrating the social identity theory, norm activation theory and justice theory, the study improves our understanding of how a collective organisational identity, perception of justice and personal values reinforce a positive reactive response towards AI bias outcomes."
Ethics and governance of trustworthy medical artificial intelligence,2023,Jie Zhang; Zongming Zhang,BMC Medical Informatics and Decision Making,257,W4315880904,10.1186/s12911-023-02103-9,https://openalex.org/W4315880904,https://bmcmedinformdecismak.biomedcentral.com/counter/pdf/10.1186/s12911-023-02103-9,Trustworthiness; Health informatics; Corporate governance; Computer science; Artificial intelligence,article,True,"Abstract Background The growing application of artificial intelligence (AI) in healthcare has brought technological breakthroughs to traditional diagnosis and treatment, but it is accompanied by many risks and challenges. These adverse effects are also seen as ethical issues and affect trustworthiness in medical AI and need to be managed through identification, prognosis and monitoring. Methods We adopted a multidisciplinary approach and summarized five subjects that influence the trustworthiness of medical AI: data quality, algorithmic bias, opacity, safety and security, and responsibility attribution, and discussed these factors from the perspectives of technology, law, and healthcare stakeholders and institutions. The ethical framework of ethical values-ethical principles-ethical norms is used to propose corresponding ethical governance countermeasures for trustworthy medical AI from the ethical, legal, and regulatory aspects. Results Medical data are primarily unstructured, lacking uniform and standardized annotation, and data quality will directly affect the quality of medical AI algorithm models. Algorithmic bias can affect AI clinical predictions and exacerbate health disparities. The opacity of algorithms affects patients’ and doctors’ trust in medical AI, and algorithmic errors or security vulnerabilities can pose significant risks and harm to patients. The involvement of medical AI in clinical practices may threaten doctors ‘and patients’ autonomy and dignity. When accidents occur with medical AI, the responsibility attribution is not clear. All these factors affect people’s trust in medical AI. Conclusions In order to make medical AI trustworthy, at the ethical level, the ethical value orientation of promoting human health should first and foremost be considered as the top-level design. At the legal level, current medical AI does not have moral status and humans remain the duty bearers. At the regulatory level, strengthening data quality management, improving algorithm transparency and traceability to reduce algorithm bias, and regulating and reviewing the whole process of the AI industry to control risks are proposed. It is also necessary to encourage multiple parties to discuss and assess AI risks and social impacts, and to strengthen international cooperation and communication."
Ethics and discrimination in artificial intelligence-enabled recruitment practices,2023,Zhisheng Chen,Humanities and Social Sciences Communications,170,W4386714740,10.1057/s41599-023-02079-x,https://openalex.org/W4386714740,https://www.nature.com/articles/s41599-023-02079-x.pdf,Transparency (behavior); Computer science; Corporate governance; Big Five personality traits; Raw data,article,True,"Abstract This study aims to address the research gap on algorithmic discrimination caused by AI-enabled recruitment and explore technical and managerial solutions. The primary research approach used is a literature review. The findings suggest that AI-enabled recruitment has the potential to enhance recruitment quality, increase efficiency, and reduce transactional work. However, algorithmic bias results in discriminatory hiring practices based on gender, race, color, and personality traits. The study indicates that algorithmic bias stems from limited raw data sets and biased algorithm designers. To mitigate this issue, it is recommended to implement technical measures, such as unbiased dataset frameworks and improved algorithmic transparency, as well as management measures like internal corporate ethical governance and external oversight. Employing Grounded Theory, the study conducted survey analysis to collect firsthand data on respondents’ experiences and perceptions of AI-driven recruitment applications and discrimination."
Race Correction and Algorithmic Bias in Atrial Fibrillation Wearable Technologies,2023,Beza Merid; Vanessa V. Volpe,Health Equity,3,W4389166084,10.1089/heq.2023.0034,https://openalex.org/W4389166084,https://doi.org/10.1089/heq.2023.0034,Race (biology); Harm; Construct (python library); Race and health; Health care,article,True,"Stakeholders in biomedicine are evaluating how race corrections in clinical algorithms inequitably allocate health care resources on the basis of a misunderstanding of race-as-genetic difference. Ostensibly used to intervene on persistent disparities in health outcomes across different racial groups, these troubling corrections in risk assessments embed essentialist ideas of race as a biological reality, rather than a social and political construct that reproduces a racial hierarchy, into practice guidelines. This article explores the harms of such race corrections by considering how the technologies we use to account for disparities in health outcomes can actually innovate and amplify these harms. Focusing on the design of wearable digital health technologies that use photoplethysmographic sensors to detect atrial fibrillation, we argue that these devices, which are notoriously poor in accurately functioning on users with darker skin tones, embed a subtle form of race correction that presupposes the need for explicit adjustments in the clinical interpretation of their data outputs. We point to research on responsible innovation in health, and its commitment to being responsive in addressing inequities and harms, as a way forward for those invested in the elimination of race correction."
A Genealogical Approach to Algorithmic Bias,2024,Marta Ziosi; David Watson; Luciano Floridi,SSRN Electronic Journal,1,W4393032343,10.2139/ssrn.4734082,https://openalex.org/W4393032343,https://ora.ox.ac.uk/objects/uuid:cba09273-3506-4b80-b3fb-aad2e61c43ec/files/rht24wk56z,Computer science; Econometrics; Mathematics,article,True,"The Fairness, Accountability, and Transparency (FAccT) literature tends to focus on bias as a problem that requires ex post solutions (e.g. fairness metrics), rather than addressing the underlying social and technical conditions that (re)produce it. In this article, we propose a complementary strategy that uses genealogy as a constructive, epistemic critique to explain algorithmic bias in terms of the conditions that enable it. We focus on XAI feature attributions (Shapley values) and counterfactual approaches as potential tools to gauge these conditions and offer two main contributions. One is constructive: we develop a theoretical framework to classify these approaches according to their relevance for bias as evidence of social disparities. We draw on Pearl's ladder of causation (2000, 2009) to order these XAI approaches concerning their ability to answer fairness-relevant questions and identify fairness-relevant solutions. The other contribution is critical: we evaluate these approaches in terms of their assumptions about the role of protected characteristics in discriminatory outcomes. We achieve this by building on Kohler-Hausmann's (2019) constructivist theory of discrimination. We derive three recommendations for XAI practitioners to develop and AI policymakers to regulate tools that address algorithmic bias in its conditions and hence mitigate its future occurrence."
Biased random-key genetic algorithms: A review,2024,Mariana A. Londe; Luciana Fontes Pessôa; Carlos E. Andrade; Maurício G. C. Resende,European Journal of Operational Research,29,W4393204274,10.1016/j.ejor.2024.03.030,https://openalex.org/W4393204274,https://doi.org/10.1016/j.ejor.2024.03.030,Metaheuristic; Computer science; Hyperparameter; Key (lock); Genetic algorithm,review,True,"This paper is a comprehensive literature review of Biased Random-Key Genetic Algorithms (BRKGA). BRKGA is a metaheuristic that employs random-key-based chromosomes with biased, uniform, and elitist mating strategies in a genetic algorithm framework. The review encompasses over 150 papers with a wide range of applications, including classical combinatorial optimization problems, real-world industrial use cases, and non-orthodox applications such as neural network hyperparameter tuning in machine learning. Scheduling is by far the most prevalent application area in this review, followed by network design and location problems. The most frequent hybridization method employed is local search, and new features aim to increase population diversity. We also detail challenges and future directions for this method. Overall, this survey provides a comprehensive overview of the BRKGA metaheuristic and its applications and highlights important areas for future research."
Algorithmic Bias: When stigmatization becomes a perception,2023,Olalekan J. Akintande,,2,W4386246861,10.1145/3600211.3604723,https://openalex.org/W4386246861,https://doi.org/10.1145/3600211.3604723,Perception; Racism; Social psychology; Perspective (graphical); Subject (documents),article,False,"In this study, the author examines how perceived stigmatization endangered the stigmatized groups within a society or community. Thus, he goes back in history to dig deep into the sources of perceived stigmatization associated with the black race and how perceived stigmatization has emigrated into AI tools and machine outputs - subjecting vulnerable communities to hypervisibility by exposing them to systems of racial surveillance. To justify the study goal, he conducted a summarized text analysis on racial stigmatization using Twitter hashtags ∈ { black people, blackness, Africa, African-Americans}, all coined out of the Twitter Users' perception of the subject and hypothesized to find high negative sentiment correlation of stigmatization perspective in association with black race and Africa. He finds that Black people are associated with Africa and have a strong negative sentiment correlation with - poorness, crime, death, abuses (stupid), among others, and a subject of racist scum and racism. Similarly, there is a weak negative sentiment correlation with being - bad, abused (such bitch), hate, violence, and protest. He also finds similar strong and weak negative sentiment correlations with other hashtags. He discusses the danger of racial stigmatization and proposes a cycle of ethical algorithmic development & deployment and recommendations."
Algorithmic bias: sexualized violence against women in GPT-3 models,2025,Sarah Wyer; Sue Black,AI and Ethics,3,W4406392547,10.1007/s43681-024-00641-0,https://openalex.org/W4406392547,https://doi.org/10.1007/s43681-024-00641-0,Psychology; Criminology; Computer security; Medical emergency; Computer science,article,True,
"Algorithmic bias, generalist models, and clinical medicine",2023,Geoff Keeling,AI and Ethics,2,W4385806884,10.1007/s43681-023-00329-x,https://openalex.org/W4385806884,https://arxiv.org/pdf/2305.04008,Generalist and specialist species; Computer science; Data science; Artificial intelligence; Machine learning,article,False,
Unlocking Monetization Potential in the Age of YouTube Algorithmic Bias: An Analysis of Botswana Filmmaking,2023,Gopolang Ditlhokwa,IntechOpen eBooks,2,W4389305620,10.5772/intechopen.113306,https://openalex.org/W4389305620,https://www.intechopen.com/citation-pdf-url/88528,Monetization; Audience measurement; Filmmaking; Political science; Advertising,book-chapter,True,"This chapter examines the challenges and opportunities faced by filmmakers in Botswana to monetize their film content on YouTube. The researcher uses a Critical Theory framework to explore the power dynamics of platforms toward cultural industries by dissecting the impact of YouTube’s algorithmic bias and geo-restrictions on content monetization potential. Additionally, this study extends to investigating the representation of diverse cultures and communities within the film industry and how YouTube’s policies may contribute to underrepresentation. With the help of qualitative research methods, the findings reveal that, indeed, filmmakers in Botswana face limitations in monetizing their content on YouTube due to regional IP restrictions, inability to meet subscription thresholds, and low viewership turnout. The study also highlights the potential for growth and market penetration through YouTube, as reaching a global audience by Botswana filmmakers can attract interest and investment from various funders. The study concludes that addressing YouTube’s algorithmic bias, geo-restrictions, and economic dynamics is crucial for promoting a more inclusive and equitable film industry in Botswana. It further suggests the need for pragmatic interventions that support filmmakers in navigating these challenges and maximizing their monetization opportunities on YouTube."
Human bias in algorithm design,2023,Carey K. Morewedge; Sendhil Mullainathan; Haaya Naushan; Cass R. Sunstein; Jon Kleinberg; Manish Raghavan; J. Ludwig,Nature Human Behaviour,31,W4388834460,10.1038/s41562-023-01724-4,https://openalex.org/W4388834460,,Affect (linguistics); Computer science; Machine learning; Artificial intelligence; User modeling,article,False,
Algorithmic Bias and Discrimination: Legal and Policy Considerations,2023,Kartik Pendharkar,SSRN Electronic Journal,1,W4389254538,10.2139/ssrn.4640433,https://openalex.org/W4389254538,,Political science; Law and economics; Computer science; Econometrics; Psychology,article,False,"This research paper examines algorithmic bias and discrimination, its impact in various sectors, and the legal and policy measures to address it. We define algorithmic bias and explore its types, including disparate impact, disparate treatment, and contextual bias, with illustrative examples. Factors contributing to bias, such as biased training data and flawed algorithms, are analyzed.The research paper delves into case studies and real-world examples that showcase the implications of algorithmic bias and discrimination. These examples may include biased hiring algorithms or racially discriminatory predictive policing tools. Drawing from the analysis, the paper proposes policy recommendations to mitigate algorithmic bias and discrimination. These recommendations encompass transparency, accountability, and algorithmic auditing improvements. The roles of government agencies, policymakers, and industry stakeholders in addressing algorithmic bias and promoting fairness in algorithmic decision-making are discussed.Policy recommendations encompass transparency, accountability, and algorithmic auditing. The roles of government, policymakers, and industry stakeholders are highlighted in promoting fairness in algorithmic decision-making. The paper speculates on future challenges posed by emerging technologies and proposes areas for further research and policy development. This comprehensive analysis contributes to creating just and inclusive digital environments."
Cognitively Biased Users Interacting with Algorithmically Biased Results in Whole-Session Search on Debated Topics,2024,Ben Wang; Jiqun Liu,,2,W4401338160,10.1145/3664190.3672520,https://openalex.org/W4401338160,https://doi.org/10.1145/3664190.3672520,Session (web analytics); Computer science; Information retrieval; Human–computer interaction; World Wide Web,article,True,"When interacting with information retrieval (IR) systems, users, affected by confirmation biases, tend to select search results that confirm their existing beliefs on socially significant contentious issues. To understand the judgments and attitude changes of users searching online, our study examined how cognitively biased users interact with algorithmically biased search engine result pages (SERPs). We designed three-query search sessions on debated topics under various bias conditions. We recruited 1,321 crowdsourcing participants and explored their attitude changes, search interactions, and the effects of confirmation bias. Three key findings emerged: 1) most attitude changes occur in the initial query of a search session; 2) Confirmation bias and result presentation on SERPs affect the number and depth of clicks in the current query and perceived familiarity with clicked results in subsequent queries; 3) The bias position also affects attitude changes of users with lower perceived openness to conflicting opinions. Our study goes beyond traditional simulation-based evaluation settings and simulated rational users, sheds light on the mixed effects of human biases and algorithmic biases in information retrieval tasks on debated topics, and can inform the design of bias-aware user models, human-centered bias mitigation techniques, and socially responsible intelligent IR systems."
Mitigating algorithmic bias in opioid risk-score modeling to ensure equitable access to pain relief,2023,Atharva M. Bhagwat; Kadija Ferryman; Jason B. Gibbons,Nature Medicine,4,W4360612372,10.1038/s41591-023-02256-0,https://openalex.org/W4360612372,,Opioid; Medicine; Opioid epidemic; Pain relief; Business,letter,False,
"Editorial Note for Special Issue on Al and Fake News, Mis(dis)information, and Algorithmic Bias",2023,Donghee Shin; Kerk F. Kee,Journal of Broadcasting & Electronic Media,4,W4380878039,10.1080/08838151.2023.2225665,https://openalex.org/W4380878039,,Misinformation; Journalism; Sociology; Digital media; Digital era,editorial,False,"Click to increase image sizeClick to decrease image size Disclosure statementNo potential conflict of interest was reported by the author(s).Additional informationNotes on contributorsDonghee ShinDonghee Shin (Ph.D. & M.A., Syracuse University) is a professor and chair at the College of Media & Communication at Texas Tech University. His research interests include digital journalism, human-computer interaction, and algorithmic media. His recent research in the algorithm as media addresses misinformation, inoculation theory, and algorithmic biases.Kerk F. KeeKerk F. Kee (Ph.D., the University of Texas at Austin) is an associate professor at the College of Media & Communication at Texas Tech University. His research primarily investigates the development, adoption, implementation, and ultimate diffusion of big data technologies in scientific organizations. He also studies the dissemination of health information in cultural communities and the spread of pro-environmental attitudes in modern societies."
Investigating What Factors Influence Users’ Rating of Harmful Algorithmic Bias and Discrimination,2024,Sara Kingsley; Jiayin Zhi; Wesley Hanwen Deng; Jaimie Lee; S.Q. Zhang; Motahhare Eslami; Kenneth Holstein; Jason Hong; Tianshi Li; Mouquan Shen,Proceedings of the AAAI Conference on Human Computation and Crowdsourcing,2,W4403434407,10.1609/hcomp.v12i1.31602,https://openalex.org/W4403434407,https://ojs.aaai.org/index.php/HCOMP/article/download/31602/33768,Psychology; Computer science; Applied psychology; Cognitive psychology,article,True,"There has been growing recognition of the crucial role users, especially those from marginalized groups, play in uncovering harmful algorithmic biases. However, it remains unclear how users’ identities and experiences might impact their rating of harmful biases. We present an online experiment (N=2,197) examining these factors: demographics, discrimination experiences, and social and technical knowledge. Participants were shown examples of image search results, including ones that previous literature has identified as biased against marginalized racial, gender, or sexual orientation groups. We found participants from marginalized gender or sexual orientation groups were more likely to rate the examples as more severely harmful. Belonging to marginalized races did not have a similar pattern. Additional factors affecting users’ ratings included discrimination experiences, and having friends or family belonging to marginalized demographics. A qualitative analysis offers insights into users' bias recognition, and why they see biases the way they do. We provide guidance for designing future methods to support effective user-driven auditing."
Unpacking Algorithmic Bias in YouTube Shorts by Analyzing Thumbnails,2025,Mert Can Çakmak; Nitin Agarwal,Proceedings of the ... Annual Hawaii International Conference on System Sciences/Proceedings of the Annual Hawaii International Conference on System Sciences,2,W4407208106,10.24251/hicss.2025.304,https://openalex.org/W4407208106,,Unpacking; Thumbnail; Computer science; Artificial intelligence; Philosophy,article,False,
Mitigating Algorithmic Bias: Strategies for Addressing Discrimination in Data,2024,Sonia Gipson Rankin,SSRN Electronic Journal,2,W4400918480,10.2139/ssrn.4902043,https://openalex.org/W4400918480,,Computer science; Data science,article,False,
Making the invisible visible: Youth designs for teaching about technological and algorithmic bias,2024,Merijke Coenraad,International Journal of Child-Computer Interaction,3,W4391650902,10.1016/j.ijcci.2024.100634,https://openalex.org/W4391650902,,Covert; Experiential learning; Computer science; Gender bias; Technological change,article,False,
Questioning AI: How Racial Identity Shapes the Perceptions of Algorithmic Bias,2023,Soojong Kim; Joomi Lee; Poong Oh,,2,W4381513294,10.31234/osf.io/afcdn,https://openalex.org/W4381513294,https://psyarxiv.com/afcdn/download,Injustice; Social psychology; Perception; Social identity theory; Psychology,preprint,True,"[Published in International Journal of Communication] Growing concerns indicate that automated decision-making (ADM) may discriminate against certain social groups, but little is known about how social identities of people influence their perceptions of biased automated decisions. Focusing on the context of racial disparity, this study examined if individuals’ social identities (white vs. People of Color) and social contexts that entail discrimination (discrimination target: the self vs. the other) affect the perceptions of algorithm outcomes. A randomized controlled experiment (N = 604) demonstrated that a participant’s social identity significantly moderated the effects of the discrimination target on the perceptions. Among POC participants, algorithms that discriminate against the subject decreased their perceived fairness and trust, whereas among white participants opposite patterns were observed. The findings imply that social disparity and inequality, and different social groups’ lived experiences of the existing discrimination and injustice should be at the center of understanding how people make sense of biased algorithms."
Raising Algorithm Bias Awareness Among Computer Science Students Through Library and Computer Science Instruction,2024,Shalini Ramachandran; Steve Cutchin; Sheree Fu,2020 ASEE Virtual Annual Conference Content Access Proceedings,2,W3191615150,10.18260/1-2--37634,https://openalex.org/W3191615150,https://peer.asee.org/37634.pdf,Raising (metalworking); Computer science; Mathematics education; Psychology; Engineering,article,True,"Abstract We are a computer science professor and two librarians who work closely with computer science students. In this paper, we outline the development of an introductory algorithm bias instruction session. As part of our lesson development, we analyzed the results of a survey we conducted of computer science students at three universities on their perceptions about search-engine and big-data algorithms. We examined whether an information literacy component focused on algorithmic bias was beneficial to offer to students in the computational sciences and designed an instructional prototype. We studied qualitative data, including feedback from students and colleagues on our initial instruction module to create the next two modules. We found that students' reception to the subject of algorithm bias can range from defensive and unaccepting to open and accepting of the existence of such bias. Since the topic ultimately deals with issues of racial, gender-based, and other discrimination, a multidisciplinary approach is needed when teaching about algorithm bias. Our assertion is that librarians have a role in partnering with computer science instructors to ensure that students who major in computer science, who will be the primary creators of algorithms as they enter the workforce, can develop an early awareness and understanding of bias in information systems. Further, when the students receive such training, the automated systems they generate will produce more fair outcomes. Our pedagogy incorporates insights from computer science, library science, medical ethics, and critical theory. The aim of our algorithm bias instruction is to help computer science students recognize and mitigate the systematic marginalization of groups within the current technological environment."
A Scoping Review On The Impact Of Algorithm Bias On The Perceived Fairness,2023,Amirhossein Hajigholam Saryazdi; Mahdi Mirhosseini,Research Square (Research Square),1,W4387574695,10.21203/rs.3.rs-3428999/v1,https://openalex.org/W4387574695,https://www.researchsquare.com/article/rs-3428999/latest.pdf,Scope (computer science); Injustice; Computer science; Task (project management); Mechanism (biology),review,True,"Abstract Artificial intelligence (AI) based algorithms increasingly shape our daily lives. These algorithms can be biased in a way that compounds injustice in society. This affects how people perceive the fairness of the algorithms, but little is known about the mechanism and scope. To address this research gap, this study conducts a scoping literature review of published papers representing the current state of the research. Then develops a novel theoretical model about the impact of algorithm bias on perceived fairness by synthesizing common themes namely: algorithm bias, algorithm fairness, perceived fairness, individual characteristics, social characteristics, task characteristics, and technology characteristics. The paper concludes by formulating propositions that highlight the significant gap in the literature, contribute to a better understanding of the relationships between identified themes and their components, and provide a roadmap for future research."
Algorithmic bias: Looking beyond data bias to ensure algorithmic accountability and equity,2023,S. J. Thais; Hannah Shumway; Austin Saragih,,1,W4386301315,10.38105/spr.5lwvw66ssy,https://openalex.org/W4386301315,https://sciencepolicyreview.org/wp-content/uploads/securepdfs/2023/08/MITSPR-v4-191618004007.pdf,Accountability; Transparency (behavior); Computer science; Equity (law); Audit,article,True,"As algorithms increasingly aid public sector decision making in the United States, it becomes important to understand how to effectively tackle algorithmic bias in systems that local, state, and federal government entities use and procure, including what kinds of policies are currently in place or proposed. There is a prevalent belief that algorithmic bias arises primarily from statistical biases present in the data used to train or develop the algorithm, but bias may also arise during data collection, problem specification, how and where algorithms are deployed, and within the broader societal contextualization of algorithms. So far, enacted policies in in the US center on temporary bans of particular types of algorithms, transparency, and post-hoc bias audits, as well as more wide-ranging (but non-binding) policy frameworks; all largely focus on quantitative notions of fairness when they assess bias, leaving room for more comprehensive legislation to meaningfully address this issue going forward."
"Focusing on Decisions, Outcomes, and Value Judgments to Confront Algorithmic Bias",2023,Apurvakumar Pandya; Jinyi Zhu,JAMA Network Open,2,W4380730826,10.1001/jamanetworkopen.2023.18501,https://openalex.org/W4380730826,https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2806104/pandya_2023_ic_230093_1686167895.4643.pdf,Value (mathematics); Psychology; Sociology; Positive economics; Statistics,article,True,"Sara Khor, MASc; Eric C. Haupt, ScM; Erin E. Hahn, PhD, MPH; Lindsay Joe L. Lyons, LVN; Veena Shankaran, MD, MS; Aasthaa Bansal, PhD"
AI and Discrimination: Sources of Algorithmic Biases,2024,Sara Moussawi; Xuefei Deng; K. D. Joshi,ACM SIGMIS Database the DATABASE for Advances in Information Systems,1,W4403624891,10.1145/3701613.3701615,https://openalex.org/W4403624891,,Harm; Computer science; Context (archaeology); Scholarship; Process (computing),article,False,"In this editorial, we define discrimination in the context of AI algorithms by focusing on understanding the biases arising throughout the lifecycle of building algorithms: input data for training, the process of algorithm development, and algorithm execution and usage. We draw insights from a few empirical studies to illustrate biases codified in algorithms that could result in harmful outcomes. We call on information systems scholars to prioritize scholarship in the area of algorithmic discrimination that can help generate new knowledge systems that would help safeguard against widespread and unaccountable harm."
Algorithmic Bias and Physician Liability,2024,Shujie Luan; Shubhranshu Singh; Tinglong Dai,,1,W4405236536,10.2139/ssrn.5046254,https://openalex.org/W4405236536,,Liability; Business; Actuarial science; Law and economics; Accounting,preprint,False,
Engineering a Fiduciary: Expanding the Regulatory Scope of Algorithmic Bias,2023,Bao Kham Chau,SSRN Electronic Journal,1,W4387886268,10.2139/ssrn.4372258,https://openalex.org/W4387886268,,Promulgation; Scope (computer science); Computer science; Fiduciary; Corporate governance,article,False,"This Commentary seeks to expand on current legal proposals attempting to regulate BigTech. It discusses the scope of current algorithmic governance frameworks and recommends expanding the regulatory purview to include both rule-based and machine learning algorithms. The Commentary suggests that this expansion will help to further dispel the myth of algorithmic idealism and address the issue of algorithmic bias. In particular, the Commentary examines the Google Chrome source code and contends that the Agile Software Development process does not produce algorithmically neutral code, and top-down proposals of external regulation do not fully address the harm of rule-based algorithms. Therefore, it is beneficial to explore bottom-up regulatory solutions, such as the promulgation and enforcement of a code of ethics for the software engineering profession."
"Data and science engineering: The ethical dilemma of our time-exploring privacy breaches, algorithmic biases, and the need for transparency",2023,Shubham Shubham; Saloni Saloni; Sidra-Tul-Muntaha,World Journal of Advanced Research and Reviews,2,W4366773799,10.30574/wjarr.2023.18.1.0677,https://openalex.org/W4366773799,https://wjarr.com/sites/default/files/WJARR-2023-0677.pdf,Transparency (behavior); Autonomy; Accountability; Engineering ethics; Ethical issues,article,True,"This paper explores the ethical dilemmas associated with data and science engineering, with a focus on privacy breaches, algorithmic biases, and the need for transparency. With the increasing reliance on data-driven decision making and machine learning algorithms, the ethical implications of these technologies have become a pressing issue in various sectors. The study aimed to identify the most significant ethical concerns, analyze their impact on society, and provide solutions to address these issues. The research utilized a systematic review of 18 studies to identify the key ethical issues in data and science engineering. The findings revealed that privacy breaches, algorithmic biases, and lack of transparency were the most prevalent ethical concerns. These issues can have significant implications for individuals and groups, including discrimination, loss of autonomy, and reputational harm. The study also identified vulnerable groups, such as marginalized communities, who may be disproportionately affected by these issues. To address these ethical concerns, the study proposed several solutions, including the development of ethical guidelines, increased transparency and accountability, and the use of diverse and representative datasets. The solutions were informed by the literature review, case studies, and analysis of real-world examples. The study also assessed the feasibility of implementing these solutions and highlighted potential barriers to implementation."
"Whither Bias Goes, I Will Go: An Integrative, Systematic Review of Algorithmic Bias Mitigation",2024,Louis Hickman; Christopher Huynh; Jessica Gass; Brandon M. Booth; Jason Kuruzovich; Louis Tay,,1,W4403627258,10.31234/osf.io/hcxbn,https://openalex.org/W4403627258,https://osf.io/hcxbn/download,Computer science; Psychology; Data science,preprint,True,"Machine learning (ML) models are increasingly used for personnel assessment and selection (e.g., resume screeners, automatically scored interviews). However, concerns have been raised throughout society that ML assessments may be biased and perpetuate or exacerbate inequality. Although organizational researchers have begun investigating ML assessments from traditional psychometric and legal perspectives, there is a need to understand, clarify, and integrate fairness operationalizations and algorithmic bias mitigation methods from the computer science, data science, and organizational research literatures. We present a four-stage model of developing ML assessments and applying bias mitigation methods, including 1) generating the training data, 2) training the model, 3) testing the model, and 4) deploying the model. When introducing the four-stage model, we describe potential sources of bias and unfairness at each stage. Then, we systematically review definitions and operationalizations of algorithmic bias, legal requirements governing personnel selection from the United States and Europe, and research on algorithmic bias mitigation across multiple domains and integrate these findings into our framework. Our review provides insights for both research and practice by elucidating possible mechanisms of algorithmic bias while identifying which bias mitigation methods are legal and effective. This integrative framework also reveals gaps in the knowledge of algorithmic bias mitigation that should be addressed by future collaborative research between organizational researchers, computer scientists, and data scientists. We provide recommendations for developing and deploying ML assessments, as well as recommendations for future research into algorithmic bias and fairness."
Enhancing Clinical Decision Support in Nephrology: Addressing Algorithmic Bias Through Artificial Intelligence Governance,2024,Benjamin A. Goldstein; Dinushika Mohottige; Sophia Bessias; Michael P. Cary,American Journal of Kidney Diseases,3,W4399447219,10.1053/j.ajkd.2024.04.008,https://openalex.org/W4399447219,,Corporate governance; Medicine; Equity (law); PsycINFO; Set (abstract data type),review,False,
Are algorithmic bias claims supported?—Response,2023,Sandra González‐Bailón; David Lazer,Science,1,W4387128734,10.1126/science.adk4899,https://openalex.org/W4387128734,,Computer science; Data science,article,False,
"Antimicrobial resistance: Impacts, challenges, and future prospects",2024,Sirwan Khalid Ahmed; Safin Hussein; Karzan Qurbani; Radhwan Hussein Ibrahim; Abdulmalik Fareeq; Kochr Ali Mahmood; Mona Gamal Mohamed,Journal of Medicine Surgery and Public Health,435,W4392349824,10.1016/j.glmedi.2024.100081,https://openalex.org/W4392349824,https://doi.org/10.1016/j.glmedi.2024.100081,Antibiotic resistance; Antimicrobial stewardship; Business; Agriculture; Pandemic,article,True,"Antimicrobial resistance (AMR) is a critical global health issue driven by antibiotic misuse and overuse in various sectors, leading to the emergence of resistant microorganisms. The history of AMR dates back to the discovery of penicillin, with the rise of multidrug-resistant pathogens posing significant challenges to healthcare systems worldwide. The misuse of antibiotics in human and animal health, as well as in agriculture, contributes to the spread of resistance genes, creating a ""Silent Pandemic"" that could surpass other causes of mortality by 2050. AMR affects both humans and animals, with resistant pathogens posing challenges in treating infections. Various mechanisms, such as enzymatic modification and biofilm formation, enable microbes to withstand the effects of antibiotics. The lack of effective antibiotics threatens routine medical procedures and could lead to millions of deaths annually if left unchecked. The economic impact of AMR is substantial, with projected losses in the trillions of dollars and significant financial burdens on healthcare systems and agriculture. Artificial intelligence is being explored as a tool to combat AMR by improving diagnostics and treatment strategies, although challenges such as data quality and algorithmic biases exist. To address AMR effectively, a One Health approach that considers human, animal, and environmental factors is crucial. This includes enhancing surveillance systems, promoting stewardship programs, and investing in research and development for new antimicrobial options. Public awareness, education, and international collaboration are essential for combating AMR and preserving the efficacy of antibiotics for future generations."
Avoidable and Unavoidable AI Algorithmic Bias,2024,Tshilidzi Marwala,,1,W4404283849,10.1007/978-981-97-9251-1_8,https://openalex.org/W4404283849,,Computer science,book-chapter,False,
IEEE Standard for Algorithmic Bias Considerations,2024,,,1,W4406755120,10.1109/ieeestd.2025.10851955,https://openalex.org/W4406755120,,Computer science,standard,False,
Algorithmic bias in anthropomorphic artificial intelligence: Critical perspectives through the practice of women media artists and designers,2023,Caterina Antonopoulou,Technoetic Arts,2,W4391184189,10.1386/tear_00109_1,https://openalex.org/W4391184189,,Computer science; Psychology; Artificial intelligence; Multimedia; Human–computer interaction,article,False,"Current research in artificial intelligence (AI) sheds light on algorithmic bias embedded in AI systems. The underrepresentation of women in the AI design sector of the tech industry, as well as in training datasets, results in technological products that encode gender bias, reinforce stereotypes and reproduce normative notions of gender and femininity. Biased behaviour is notably reflected in anthropomorphic AI systems, such as personal intelligent assistants (PIAs) and chatbots, that are usually feminized through various design parameters, such as names, voices and traits. Gendering of AI entities, however, is often reduced to the encoding of stereotypical behavioural patterns that perpetuate normative assumptions about the role of women in society. The impact of this behaviour on social life increases, as human-to-(anthropomorphic)machine interactions are mirrored in human-to-human social interactions. This article presents current critical research on AI bias, focusing on anthropomorphic systems. Moreover, it discusses the significance of women’s engagement in AI design and programming, by presenting selected case studies of contemporary female media artists and designers. Finally, it suggests that women, through their creative practice, provide feminist and critical approaches to AI design which are essential for imagining alternative, inclusive, ethic and de-biased futures for anthropomorphic AIs."
Do not treat Bill Gates for prostate cancer! Algorithmic bias and causality in medical prediction,2023,Andrew J. Vickers,BJU International,2,W4318670249,10.1111/bju.15951,https://openalex.org/W4318670249,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/bju.15951,Life expectancy; Prostate cancer; Prostatectomy; Medicine; Wife,letter,True,"Chase et al. [1] recently published in the BJUI a life-expectancy prediction model for patients with prostate cancer. They expressly recommended this be used to guide treatment decision-making for patients, and even provide an easy-to-use app for doing so. It is the authors’ explicit intention that urologists use their prediction model to decide who should and should not receive curative treatment depending on whether life expectancy is >10 years. The underlying principle is completely sound. Life expectancy is indeed a critical determinant of prostate cancer treatment and many current approaches, such as using social security tables, or informal clinical assessment, are known to be invalid [2]. The problem with the Chase et al. [1] model is that it includes education and marital status. If you take two men with identical tumours, and who are also the same age and have the same comorbidities, you might recommend one but not the other to undergo curative treatment based on his higher level of education, or because he is married. Indeed, one might imagine a situation where a man is scheduled for a radical prostatectomy, but then his wife leaves him and the urologist, after using the app to make a quick calculation, tells the patient that he is no longer eligible for surgery. Perhaps even more problematic, use of the prediction model would systematically discriminate against Black Americans. Centuries of racism has left Blacks less likely to be well-educated or married. So randomly select a Black man and a White man of the same age, cancer and comorbidity profile, there is a good chance that the White man will be referred for surgery or radiotherapy whereas the Black man is denied curative treatment. This can only worsen the severe and chronic racial disparities in prostate cancer outcome that continue to blight American healthcare. To their credit, the authors are not unaware of the problem of algorithmic bias, even citing the seminal work of Kent and Paulus [3]. My own view, however, is that their arguments are weak. Take, for instance, their assertion that ‘<5% of patients … would have had their prediction appreciably changed by a modification to their marital status or educational attainment’. This is far from reassuring. Let us be 100% clear: no-one should be denied treatment because they are unmarried or never got a degree. As it turns out, Bill Gates, an unmarried college drop-out billionaire, might be one of the men who would be given a different prediction because the Chase et al. [1] model includes education and marital status. Indeed, the Bill Gates problem tells us exactly why we should avoid correlates of social determinants of health in prediction models. Causality is not something we normally worry about when making predictions: if it predicts, it predicts. For instance, grip strength in older patients predicts life expectancy and so might be used as a simple, in-office test to help guide shared decision-making [4]. No one thinks that poor grip causes an early death, but that is irrelevant. However, it is rather different for predictors that have social consequences. We must think about causality if our model might exacerbate disparities. Educational and marital status are predictive of life expectancy in patients with prostate cancer because they are associated with social support and access to care and so, as in the case of Bill Gates, we are better off examining a patient's level of social support and access to care than to simply ask about their education and marital status. The numerous medical prediction models that directly include race as a predictor [5] should similarly explore causality. In many prostate-cancer models, Black race is a correlate for genetic differences that are yet to be fully elucidated, and it seems appropriate to include race until better markers of genetic risk are available. In other models, it seems that race is used simply because it is predictive and that will often be because of racism. This has included life-expectancy models in prostate cancer [6], where Black men are given lower life expectancy, and less chance of treatment, simply because they are Black. I am pretty sure that none of us would agree with ‘discriminate against black people because, as a result of previous discrimination, they have poorer outcomes’. Yet, that is often exactly what is implied by including race, or social correlates of race, in prediction models. Laudably, Chase et al. [1] provide a version of their model that avoids education and marital status as inputs. I would encourage anyone interested in using the model to use this version. Moving forward, we need to extremely careful about including race or social determinants of health in prediction models or clinical algorithms, and carefully evaluate how such models or algorithms might impact disparities. None relevant. This work was supported in part by the National Institutes of Health/National Cancer Institute (NIH/NCI) with a Cancer Center Support Grant to Memorial Sloan Kettering Cancer Center [P30-CA008748]."
Algorithmic Bias in Image-Generating Artificial Intelligence: Prevalence and User Perceptions,2024,Tanja Messingschlager; Markus Appel,,1,W4393864832,10.31234/osf.io/wjeps,https://openalex.org/W4393864832,https://osf.io/wjeps/download,Perception; Image (mathematics); Artificial intelligence; Computer science; Psychology,preprint,True,"Image-generating AI is among the most popular generative AI applications, likely changing the visual mediated environments humans are exposed to on a mass scale. Prior work found that AI can be biased against women and minorities (algorithmic bias), whereas humans attribute rather high objectivity to AI. We focused on image-generating AI, analyzing the extent of algorithmic bias in AI-generated pictures, as well as human responses to bias in image-generating AI. Study 1 showed that AI-generated portraits of people in STEM professions were almost exclusively depicting male, white (and older) individuals. Study 2 (experimental, N = 495) showed that the responses to AI-generated pictures vary, depending on the portrayed group. Participants perceived pictures to be less biased if they were introduced as AI-generated, but only if pictures showed college students (vs. older people). If images showed older people, participants reported higher moral outrage if pictures were supposedly generated by AI (vs. human creators)."
EAB-FL: Exacerbating Algorithmic Bias through Model Poisoning Attacks in Federated Learning,2024,Syed Irfan Ali Meerza; Jian Liu,,2,W4401024118,10.24963/ijcai.2024/51,https://openalex.org/W4401024118,http://arxiv.org/pdf/2410.02042,Computer science; Federated learning; Computer security; Artificial intelligence,preprint,True,"Federated Learning (FL) is a technique that allows multiple parties to train a shared model collaboratively without disclosing their private data. It has become increasingly popular due to its distinct privacy advantages. However, FL models can suffer from biases against certain demographic groups (e.g., racial and gender groups) due to the heterogeneity of data and party selection. Researchers have proposed various strategies for characterizing the group fairness of FL algorithms to address this issue. However, the effectiveness of these strategies in the face of deliberate adversarial attacks has not been fully explored. Although existing studies have revealed various threats (e.g., model poisoning attacks) against FL systems caused by malicious participants, their primary aim is to decrease model accuracy, while the potential of leveraging poisonous model updates to exacerbate model unfairness remains unexplored. In this paper, we propose a new type of model poisoning attack, EAB-FL, with a focus on exacerbating group unfairness while maintaining a good level of model utility. Extensive experiments on three datasets demonstrate the effectiveness and efficiency of our attack, even with state-of-the-art fairness optimization algorithms and secure aggregation rules employed."
Addressing Algorithmic Bias in India: Ethical Implications and Pitfalls,2023,Yoshita Sood,SSRN Electronic Journal,1,W4379399777,10.2139/ssrn.4466681,https://openalex.org/W4379399777,,Political science; Computer science,article,False,"In the realm of artificial intelligence (AI), where its pervasive influence spans numerous domains, it is imperative to acknowledge and rectify the biases that can permeate its algorithms. This research paper explores the reasons behind the emergence of unconscious bias in AI systems, with a focus on India. It discusses the ethical issues arising from biased algorithms and their impact on Indian society. The paper also presents potential solutions for Indian entities to combat AI unfairness. The first part examines the theories of biased training data and biased programmers as root causes of machine prejudice. It highlights the importance of managerial and educational interventions to reduce bias. The second part of the paper explores the unique challenges faced in India, encompassing the underrepresentation and lack of diversity in datasets, as well as the western-centric bias ingrained within AI technologies. It critically examines the repercussions of biased AI in areas like predictive policing, facial recognition, and healthcare. The paper emphasizes the need for a socio-technical perspective to understand the complex factors contributing to AI bias and recommends a feminist, decolonial, and anti-caste lens to achieve algorithmic fairness in India. The proposed solutions include real-time problem identification, recontextualization of data models, and fostering dialogue with community workers to build high-quality datasets."
On the Implicit Bias in Deep-Learning Algorithms,2023,Gal Vardi,Communications of the ACM,22,W4377941395,10.1145/3571070,https://openalex.org/W4377941395,https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3571070&file=p86-vardi-supp.pdf,Computer science; Artificial neural network; Artificial intelligence; Algorithm; Deep learning,article,True,Examining the implicit bias in training neural networks using gradient-based methods.
Algorithmic Bias and Data Justice: ethical challenges in Artificial Intelligence Systems,2025,Javier González‐Argote; Emilia Urrutia Maldonado; Karina Maldonado,,1,W4411475682,10.56294/ai2025159,https://openalex.org/W4411475682,,Equity (law); Economic Justice; Corporate governance; Social justice; Inequality,article,False,"This article examines the critical ethical challenges posed by algorithmic bias in artificial intelligence (AI) systems, focusing on its implications for social justice and data equity. Through a systematic review of case studies and theoretical frameworks, we analyze how biased datasets and algorithmic designs perpetuate structural inequalities, particularly affecting marginalized communities. The study highlights key examples, such as gender and racial biases in facial recognition and hiring algorithms, while exploring mitigation strategies rooted in data justice principles. Additionally, we evaluate regulatory responses, including the European Union's AI Act, which proposes a risk-based governance framework. The findings underscore the urgent need for interdisciplinary approaches to develop fairer AI systems that align with ethical standards and human rights."
The dark side of generative artificial intelligence: A critical analysis of controversies and risks of ChatGPT,2023,Krzysztof Wach; Cong Doanh Duong; Joanna Ejdys; Rūta Kazlauskaitė; Paweł Korzyński; Grzegorz Mazurek; Joanna Paliszkiewicz; Ewa Ziemba,Entrepreneurial Business and Economics Review,345,W4383959108,10.15678/eber.2023.110201,https://openalex.org/W4383959108,https://eber.uek.krakow.pl/index.php/eber/article/view/2113/852,Great Rift; Generative grammar; Psychology; Artificial intelligence; Computer science,article,True,"Objective: The objective of the article is to provide a comprehensive identification and understanding of the challenges and opportunities associated with the use of generative artificial intelligence (GAI) in business.This study sought to develop a conceptual framework that gathers the negative aspects of GAI development in management and economics, with a focus on ChatGPT. Research Design & Methods:The study employed a narrative and critical literature review and developed a conceptual framework based on prior literature.We used a line of deductive reasoning in formulating our theoretical framework to make the study's overall structure rational and productive.Therefore, this article should be viewed as a conceptual article that highlights the controversies and threats of GAI in management and economics, with ChatGPT as a case study.Findings: Based on the conducted deep and extensive query of academic literature on the subject as well as professional press and Internet portals, we identified various controversies, threats, defects, and disadvantages of GAI, in particular ChatGPT.Next, we grouped the identified threats into clusters to summarize the seven main threats we see.In our opinion they are as follows: (i) no regulation of the AI market and urgent need for regulation, (ii) poor quality, lack of quality control, disinformation, deepfake content, algorithmic bias, (iii) automationspurred job losses, (iv) personal data violation, social surveillance, and privacy violation, (v) social manipulation, weakening ethics and goodwill, (vi) widening socio-economic inequalities, and (vii) AI technostress.Implications & Recommendations: It is important to regulate the AI/GAI market.Advocating for the regulation of the AI market is crucial to ensure a level playing field, promote fair competition, protect intellectual property rights and privacy, and prevent potential geopolitical risks.The changing job market requires workers to continuously acquire new (digital) skills through education and retraining.As the training of AI systems becomes a prominent job category, it is important to adapt and take advantage of new opportunities.To mitigate the risks related to personal data violation, social surveillance, and privacy violation, GAI developers must prioritize ethical considerations and work to develop systems that prioritize user privacy and security.To avoid social manipulation and weaken ethics and goodwill, it is important to implement responsible AI practices and ethical guidelines: transparency in data usage, bias mitigation techniques, and monitoring of generated content for harmful or misleading information.Contribution & Value Added: This article may aid in bringing attention to the significance of resolving the ethical and legal considerations that arise from the use of GAI and ChatGPT by drawing attention to the controversies and hazards associated with these technologies."
Identifying and mitigating algorithmic bias in the safety net,2025,Shaina Mackin; Vincent J. Major; Rumi Chunara; Remle Newton-Dame,npj Digital Medicine,1,W4411067641,10.1038/s41746-025-01732-w,https://openalex.org/W4411067641,https://doi.org/10.1038/s41746-025-01732-w,Computer science; Risk analysis (engineering); Business,article,True,
Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling,2023,Wendy Hui; John W. Lau,arXiv (Cornell University),1,W4387838730,10.48550/arxiv.2310.12421,https://openalex.org/W4387838730,https://arxiv.org/abs/2310.12421,Computer science; Black box; Gender bias; Binary number; Focus (optics),preprint,True,"This paper proposes the use of causal modeling to detect and mitigate algorithmic bias. We provide a brief description of causal modeling and a general overview of our approach. We then use the Adult dataset, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on gender bias and the problem of binary classification. We show that gender bias in the prediction model is statistically significant at the 0.05 level. We demonstrate the effectiveness of the causal model in mitigating gender bias by cross-validation. Furthermore, we show that the overall classification accuracy is improved slightly. Our novel approach is intuitive, easy-to-use, and can be implemented using existing statistical software tools such as ""lavaan"" in R. Hence, it enhances explainability and promotes trust."
"TrustScapes: A Visualisation Tool to Capture Stakeholders’ Concerns and Recommendations About Data Protection, Algorithmic Bias, and Online Safety",2023,Sachiyo Ito‐Jaeger; Giles Lane; Liz Dowthwaite; Helena Webb; Menisha Patel; Mat Rawsthorne; Virginia Portillo; Marina Jirotka; Elvira Pérez Vallejos,International Journal of Qualitative Methods,2,W4383112433,10.1177/16094069231186965,https://openalex.org/W4383112433,https://journals.sagepub.com/doi/pdf/10.1177/16094069231186965,Worksheet; Flexibility (engineering); Computer science; Focus group; Data collection,article,True,"This paper presents a new methodological approach, TrustScapes, an open access tool designed to identify and visualise stakeholders’ concerns and policy recommendations on data protection, algorithmic bias, and online safety for a fairer and more trustworthy online world. We first describe how the tool was co-created with young people and other stakeholders through a series of workshops. We then present two sets of TrustScapes focus groups to illustrate how the tool can be used, and the data analysed. The paper then provides the methodological insights, including the strengths of the TrustScapes and the lessons for future research using TrustScapes. A key strength of this method is that it allows people to visualise their ideas and thoughts on the worksheet, using the keywords and sketches provided. The flexibility in the mode of delivery is another strength of the TrustScapes method. The TrustScapes focus groups can be conducted in a relatively short time (1.5–2 hours), either in person or online depending on the participants’ needs, geological locations, and practicality. Our experience with the TrustScapes offers some lessons (related to the data collection and analysis) for researchers who wish to use this method in the future. Finally, we describe how the outcomes from the TrustScapes focus groups should help to inform future policy decisions."
Computer Vision: Anthropology of Algorithmic Bias in Facial Analysis Tool,2023,Mayane Batista Lima,IntechOpen eBooks,1,W4379619635,10.5772/intechopen.110330,https://openalex.org/W4379619635,https://www.intechopen.com/citation-pdf-url/87206,Computer science; Artificial intelligence; Machine learning; Ethnic group; Race (biology),book-chapter,True,"The usage of Computer Vision (CV) has led to debates about the bias within the technology. Despite machines being labeled as autonomous, human bias is embedded in data labeling for effective machine learning. Proper training of neural network machines requires massive amounts of “relevant data,” however, not all data is collected. This contributes to a one-sided view and feeds a “standard of data that is not collected.” The machine develops algorithmic decision-making based on the data it is presented, which can create machinic biases such as differences in gender, race/ethnicity, and class. This raises questions about which bodies are recognized by machines and how they are taught to “see” beyond binary “male or female” limitations. The study aims to understand how Amazon’s Rekognition, a facial recognition and analysis tool, analyzes and classifies people of dissident genders who do not conform to “conventional” gender norms. Understanding the mechanisms behind the technology’s decision-making processes can lead to more equitable and inclusive outcomes."
Female perspectives on algorithmic bias: implications for AI researchers and practitioners,2025,Belen Fraile-Rojas; Carmen De‐Pablos‐Heredero; Mariano Méndez-Suárez,Management Decision,1,W4406230024,10.1108/md-04-2024-0884,https://openalex.org/W4406230024,,Empowerment; Injustice; Inequality; Constructive; Empirical research,article,False,"Purpose This article explores the use of natural language processing (NLP) techniques and machine learning (ML) models to discover underlying concepts of gender inequality applied to artificial intelligence (AI) technologies in female social media conversations. The first purpose is to characterize female users who use this platform to share content around this area. The second is to identify the most prominent themes among female users’ digital production of gender inequality concepts, applied to AI technologies. Design/methodology/approach Social opinion mining has been applied to historical Twitter data. Data were gathered using a combination of analytical methods such as word clouds, sentiment analyses and clustering. It examines 172,041 tweets worldwide over a limited period of 359 days. Findings Empirical data gathered from interactions of female users in digital dialogues highlight that the most prominent topics of interest are the future of AI technologies and the active role of women to guarantee gender balanced systems. Algorithmic bias impacts female user behaviours in response to injustice and inequality in algorithmic outcomes. They share topics of interest and lead constructive conversations with profiles affiliated with gender or race empowerment associations. Women challenged by stereotypes and prejudices are likely to fund entrepreneurial solutions to create opportunities for change. Research limitations/implications This study does have its limitations, however. First, different keywords are likely to result in a different pool of related research. Moreover, due to the nature of our sample, the largest proportion of posts are from native English speakers, predominantly (88%) from the US, UK, Australia and Canada. This demographic concentration reflects specific social structures and practices that influence gender equity priorities within the sample. These cultural contexts, which often emphasize inclusivity and equity, play a significant role in shaping the discourse around gender issues. These cultural norms, preferences and practices are critical in understanding the individual behaviours, perspectives and priorities expressed in the posts; in other words, it is vital to consider cultural context and economic determinants in an analysis of gender equity discussions. The US, UK, Australia and Canada share a cultural and legal heritage, a common language, values, democracy and the rule of law. Bennett (2007) emphasizes the potential for enhanced cooperation in areas like technology, trade and security, suggesting that the anglosphere’s cultural and institutional commonalities create a natural foundation for a cohesive, influential global network. These shared characteristics further influence the common approaches and perspectives on gender equity in public discourse. Yet findings from Western nations should not be assumed to apply easily to the contexts of other countries. Practical implications From a practical perspective, the results help us understand the role of female influencers and scrutinize public conversations. From a theoretical one, this research upholds the argument that feminist critical thought is indispensable in the development of balanced AI systems. Social implications The results also help us understand the role of female influencers: ordinary individuals often challenged by gender and race discrimination. They request an intersectional, collaborative and pluralistic understanding of gender and race in AI. They act alone and endure the consequences of stigmatized products and services. AI curators should strongly consider advocating for responsible, impartial technologies, recognizing the indispensable role of women. This must consider all stakeholders, including representatives from industry, small and medium-sized enterprises (SMEs), civil society and academia. Originality/value This study aims to fill critical research gaps by addressing the lack of a socio-technical perspective on AI-based decision-making systems, the shortage of empirical studies in the field and the need for a critical analysis using feminist theories. The study offers valuable insights that can guide managerial decision-making for AI researchers and practitioners, providing a comprehensive understanding of the topic through a critical lens."
People see more of their biases in algorithms,2024,Begum Celiktutan; Romain Cadario; Carey K. Morewedge,Proceedings of the National Academy of Sciences,9,W4394675493,10.1073/pnas.2317602121,https://openalex.org/W4394675493,https://www.pnas.org/doi/pdf/10.1073/pnas.2317602121,Debiasing; Cognitive bias; Algorithm; Confirmation bias; Computer science,article,True,"Algorithmic bias occurs when algorithms incorporate biases in the human decisions on which they are trained. We find that people see more of their biases (e.g., age, gender, race) in the decisions of algorithms than in their own decisions. Research participants saw more bias in the decisions of algorithms trained on their decisions than in their own decisions, even when those decisions were the same and participants were incentivized to reveal their true beliefs. By contrast, participants saw as much bias in the decisions of algorithms trained on their decisions as in the decisions of other participants and algorithms trained on the decisions of other participants. Cognitive psychological processes and motivated reasoning help explain why people see more of their biases in algorithms. Research participants most susceptible to bias blind spot were most likely to see more bias in algorithms than self. Participants were also more likely to perceive algorithms than themselves to have been influenced by irrelevant biasing attributes (e.g., race) but not by relevant attributes (e.g., user reviews). Because participants saw more of their biases in algorithms than themselves, they were more likely to make debiasing corrections to decisions attributed to an algorithm than to themselves. Our findings show that bias is more readily perceived in algorithms than in self and suggest how to use algorithms to reveal and correct biased human decisions."
Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices,2023,Manish Raghavan,ACM eBooks,18,W2972445641,10.1145/3603195.3603203,https://openalex.org/W2972445641,https://arxiv.org/pdf/1906.09208,Citation; Computer science; Operations research; Engineering; Data science,book-chapter,True,"chapter Share on Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices Author: Manish Raghavan Massachusetts Institute of Technology, Sloan School of Management and Department of Electrical Engineering and Computer Science Massachusetts Institute of Technology, Sloan School of Management and Department of Electrical Engineering and Computer ScienceView Profile Authors Info & Claims The Societal Impacts of Algorithmic Decision-MakingSeptember 2023https://doi.org/10.1145/3603195.3603203Published:08 September 2023Publication History 0citation0DownloadsMetricsTotal Citations0Total Downloads0Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access"
AI’s Diversity Problem in Radiology: Addressing Algorithm Bias,2024,Kerri Reeves,Applied Radiology,1,W4391547243,10.37549/ar2952,https://openalex.org/W4391547243,https://doi.org/10.37549/ar2952,Diversity (politics); Computer science; Psychology; Algorithm; Artificial intelligence,article,True,
Algorithmic Bias and Historical Injustice: Race and Digital Profiling,2024,A. Matthew; Amalia R. Miller; Catherine E. Tucker,SSRN Electronic Journal,1,W4399110376,10.2139/ssrn.4843044,https://openalex.org/W4399110376,,Injustice; Download; Profiling (computer programming); Racial profiling; Ethnic group,article,False,"This paper studies the implications of attempts at ""ethnic-affinity"" profiling on Facebook that reflects users' engagement with content on Facebook. Profiling by ethnic-affinity is highly correlated with Census estimates of population race by geography. However, more users were profiled as African-American in former slave states relative to the baseline population. This occurs because the targeting algorithm was better at identifying Black users through differentiated engagement with cultural content in these states. This implies that policies restricting the collection of racial identity data will be unsuccessful due to the existence of proxies, and that relying on proxies may introduce troubling biases.Institutional subscribers to the NBER working paper series, and residents of developing countries may download this paper without additional charge at www.nber.org."
Human Freedom from Algorithmic Bias: What is the role of Accountability in addressing Health Disparities?,2024,Sajda Qureshi; Blessing Oladokun,Medical Research Archives,1,W4402143614,10.18103/mra.v12i8.5635,https://openalex.org/W4402143614,,Accountability; Human health; Degrees of freedom (physics and chemistry); Computer science; Political science,article,False,"While there are many causes of health disparities, the application of Artificial Intelligence tools in healthcare may have mixed results. The purpose of this paper is to investigate the role of human freedoms and accountability to achieving digital inclusion. It discovers the role of algorithmic bias in mediating the relationship between human freedom and mobile health. The following research questions are investigated: 1) How do human freedoms effect digital inclusion and mobile health? 2) Do human freedoms effect mobile health? And 3) Does AI accountability mediate the relationship between human freedoms and mobile health? The findings suggest that human freedoms are central to digital inclusion and mobile health. Accountability does affect the extent to which digital inclusion can be achieved through human freedoms. AI accountability significantly mediates the relationship between human freedoms and the mobile index. This offers an important contribution in uncovering the role of algorithmic bias in human freedom and mobile health, and of accountability between human freedom and digital inclusion."
Uncovering the Challenges From Algorithmic Bias Affecting the Marginalized Patient Groups in Healthcare,2024,Sarthak Bhatia; Shobhit; Anuj Kumar; Stuti Tandon,SSRN Electronic Journal,1,W4399153522,10.2139/ssrn.4848690,https://openalex.org/W4399153522,,Health care; Psychology; Political science; Public relations; Data science,article,False,"Data-driven insights from artificial intelligence (AI) offer increased diagnostic accuracy and consistency in healthcare delivery. However, there is a chance that algorithmic biases in medical AI could exacerbate already-existing health inequities for marginalised populations. This study examines realworld case studies and recorded evidence of biassed AI systems that negatively impact underprivileged patient populations. We critically evaluate key technological factors such as unbalanced datasets, limited feature scope, feedback loops of reinforcement, and correlational proxies that propagate preconceptions among groups. Upstream mitigation strategies include varying the makeup of training data, conducting institutional audits to identify representation gaps, and forming interdisciplinary teams to supervise product development. The efficacy of midstream algorithmic techniques for debiasing AI models varies. There are still significant gaps in the areas of benchmarking, transparency, and the absence of common criteria establishing fairness benchmarks applicable to healthcare settings. Moving beyond technical fixes and towards a comprehensive assessment of the patient journey, cultural norms, and sociotechnical ecosystems influencing healthcare experiences is necessary to achieve equitable AI. Innovation and inclusive development may be actively balanced by putting stakeholder voices front and centre through participatory design, together with downstream policy levers around transparency mandates and tackling the underlying reasons of unequal access."
Reinvention mediates impacts of skin tone bias in algorithms: implications for technology diffusion,2024,Hannah Overbye-Thompson; Kristy A. Hamilton; Dana Mastro,Journal of Computer-Mediated Communication,11,W4402741719,10.1093/jcmc/zmae016,https://openalex.org/W4402741719,https://doi.org/10.1093/jcmc/zmae016,Tone (literature); Diffusion; Algorithm; Computer science; Physics,article,True,"Abstract Two studies examine how skin tone bias in image recognition algorithms impacts users’ adoption and usage of image recognition technology. We employed a diffusion of innovations framework to explore perceptions of compatibility, complexity, observability, relative advantage, and reinvention to determine their influence on participants' utilization of image recognition algorithms. Despite being more likely to encounter algorithm bias, individuals with darker skin tones perceived image recognition algorithms as having greater levels of compatibility and relative advantage, being more observable, and less complex and thus used them more extensively compared to those with lighter skin tones. Individuals with darker skin tones also displayed higher levels of reinvention behaviors, suggesting a potential adaptive response to counteract algorithm biases."
"Balancing Privacy and Progress: A Review of Privacy Challenges, Systemic Oversight, and Patient Perceptions in AI-Driven Healthcare",2024,S. Williamson; Victor R. Prybutok,Applied Sciences,218,W4390829176,10.3390/app14020675,https://openalex.org/W4390829176,https://www.mdpi.com/2076-3417/14/2/675/pdf?version=1705396255,Health care; Autonomy; Confidentiality; Transformative learning; Information privacy,review,True,"Integrating Artificial Intelligence (AI) in healthcare represents a transformative shift with substantial potential for enhancing patient care. This paper critically examines this integration, confronting significant ethical, legal, and technological challenges, particularly in patient privacy, decision-making autonomy, and data integrity. A structured exploration of these issues focuses on Differential Privacy as a critical method for preserving patient confidentiality in AI-driven healthcare systems. We analyze the balance between privacy preservation and the practical utility of healthcare data, emphasizing the effectiveness of encryption, Differential Privacy, and mixed-model approaches. The paper navigates the complex ethical and legal frameworks essential for AI integration in healthcare. We comprehensively examine patient rights and the nuances of informed consent, along with the challenges of harmonizing advanced technologies like blockchain with the General Data Protection Regulation (GDPR). The issue of algorithmic bias in healthcare is also explored, underscoring the urgent need for effective bias detection and mitigation strategies to build patient trust. The evolving roles of decentralized data sharing, regulatory frameworks, and patient agency are discussed in depth. Advocating for an interdisciplinary, multi-stakeholder approach and responsive governance, the paper aims to align healthcare AI with ethical principles, prioritize patient-centered outcomes, and steer AI towards responsible and equitable enhancements in patient care."
Group Fairness for Content Creators: the Role of Human and Algorithmic Biases under Popularity-based Recommendations,2023,Stefania Ionescu; Anikó Hannák; Nicolò Pagan,,2,W4386728800,10.1145/3604915.3608841,https://openalex.org/W4386728800,https://dl.acm.org/doi/pdf/10.1145/3604915.3608841,Popularity; Computer science; Content (measure theory); Group (periodic table); Internet privacy,article,True,"The Creator Economy faces concerning levels of unfairness. Content creators (CCs) publicly accuse platforms of purposefully reducing the visibility of their content based on protected attributes, while platforms place the blame on viewer biases. Meanwhile, prior work warns about the ""rich-get-richer"" effect perpetuated by existing popularity biases in recommender systems: Any initial advantage in visibility will likely be exacerbated over time. What remains unclear is how the biases based on protected attributes from platforms and viewers interact and contribute to the observed inequality in the context of popularity-biased recommender systems. The difficulty of the question lies in the complexity and opacity of the system. To overcome this challenge, we design a simple agent-based model (ABM) that unifies the platform systems which allocate the visibility of CCs (e.g., recommender systems, moderation) into a single popularity-based function, which we call the visibility allocation system (VAS). Through simulations, we find that although viewer homophilic biases do alone create inequalities, small levels of additional biases in VAS are more harmful. From the perspective of interventions, our results suggest that (a) attempts to reduce attribute-biases in moderation and recommendations should precede those reducing viewers' homophilic tendencies, (b) decreasing the popularity-biases in VAS decreases but not eliminates inequalities, (c) boosting the visibility of protected CCs to overcome viewers' homophily with respect to one fairness metric is unlikely to produce fair outcomes with respect to all metrics, and (d) the process is also unfair for viewers and this unfairness could be overcome through the same interventions. More generally, this work demonstrates the potential of using ABMs to better understand the causes and effects of biases and interventions within complex sociotechnical systems."
The role of Artificial Intelligence in shaping the future of Media production and the application of Algorithm bias theory in Storytelling,2024,Mary Y. Habib; Maha ElTarabishi,Journal of Media and Interdisciplinary Studies,2,W4400923167,10.21608/jmis.2024.299137.1033,https://openalex.org/W4400923167,,Storytelling; Production (economics); Computer science; Artificial intelligence; Algorithm,article,False,
Fairness and Bias in Algorithmic Hiring: A Multidisciplinary Survey,2024,Alessandro Fabris; Nina Baranowska; Matthew Dennis; David Graus; Philipp Hacker; Jorge Saldivar; Frederik Zuiderveen Borgesius; Asia J. Biega,ACM Transactions on Intelligent Systems and Technology,14,W4402747641,10.1145/3696457,https://openalex.org/W4402747641,,Computer science; Multidisciplinary approach; Data science; Artificial intelligence; Law,article,False,"Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of , algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders."
"The emergent role of artificial intelligence, natural learning processing, and large language models in higher education and research",2023,Tariq Alqahtani; Hisham A. Badreldin; Mohammed Alrashed; Abdulrahman Alshaya; Sahar S. Alghamdi; Khalid Bin Saleh; Shuroug A. Alowais; Omar A. Alshaya; Ishrat Rahman; Majed S. Al Yami; Abdulkareem Albekairy,Research in Social and Administrative Pharmacy,285,W4379284826,10.1016/j.sapharm.2023.05.016,https://openalex.org/W4379284826,https://doi.org/10.1016/j.sapharm.2023.05.016,Computer science; Curriculum; Artificial intelligence; Engineering ethics; Field (mathematics),review,True,"Artificial Intelligence (AI) has revolutionized various domains, including education and research. Natural language processing (NLP) techniques and large language models (LLMs) such as GPT-4 and BARD have significantly advanced our comprehension and application of AI in these fields. This paper provides an in-depth introduction to AI, NLP, and LLMs, discussing their potential impact on education and research. By exploring the advantages, challenges, and innovative applications of these technologies, this review gives educators, researchers, students, and readers a comprehensive view of how AI could shape educational and research practices in the future, ultimately leading to improved outcomes. Key applications discussed in the field of research include text generation, data analysis and interpretation, literature review, formatting and editing, and peer review. AI applications in academics and education include educational support and constructive feedback, assessment, grading, tailored curricula, personalized career guidance, and mental health support. Addressing the challenges associated with these technologies, such as ethical concerns and algorithmic biases, is essential for maximizing their potential to improve education and research outcomes. Ultimately, the paper aims to contribute to the ongoing discussion about the role of AI in education and research and highlight its potential to lead to better outcomes for students, educators, and researchers."
Fourth International Workshop on Algorithmic Bias in Search and Recommendation (Bias 2023),2023,Ludovico Boratto; Stefano Faralli; Mirko Marras; Giovanni Stilo,Lecture notes in computer science,1,W4327499177,10.1007/978-3-031-28241-6_39,https://openalex.org/W4327499177,,Debiasing; Computer science; Trustworthiness; Audit; Field (mathematics),book-chapter,False,
Unbias me! Mitigating Algorithmic Bias for Less-studied Demographic Groups in the Context of Language Learning Technology,2023,Nathalie Rzepka; Linda Fernsel; Hans‐Georg Müller; Katharina Simbeck; N. Pinkwart,,1,W4380854637,10.35542/osf.io/qa9vz,https://openalex.org/W4380854637,https://osf.io/qa9vz/download,Computer science; Context (archaeology); Odds; Machine learning; Artificial intelligence,preprint,True,"Rzepka, N., Fernsel, L., Müller, H., Simbeck, K., &amp;amp; Pinkwart, N., (2023) Unbias me! Mitigating Algorithmic Bias for Less-studied Demographic Groups in the Context of Language Learning Technology. Computer-Based Learning in Context, 6 (1), 1-23. 10.5281/zenodo.7996194 Algorithms and machine learning models are being used more frequently in educational settings, but there are concerns that they may discriminate against certain groups. While there is some research on algorithmic fairness, there are two main issues with the current research. Firstly, it often focuses on gender and race and ignores other groups. Secondly, studies often find algorithmic bias in educational models but don't explore ways to reduce it. This study evaluates three drop-out prediction models used in an online learning platform to teach German spelling skills. The aim is to assess the fairness of the models for (in part) less-studied demographic groups, including first spoken language, home literacy environment, parental education background, and gender. To evaluate the models, four fairness metrics are used: predictive parity, equalized odds, predictive equality, and ABROCA. The study also examines ways to reduce algorithmic bias by analyzing the models at each stage of the machine learning process. The results show that all three models had biases that affected the fairness of all four demographic groups to varying degrees. However, the study found that most biases could be mitigated during the process. The methods used to mitigate bias differed by demographic group, and some methods improved fairness for one group but worsened it for others. Therefore, the study concludes that reducing algorithmic bias for less-studied demographic groups is possible, but finding the right method for each algorithm and demographic group is crucial."
Algorithmic Bias in BERT for Response Accuracy Prediction: A Case Study for Investigating Population Validity,2024,Guher Gorgun; Seyma N. Yildirim‐Erbasli,Journal of Educational Measurement,1,W4403847593,10.1111/jedm.12420,https://openalex.org/W4403847593,https://doi.org/10.1111/jedm.12420,Item response theory; Population; Psychology; Test validity; Predictive validity,article,True,"Abstract Pretrained large language models (LLMs) have gained popularity in recent years due to their high performance in various educational tasks such as learner modeling, automated scoring, automatic item generation, and prediction. Nevertheless, LLMs are black box approaches where models are less interpretable, and they may carry human biases and prejudices because historical human data have been used for pretraining these large‐scale models. For these reasons, the prediction tasks based on LLMs require scrutiny to ensure that the prediction models are fair and unbiased. In this study, we used BERT—a pretrained encoder‐only LLM for predicting response accuracy using action sequences extracted from the 2012 PIAAC assessment. We selected three countries (i.e., Finland, Slovakia, and the United States) representing different performance levels in the overall PIAAC assessment. We found promising results for predicting response accuracy using the fine‐tuned BERT model. Additionally, we examined algorithmic bias in the prediction models trained with different countries. We found differences in model performance, suggesting that some trained models are not free from bias, and thus the models are less generalizable across countries. Our results highlighted the importance of investigating algorithmic fairness in prediction models utilizing algorithmic systems to ensure models are bias‐free."
Deep Isolation Forest for Anomaly Detection,2023,Hongzuo Xu; Guansong Pang; Yijie Wang; Yongjun Wang,IEEE Transactions on Knowledge and Data Engineering,218,W4367016885,10.1109/tkde.2023.3270293,https://openalex.org/W4367016885,http://arxiv.org/pdf/2206.06602,Computer science; Partition (number theory); Anomaly detection; Scalability; Linear subspace,article,True,"Isolation forest (iForest) has been emerging as arguably the most popular anomaly detector in recent years due to its general effectiveness across different benchmarks and strong scalability. Nevertheless, its linear axis-parallel isolation method often leads to (i) failure in detecting hard anomalies that are difficult to isolate in high-dimensional/non-linear-separable data space, and (ii) notorious algorithmic bias that assigns unexpectedly lower anomaly scores to artefact regions. These issues contribute to high false negative errors. Several iForest extensions are introduced, but they essentially still employ shallow, linear data partition, restricting their power in isolating true anomalies. Therefore, this paper proposes deep isolation forest. We introduce a new representation scheme that utilises casually initialised neural networks to map original data into random representation ensembles, where random axis-parallel cuts are subsequently applied to perform the data partition. This representation scheme facilitates high freedom of the partition in the original data space (equivalent to non-linear partition on subspaces of varying sizes), encouraging a unique synergy between random representations and random partition-based isolation. Extensive experiments show that our model achieves significant improvement over state-of-the-art isolation-based methods and deep detectors on tabular, graph and time series datasets; our model also inherits desired scalability from iForest."
Understanding how users may work around algorithmic bias,2025,Hannah Overbye-Thompson; Ronald E. Rice,AI & Society,1,W4412676577,10.1007/s00146-025-02498-1,https://openalex.org/W4412676577,https://doi.org/10.1007/s00146-025-02498-1,Work (physics); Performing arts; Computer science; Data science; Human–computer interaction,article,True,
Laissez-Faire Harms: Algorithmic Biases in Generative Language Models,2024,Evan Shieh; Faye-Marie Vassel; Cassidy R. Sugimoto; Thema Monroe‐White,arXiv (Cornell University),1,W4394780976,10.48550/arxiv.2404.07475,https://openalex.org/W4394780976,https://arxiv.org/pdf/2404.07475,Psychology; Identity (music); Social psychology; Generative grammar; Sexual orientation,preprint,True,"The rapid deployment of generative language models (LMs) has raised concerns about social biases affecting the well-being of diverse consumers. The extant literature on generative LMs has primarily examined bias via explicit identity prompting. However, prior research on bias in earlier language-based technology platforms, including search engines, has shown that discrimination can occur even when identity terms are not specified explicitly. Studies of bias in LM responses to open-ended prompts (where identity classifications are left unspecified) are lacking and have not yet been grounded in end-consumer harms. Here, we advance studies of generative LM bias by considering a broader set of natural use cases via open-ended prompting. In this ""laissez-faire"" setting, we find that synthetically generated texts from five of the most pervasive LMs (ChatGPT3.5, ChatGPT4, Claude2.0, Llama2, and PaLM2) perpetuate harms of omission, subordination, and stereotyping for minoritized individuals with intersectional race, gender, and/or sexual orientation identities (AI/AN, Asian, Black, Latine, MENA, NH/PI, Female, Non-binary, Queer). We find widespread evidence of bias to an extent that such individuals are hundreds to thousands of times more likely to encounter LM-generated outputs that portray their identities in a subordinated manner compared to representative or empowering portrayals. We also document a prevalence of stereotypes (e.g. perpetual foreigner) in LM-generated outputs that are known to trigger psychological harms that disproportionately affect minoritized individuals. These include stereotype threat, which leads to impaired cognitive performance and increased negative self-perception. Our findings highlight the urgent need to protect consumers from discriminatory harms caused by language models and invest in critical AI education programs tailored towards empowering diverse consumers."
The Tangled Web of Social Bias: Unraveling Human and Algorithmic Biases on Digital Platforms,2024,Runshan Fu,,1,W4405596493,10.2139/ssrn.5008463,https://openalex.org/W4405596493,,Computer science; Data science; World Wide Web; Internet privacy,preprint,False,
Algorithmic Bias as a Core Legal Dilemma in the Age of Artificial Intelligence: Conceptual Basis and the Current State of Regulation,2025,Gergely Ferenc Lendvai; Gergely Gosztonyi,Laws,1,W4411243228,10.3390/laws14030041,https://openalex.org/W4411243228,https://doi.org/10.3390/laws14030041,Dilemma; Core (optical fiber); Current (fluid); State (computer science); Artificial intelligence,article,True,"This article examines algorithmic bias as a pressing legal challenge, situating the issue within the broader context of artificial intelligence (AI) governance. We employed comparative legal analysis and reviewed pertinent regulatory documents to examine how the fragmented U.S. approaches and the EU’s user-centric legal frameworks, such as the GDPR, DSA, and AI Act, address the systemic risks posed by biased algorithms. The findings underscore persistent enforcement gaps, particularly concerning opaque black-box algorithmic design, which hampers bias detection and remediation. The paper highlights how current regulatory efforts disproportionately affect marginalized communities and fail to provide effective protection across jurisdictions. It also identifies structural imbalances in legal instruments, particularly in relation to risk classification, transparency, and fairness standards. Notably, emerging regulations often lack the technical and ethical capacity for implementation. We argue that global cooperation is not only necessary but inevitable, as regional solutions alone are insufficient to govern transnational AI systems. Without harmonized international standards, algorithmic bias will continue to reproduce existing inequalities under the guise of objectivity. The article advocates for inclusive, cross-sectoral collaboration among governments, developers, and civil society to ensure the responsible development of AI and uphold fundamental rights."
"Structural Racism in Tech: Social Media Platforms, Algorithmic Bias, and Racist Tech",2024,Safiya Noble; Sarah Roberts; Matthew Bui; André Brock; Olivia Snow,,1,W4405059960,10.1007/978-3-031-69362-5_37,https://openalex.org/W4405059960,https://doi.org/10.1007/978-3-031-69362-5_37,Racism; Key (lock); Social media; Racial bias; High tech,book-chapter,False,"Abstract Covers key studies published over the last decade documenting the harmful effects of racist technologies, which include how algorithms are racially biased and produce harmful effects."
A COMPREHENSIVE REVIEW OF BIAS IN AI ALGORITHMS,2024,Abdul Wajid Fazil; Musawer Hakimi; Amir Kror Shahidzay,Nusantara Hasana Journal,11,W4391069191,10.59003/nhj.v3i8.1052,https://openalex.org/W4391069191,https://nusantarahasanajournal.com/index.php/nhj/article/download/1052/853,Scopus; Computer science; Software deployment; Artificial intelligence; Inclusion (mineral),review,True,"This comprehensive review aims to analyze and synthesize the existing literature on bias in AI algorithms, providing a thorough understanding of the challenges, methodologies, and implications associated with biased artificial intelligence systems.Employing a narrative synthesis and systematic literature review approach, this study systematically explores a wide array of sources from prominent databases such as PubMed, Google Scholar, Scopus, Web of Science, and ScienceDirect. The inclusion criteria focused on studies that distinctly defined artificial intelligence in the education sector, were published in English, and underwent peer-review. Five independent reviewers meticulously evaluated search results, extracted pertinent data, and assessed the quality of included studies, ensuring a rigorous and comprehensive analysis. The synthesis of findings reveals pervasive patterns of bias in AI algorithms across various domains, shedding light on the nuanced aspects of discriminatory practices. The systematic review highlights the need for continued research, emphasizing the intricate interplay between bias, technological advancements, and societal impacts. The comprehensive analysis underscores the complexity of bias in AI algorithms, emphasizing the critical importance of addressing these issues in future developments. Recognizing the limitations and potential consequences, the study calls for a concerted effort from researchers, developers, and policymakers to mitigate bias and foster the responsible deployment of AI technologies. Based on the findings, recommendations include implementing robust bias detection mechanisms, enhancing diversity in AI development teams, and establishing transparent frameworks for algorithmic decision-making. The implications of this study extend beyond academia, informing industry practices and policy formulations to create a more equitable and ethically grounded AI landscape."
The bias algorithm: how AI in healthcare exacerbates ethnic and racial disparities – a scoping review,2024,Syed Ali Hussain; Mary Bresnahan; Jie Zhang,Ethnicity and Health,12,W4404016455,10.1080/13557858.2024.2422848,https://openalex.org/W4404016455,,Health equity; Ethnic group; Equity (law); Health care; Transparency (behavior),review,False,"This scoping review examined racial and ethnic bias in artificial intelligence health algorithms (AIHA), the role of stakeholders in oversight, and the consequences of AIHA for health equity. Using the PRISMA-ScR guidelines, databases were searched between 2020 and 2024 using the terms racial and ethnic bias in health algorithms resulting in a final sample of 23 sources. Suggestions for how to mitigate algorithmic bias were compiled and evaluated, roles played by stakeholders were identified, and governance and stewardship plans for AIHA were examined. While AIHA represent a significant breakthrough in predictive analytics and treatment optimization, regularly outperforming humans in diagnostic precision and accuracy, they also present serious challenges to patient privacy, data security, institutional transparency, and health equity. Evidence from extant sources including those in this review showed that AIHA carry the potential to perpetuate health inequities. While the current study considered AIHA in the US, the use of AIHA carries implications for global health equity."
Generative AI in Writing Research Papers: A New Type of Algorithmic Bias and Uncertainty in Scholarly Work,2023,Rishab Jain; Aditya Jain,arXiv (Cornell University),1,W4389982241,10.48550/arxiv.2312.10057,https://openalex.org/W4389982241,https://arxiv.org/abs/2312.10057,Generative grammar; Interpretability; Computer science; Context (archaeology); Artificial intelligence,preprint,True,"The use of artificial intelligence (AI) in research across all disciplines is becoming ubiquitous. However, this ubiquity is largely driven by hyperspecific AI models developed during scientific studies for accomplishing a well-defined, data-dense task. These AI models introduce apparent, human-recognizable biases because they are trained with finite, specific data sets and parameters. However, the efficacy of using large language models (LLMs) -- and LLM-powered generative AI tools, such as ChatGPT -- to assist the research process is currently indeterminate. These generative AI tools, trained on general and imperceptibly large datasets along with human feedback, present challenges in identifying and addressing biases. Furthermore, these models are susceptible to goal misgeneralization, hallucinations, and adversarial attacks such as red teaming prompts -- which can be unintentionally performed by human researchers, resulting in harmful outputs. These outputs are reinforced in research -- where an increasing number of individuals have begun to use generative AI to compose manuscripts. Efforts into AI interpretability lag behind development, and the implicit variations that occur when prompting and providing context to a chatbot introduce uncertainty and irreproducibility. We thereby find that incorporating generative AI in the process of writing research manuscripts introduces a new type of context-induced algorithmic bias and has unintended side effects that are largely detrimental to academia, knowledge production, and communicating research."
The value of standards for health datasets in artificial intelligence-based applications,2023,Anmol Arora; Joseph Alderman; Joanne Palmer; Shaswath Ganapathi; Elinor Laws; Melissa D. McCradden; Lauren Oakden‐Rayner; Stephen Pfohl; Marzyeh Ghassemi; Francis McKay; Darren Treanor; Negar Rostamzadeh; Bilal A. Mateen; Jacqui Gath; Adewole O. Adebajo; Stephanie Kuku; Rubeta Matin; Katherine Heller; Elizabeth Sapey; Neil J. Sebire; Heather Cole-Lewis; Melanie Calvert; Alastair K. Denniston; Xiaoxuan Liu,Nature Medicine,152,W4387966489,10.1038/s41591-023-02608-w,https://openalex.org/W4387966489,https://www.nature.com/articles/s41591-023-02608-w.pdf,Best practice; Data science; Diversity (politics); Health care; Computer science,article,True,"Abstract Artificial intelligence as a medical device is increasingly being applied to healthcare for diagnosis, risk stratification and resource allocation. However, a growing body of evidence has highlighted the risk of algorithmic bias, which may perpetuate existing health inequity. This problem arises in part because of systemic inequalities in dataset curation, unequal opportunity to participate in research and inequalities of access. This study aims to explore existing standards, frameworks and best practices for ensuring adequate data diversity in health datasets. Exploring the body of existing literature and expert views is an important step towards the development of consensus-based guidelines. The study comprises two parts: a systematic review of existing standards, frameworks and best practices for healthcare datasets; and a survey and thematic analysis of stakeholder views of bias, health equity and best practices for artificial intelligence as a medical device. We found that the need for dataset diversity was well described in literature, and experts generally favored the development of a robust set of guidelines, but there were mixed views about how these could be implemented practically. The outputs of this study will be used to inform the development of standards for transparency of data diversity in health datasets (the STANDING Together initiative)."
"ALGORITHMIC BIAS IN MEDIA CONTENT DISTRIBUTION AND ITS INFLUENCE ON MEDIA CONSUMPTION: IMPLICATIONS FOR DIVERSITY, EQUITY, AND INCLUSION (DEI)",2024,CHIZOROM EBOSIE OKORONKWO,International Journal of Social Sciences and Management Review,1,W4403433610,10.37602/ijssmr.2024.7523,https://openalex.org/W4403433610,https://doi.org/10.37602/ijssmr.2024.7523,Inclusion (mineral); Diversity (politics); Content distribution; Consumption (sociology); Equity (law),article,True,"In today’s digital age, algorithms play a pivotal role in shaping media content distribution, which may possibly influence what content individuals are exposed to. Consequently, this may have implications for diversity, equity, and inclusion (DEI). Hence, this review analyzes algorithmic bias in media material distribution and its impact on media consumption and the implications for diversity, equity, and inclusion (DEI). The study concludes that algorithm bias limits the visibility of underprivileged groups and perpetuates current social injustices, posing serious problems for media distribution. Moreover, there are risks and opportunities associated with the development of artificial intelligence (AI) and machine learning in tackling algorithmic inequities. Furthermore, there is a need for collaborative efforts among different stakeholders (engineers, policymakers, and media platforms) in creating a more inclusive and equitable algorithms in order to ensure that media distribution systems promote fairness and diversity."
Framework for Algorithmic Bias Quantification and its Application to Automated Sleep Scoring,2024,Michal Bechný; Giuliana Monachino; Luigi Fiorillo; Julia van der Meer; Markus H. Schmidt; Claudio L. Bassetti; Athina Tzovara; Francesca Dalia Faraci,,1,W4402594212,10.1109/sds60720.2024.00045,https://openalex.org/W4402594212,,Computer science; Sleep (system call); Artificial intelligence; Machine learning; Programming language,article,False,
"Algorithmic bias, fairness, and inclusivity: a multilevel framework for justice-oriented AI",2025,Paola Panarese; Marco Grasso; Claudia Solinas,AI & Society,1,W4412420621,10.1007/s00146-025-02451-2,https://openalex.org/W4412420621,,Economic Justice; Sociology; Fairness measure; Computer science; Psychology,article,False,
Michael is better than Mehmet: exploring the perils of algorithmic biases and selective adherence to advice from automated decision support systems in hiring,2024,Astrid M. Rosenthal‐von der Pütten; Alexandra Sach,Frontiers in Psychology,2,W4402418959,10.3389/fpsyg.2024.1416504,https://openalex.org/W4402418959,https://doi.org/10.3389/fpsyg.2024.1416504,Advice (programming); Psychology; Social psychology; Applied psychology; Computer science,article,True,"Artificial intelligence algorithms are increasingly adopted as decisional aides in many contexts such as human resources, often with the promise of being fast, efficient, and even capable of overcoming biases of human decision-makers. Simultaneously, this promise of objectivity and the increasing supervisory role of humans may make it more likely for existing biases in algorithms to be overlooked, as humans are prone to over-rely on such automated systems. This study therefore aims to investigate such reliance on biased algorithmic advice in a hiring context."
Analysing Coping Strategies of Teenage Girls Towards Instagram’s Algorithmic Bias,2024,Intisãr Constant; Pitso Tsibolane; Adheesh Budree; Grant Oosterwyk,Lecture notes in computer science,1,W4399267544,10.1007/978-3-031-61281-7_10,https://openalex.org/W4399267544,,Computer science; Coping (psychology); Psychology; Psychotherapist,book-chapter,False,
Gender Bias in Hiring: An Analysis of the Impact of Amazon's Recruiting Algorithm,2023,Xinyu Chang,Advances in Economics Management and Political Sciences,7,W4386641279,10.54254/2754-1169/23/20230367,https://openalex.org/W4386641279,https://www.ewadirect.com/proceedings/aemps/article/view/3655/pdf,Gender bias; Amazon rainforest; Subject (documents); Implicit bias; Diversity (politics),article,True,"Algorithmic bias in artificial intelligence (AI) is a growing concern, especially in the employment sector, where it can have devastating effects on both individuals and society. Gender discrimination is one of the most prevalent forms of algorithmic bias seen in numerous industries, including technology. The underrepresentation of women in the field of information technology is a well-known issue, and several organizations have made tackling this issue a top priority. Amazon, one of the world's top technology businesses, has been at the forefront of initiatives to increase inclusiveness and diversity in the sector. Concerns exist, however, that algorithmic bias in their recruitment process may perpetuate discrimination based on gender. This study intends to investigate these issues by employing an interpretive epistemology and utilizing interviews and focus groups to acquire a more nuanced knowledge of the subject,with keyfactors contributing to algorithmic gender bias in Amazon's recruitmentprocess andrecommend strategies for improving women's employment in information technology."
Exposing the chimp optimization algorithm: A misleading metaheuristic technique with structural bias,2024,Lingyun Deng; Sanyang Liu,Applied Soft Computing,24,W4394566650,10.1016/j.asoc.2024.111574,https://openalex.org/W4394566650,,Metaheuristic; Computer science; Optimization algorithm; Algorithm; Mathematical optimization,article,False,
Addressing Bias in Machine Learning Algorithms: Promoting Fairness and Ethical Design,2024,Dharmesh Dhabliya; Sukhvinder Singh Dari; Anishkumar Dhablia; N. Akhila; Renu Kachhoria; Vinit Khetani,E3S Web of Conferences,13,W4391998543,10.1051/e3sconf/202449102040,https://openalex.org/W4391998543,https://www.e3s-conferences.org/articles/e3sconf/pdf/2024/21/e3sconf_icecs2024_02040.pdf,Computer science; Artificial intelligence; Machine learning; Psychology,article,True,"Machine learning algorithms have quickly risen to the top of several fields' decision-making processes in recent years. However, it is simple for these algorithms to confirm already present prejudices in data, leading to biassed and unfair choices. In this work, we examine bias in machine learning in great detail and offer strategies for promoting fair and moral algorithm design. The paper then emphasises the value of fairnessaware machine learning algorithms, which aim to lessen bias by including fairness constraints into the training and evaluation procedures. Reweighting, adversarial training, and resampling are a few strategies that could be used to overcome prejudice. Machine learning systems that better serve society and respect ethical ideals can be developed by promoting justice, transparency, and inclusivity. This paper lays the groundwork for researchers, practitioners, and policymakers to forward the cause of ethical and fair machine learning through concerted effort."
Strategic Integration of AIGC in Asian Elderly Fashion: Human-Centric Design Enhancement and Algorithmic Bias Neutralization,2024,Hongcai Chen; Vongphantuset Jirawat; Yan Wang,AHFE international,1,W4399479261,10.54941/ahfe1004658,https://openalex.org/W4399479261,,Neutralization; Computer science; Human–computer interaction; Medicine; Immunology,article,False,"The advent of Artificial Intelligence Generated Content (AIGC) has catalyzed transformative shifts in the domain of fashion design, providing novel opportunities for customization and innovation. This research delineates the strategic integration of AIGC within Asian elderly fashion design, critically examining its role in augmenting human-centric design principles while addressing the prevalent algorithmic biases. The objective is to empirically assess the efficacy of AIGC in creating designs that resonate with the functional and aesthetic preferences of the elderly Asian demographic. Employing a mixed-methods approach, the study first delineates the current limitations and potential enhancements AIGC offers to the fashion design process. Through iterative design experiments, AIGC applications are evaluated for their capacity to accommodate the nuanced needs of the target population. Concurrently, a fuzzy evaluation method systematically quantifies the feedback from design practitioners, revealing the salient factors and their relative influence on the AIGC-driven design process. Findings from the study highlight the dichotomy between AIGC's potential for personalized design and the inherent risks of reinforcing biases. The analysis provides a granular understanding of the interplay between AIGC capabilities and user-centered design requirements, emphasizing the necessity for a calibrated approach that prioritizes ethical considerations. The study culminates in a set of actionable guidelines that advocate for the integration of comprehensive educational modules on AIGC technologies in design curricula, aiming to bridge the interdisciplinary gap and enhance designer preparedness. The conclusion underscores the imperative for ongoing scrutiny of AIGC outputs, advocating for the development of robust frameworks that ensure equitable and inclusive design practices. Through this research, a path is charted toward the responsible utilization of AIGC, fostering a fashion industry that is adaptive, empathetic, and attuned to the diverse spectrum of aging consumers in Asia."
Engaging an advisory board in discussions about the ethical relevance of algorithmic bias and fairness,2025,Laura Brandt; Larry Au; Clinton Castro; Gabriel Odom,npj Digital Medicine,1,W4410469255,10.1038/s41746-025-01711-1,https://openalex.org/W4410469255,https://doi.org/10.1038/s41746-025-01711-1,Relevance (law); Advisory committee; Ethical issues; Psychology; Political science,review,True,"We are an interdisciplinary team of researchers that are working to advance algorithmic fairness in the research of opioid use disorders. We discuss challenges that our research team faced when engaging with our Advisory Board, as well as several strategies that we came up with to help us find a common language to ensure semantic transparency and to ensure the thick alignment with values of affected parties."
In This Special Section: Algorithmic Bias—Australia’s Robodebt and Its Human Rights Aftermath,2024,Katina Michael,IEEE Transactions on Technology and Society,1,W4402704508,10.1109/tts.2024.3444248,https://openalex.org/W4402704508,,Section (typography); Human rights; Special section; Political science; Computer science,article,False,
Sociodemographic bias in clinical machine learning models: A scoping review of algorithmic bias instances and mechanisms,2024,Michael Colacci; Yu Qing Huang; Gemma Postill; Pavel Zhelnov; Orna Fennelly; Amol A. Verma; Sharon E. Straus; Andrea C. Tricco,Journal of Clinical Epidemiology,1,W4404210001,10.1016/j.jclinepi.2024.111606,https://openalex.org/W4404210001,https://doi.org/10.1016/j.jclinepi.2024.111606,Machine learning; Artificial intelligence; Computer science; Psychology; MEDLINE,review,False,
Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation,2023,Hao Liang; Pietro Perona; Guha Balakrishnan,arXiv (Cornell University),1,W4385774866,10.48550/arxiv.2308.05441,https://openalex.org/W4385774866,https://arxiv.org/abs/2308.05441,Artificial intelligence; Computer science; Benchmark (surveying); Synthetic data; Pattern recognition (psychology),preprint,True,"We propose an experimental method for measuring bias in face recognition systems. Existing methods to measure bias depend on benchmark datasets that are collected in the wild and annotated for protected (e.g., race, gender) and non-protected (e.g., pose, lighting) attributes. Such observational datasets only permit correlational conclusions, e.g., ""Algorithm A's accuracy is different on female and male faces in dataset X."". By contrast, experimental methods manipulate attributes individually and thus permit causal conclusions, e.g., ""Algorithm A's accuracy is affected by gender and skin color."" Our method is based on generating synthetic faces using a neural face generator, where each attribute of interest is modified independently while leaving all other attributes constant. Human observers crucially provide the ground truth on perceptual identity similarity between synthetic image pairs. We validate our method quantitatively by evaluating race and gender biases of three research-grade face recognition models. Our synthetic pipeline reveals that for these algorithms, accuracy is lower for Black and East Asian population subgroups. Our method can also quantify how perceptual changes in attributes affect face identity distances reported by these models. Our large synthetic dataset, consisting of 48,000 synthetic face image pairs (10,200 unique synthetic faces) and 555,000 human annotations (individual attributes and pairwise identity comparisons) is available to researchers in this important area."
Assessing racial bias in type 2 diabetes risk prediction algorithms,2023,Héléne T. Cronjé; Alexandros Katsiferis; Leonie K. Elsenburg; Thea Otte Andersen; Naja Hulvej Rod; Tri‐Long Nguyen; Tibor V. Varga,PLOS Global Public Health,19,W4376959293,10.1371/journal.pgph.0001556,https://openalex.org/W4376959293,https://journals.plos.org/globalpublichealth/article/file?id=10.1371/journal.pgph.0001556&type=printable,Medicine; Framingham Risk Score; Type 2 diabetes; National Health and Nutrition Examination Survey; Demography,article,True,"Risk prediction models for type 2 diabetes can be useful for the early detection of individuals at high risk. However, models may also bias clinical decision-making processes, for instance by differential risk miscalibration across racial groups. We investigated whether the Prediabetes Risk Test (PRT) issued by the National Diabetes Prevention Program, and two prognostic models, the Framingham Offspring Risk Score, and the ARIC Model, demonstrate racial bias between non-Hispanic Whites and non-Hispanic Blacks. We used National Health and Nutrition Examination Survey (NHANES) data, sampled in six independent two-year batches between 1999 and 2010. A total of 9,987 adults without a prior diagnosis of diabetes and with fasting blood samples available were included. We calculated race- and year-specific average predicted risks of type 2 diabetes according to the risk models. We compared the predicted risks with observed ones extracted from the US Diabetes Surveillance System across racial groups (summary calibration). All investigated models were found to be miscalibrated with regard to race, consistently across the survey years. The Framingham Offspring Risk Score overestimated type 2 diabetes risk for non-Hispanic Whites and underestimated risk for non-Hispanic Blacks. The PRT and the ARIC models overestimated risk for both races, but more so for non-Hispanic Whites. These landmark models overestimated the risk of type 2 diabetes for non-Hispanic Whites more severely than for non-Hispanic Blacks. This may result in a larger proportion of non-Hispanic Whites being prioritized for preventive interventions, but it also increases the risk of overdiagnosis and overtreatment in this group. On the other hand, a larger proportion of non-Hispanic Blacks may be potentially underprioritized and undertreated."
Can large language models estimate public opinion about global warming? An empirical assessment of algorithmic fidelity and bias,2024,Sanguk Lee; Tai‐Quan Peng; Matthew H. Goldberg; Seth A. Rosenthal; John Kotcher; Edward Maibach; Anthony Leiserowitz,PLOS Climate,11,W4401389158,10.1371/journal.pclm.0000429,https://openalex.org/W4401389158,https://journals.plos.org/climate/article/file?id=10.1371/journal.pclm.0000429&type=printable,Global warming; Fidelity; Covariate; Public opinion; Psychology,article,True,"Large language models (LLMs) can be used to estimate human attitudes and behavior, including measures of public opinion, a concept referred to as algorithmic fidelity. This study assesses the algorithmic fidelity and bias of LLMs in estimating public opinion about global warming. LLMs were conditioned on demographics and/or psychological covariates to simulate survey responses. Findings indicate that LLMs can effectively reproduce presidential voting behaviors but not global warming opinions unless the issue relevant covariates are included. When conditioned on both demographic and covariates, GPT-4 demonstrates improved accuracy, ranging from 53% to 91%, in predicting beliefs and attitudes about global warming. Additionally, we find an algorithmic bias that underestimates the global warming opinions of Black Americans. While highlighting the potential of LLMs to aid social science research, these results underscore the importance of conditioning, model selection, survey question format, and bias assessment when employing LLMs for survey simulation."
Measuring algorithmic bias to analyze the reliability of AI tools that predict depression risk using smartphone sensed-behavioral data,2024,Daniel A. Adler; Caitlin A. Stamatis; Jonah Meyerhoff; David C. Mohr; Fei Wang; Gabriel J. Aranovich; Srijan Sen; Tanzeem Choudhury,Research Square (Research Square),1,W4395007622,10.21203/rs.3.rs-3044613/v1,https://openalex.org/W4395007622,https://www.researchsquare.com/article/rs-3044613/latest.pdf,Reliability (semiconductor); Computer science; Depression (economics); Artificial intelligence; Psychology,preprint,True,"<title>Abstract</title> AI tools intend to transform mental healthcare by providing remote estimates of depression risk using behavioral data collected by sensors embedded in smartphones. While these tools accurately predict elevated symptoms in small, homogenous populations, recent studies show that these tools are less accurate in larger, more diverse populations. In this work, we show that accuracy is reduced because sensed-behaviors are unreliable predictors of depression across individuals; specifically the sensed-behaviors that predict depression risk are inconsistent across demographic and socioeconomic subgroups. We first identified subgroups where a developed AI tool underperformed by measuring algorithmic bias, where subgroups with depression were incorrectly predicted to be at lower risk than healthier subgroups. We then found inconsistencies between sensed-behaviors predictive of depression across these subgroups. Our findings suggest that researchers developing AI tools predicting mental health from behavior should think critically about the generalizability of these tools, and consider tailored solutions for targeted populations."
Algorithmic bias in public health AI: a silent threat to equity in low-resource settings,2025,Jeena Joseph,Frontiers in Public Health,1,W4412583896,10.3389/fpubh.2025.1643180,https://openalex.org/W4412583896,https://doi.org/10.3389/fpubh.2025.1643180,Health equity; Equity (law); Public health; Resource (disambiguation); Computer science,article,True,
The Layered Injection Model of Algorithmic Bias as a Conceptual Framework to Understand Biases Impacting the Output of Text-To-Image Models,2025,Dirk Spennemann,,1,W4413477093,10.2139/ssrn.5393987,https://openalex.org/W4413477093,,Image (mathematics); Computer science; Econometrics; Conceptual model; Conceptual framework,preprint,False,
U.S. Antidiscrimination Law in Healthcare &amp; Algorithmic Bias : Its Lessons for South Korean Legislation,2024,Won Bok Lee,Public Law Journal,1,W4399904163,10.31779/plj.25.2.202405.007,https://openalex.org/W4399904163,https://doi.org/10.31779/plj.25.2.202405.007,Legislation; Law; Health care; Political science; Law and economics,article,True,"미국 의료 분야의 차별금지법은 1964년 제정된 민권법에서 시작하여 지금은 Patient Protection and Affordable Care Act의 제1557조로 통합되었다. 이는 의료 인공지능이야기하는 편향이나 차별에 적용되는 법적 프레임워크를 형성한다. 이와 같이 미국은 성문법과 판례가 축적한 차별금지법리가 오랜 세월을 통하여 섬세하게 구축되어 있음에도 불구하고 이를 알고리즘에 적용하는 것에는 어려움이 적지 않다. 특히 의료 분야 차별금지법은 결과적 차별에 대한 개인의 소권을 인정하는데 대하여 소극적인 미국 항소법원들의 입장 때문에 알고리즘의 차별에 적극적으로 대처하기에 미흡하다는 지적도 있다.BR 우리나라의 경우에는 국회에 상정되었던 인공지능 규제 법률안들이나 의료 인공지능에 적용되는 윤리지침 등에서 공히 차별 또는 편향의 예방을 중요한 원칙으로 규정하지만, 의료 분야 차별금지법의 부재 및 외국에 비하여 높은 의료 형평성의 현실로 인하여 과연 어떤 사유를 이유로 하는 차별이나 편향을 예방하여야 할 것인지 아무런 아이디어를 얻을 수 없는 상황이다. 이를 해소하기 위하여는 (실제 입법까지 이르는 것을 상정하지 않더라도) 우리나라에서 의료 분야 차별금지법을 제정할 경우 어떤 모습이 되어야 하는지를 가정적으로 논의하고, 그 과정에서 우리나라 현실에 부합하는 차별금지사유를 추출하는 작업이 먼저 이루어져야 할 것이다."
"Algorithmic Bias in AI-Based Diabetes Care: Systematic Review of Model Performance, Equity Reporting, and Physiological Label Bias",2025,Mohammad Amin Alipour; Abolfazl Alipour,InfoScience Trends,1,W4410482192,10.61186/ist.202502.05.04,https://openalex.org/W4410482192,https://doi.org/10.61186/ist.202502.05.04,Equity (law); Computer science; Psychology; Actuarial science; Econometrics,article,True,
Mind evolutionary algorithm optimization in the prediction of satellite clock bias using the back propagation neural network,2023,Hongwei Bai; Qianqian Cao; Subang An,Scientific Reports,15,W4319298381,10.1038/s41598-023-28855-y,https://openalex.org/W4319298381,https://www.nature.com/articles/s41598-023-28855-y.pdf,Artificial neural network; Computer science; Algorithm; Satellite; Mean squared error,article,True,"Satellite clock bias is the key factor affecting the accuracy of the single point positioning of a global navigation satellite system. The traditional model back propagation (BP) neural network is prone to local optimum problems. This paper presents a prediction model and algorithm for the clock bias of the BP neural network based on the optimization of the mind evolutionary algorithm (MEA), which is used to optimize the initial weights and thresholds of the BP neural network. The accuracy of the comparison between clock bias data is verified with and without one-time difference processing. Compared with grey model (GM (1,1)) and BP neural network, this paper discusses the advantages and general applicability of this method from different constellation satellites, different atomic clock type satellites, and the amount of modeling data. The accuracy of the grey model (GM(1,1)), BP, and MEA-BP models for satellite clock bias prediction is analyzed and the root mean square error, range difference error, and the mean of the clock bias data compared. The results demonstrate that the prediction accuracy of the three satellites significantly increased after one-time difference processing and that they have good stability. The prediction accuracy of four sessions of 2 h, 3 h, 6 h, and 12 h obtained using the MEA-BP model was better than 0.74, 0.80, 1.12, and 0.87 ns, respectively. The MEA-BP model has a specific degree of improvement in the prediction accuracy of the different sessions. Additionally, the prediction accuracy of different models has a specific relationship with the length of the original modeling sequence, of which BP model is the most affected, and MEABP is relatively less affected by the length of the modeling sequence, indicating that the MEA-BP model has strong anti-interference ability."
Opening the ‘black box’ of HRM algorithmic biases – How hiring practices induce discrimination on freelancing platforms,2025,Yannik Trautwein; Felix Zechiel; Kristof Coussement; Matthijs Meire; Marion Büttgen,Journal of Business Research,1,W4408310373,10.1016/j.jbusres.2025.115298,https://openalex.org/W4408310373,https://doi.org/10.1016/j.jbusres.2025.115298,Black box; Business; Computer science; Labour economics; Data science,article,True,
Organizational Dynamics and Bias in Artificial Intelligence (AI) Recruitment Algorithms,2023,Marwan Omar; Darrell Norman Burrell,Advances in business information systems and analytics book series,10,W4391603409,10.4018/979-8-3693-1970-3.ch015,https://openalex.org/W4391603409,,Artificial intelligence; Computer science; Machine learning; Algorithm,book-chapter,False,"The integration of artificial intelligence (AI) into recruitment processes has promised to revolutionize and optimize the hiring landscape. However, recent legal proceedings have shed light on the alarming implications of AI algorithms in the employment sector. This chapter delves into a significant case study where African American, Latina American, Arab American, and other marginalized job applicants and employees filed a 100-million-dollar class action lawsuit against a prominent organization, Context Systems. The suit alleges that AI screening tools, entrusted with the crucial task of selecting candidates, have been marred by programming bias, leading to discriminatory outcomes. This case study critically examines the multifaceted problems arising from bias in AI algorithms, revealing their detrimental effects on marginalized communities in the employment sector. By scrutinizing this pivotal case, the authors aim to provide insights into the urgent need for transparency, accountability, and ethical considerations in the development and deployment of AI-driven recruitment tools."
"Missingness and algorithmic bias: an example from the United States National Outbreak Reporting System, 2009–2019",2024,Emily Diemer; Elena N. Naumova,Journal of Public Health Policy,1,W4396602160,10.1057/s41271-024-00477-2,https://openalex.org/W4396602160,,Public health law; Medical sociology; Social policy; Public health; Health care reform,article,False,
THE ROLE OF AI IN MARKETING PERSONALIZATION: A THEORETICAL EXPLORATION OF CONSUMER ENGAGEMENT STRATEGIES,2024,Sodiq Odetunde Babatunde; Opeyemi Abayomi Odejide; Tolulope Esther Edunjobi; Damilola Oluwaseun Ogundipe,International Journal of Management & Entrepreneurship Research,165,W4393278147,10.51594/ijmer.v6i3.964,https://openalex.org/W4393278147,https://fepbl.com/index.php/ijmer/article/download/964/1179,Personalization; Customer engagement; Marketing; Business; Psychology,article,True,"This paper explores the transformative potential of Artificial Intelligence (AI) in personalizing marketing strategies. It delves into the theoretical underpinnings of consumer engagement sand investigates how AI can be leveraged to develop targeted and relevant marketing experiences. AI can personalize messages based on consumer behavior and demographics, influencing the processing route and maximizing engagement. This theory explores the use of game mechanics to motivate and engage users. AI can personalize gamified marketing experiences, tailoring rewards and challenges to individual consumer preferences, driving deeper engagement. Algorithms can analyze vast amounts of customer data to predict individual preferences and behaviors. This allows for targeted advertising, product recommendations, and content that resonates with specific consumer segments. Natural Language Processing (NLP), AI-powered NLP tools analyze customer reviews, social media conversations, and other forms of unstructured data. This allows brands to understand customer sentiment and personalize communication styles for optimal engagement AI-powered chatbots and virtual assistants can provide personalized customer support and product recommendations in real-time, fostering a more interactive and engaging brand experience. Potential Benefits and Considerations Personalized marketing messages and experiences cater to individual needs and preferences, leading to higher satisfaction and loyalty. By tailoring content and offerings to specific consumer segments, brands can establish a more relevant and relatable image. Improved Conversion Rates, Personalized marketing campaigns can be highly targeted and effective, leading to increased conversions and sales. Balancing personalization with data privacy concerns is crucial. Transparency and user control over data collection practices are essential. AI algorithms can perpetuate biases present in training data. Ensuring fairness and inclusivity in AI-powered marketing is paramount. AI is revolutionizing marketing personalization. By leveraging AI's analytical capabilities and understanding the theoretical aspects of consumer engagement, brands can develop targeted and relevant marketing strategies that foster deeper customer connections and drive business growth.&#x0D; Keywords: AI Personalization, Consumer Engagement, Marketing Strategy, Theoretical Exploration, Data Privacy, Algorithmic Bias."
Unbias me! Mitigating Algorithmic Bias for Less-studied Demographic Groups in the Context of Language  Learning Technology,2023,Nathalie Rzepka; Linda Fernsel; Hans‐Georg Müller; Katharina Simbeck; Niels Pinkwart,Zenodo (CERN European Organization for Nuclear Research),1,W4379092151,10.5281/zenodo.7996194,https://openalex.org/W4379092151,https://zenodo.org/records/7996194/files/CBLC-2023-6-1-1.pdf,Context (archaeology); Computer science; Psychology; Natural language processing; Cognitive psychology,article,True,
Equal accuracy for Andrew and Abubakar—detecting and mitigating bias in name-ethnicity classification algorithms,2023,Lena Hafner; Theodor Peter Peifer; Franziska Sofia Hafner,AI & Society,12,W4319661985,10.1007/s00146-022-01619-4,https://openalex.org/W4319661985,https://link.springer.com/content/pdf/10.1007/s00146-022-01619-4.pdf,Ethnic group; Computer science; Audit; Artificial intelligence; Contrast (vision),article,True,"Abstract Uncovering the world’s ethnic inequalities is hampered by a lack of ethnicity-annotated datasets. Name-ethnicity classifiers (NECs) can help, as they are able to infer people’s ethnicities from their names. However, since the latest generation of NECs rely on machine learning and artificial intelligence (AI), they may suffer from the same racist and sexist biases found in many AIs. Therefore, this paper offers an algorithmic fairness audit of three NECs. It finds that the UK-Census-trained EthnicityEstimator displays large accuracy biases with regards to ethnicity, but relatively less among gender and age groups. In contrast, the Twitter-trained NamePrism and the Wikipedia-trained Ethnicolr are more balanced among ethnicity, but less among gender and age. We relate these biases to global power structures manifested in naming conventions and NECs’ input distribution of names. To improve on the uncovered biases, we program a novel NEC, N2E , using fairness-aware AI techniques. We make N2E freely available at www.name-to-ethnicity.com ."
Using artificial intelligence in craft education: crafting with text-to-image generative models,2023,Henriikka Vartiainen; Matti Tedre,Digital Creativity,145,W4321596574,10.1080/14626268.2023.2174557,https://openalex.org/W4321596574,https://doi.org/10.1080/14626268.2023.2174557,Craft; Computer science; Generative grammar; Multimedia; Artificial intelligence,article,True,"Artificial intelligence (AI) and the automation of creative work have received little attention in craft education. This study aimed to address this gap by exploring Finnish pre-service craft teachers' and teacher educators' (N = 15) insights into the potential benefits and challenges of AI, particularly text-to-image generative AI. This study implemented a hands-on workshop on creative making with text-to-image generative AI in order to stimulate discourses and capture imaginaries concerning generative AI. The results revealed that making with AI inspired teachers to consider the unique nature of crafts as well as the tensions and tradeoffs of adopting generative AI in craft practices. The teachers identified concerns in data-driven design, including algorithmic bias, copyright violations and black-boxing creativity, as well as in power relationships, hybrid influencing and behaviour engineering. The article concludes with a discussion of the complicated relationships the results uncovered between creative making and generative AI."
Call for algorithmic fairness to mitigate amplification of racial biases in artificial intelligence models used in orthodontics and craniofacial health,2023,Veerasathpurush Allareddy; Maysaa Oubaidin; Sankeerth Rampa; Shankar Rengasamy Venugopalan; Mohammed H. Elnagar; Sumit Yadav; Min Kyeong Lee,Orthodontics and Craniofacial Research,14,W4387692646,10.1111/ocr.12721,https://openalex.org/W4387692646,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/ocr.12721,Orthodontics; Craniofacial; Computer science; Psychology; Artificial intelligence,review,True,"Machine Learning (ML), a subfield of Artificial Intelligence (AI), is being increasingly used in Orthodontics and craniofacial health for predicting clinical outcomes. Current ML/AI models are prone to accentuate racial disparities. The objective of this narrative review is to provide an overview of how AI/ML models perpetuate racial biases and how we can mitigate this situation. A narrative review of articles published in the medical literature on racial biases and the use of AI/ML models was undertaken. Current AI/ML models are built on homogenous clinical datasets that have a gross underrepresentation of historically disadvantages demographic groups, especially the ethno-racial minorities. The consequence of such AI/ML models is that they perform poorly when deployed on ethno-racial minorities thus further amplifying racial biases. Healthcare providers, policymakers, AI developers and all stakeholders should pay close attention to various steps in the pipeline of building AI/ML models and every effort must be made to establish algorithmic fairness to redress inequities."
"Fairness and Bias in Artificial Intelligence: A Brief Survey of Sources, Impacts, and Mitigation Strategies",2023,Emilio Ferrara,Sci,371,W4390229867,10.3390/sci6010003,https://openalex.org/W4390229867,https://www.mdpi.com/2413-4155/6/1/3/pdf?version=1703577406,Generative grammar; Artificial intelligence; Generative model; Computer science; Selection bias,article,True,"The significant advancements in applying artificial intelligence (AI) to healthcare decision-making, medical diagnosis, and other domains have simultaneously raised concerns about the fairness and bias of AI systems. This is particularly critical in areas like healthcare, employment, criminal justice, credit scoring, and increasingly, in generative AI models (GenAI) that produce synthetic media. Such systems can lead to unfair outcomes and perpetuate existing inequalities, including generative biases that affect the representation of individuals in synthetic data. This survey study offers a succinct, comprehensive overview of fairness and bias in AI, addressing their sources, impacts, and mitigation strategies. We review sources of bias, such as data, algorithm, and human decision biases—highlighting the emergent issue of generative AI bias, where models may reproduce and amplify societal stereotypes. We assess the societal impact of biased AI systems, focusing on perpetuating inequalities and reinforcing harmful stereotypes, especially as generative AI becomes more prevalent in creating content that influences public perception. We explore various proposed mitigation strategies, discuss the ethical considerations of their implementation, and emphasize the need for interdisciplinary collaboration to ensure effectiveness. Through a systematic literature review spanning multiple academic disciplines, we present definitions of AI bias and its different types, including a detailed look at generative AI bias. We discuss the negative impacts of AI bias on individuals and society and provide an overview of current approaches to mitigate AI bias, including data pre-processing, model selection, and post-processing. We emphasize the unique challenges presented by generative AI models and the importance of strategies specifically tailored to address these. Addressing bias in AI requires a holistic approach involving diverse and representative datasets, enhanced transparency and accountability in AI systems, and the exploration of alternative AI paradigms that prioritize fairness and ethical considerations. This survey contributes to the ongoing discussion on developing fair and unbiased AI systems by providing an overview of the sources, impacts, and mitigation strategies related to AI bias, with a particular focus on the emerging field of generative AI."
AI in education: A review of personalized learning and educational technology,2024,Oyebola Olusola Ayeni; Nancy Mohd Al Hamad; Onyebuchi Nneamaka Chisom; Blessing Osawaru; Ololade Elizabeth Adewusi,GSC Advanced Research and Reviews,169,W4392772072,10.30574/gscarr.2024.18.2.0062,https://openalex.org/W4392772072,https://gsconlinepress.com/journals/gscarr/sites/default/files/GSCARR-2024-0062.pdf,Personalized learning; Mathematics education; Educational technology; Computer science; Psychology,review,True,"The integration of Artificial Intelligence (AI) in education has ushered in a transformative era, redefining traditional teaching and learning methods. This review explores the multifaceted role of AI in education, with a particular focus on personalized learning and educational technology. The synergy between AI and education promises to address individualized needs, enhance student engagement, and optimize learning outcomes. Personalized learning, enabled by AI algorithms, tailors educational experiences to the unique needs, preferences, and pace of each student. This approach goes beyond a one-size-fits-all model, fostering a more inclusive and effective learning environment. The review delves into the diverse applications of AI-driven personalized learning, ranging from adaptive content delivery and real-time feedback to intelligent tutoring systems. It analyzes the impact of these technologies on student performance, highlighting the potential to narrow educational gaps and cater to diverse learning styles. Educational technology, powered by AI, extends beyond the classroom, encompassing online platforms, virtual reality, and interactive tools. The review explores the integration of AI in curriculum development, content creation, and assessment methods, offering insights into how these technologies augment the teaching and learning experience. Furthermore, the review examines the role of AI in automating administrative tasks, allowing educators to redirect their focus towards personalized instruction. Challenges and ethical considerations associated with the adoption of AI in education are also scrutinized. Privacy concerns, algorithmic biases, and the digital divide are discussed, emphasizing the importance of responsible AI implementation. The review underscores the need for collaborative efforts among educators, policymakers, and technologists to establish ethical guidelines and ensure the equitable distribution of AI-enhanced educational resources. This review provides a comprehensive examination of the evolving landscape of AI in education, with a spotlight on personalized learning and educational technology. As the symbiosis between AI and education continues to evolve, this synthesis of current research and trends aims to guide future developments, fostering an informed and progressive approach to the integration of AI in the educational sphere."
Algorithms propagate gender bias in the marketplace—with consumers’ cooperation,2023,Shelly Rathee; Sachin Banker; Arul Mishra; Himanshu Mishra,Journal of Consumer Psychology,13,W4365449190,10.1002/jcpy.1351,https://openalex.org/W4365449190,,Psychographic; Gender bias; Product (mathematics); Field (mathematics); Digital advertising,article,False,
"Data and science engineering: The ethical dilemma of our time-exploring privacy breaches, algorithmic biases, and the need for transparency",2023,Shubham Shubham; Saloni Saloni; Sidra-Tul-Muntaha,Zenodo (CERN European Organization for Nuclear Research),1,W4384921908,10.5281/zenodo.8172333,https://openalex.org/W4384921908,https://zenodo.org/records/8172333/files/WJARR-2023-0677.pdf,Transparency (behavior); Dilemma; Internet privacy; Ethical dilemma; Information privacy,article,True,
Biased stochastic conjugate gradient algorithm with adaptive step size for nonconvex problems,2023,Ruping Huang; Yan Qin; Kejun Liu; Gonglin Yuan,Expert Systems with Applications,17,W4386969692,10.1016/j.eswa.2023.121556,https://openalex.org/W4386969692,,Conjugate gradient method; Stochastic gradient descent; Convergence (economics); Algorithm; Gradient method,article,False,
Using Cognitive Models to Understand and Counteract the Effect of Self-Induced Bias on Recommendation Algorithms,2023,Justyna Pawłowska; Klara Rydzewska; Adam Wierzbicki,Journal of Artificial Intelligence and Soft Computing Research,10,W4323928465,10.2478/jaiscr-2023-0008,https://openalex.org/W4323928465,https://sciendo.com/pdf/10.2478/jaiscr-2023-0008,Collaborative filtering; Computer science; Cognition; Recommender system; Set (abstract data type),article,True,"Abstract Recommendation algorithms trained on a training set containing sub-optimal decisions may increase the likelihood of making more bad decisions in the future. We call this harmful effect self-induced bias, to emphasize that the bias is driven directly by the user’s past choices. In order to better understand the nature of self-induced bias of recommendation algorithms that are used by older adults with cognitive limitations, we have used agent-based simulation. Based on state-of-the-art results in psychology of aging and cognitive science, as well as our own empirical results, we have developed a cognitive model of an e-commerce client that incorporates cognitive decision-making abilities. We have evaluated the magnitude of self-induced bias by comparing results achieved by simulated agents with and without cognitive limitations due to age. We have also proposed new recommendation algorithms designed to counteract self-induced bias. The algorithms take into account user preferences and cognitive abilities relevant to decision making. To evaluate the algorithms, we have introduced 3 benchmarks: a simple product filtering method and two types of widely used recommendation algorithms: Content-Based and Collaborative filtering. Results indicate that the new algorithms outperform benchmarks both in terms of increasing the utility of simulated agents (both old and young), and in reducing self-induced bias."
Analyzing Bias in Recommender Systems: A Comprehensive Evaluation of YouTube's Recommendation Algorithm,2023,Mert Can Çakmak; Obianuju Okeke; Ugochukwu Onyepunuka; Billy Spann; Nitin Agarwal,,9,W4392845346,10.1145/3625007.3627300,https://openalex.org/W4392845346,https://dl.acm.org/doi/pdf/10.1145/3625007.3627300,Recommender system; Computer science; Collaborative filtering; Filter (signal processing); Information retrieval,article,True,"Recommender systems play a crucial role in suggesting relevant content to users based on their past activities. They employ a process known as ""collaborative filtering"" to efficiently navigate extensive content repositories. However, concerns have been raised regarding the potential bias and homogeneity in recommendations, resulting in filter-bubbles and echo-chambers. Detecting and mitigating these biases is crucial for ensuring fair and diverse automated decision-making systems. This study investigates the impact of YouTube's recommendation algorithm on three distinct narratives across multiple dimensions. Our objective is to identify potential biases and gain insights into its decision-making behavior. We applied a multi-method approach to evaluate emotional content, moral foundations, lexical similarity, and social network analysis across 5 depths of YouTube recommendations. The results of our analysis showed diversity in emotions, significant drift in topics, and a push toward non-related, but highly influential videos across multiple recommendation depths. The findings from this study contribute to the understanding of bias in recommender systems. These insights inform the development of strategies to mitigate biases and improve the user experience. Policymakers and platform developers can utilize this knowledge to establish effective guidelines and policies for their recommender systems, enhancing decision-making processes."
"Research on Ethical Issues, Data Privacy Protection, Algorithmic Bias, and Regulatory Policy of Artificial Intelligence Technology in Digital Transformation",2025,R Zou,Applied economics and policy studies,1,W4409136616,10.1007/978-981-96-3236-7_21,https://openalex.org/W4409136616,,Digital transformation; Data Protection Act 1998; Transformation (genetics); Privacy protection; Privacy policy,book-chapter,False,
Addressing algorithmic bias in AI-based pain management: a comparative analysis of inference models and pretrained large language models,2025,Xiang Liu; Wei Gao; Pengfei Li; Xiaohua Fan; Chao Wang,Pain,1,W4413394921,10.1097/j.pain.0000000000003694,https://openalex.org/W4413394921,,Inference; Computer science; Natural language processing; Pain management; Artificial intelligence,article,False,
Ethical implications of AI and robotics in healthcare: A review,2023,Chukwuka Elendu; Dependable C. Amaechi; Tochi C. Elendu; Klein A. Jingwa; Osinachi K. Okoye; Minichimso John Okah; John A. Ladele; Abdirahman H. Farah; Hameed A. Alimi,Medicine,183,W4389775637,10.1097/md.0000000000036671,https://openalex.org/W4389775637,https://doi.org/10.1097/md.0000000000036671,Accountability; Health care; Transparency (behavior); Robotics; Artificial intelligence,review,True,"Integrating Artificial Intelligence (AI) and robotics in healthcare heralds a new era of medical innovation, promising enhanced diagnostics, streamlined processes, and improved patient care. However, this technological revolution is accompanied by intricate ethical implications that demand meticulous consideration. This article navigates the complex ethical terrain surrounding AI and robotics in healthcare, delving into specific dimensions and providing strategies and best practices for ethical navigation. Privacy and data security are paramount concerns, necessitating robust encryption and anonymization techniques to safeguard patient data. Responsible data handling practices, including decentralized data sharing, are critical to preserve patient privacy. Algorithmic bias poses a significant challenge, demanding diverse datasets and ongoing monitoring to ensure fairness. Transparency and explainability in AI decision-making processes enhance trust and accountability. Clear responsibility frameworks are essential to address the accountability of manufacturers, healthcare institutions, and professionals. Ethical guidelines, regularly updated and accessible to all stakeholders, guide decision-making in this dynamic landscape. Moreover, the societal implications of AI and robotics extend to accessibility, equity, and societal trust. Strategies to bridge the digital divide and ensure equitable access must be prioritized. Global collaboration is pivotal in developing adaptable regulations and addressing legal challenges like liability and intellectual property. Ethics must remain at the forefront in the ever-evolving realm of healthcare technology. By embracing these strategies and best practices, healthcare systems and professionals can harness the potential of AI and robotics, ensuring responsible and ethical integration that benefits patients while upholding the highest ethical standards."
"Regulatory and Ethical Challenges in AI-Driven and Machine learning Credit Risk Assessment for Buy Now, Pay Later (BNPL) in U.S. E-Commerce: Compliance, Fair Lending, and Algorithmic Bias",2025,Aditya Mishra; Sanjida Nowshin Mou; Jannat Ara; M. Sarkar,Journal of Business and Management Studies,1,W4408289860,10.32996/jbms.2025.7.2.3,https://openalex.org/W4408289860,https://doi.org/10.32996/jbms.2025.7.2.3,Compliance (psychology); Business; Actuarial science; Credit risk; Accounting,article,True,"The integration of artificial intelligence (AI) and machine learning (ML) in credit risk assessment for Buy Now, Pay Later (BNPL) services has transformed the U.S. e-commerce landscape. However, these advancements present significant regulatory and ethical challenges, particularly regarding compliance, fair lending practices, and algorithmic bias. This study examines the legal framework governing BNPL credit assessments, including adherence to the Equal Credit Opportunity Act (ECOA), Fair Credit Reporting Act (FCRA), and other consumer protection regulations (Federal Trade Commission [FTC], 2022; U.S. Consumer Financial Protection Bureau [CFPB], 2023). Additionally, the paper explores the implications of algorithmic bias in AI-driven credit decisions, highlighting the potential for disparate impacts on marginalized communities (Bartlett et al., 2022; Bragg, 2021; Zarsky, 2016). The ethical concerns surrounding transparency, explain ability, and consumer rights are also discussed (Kroll et al., 2017; Pasquale, 2020). A comparative analysis of current regulatory approaches and proposed reforms is conducted, with a focus on mitigating bias and ensuring equitable access to credit. This research concludes with recommendations for policymakers, regulators, and financial technology firms to foster responsible AI deployment in BNPL services while safeguarding consumer protection and financial inclusion."
"Ethical Considerations of AI in Financial Services: Privacy, Bias, and Algorithmic Transparency",2024,Naila Iqbal Qureshi; Saurabh Suman Choudhuri; Yaramala Nagamani; Raj Varma; Rutul Shah,,14,W4401387946,10.1109/ickecs61492.2024.10616483,https://openalex.org/W4401387946,,Transparency (behavior); Information privacy; Computer science; Internet privacy; Accounting,article,False,
A biased random-key genetic algorithm for the chordal completion problem,2023,Samuel Eduardo da Silva; Celso C. Ribeiro; Uéverton S. Souza,RAIRO - Operations Research,8,W4379387944,10.1051/ro/2023081,https://openalex.org/W4379387944,https://doi.org/10.1051/ro/2023081,Chordal graph; Heuristics; Chord (peer-to-peer); Algorithm; Implementation,article,True,"A graph is chordal if all its cycles of length greater than or equal to four contain a chord, i.e. , an edge connecting two nonconsecutive vertices of the cycle. Given a graph G = ( V , E ), the chordal completion problem consists in finding the minimum set of edges to be added to G to obtain a chordal graph. It has applications in sparse linear systems, database management and computer vision programming. In this article, we developed a biased random-key genetic algorithm (BRKGA) for solving the chordal completion problem, based on the strategy of manipulating permutations that represent perfect elimination orderings of triangulations. Computational results show that the proposed heuristic improve the results of the constructive heuristics fill-in and min-degree. We also developed a strategy for injecting externally constructed feasible solutions coded as random keys into the initial population of the BRKGA that significantly improves the solutions obtained and may benefit other implementations of biased random-key genetic algorithms."
"Robust Bias-Compensated LMS Algorithm: Design, Performance Analysis and Applications",2023,Fuyi Huang; Fan Song; Sheng Zhang; Hing Cheung So; Jun Yang,IEEE Transactions on Vehicular Technology,8,W4376851446,10.1109/tvt.2023.3276573,https://openalex.org/W4376851446,,Least mean squares filter; Estimator; Algorithm; Estimation theory; Adaptive filter,article,False,"This paper considers the problem of system parameter estimation using adaptive filter. Conventional adaptive algorithms will result in degraded performance in the presence of impulsive noise and biased estimation when the input signal is noisy. To address these issues, this paper proposes a robust bias-compensated least mean squares (R-BC-LMS) algorithm. It is derived by performing the maximum- <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">a</b> - <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">posteriori</b> estimation subject to a constraint on the squared norm of the weight vector difference, and then introducing an unbiasedness criterion to insert a bias compensation term in the update. Under common statistical assumptions, the mean and mean square behaviors of weight deviation are derived for the R-BC-LMS algorithm. In addition, we develop the estimator for the input and output noise variances. Simulations in channel estimation, vehicle handsfree echo cancellation, and direction-of-arrival estimation demonstrate that our method outperforms the competing algorithms."
The bias beneath: analyzing drift in YouTube’s algorithmic recommendations,2024,Mert Can Çakmak; Nitin Agarwal; Remi Oni,Social Network Analysis and Mining,7,W4401843528,10.1007/s13278-024-01343-5,https://openalex.org/W4401843528,https://doi.org/10.1007/s13278-024-01343-5,Computer science; Limiting; Transparency (behavior); Variety (cybernetics); Space (punctuation),article,True,"Abstract In today’s digital world, understanding how YouTube’s recommendation systems guide what we watch is crucial. This study dives into these systems, revealing how they influence the content we see over time. We found that YouTube’s algorithms tend to push content in certain directions, affecting the variety and type of videos recommended to viewers. To uncover these patterns, we used a mixed methods approach to analyze videos recommended by YouTube. We looked at the emotions conveyed in videos, the moral messages they might carry, and whether they contained harmful content. Our research also involved statistical analysis to detect biases in how these videos are recommended and network analysis to see how certain videos become more influential than others. Our findings show that YouTube’s algorithms can lead to a narrowing of the content landscape, limiting the diversity of what gets recommended. This has important implications for how information is spread and consumed online, suggesting a need for more transparency and fairness in how these algorithms work. In summary, this paper highlights the need for a more inclusive approach to how digital platforms recommend content. By better understanding the impact of YouTube’s algorithms, we can work towards creating a digital space that offers a wider range of perspectives and voices, affording fairness, and enriching everyone’s online experience."
Making decisions: Bias in artificial intelligence and data‑driven diagnostic tools,2023,Yves Saint James Aquino,Australian Journal of General Practice,35,W4383710950,10.31128/ajgp-12-22-6630,https://openalex.org/W4383710950,https://www1.racgp.org.au/getattachment/ae297919-ed4c-4add-9dc0-244c3acc08cc/Making-decisions.aspx,Compromise; Replicate; Computer science; Artificial intelligence; Disadvantaged,article,True,"Although numerous studies have shown the potential of artificial intelligence (AI) systems in drastically improving clinical practice, there are concerns that these AI systems could replicate existing biases.This paper provides a brief overview of 'algorithmic bias', which refers to the tendency of some AI systems to perform poorly for disadvantaged or marginalised groups.AI relies on data generated, collected, recorded and labelled by humans. If AI systems remain unchecked, whatever biases that exist in the real world that are embedded in data will be incorporated into the AI algorithms. Algorithmic bias can be considered as an extension, if not a new manifestation, of existing social biases, understood as negative attitudes towards or the discriminatory treatment of some groups. In medicine, algorithmic bias can compromise patient safety and risks perpetuating disparities in care and outcome. Thus, clinicians should consider the risk of bias when deploying AI-enabled tools in their practice."
Predictive algorithms and racial bias: a qualitative descriptive study on the perceptions of algorithm accuracy in higher education,2023,Stacey Lynn von Winckelmann,Information and Learning Sciences,7,W4387024961,10.1108/ils-05-2023-0045,https://openalex.org/W4387024961,,Data collection; Algorithm; Originality; Stakeholder; Perception,article,False,"Purpose This study aims to explore the perception of algorithm accuracy among data professionals in higher education. Design/methodology/approach Social justice theory guided the qualitative descriptive study and emphasized four principles: access, participation, equity and human rights. Data collection included eight online open-ended questionnaires and six semi-structured interviews. Participants included higher education professionals who have worked with predictive algorithm (PA) recommendations programmed with student data. Findings Participants are aware of systemic and racial bias in their PA inputs and outputs and acknowledge their responsibility to ethically use PA recommendations with students in historically underrepresented groups (HUGs). For some participants, examining these topics through the lens of social justice was a new experience, which caused them to look at PAs in new ways. Research limitations/implications Small sample size is a limitation of the study. Implications for practice include increased stakeholder training, creating an ethical data strategy that protects students, incorporating adverse childhood experiences data with algorithm recommendations, and applying a modified critical race theory framework to algorithm outputs. Originality/value The study explored the perception of algorithm accuracy among data professionals in higher education. Examining this topic through a social justice lens contributes to limited research in the field. It also presents implications for addressing racial bias when using PAs with students in HUGs."
Fairness of artificial intelligence in healthcare: review and recommendations,2023,Daiju Ueda; Taichi Kakinuma; Shohei Fujita; Koji Kamagata; Yasutaka Fushimi; Rintaro Ito; Yusuke Matsui; Taiki Nozaki; Takeshi Nakaura; Noriyuki Fujima; Fuminari Tatsugami; Masahiro Yanagawa; Kenji Hirata; Akira Yamada; Takahiro Tsuboyama; Mariko Kawamura; Tomoyuki Fujioka; Shinji Naganawa,Japanese Journal of Radiology,286,W4385564466,10.1007/s11604-023-01474-3,https://openalex.org/W4385564466,https://link.springer.com/content/pdf/10.1007/s11604-023-01474-3.pdf,Accountability; Health care; Transparency (behavior); Computer science; Software deployment,review,True,"Abstract In this review, we address the issue of fairness in the clinical integration of artificial intelligence (AI) in the medical field. As the clinical adoption of deep learning algorithms, a subfield of AI, progresses, concerns have arisen regarding the impact of AI biases and discrimination on patient health. This review aims to provide a comprehensive overview of concerns associated with AI fairness; discuss strategies to mitigate AI biases; and emphasize the need for cooperation among physicians, AI researchers, AI developers, policymakers, and patients to ensure equitable AI integration. First, we define and introduce the concept of fairness in AI applications in healthcare and radiology, emphasizing the benefits and challenges of incorporating AI into clinical practice. Next, we delve into concerns regarding fairness in healthcare, addressing the various causes of biases in AI and potential concerns such as misdiagnosis, unequal access to treatment, and ethical considerations. We then outline strategies for addressing fairness, such as the importance of diverse and representative data and algorithm audits. Additionally, we discuss ethical and legal considerations such as data privacy, responsibility, accountability, transparency, and explainability in AI. Finally, we present the Fairness of Artificial Intelligence Recommendations in healthcare (FAIR) statement to offer best practices. Through these efforts, we aim to provide a foundation for discussing the responsible and equitable implementation and deployment of AI in healthcare."
Large language models to identify social determinants of health in electronic health records,2024,Marco Guevara-Vega; Shan Chen; Spencer A. Thomas; Tafadzwa L. Chaunzwa; Idalid Franco; Benjamin H. Kann; Shalini Moningi; Jack M. Qian; Madeleine Goldstein; Susan Harper; Hugo J.W.L. Aerts; Paul J. Catalano; Guergana Savova; Raymond H. Mak; Danielle S. Bitterman,npj Digital Medicine,147,W4390745503,10.1038/s41746-023-00970-0,https://openalex.org/W4390745503,https://www.nature.com/articles/s41746-023-00970-0.pdf,Social determinants of health; Health care; Medicine; Political science; Law,article,True,"Abstract Social determinants of health (SDoH) play a critical role in patient outcomes, yet their documentation is often missing or incomplete in the structured data of electronic health records (EHRs). Large language models (LLMs) could enable high-throughput extraction of SDoH from the EHR to support research and clinical care. However, class imbalance and data limitations present challenges for this sparsely documented yet critical information. Here, we investigated the optimal methods for using LLMs to extract six SDoH categories from narrative text in the EHR: employment, housing, transportation, parental status, relationship, and social support. The best-performing models were fine-tuned Flan-T5 XL for any SDoH mentions (macro-F1 0.71), and Flan-T5 XXL for adverse SDoH mentions (macro-F1 0.70). Adding LLM-generated synthetic data to training varied across models and architecture, but improved the performance of smaller Flan-T5 models (delta F1 + 0.12 to +0.23). Our best-fine-tuned models outperformed zero- and few-shot performance of ChatGPT-family models in the zero- and few-shot setting, except GPT4 with 10-shot prompting for adverse SDoH. Fine-tuned models were less likely than ChatGPT to change their prediction when race/ethnicity and gender descriptors were added to the text, suggesting less algorithmic bias ( p &lt; 0.05). Our models identified 93.8% of patients with adverse SDoH, while ICD-10 codes captured 2.0%. These results demonstrate the potential of LLMs in improving real-world evidence on SDoH and assisting in identifying patients who could benefit from resource support."
Uncovering structural bias in population-based optimization algorithms: A theoretical and simulation-based analysis of the Generalized Signature Test,2023,Kanchan Rajwar; Kusum Deep,Expert Systems with Applications,19,W4388089286,10.1016/j.eswa.2023.122332,https://openalex.org/W4388089286,,Population; Metaheuristic; Algorithm; Computer science; Differential evolution,article,False,
Solution of SAT problems with the adaptive-bias quantum approximate optimization algorithm,2023,Yunlong Yu; Chenfeng Cao; Xiang‐Bin Wang; Nic Shannon; Robert Joynt,Physical Review Research,8,W4379231024,10.1103/physrevresearch.5.023147,https://openalex.org/W4379231024,http://link.aps.org/pdf/10.1103/PhysRevResearch.5.023147,Computer science; Optimization problem; Quantum entanglement; Quantum; Mathematical optimization,article,True,"The quantum approximate optimization algorithm (QAOA) is a promising method for solving certain classical combinatorial optimization problems on near-term quantum devices. When employing the QAOA to 3-SAT and Max-3-SAT problems, the quantum cost exhibits an easy-hard-easy or easy-hard pattern, respectively, as the clause density is changed. The quantum resources needed in the hard-region problems are out of reach for current noisy intermediate-scale quantum (NISQ) devices. We show by numerical simulations with up to 14 variables and analytical arguments that the adaptive-bias QAOA (ab-QAOA) greatly improves performance in the hard region of the 3-SAT problems and hard region of the Max-3-SAT problems. For similar accuracy, on average, ab-QAOA needs 3 levels for 10-variable 3-SAT problems as compared to 22 for QAOA. For 10-variable Max-3-SAT problems, the numbers are 7 levels and 62 levels. The improvement comes from a more targeted and more limited generation of entanglement during the evolution. We demonstrate that classical optimization is not strictly necessary in the ab-QAOA since local fields are used to guide the evolution. This leads us to propose an optimization-free ab-QAOA that can solve the hard-region 3-SAT and Max-3-SAT problems effectively with significantly fewer quantum gates as compared to the original ab-QAOA. Our work paves the way for realizing quantum advantages for optimization problems on NISQ devices."
REVOLUTIONIZING EDUCATION THROUGH AI: A COMPREHENSIVE REVIEW OF ENHANCING LEARNING EXPERIENCES,2024,Oseremi Onesi-Ozigagun; Yinka James Ololade; Nsisong Louis Eyo-Udo; Damilola Oluwaseun Ogundipe,International Journal of Applied Research in Social Sciences,155,W4394688113,10.51594/ijarss.v6i4.1011,https://openalex.org/W4394688113,https://fepbl.com/index.php/ijarss/article/download/1011/1233,Engineering ethics; Psychology; Engineering,review,True,"Artificial Intelligence (AI) is transforming the landscape of education, offering innovative solutions to enhance learning experiences. This review provides a comprehensive overview of how AI is revolutionizing education, focusing on its impact on learning outcomes, teaching methodologies, and the overall educational ecosystem. The adoption of AI in education has led to personalized learning experiences tailored to individual student needs. AI-powered adaptive learning systems analyze student performance data to create customized learning paths, ensuring that students receive content at their pace and level of understanding. This personalized approach improves student engagement and academic performance. AI is also reshaping teaching methodologies, providing educators with tools to streamline administrative tasks and enhance instructional strategies. AI-powered tools can automate grading, create interactive lessons, and provide real-time feedback to students. This allows teachers to focus more on facilitating learning and developing critical thinking skills in students. Furthermore, AI is revolutionizing the assessment process, moving beyond traditional exams to more dynamic and insightful evaluation methods. AI-powered assessment tools can analyze student responses in real-time, providing immediate feedback and insights into student comprehension and learning progress. The integration of AI in education also extends to administrative functions, such as student enrollment, scheduling, and resource allocation. AI-powered systems can optimize these processes, leading to more efficient and effective management of educational institutions. Despite the numerous benefits of AI in education, challenges remain, including concerns about data privacy, algorithmic bias, and the need for teacher training. Addressing these challenges will be crucial to maximizing the potential of AI in education and ensuring equitable access to quality education for all. In conclusion, AI is revolutionizing education by enhancing learning experiences, transforming teaching methodologies, and optimizing administrative processes. As AI continues to evolve, its impact on education is expected to grow, offering new opportunities to improve learning outcomes and prepare students for success in the digital age.&#x0D; Keywords: Revolutionizing, AI, Enhancing, Learning, Experiences."
E-commerce and consumer behavior: A review of AI-powered personalization and market trends,2024,Mustafa Ayobami Raji; Hameedat Bukola Olodo; Timothy Tolulope Oke; Wilhelmina Afua Addy; Onyeka Chrisanctus Ofodile; Adedoyin Tolulope Oyewole,GSC Advanced Research and Reviews,109,W4392621828,10.30574/gscarr.2024.18.3.0090,https://openalex.org/W4392621828,https://gsconlinepress.com/journals/gscarr/sites/default/files/GSCARR-2024-0090.pdf,Personalization; E-commerce; Business; Advertising; Commerce,review,True,"In the dynamic landscape of electronic commerce (e-commerce), understanding and adapting to evolving consumer behavior is critical for the sustained success of online businesses. This review delves into the intersection of e-commerce and consumer behavior, focusing on the transformative role of Artificial Intelligence (AI)-powered personalization and its impact on market trends. The advent of AI has revolutionized the way e-commerce platforms engage with and cater to individual consumer preferences. AI-powered personalization techniques leverage advanced algorithms to analyze vast datasets, enabling the delivery of highly tailored and relevant content, product recommendations, and user experiences. This review explores the intricate mechanisms of AI-driven personalization, examining how it enhances customer engagement, satisfaction, and loyalty. Furthermore, the study investigates the prominent market trends shaped by AI in e-commerce. From chatbots and virtual assistants facilitating seamless customer interactions to predictive analytics optimizing inventory management, AI is driving innovation across various facets of the online retail landscape. The analysis delves into the integration of machine learning algorithms in predicting consumer preferences, streamlining the purchasing process, and fostering a more personalized shopping journey. As e-commerce continues to evolve, the review also explores the challenges and ethical considerations associated with AI-powered personalization. Issues such as data privacy, algorithmic bias, and the delicate balance between customization and intrusiveness are examined to provide a comprehensive understanding of the broader implications of AI in shaping consumer behavior. Ultimately, this review offers valuable insights into the symbiotic relationship between e-commerce and consumer behavior, shedding light on the transformative power of AI-powered personalization and its influence on emerging market trends. As businesses navigate the digital landscape, understanding and harnessing the potential of AI-driven strategies become imperative for staying competitive and meeting the evolving expectations of tech-savvy consumers."
Exploring Gender Bias and Algorithm Transparency: Ethical Considerations of AI in HRM,2024,Jiaxing Du,Journal of Theory and Practice of Management Science,6,W4393522655,10.53469/jtpms.2024.04(03).06,https://openalex.org/W4393522655,https://centuryscipub.com/index.php/JTPMS/article/download/539/461,Transparency (behavior); Computer science; Psychology; Algorithm; Computer security,article,True,"Opportunities and challenges are introduced by the integration of Artificial Intelligence (AI) into Human Resource Management (HRM). The paragraph discusses the ethical implications of AI applications in HRM, focusing on gender bias and algorithm transparency. It explores how AI-driven decision-making in HRM perpetuates gender bias, the importance of transparent algorithms for trust and accountability, and the role of regulatory frameworks in safeguarding ethical standards. The paper aims to provide a comprehensive analysis of the ethical landscape of AI in HRM and offers policy recommendations to mitigate bias and enhance transparency."
Ethical and Bias Considerations in Artificial Intelligence (AI)/Machine Learning,2024,Matthew G. Hanna; Liron Pantanowitz; Brian Jackson; Octavia M. Peck Palmer; Shyam Visweswaran; Joshua Pantanowitz; Mustafa Deebajah; Hooman H. Rashidi,Modern Pathology,102,W4405439405,10.1016/j.modpat.2024.100686,https://openalex.org/W4405439405,https://doi.org/10.1016/j.modpat.2024.100686,Artificial intelligence; Psychology; Computer science,review,True,"As artificial intelligence (AI) gains prominence in pathology and medicine, the ethical implications and potential biases within such integrated AI models will require careful scrutiny. Ethics and bias are important considerations in our practice settings, especially as increased number of machine learning (ML) systems are being integrated within our various medical domains. Such machine learning based systems, have demonstrated remarkable capabilities in specified tasks such as but not limited to image recognition, natural language processing, and predictive analytics. However, the potential bias that may exist within such AI-ML models can also inadvertently lead to unfair and potentially detrimental outcomes. The source of bias within such machine learning models can be due to numerous factors but can be typically put in three main buckets (data bias, development bias and interaction bias). These could be due to the training data, algorithmic bias, feature engineering and selection issues, clinical and institutional bias (i.e. practice variability), reporting bias, and temporal bias (i.e. changes in technology, clinical practice or disease patterns). Therefore despite the potential of these AI-ML applications, their deployment in our day to day practice also raises noteworthy ethical concerns. To address ethics and bias in medicine, a comprehensive evaluation process is required which will encompass all aspects such systems, from model development through clinical deployment. Addressing these biases is crucial to ensure that AI-ML systems remain fair, transparent, and beneficial to all. This review will discuss the relevant ethical and bias considerations in AI-ML specifically within the pathology and medical domain."
"Digital Trace Data Collection for Social Media Effects Research: APIs, Data Donation, and (Screen) Tracking",2023,Jakob Ohme; Theo Araujo; Laura Boeschoten; Deen Freelon; Nilàm Ram; Byron Reeves; Thomas N. Robinson,Communication Methods and Measures,88,W4322500410,10.1080/19312458.2023.2181319,https://openalex.org/W4322500410,https://doi.org/10.1080/19312458.2023.2181319,Social media; Computer science; TRACE (psycholinguistics); Misinformation; Data collection,article,True,"In social media effects research, the role of specific social media content is understudied, in part attributable to the fact that communication science previously lacked methods to access social media content directly. Digital trace data (DTD) can shed light on textual and audio-visual content of social media use and enable the analysis of content usage on a granular individual level that has been previously unavailable. However, because digital trace data are not specifically designed for research purposes, collection and analysis present several uncertainties. This article is a collaborative effort by scholars to provide an overview of how three methods of digital trace data collection - APIs, data donations, and tracking - can be used in studying the effects of social media content in three important topic areas of communication research: misinformation, algorithmic bias, and well-being. We address the question of how to collect raw social media content data and arrive at meaningful measures with multiple state-of-the-art data collection techniques that can be used to study the effects of social media use on different levels of detail. We conclude with a discussion of best practices for the implementation of each technique, and a comparison of their advantages and disadvantages."
Analysis of Marine Predators Algorithm using BIAS toolbox and Generalized Signature Test,2024,Manish Kumar; Kanchan Rajwar; Kusum Deep,Alexandria Engineering Journal,7,W4393305890,10.1016/j.aej.2024.03.060,https://openalex.org/W4393305890,https://doi.org/10.1016/j.aej.2024.03.060,Foraging; Algorithm; Toolbox; Convergence (economics); Computer science,article,True,"The Marine Predators Algorithm (MPA) is a prominent Nature-Inspired Optimization Algorithm (NIOA) that has garnered significant research interest due to its effectiveness. It draws inspiration from the foraging behaviors of marine predators, predominantly using the Lévy or Brownian approach for its foraging strategy. Despite its acclaim, the structural bias within MPA has not been thoroughly investigated, marking a significant gap in the current research. This absence of targeted research forms the core rationale behind initiating this study. Structural bias has recently been identified in NIOAs, causing the population to revisit specific regions of the search space without gaining new information. As a result, it may lead to increased computational costs and slow down the rate of convergence. Therefore, identifying structural bias is essential to better understand the search mechanism of MPA. To ascertain the presence of any structural bias, two recently introduced models are employed: the BIAS toolbox and the Generalized Signature Test. These examinations reveal a notable structural bias in MPA, predominantly towards the center of the search space. Also, possible future research directions for MPA are discussed. Our findings provide valuable insights into the search dynamics of the algorithm, fostering the development of new, unbiased, and efficient algorithms."
A novel approach for bias mitigation of gender classification algorithms using consistency regularization,2023,Anoop Krishnan; Ajita Rattani,Image and Vision Computing,12,W4385597536,10.1016/j.imavis.2023.104793,https://openalex.org/W4385597536,https://www.sciencedirect.com/science/article/am/pii/S0262885623001671,Generalizability theory; Computer science; Classifier (UML); Artificial intelligence; Machine learning,article,True,
Racial Bias in Clinical and Population Health Algorithms: A Critical Review of Current Debates,2024,Madison Coots; Kristin A. Linn; Sharad Goel; Amol S. Navathe; Ravi B. Parikh,Annual Review of Public Health,7,W4404945662,10.1146/annurev-publhealth-071823-112058,https://openalex.org/W4404945662,,Health care; Health equity; Ethnic group; Population; Algorithm,review,False,"Among health care researchers, there is increasing debate over how best to assess and ensure the fairness of algorithms used for clinical decision support and population health, particularly concerning potential racial bias. Here we first distill concerns over the fairness of health care algorithms into four broad categories: ( a ) the explicit inclusion (or, conversely, the exclusion) of race and ethnicity in algorithms, ( b ) unequal algorithm decision rates across groups, ( c ) unequal error rates across groups, and ( d ) potential bias in the target variable used in prediction. With this taxonomy, we critically examine seven prominent and controversial health care algorithms. We show that popular approaches that aim to improve the fairness of health care algorithms can in fact worsen outcomes for individuals across all racial and ethnic groups. We conclude by offering an alternative, consequentialist framework for algorithm design that mitigates these harms by instead foregrounding outcomes and clarifying trade-offs in the pursuit of equitable decision-making."
Algorithmic gender bias: investigating perceptions of discrimination in automated decision-making,2024,Soojong Kim; Poong Oh; Joomi Lee,Behaviour and Information Technology,4,W4391384911,10.1080/0144929x.2024.2306484,https://openalex.org/W4391384911,https://osf.io/zpqrt/download,Perception; Psychology; Situational ethics; Injustice; Social psychology,article,True,"With the widespread use of artificial intelligence and automated decision-making (ADM), concerns are increasing about automated decisions biased against certain social groups, such as women and racial minorities. The public's skepticism and the danger of algorithmic discrimination are widely acknowledged, yet the role of key factors constituting the context of discriminatory situations is underexplored. This study examined people's perceptions of gender bias in ADM, focusing on three factors influencing the responses to discriminatory automated decisions: the target of discrimination (subject vs. other), the gender identity of the subject, and situational contexts that engender biases. Based on a randomised experiment (N = 602), we found stronger negative reactions to automated decisions that discriminate against the gender group of the subject than those discriminating against other gender groups, evidenced by lower perceived fairness and trust in ADM, and greater negative emotion and tendency to question the outcome. The negative reactions were more pronounced among participants in underserved gender groups than men. Also, participants were more sensitive to biases in economic and occupational contexts than in other situations. These findings suggest that perceptions of algorithmic biases should be understood in relation to the public's lived experience of inequality and injustice in society."
Combining Transition Path Sampling with Data-Driven Collective Variables through a Reactivity-Biased Shooting Algorithm,2024,Jintu Zhang; Odin Zhang; Luigi Bonati; Tingjun Hou,Journal of Chemical Theory and Computation,10,W4399038084,10.1021/acs.jctc.4c00423,https://openalex.org/W4399038084,https://arxiv.org/pdf/2404.02597,Computer science; Path (computing); Algorithm; Sampling (signal processing); Transition (genetics),article,True,"Rare event sampling is a central problem in modern computational chemistry research. Among the existing methods, transition path sampling (TPS) can generate unbiased representations of reaction processes. However, its efficiency depends on the ability to generate reactive trial paths, which in turn depends on the quality of the shooting algorithm used. We propose a new algorithm based on the shooting success rate, i.e., reactivity, measured as a function of a reduced set of collective variables (CVs). These variables are extracted with a machine learning approach directly from TPS simulations, using a multitask objective function. Iteratively, this workflow significantly improves the shooting efficiency without any prior knowledge of the process. In addition, the optimized CVs can be used with biased enhanced sampling methodologies to accurately reconstruct the free energy profiles. We tested the method on three different systems: a two-dimensional toy model, conformational transitions of alanine dipeptide, and hydrolysis of acetyl chloride in bulk water. In the latter, we integrated our workflow with an active learning scheme to learn a reactive machine learning-based potential, which allowed us to study the mechanism and free energy profile with an ab initio-like accuracy."
<scp>SeroTracker‐RoB</scp>: A decision rule‐based algorithm for reproducible risk of bias assessment of seroprevalence studies,2023,Niklas Bobrovitz; Kim C. Noël; Zihan Li; Christian Cao; Gabriel Deveaux; Anabel Selemon; David A. Clifton; Mercedes Yanes‐Lane; Tingting Yan; Rahul K. Arora,Research Synthesis Methods,9,W4315754436,10.1002/jrsm.1620,https://openalex.org/W4315754436,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/jrsm.1620,Reliability (semiconductor); Algorithm; Computer science; Checklist; Intraclass correlation,article,True,"Risk of bias (RoB) assessments are a core element of evidence synthesis but can be time consuming and subjective. We aimed to develop a decision rule-based algorithm for RoB assessment of seroprevalence studies. We developed the SeroTracker-RoB algorithm. The algorithm derives seven objective and two subjective critical appraisal items from the Joanna Briggs Institute Critical Appraisal Checklist for Prevalence studies and implements decision rules that determine study risk of bias based on the items. Decision rules were validated using the SeroTracker seroprevalence study database, which included non-algorithmic RoB judgments from two reviewers. We quantified efficiency as the mean difference in time for the algorithmic and non-algorithmic assessments of 80 randomly selected articles, coverage as the proportion of studies where the decision rules yielded an assessment, and reliability using intraclass correlations comparing algorithmic and non-algorithmic assessments for 2070 articles. A set of decision rules with 61 branches was developed using responses to the nine critical appraisal items. The algorithmic approach was faster than non-algorithmic assessment (mean reduction 2.32 min [SD 1.09] per article), classified 100% (n = 2070) of studies, and had good reliability compared to non-algorithmic assessment (ICC 0.77, 95% CI 0.74-0.80). We built the SeroTracker-RoB Excel Tool, which embeds this algorithm for use by other researchers. The SeroTracker-RoB decision-rule based algorithm was faster than non-algorithmic assessment with complete coverage and good reliability. This algorithm enabled rapid, transparent, and reproducible RoB evaluations of seroprevalence studies and may support evidence synthesis efforts during future disease outbreaks. This decision rule-based approach could be applied to other types of prevalence studies."
A Graph-Based Algorithm for Robust Sequential Localization Exploiting Multipath for Obstructed-LOS-Bias Mitigation,2023,Alexander Venus; Erik Leitinger; Stefan Tertinek; Klaus Witrisal,IEEE Transactions on Wireless Communications,13,W4380534835,10.1109/twc.2023.3285530,https://openalex.org/W4380534835,https://ieeexplore.ieee.org/ielx7/7693/4656680/10151807.pdf,Computer science; Multipath propagation; Algorithm; Graph; Theoretical computer science,article,True,"This paper presents a factor graph formulation and particle-based sum-product algorithm (SPA) for robust sequential localization in multipath-prone environments. The proposed algorithm jointly performs data association, sequential estimation of a mobile agent position, and adapts all relevant model parameters. We derive a novel non-uniform false alarm (FA) model that captures the delay and amplitude statistics of the multipath radio channel. This model enables the algorithm to indirectly exploit position-related information contained in the multipath components (MPCs) for the estimation of the agent position without using any prior information such as floorplan information or training data. Using simulated and real measurements in different channel conditions, we demonstrate that the algorithm can provide high-accuracy position estimates even in fully obstructed line-of-sight (OLOS) situations and show that the performance of our algorithm constantly attains the posterior Cramér-Rao lower bound (P-CRLB), facilitating the additional information contained in the presented FA model. The algorithm is shown to provide robust estimates in both, dense multipath channels as well as channels showing specular, resolved MPCs, significantly outperforming state-of-the-art radio-based localization methods."
Towards a holistic view of bias in machine learning: bridging algorithmic fairness and imbalanced learning,2024,Damien Dablain; Bartosz Krawczyk; Nitesh V. Chawla,Discover Data,7,W4393952022,10.1007/s44248-024-00007-1,https://openalex.org/W4393952022,https://link.springer.com/content/pdf/10.1007/s44248-024-00007-1.pdf,Bridging (networking); Computer science; Artificial intelligence; Psychology; Machine learning,article,True,"Abstract Machine learning (ML) is playing an increasingly important role in rendering decisions that affect a broad range of groups in society. This posits the requirement of algorithmic fairness , which holds that automated decisions should be equitable with respect to protected features (e.g., gender, race). Training datasets can contain both class imbalance and protected feature bias. We postulate that, to be effective, both class and protected feature bias should be reduced—which allows for an increase in model accuracy and fairness. Our method, Fair OverSampling (FOS), uses SMOTE (Chawla in J Artif Intell Res 16:321–357, 2002) to reduce class imbalance and feature blurring to enhance group fairness. Because we view bias in imbalanced learning and algorithmic fairness differently, we do not attempt to balance classes and features; instead, we seek to de-bias features and balance the number of class instances. FOS restores numerical class balance through the creation of synthetic minority class instances and causes a classifier to pay less attention to protected features. Therefore, it reduces bias for both classes and protected features . Additionally, we take a step toward bridging the gap between fairness and imbalanced learning with a new metric, Fair Utility , that measures model effectiveness with respect to accuracy and fairness. Our source code and data are publicly available at https://github.com/dd1github/Fair-Over-Sampling."
"Simplicity bias, algorithmic probability, and the random logistic map",2024,Boumediene Hamzi; Kamaludin Dingle,Physica D Nonlinear Phenomena,5,W4394760427,10.1016/j.physd.2024.134160,https://openalex.org/W4394760427,https://doi.org/10.1016/j.physd.2024.134160,Simplicity; Noise (video); Mathematics; Logistic map; Algorithm,article,True,"Simplicity bias is an intriguing phenomenon prevalent in various input–output maps, characterized by a preference for simpler, more regular, or symmetric outputs. Notably, these maps typically feature high-probability outputs with simple patterns, whereas complex patterns are exponentially less probable. This bias has been extensively examined and attributed to principles derived from algorithmic information theory and algorithmic probability. In a significant advancement, it has been demonstrated that the renowned logistic map xk+1=μxk(1−xk), a staple in dynamical systems theory, and other one-dimensional maps exhibit simplicity bias when conceptualized as input–output systems. Building upon this work, our research delves into the manifestations of simplicity bias within the random logistic map, specifically focusing on scenarios involving additive noise. This investigation is driven by the overarching goal of formulating a comprehensive theory for the prediction and analysis of time series. Our primary contributions are multifaceted. We discover that simplicity bias is observable in the random logistic map for specific ranges of μ and noise magnitudes. Additionally, we find that this bias persists even with the introduction of small measurement noise, though it diminishes as noise levels increase. Our studies also revisit the phenomenon of noise-induced chaos, particularly when μ=3.83, revealing its characteristics through complexity-probability plots. Intriguingly, we employ the logistic map to illustrate a paradoxical aspect of data analysis: more data adhering to a consistent trend can occasionally lead to reduced confidence in extrapolation predictions, challenging conventional wisdom. We propose that adopting a probability-complexity perspective in analyzing dynamical systems could significantly enrich statistical learning theories related to series prediction and analysis. This approach not only facilitates a deeper understanding of simplicity bias and its implications but also paves the way for novel methodologies in forecasting complex systems behavior, especially in scenarios dominated by uncertainty and stochasticity."
Algorithmic Fairness and Bias in Machine Learning Systems,2023,Rushil Chandra; Karun Sanjaya; AR Aravind; Ahmed Abbas; Ruzieva Gulrukh; T. S. Senthil kumar,E3S Web of Conferences,2,W4384572189,10.1051/e3sconf/202339904036,https://openalex.org/W4384572189,https://www.e3s-conferences.org/articles/e3sconf/pdf/2023/36/e3sconf_iconnect2023_04036.pdf,Interpretability; Transparency (behavior); Machine learning; Computer science; Artificial intelligence,article,True,"In recent years, research into and concern over algorithmic fairness and bias in machine learning systems has grown significantly. It is vital to make sure that these systems are fair, impartial, and do not support discrimination or social injustices since machine learning algorithms are becoming more and more prevalent in decision-making processes across a variety of disciplines. This abstract gives a general explanation of the idea of algorithmic fairness, the difficulties posed by bias in machine learning systems, and different solutions to these problems. Algorithmic bias and fairness in machine learning systems are crucial issues in this regard that demand the attention of academics, practitioners, and policymakers. Building fair and unbiased machine learning systems that uphold equality and prevent discrimination requires addressing biases in training data, creating fairness-aware algorithms, encouraging transparency and interpretability, and encouraging diversity and inclusivity."
Bias correction of <scp>CMIP6</scp> simulations of precipitation over Indian monsoon core region using deep learning algorithms,2023,T. Kesavavarthini; A. Naga Rajesh; C. V. Srinivas; T. V. Lakshmi Kumar,International Journal of Climatology,10,W4322721694,10.1002/joc.8056,https://openalex.org/W4322721694,,Climatology; Indian Ocean Dipole; Coupled model intercomparison project; Monsoon; Precipitation,article,False,"Abstract General Circulation Models or Global Climate Models (GCMs) output consists of inevitable bias due to insufficient knowledge about parameterization schemes and other mathematical computations that involve thermodynamical and physical laws while designing climate models. Indian summer monsoon (southwest monsoon) accounts for 75%–90% of the annual rainfall over most climatic zones of India during the months, June, July, August, and September, which has a direct impact on the agricultural economy of India. The aim of this study is to bias correct the Coupled Model Intercomparison Project Phase – 6 (CMIP6) GCMs' precipitation data for the historical period from 1985 to 2014 and two Shared Socioeconomic Pathways (SSP) SSP1‐2.6 and SSP5‐8.5, from the period 2015 to 2100, with reference to the India Meteorological Department (IMD) observed rainfall gridded dataset. The datasets used are for the rain‐bearing Indian southwest monsoon season from the months, June to September. Monsoon Core Region is selected to carry out the bias correction using a couple of deep learning algorithms, namely one‐dimensional Convolutional Neural Network (CNN1D) and Long Short‐Term Memory Encoder‐Decoder (LSTM‐ED) Neural Network. The performance of both algorithms is evaluated with metrics. The LSTM‐ED algorithm yielded better results with least error output. The bias‐corrected data obtained using the LSTM‐ED algorithm is then compared with IMD observed rainfall data for the climatic events such as ENSO (El Niño and La Niña) and Positive and Negative IOD (Indian Ocean Dipole)."
"Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare",2023,Eran Tal,,8,W4385644241,10.1145/3600211.3604678,https://openalex.org/W4385644241,https://doi.org/10.1145/3600211.3604678,Counterfactual thinking; Computer science; Health care; Fairness measure; Econometrics,preprint,True,"Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology – the science of measurement – suggests ways of counteracting target specification bias and avoiding its harmful consequences."
Fairness and Bias in Algorithmic Hiring,2023,Alessandro Fabris; Nina Baranowska; Matthew Dennis; Philipp Hacker; Jorge Saldivar; Frederik Zuiderveen Borgesius; Asia J. Biega,arXiv (Cornell University),3,W4387075971,10.48550/arxiv.2309.13933,https://openalex.org/W4387075971,https://arxiv.org/abs/2309.13933,Work (physics); Pipeline (software); Corporate governance; Domain (mathematical analysis); Computer science,preprint,True,"Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of, algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders."
"Artifical Intelligence and Bias: Challenges, Implications, and Remedies",2023,A. Min,Journal Of Social Research,27,W4387369817,10.55324/josr.v2i11.1477,https://openalex.org/W4387369817,https://ijsr.internationaljournallabs.com/index.php/ijsr/article/download/1477/976,Multidisciplinary approach; Dignity; Engineering ethics; Construct (python library); Human rights,article,True,"This paper investigates the multifaceted issue of algorithmic bias in artificial intelligence (AI) systems and explores its ethical and human rights implications. The study encompasses a comprehensive analysis of AI bias, its causes, and potential remedies, with a particular focus on its impact on individuals and marginalized communities. The primary objectives of this research are to examine the concept of algorithmic bias, assess its ethical and human rights implications, identify its causes and mechanisms, evaluate its societal impact, explore mitigation strategies, and examine regulatory and community-driven approaches to address this critical issue. The research employs a multidisciplinary approach, drawing from literature reviews, case studies, and ethical analyses. It synthesizes insights from academic papers, governmental reports, and industry guidelines to construct a comprehensive overview of algorithmic bias and its ramifications. This research paper underscores the urgency of addressing algorithmic bias, as it raises profound ethical and human rights concerns. It advocates for comprehensive approaches, spanning technical, ethical, regulatory, and community-driven dimensions, to ensure that AI technologies respect the rights and dignity of individuals and communities in our increasingly AI-driven world."
IARC-NCI workshop on an epidemiological toolkit to assess biases in human cancer studies for hazard identification: beyond the algorithm,2023,Mary K. Schubauer‐Berigan; David B. Richardson; Matthew P. Fox; Lin Fritschi; Irina Guseva Canu; Neil Pearce; Leslie Stayner; Amy Berrington de González,Occupational and Environmental Medicine,14,W4318499601,10.1136/oemed-2022-108724,https://openalex.org/W4318499601,https://oem.bmj.com/content/oemed/80/3/119.full.pdf,Identification (biology); Epidemiology; Hazard; Cancer; Hazard analysis,editorial,True,
Small Target Detection in Sea Clutter by Weighted Biased Soft-Margin SVM Algorithm in Feature Spaces,2024,Peng‐Lang Shui; L.R. Zhang; Xiaohui Bai,IEEE Sensors Journal,8,W4392182816,10.1109/jsen.2024.3350571,https://openalex.org/W4392182816,,Clutter; Margin (machine learning); Support vector machine; Feature (linguistics); Pattern recognition (psychology),article,False,"Sea-surface small target detection in high-resolution sea clutter is always an intractable problem. Feature-based detection in multidimensional feature spaces is recognized to be an effective way, and therein, learning algorithms with controllable false alarm rate play an important role. In this article, a weighted biased soft-margin support vector machine (WBSM-SVM) algorithm is proposed to design two-class classifiers with controllable false alarm rate in feature spaces and the induced feature-based detectors can accurately control false alarm rate and have excellent detection ability of small targets in sea clutter. The WBSM-SVM algorithm contains three innovations. First, special two-class SVM classifiers are used to lower the loss from the one-class <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$\nu $ </tex-math></inline-formula> -SVM classifiers by additional use of the training samples of the feature vector from simulated returns of typical targets plus measured sea clutter. Second, extremely unbalanced misclassification weight of penalty factors of the two classes and the Mahalanobis distance of the training sample vectors are introduced in the SVM to meet the demand of extremely unbalanced false alarm rate versus target missing probability. Third, a biased classification boundary is used to tune the false alarm rate to the expected one. The experimental results on the two recognized databases for sea-surface small target detection show that the WBSM-SVM-based detectors attain better detection performance than existing feature-based detectors."
The ethical implications of using generative chatbots in higher education,2024,Ryan Williams,Frontiers in Education,73,W4390665705,10.3389/feduc.2023.1331607,https://openalex.org/W4390665705,https://www.frontiersin.org/articles/10.3389/feduc.2023.1331607/pdf?isPublishedV2=False,Autonomy; Engineering ethics; Generative grammar; Ethical issues; Psychology,article,True,"Incorporating artificial intelligence (AI) into education, specifically through generative chatbots, can transform teaching and learning for education professionals in both administrative and pedagogical ways. However, the ethical implications of using generative chatbots in education must be carefully considered. Ethical concerns about advanced chatbots have yet to be explored in the education sector. This short article introduces the ethical concerns associated with introducing platforms such as ChatGPT in education. The article outlines how handling sensitive student data by chatbots presents significant privacy challenges, thus requiring adherence to data protection regulations, which may not always be possible. It highlights the risk of algorithmic bias in chatbots, which could perpetuate societal biases, which can be problematic. The article also examines the balance between fostering student autonomy in learning and the potential impact on academic self-efficacy, noting the risk of over-reliance on AI for educational purposes. Plagiarism continues to emerge as a critical ethical concern, with AI-generated content threatening academic integrity. The article advocates for comprehensive measures to address these ethical issues, including clear policies, advanced plagiarism detection techniques, and innovative assessment methods. By addressing these ethical challenges, the article argues that educators, AI developers, policymakers, and students can fully harness the potential of chatbots in education, creating a more inclusive, empowering, and ethically sound educational future."
Ethical Considerations in the Use of Artificial Intelligence and Machine Learning in Health Care: A Comprehensive Review,2024,Mitul Harishbhai Tilala; Pradeep Kumar Chenchala; Ashok Choppadandi; Jagbir Kaur; Savitha Naguri; Rahul Saoji; Bhanu Devaguptapu,Cureus,93,W4399698937,10.7759/cureus.62443,https://openalex.org/W4399698937,https://assets.cureus.com/uploads/review_article/pdf/259236/20240615-15665-hz42ob.pdf,Artificial intelligence; Health care; Computer science; Engineering ethics; Psychology,review,True,"Artificial intelligence (AI) and machine learning (ML) technologies are revolutionizing health care by offering unprecedented opportunities to enhance patient care, optimize clinical workflows, and advance medical research. However, the integration of AI and ML into healthcare systems raises significant ethical considerations that must be carefully addressed to ensure responsible and equitable deployment. This comprehensive review explored the multifaceted ethical considerations surrounding the use of AI and ML in health care, including privacy and data security, algorithmic bias, transparency, clinical validation, and professional responsibility. By critically examining these ethical dimensions, stakeholders can navigate the ethical complexities of AI and ML integration in health care, while safeguarding patient welfare and upholding ethical principles. By embracing ethical best practices and fostering collaboration across interdisciplinary teams, the healthcare community can harness the full potential of AI and ML technologies to usher in a new era of personalized data-driven health care that prioritizes patient well-being and equity."
Biased Random-Key Genetic Algorithm with Local Search Applied to the Maximum Diversity Problem,2023,Geiza Silva; André Ferreira Leite; Raydonal Ospina; Víctor Leiva; Jorge Figueroa-Zúñiga; Cecília Castro,Mathematics,6,W4384202382,10.3390/math11143072,https://openalex.org/W4384202382,https://www.mdpi.com/2227-7390/11/14/3072/pdf?version=1689151046,Key (lock); Computer science; Set (abstract data type); Genetic algorithm; Mathematical optimization,article,True,"The maximum diversity problem (MDP) aims to select a subset with a predetermined number of elements from a given set, maximizing the diversity among them. This NP-hard problem requires efficient algorithms that can generate high-quality solutions within reasonable computational time. In this study, we propose a novel approach that combines the biased random-key genetic algorithm (BRKGA) with local search to tackle the MDP. Our computational study utilizes a comprehensive set of MDPLib instances, and demonstrates the superior average performance of our proposed algorithm compared to existing literature results. The MDP has a wide range of practical applications, including biology, ecology, and management. We provide future research directions for improving the algorithm’s performance and exploring its applicability in real-world scenarios."
Empowering Education through Generative AI: Innovative Instructional Strategies for Tomorrow's Learners,2023,Kadaruddin Kadaruddin,International Journal of Business Law and Education,93,W4385497035,10.56442/ijble.v4i2.215,https://openalex.org/W4385497035,https://ijble.com/index.php/journal/article/download/215/225,Generative grammar; Generative model; Computer science; Instructional design; Artificial intelligence,article,True,"As the educational landscape endures continuous change, artificial intelligence (AI) has presented unprecedented opportunities to revolutionize instructional methods. Among these cutting-edge AI technologies, Generative AI has emerged as a promising instrument with the potential to empower educators and students through innovative instructional strategies. This article aims to investigate the various applications of Generative AI in education and cast light on its role in shaping the future of education. The objectives of this study are twofold: first, to investigate the various instructional strategies that can be enhanced by employing Generative AI, and second, to assess the potential impact of these strategies on student learning outcomes. To accomplish these goals, a comprehensive literature review was conducted analyzing existing studies and applications of Generative AI in educational settings. The results and discussions emphasize the numerous educational benefits of Generative AI. Educators can personalize learning experiences, create interactive content, and facilitate adaptive assessments by leveraging the capabilities of Generative AI. This individualized strategy has the potential to boost learner engagement and knowledge retention. However, despite the numerous advantages, ethical concerns and difficulties arise. The responsible incorporation of Generative AI in education requires addressing issues such as data privacy, algorithmic bias, and the educator's role in directing AI-driven learning experiences. The research concludes by emphasizing that Generative AI holds enormous promise for empowering education and transforming instructional practices. The findings highlight the importance of ongoing collaboration between educators, policymakers, and AI developers to ensure the ethical and equitable integration of Generative AI into educational environments. By embracing the potential of Generative AI while remaining vigilant regarding its challenges, the field of education can unlock novel opportunities to nurture an inclusive, adaptive, and learner-centric pedagogical landscape for tomorrow's learners.&#x0D;"
Bias in AI-based models for medical applications: challenges and mitigation strategies,2023,Mirja Mittermaier; Marium Raza; Joseph C. Kvedar,npj Digital Medicine,203,W4380538374,10.1038/s41746-023-00858-z,https://openalex.org/W4380538374,https://www.nature.com/articles/s41746-023-00858-z.pdf,Disadvantaged; Socioeconomic status; Health care; Sexual orientation; Prejudice (legal term),editorial,True,"Artificial intelligence systems are increasingly being applied to healthcare. In surgery, AI applications hold promise as tools to predict surgical outcomes, assess technical skills, or guide surgeons intraoperatively via computer vision. On the other hand, AI systems can also suffer from bias, compounding existing inequities in socioeconomic status, race, ethnicity, religion, gender, disability, or sexual orientation. Bias particularly impacts disadvantaged populations, which can be subject to algorithmic predictions that are less accurate or underestimate the need for care. Thus, strategies for detecting and mitigating bias are pivotal for creating AI technology that is generalizable and fair. Here, we discuss a recent study that developed a new strategy to mitigate bias in surgical AI systems."
Social norm bias: residual harms of fairness-aware algorithms,2023,Myra Cheng; Maria De‐Arteaga; Lester Mackey; Adam Tauman Kalai,Data Mining and Knowledge Discovery,6,W4317734474,10.1007/s10618-022-00910-8,https://openalex.org/W4317734474,https://arxiv.org/pdf/2108.11056,Conformity; Norm (philosophy); Computer science; Artificial intelligence; Machine learning,article,False,
A Conceptual Model for Inclusive Technology: Advancing Disability Inclusion through Artificial Intelligence,2024,Maram Fahaad Almufareh; Sumaira Kausar; Mamoona Humayun; Samabia Tehsin,Deleted Journal,70,W4390584313,10.57197/jdr-2023-0060,https://openalex.org/W4390584313,https://www.scienceopen.com/document_file/10696e8f-ab40-4c4f-be70-09fda6533ae8/ScienceOpen/jdr20230060.pdf,Inclusion (mineral); Safeguarding; Transformative learning; Realm; Computer science,article,True,"Artificial intelligence (AI) has ushered in transformative changes, championing inclusion and accessibility for individuals with disabilities. This article delves into the remarkable AI-driven solutions that have revolutionized their lives across various domains. From assistive technologies such as voice recognition and AI-powered smart glasses catering to diverse needs, to healthcare benefiting from early disease detection algorithms and wearable devices that monitor vital signs and alert caregivers in emergencies, AI has steered in significant enhancements. Moreover, AI-driven prosthetics and exoskeletons have substantially improved mobility for those with limb impairments. The realm of education has not been left untouched, with AI tools creating inclusive learning environments that adapt to individual learning styles, paving the way for academic success among students with disabilities. However, the boundless potential of AI also presents ethical concerns and challenges. Issues like safeguarding data privacy, mitigating algorithmic bias, and bridging the digital divide must be thoughtfully addressed to fully harness AI’s potential in empowering individuals with disabilities. To complement these achievements, a robust conceptual model for AI disability inclusion serves as the theoretical framework, guiding the development of tailored AI solutions. By striking a harmonious balance between innovation and ethics, AI has the power to significantly enhance the overall quality of life for individuals with disabilities across a spectrum of vital areas."
Trust and reliance on AI — An experimental study on the extent and costs of overreliance on AI,2024,Artur Klingbeil; Cassandra Grützner; Philipp Schreck,Computers in Human Behavior,83,W4399992361,10.1016/j.chb.2024.108352,https://openalex.org/W4399992361,https://doi.org/10.1016/j.chb.2024.108352,Psychology; Social psychology; Epistemology; Positive economics; Sociology,article,True,"Decision-making is undergoing rapid changes due to the introduction of artificial intelligence (AI), as AI recommender systems can help mitigate human flaws and increase decision accuracy and efficiency. However, AI can also commit errors or suffer from algorithmic bias. Hence, blind trust in technologies carries risks, as users may follow detrimental advice resulting in undesired consequences. Building upon research on algorithm appreciation and trust in AI, the current study investigates whether users who receive AI advice in an uncertain situation overrely on this advice — to their own detriment and that of other parties. In a domain-independent, incentivized, and interactive behavioral experiment, we find that the mere knowledge of advice being generated by an AI causes people to overrely on it, that is, to follow AI advice even when it contradicts available contextual information as well as their own assessment. Frequently, this overreliance leads not only to inefficient outcomes for the advisee, but also to undesired effects regarding third parties. The results call into question how AI is being used in assisted decision making, emphasizing the importance of AI literacy and effective trust calibration for productive deployment of such systems."
"Opaque algorithms, transparent biases: Automated content moderation during the Sheikh Jarrah Crisis",2024,Norah Abokhodair; Yarden Skop; Sarah Rüller; Konstantin Aal; Houda Elmimouni,First Monday,8,W4394806508,10.5210/fm.v29i4.13620,https://openalex.org/W4394806508,https://firstmonday.org/ojs/index.php/fm/article/download/13620/11609,Content (measure theory); Moderation; Opacity; Computer science; Algorithm,article,True,"Social media platforms, while influential tools for human rights activism, free speech, and mobilization, also bear the influence of corporate ownership and commercial interests. This dual character can lead to clashing interests in the operations of these platforms. This study centers on the May 2021 Sheikh Jarrah events in East Jerusalem, a focal point in the Israeli-Palestinian conflict that garnered global attention. During this period, Palestinian activists and their allies observed and encountered a notable increase in automated content moderation actions, like shadow banning and content removal. We surveyed 201 users who faced content moderation and conducted 12 interviews with political influencers to assess the impact of these practices on activism. Our analysis centers on automated content moderation and transparency, investigating how users and activists perceive the content moderation systems employed by social media platforms, and their opacity. Findings reveal perceived censorship by pro-Palestinian activists due to opaque and obfuscated technological mechanisms of content demotion, complicating harm substantiation and lack of redress mechanisms. We view this difficulty as part of algorithmic harms, in the realm of automated content moderation. This dynamic has far-reaching implications for activism’s future and it raises questions about power centralization in digital spaces."
Artificial Intelligence and Machine Learning in Pharmacological Research: Bridging the Gap Between Data and Drug Discovery,2023,Shruti Singh; Rajesh Kumar; Shuvasree Payra; Sunil Kumar Singh,Cureus,101,W4386291745,10.7759/cureus.44359,https://openalex.org/W4386291745,https://assets.cureus.com/uploads/review_article/pdf/172577/20230830-19258-1lnt6y5.pdf,Artificial intelligence; Repurposing; Deep learning; Medicine; Identification (biology),review,True,"Artificial intelligence (AI) has transformed pharmacological research through machine learning, deep learning, and natural language processing. These advancements have greatly influenced drug discovery, development, and precision medicine. AI algorithms analyze vast biomedical data identifying potential drug targets, predicting efficacy, and optimizing lead compounds. AI has diverse applications in pharmacological research, including target identification, drug repurposing, virtual screening, de novo drug design, toxicity prediction, and personalized medicine. AI improves patient selection, trial design, and real-time data analysis in clinical trials, leading to enhanced safety and efficacy outcomes. Post-marketing surveillance utilizes AI-based systems to monitor adverse events, detect drug interactions, and support pharmacovigilance efforts. Machine learning models extract patterns from complex datasets, enabling accurate predictions and informed decision-making, thus accelerating drug discovery. Deep learning, specifically convolutional neural networks (CNN), excels in image analysis, aiding biomarker identification and optimizing drug formulation. Natural language processing facilitates the mining and analysis of scientific literature, unlocking valuable insights and information. However, the adoption of AI in pharmacological research raises ethical considerations. Ensuring data privacy and security, addressing algorithm bias and transparency, obtaining informed consent, and maintaining human oversight in decision-making are crucial ethical concerns. The responsible deployment of AI necessitates robust frameworks and regulations. The future of AI in pharmacological research is promising, with integration with emerging technologies like genomics, proteomics, and metabolomics offering the potential for personalized medicine and targeted therapies. Collaboration among academia, industry, and regulatory bodies is essential for the ethical implementation of AI in drug discovery and development. Continuous research and development in AI techniques and comprehensive training programs will empower scientists and healthcare professionals to fully exploit AI's potential, leading to improved patient outcomes and innovative pharmacological interventions."
"Use of Evolutionary Optimization Algorithms for the Design and Analysis of Low Bias, Low Phase Noise Photodetectors",2023,Ishraq Md Anjum; Ergün Şimşek; Seyed Ehsan Jamali Mahabadi; Thomas F. Carruthers; Curtis R. Menyuk; Joe C. Campbell; D.A. Tulchinsky; Keith J. Williams,Journal of Lightwave Technology,6,W4388283811,10.1109/jlt.2023.3330099,https://openalex.org/W4388283811,https://mdsoar.org/bitstreams/31d1a5ec-92ff-42ee-8de5-2eff43296a9e/download,Particle swarm optimization; Computer science; Phase noise; Noise (video); Photonics,article,True,"With the rapid advance of machine learning techniques and the increased availability of high-speed computing resources, it has become possible to exploit machine-learning technologies to aid in the design of photonic devices. In this work we use evolutionary optimization algorithms, machine learning techniques, and the drift-diffusion equations to optimize a modified uni-traveling-carrier (MUTC) photodetector for low phase noise at a relatively low bias of 5 V. We compare the particle swarm optimization (PSO), genetic, and surrogate optimization algorithms. We find that PSO yields the solution with the lowest phase noise, with an improvement over a current design of 4.4 dBc/Hz. We then analyze the machine-optimized design to understand the physics behind the phase noise reduction and show that the optimized design removes electrical bottlenecks in the current design."
Digitalization and inclusiveness of HRM practices: The example of neurodiversity initiatives,2023,Emmanuelle Walkowiak,Human Resource Management Journal,47,W4321377417,10.1111/1748-8583.12499,https://openalex.org/W4321377417,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/1748-8583.12499,Facilitation; Digital transformation; Multidisciplinary approach; Perspective (graphical); Knowledge management,article,True,"Abstract The transformation of the intelligence ecosystem associated with the digital transformation represents a critical juncture for diversity and inclusion (D&amp;I). We present a multidisciplinary perspective on digital transformation and D&amp;I that demonstrates that, in the context of automated decision making, where algorithmic biases and the standardisation of thought represent new risks, neurodiversity initiatives become a cornerstone for advancing D&amp;I. Based on interviews with neurodiversity experts, we identify innovative ways to efficiently configure an inclusive organisational design targeting neurodiversity by leveraging technologies. We identify several properties of technologies that support D&amp;I in neurodiversity initiatives: the neutralisation of biases during interviews, the development of digital support for physical and mental well‐being and the facilitation of different cognition modes. Finally, we critically discuss the risks and opportunities offered by various technologies in terms of performance evaluation, new forms of dominance, and design of a digital ecosystem for mental well‐being."
Eliminating Algorithmic Racial Bias in Clinical Decision Support Algorithms: Use Cases from the Veterans Health Administration,2023,Justin M. List; Paul M. Palevsky; Suzanne Tamang; Susan T. Crowley; David H. Au; William C. Yarbrough; Amol S. Navathe; Craig Kreisler; Ravi B. Parikh; Jessica Wang‐Rodriguez; J. Stacey Klutts; Paul R. Conlin; Leonard Pogach; Esther L. Meerwijk; Ernest Moy,Health Equity,5,W4389162187,10.1089/heq.2023.0037,https://openalex.org/W4389162187,https://doi.org/10.1089/heq.2023.0037,Equity (law); Clinical decision support system; Health care; Algorithm; Clinical decision making,article,True,"The Veterans Health Administration uses equity- and evidence-based principles to examine, correct, and eliminate use of potentially biased clinical equations and predictive models. We discuss the processes, successes, challenges, and next steps in four examples. We detail elimination of the race modifier for estimated kidney function and discuss steps to achieve more equitable pulmonary function testing measurement. We detail the use of equity lenses in two predictive clinical modeling tools: Stratification Tool for Opioid Risk Mitigation (STORM) and Care Assessment Need (CAN) predictive models. We conclude with consideration of ways to advance racial health equity in clinical decision support algorithms."
Your robot therapist is not your therapist: understanding the role of AI-powered mental health chatbots,2023,Zoha Khawaja; Jean‐Christophe Bélisle‐Pipon,Frontiers in Digital Health,82,W4388524014,10.3389/fdgth.2023.1278186,https://openalex.org/W4388524014,https://www.frontiersin.org/articles/10.3389/fdgth.2023.1278186/pdf?isPublishedV2=False,Chatbot; Ignorance; Mental health; Internet privacy; Psychology,article,True,"Artificial intelligence (AI)-powered chatbots have the potential to substantially increase access to affordable and effective mental health services by supplementing the work of clinicians. Their 24/7 availability and accessibility through a mobile phone allow individuals to obtain help whenever and wherever needed, overcoming financial and logistical barriers. Although psychological AI chatbots have the ability to make significant improvements in providing mental health care services, they do not come without ethical and technical challenges. Some major concerns include providing inadequate or harmful support, exploiting vulnerable populations, and potentially producing discriminatory advice due to algorithmic bias. However, it is not always obvious for users to fully understand the nature of the relationship they have with chatbots. There can be significant misunderstandings about the exact purpose of the chatbot, particularly in terms of care expectations, ability to adapt to the particularities of users and responsiveness in terms of the needs and resources/treatments that can be offered. Hence, it is imperative that users are aware of the limited therapeutic relationship they can enjoy when interacting with mental health chatbots. Ignorance or misunderstanding of such limitations or of the role of psychological AI chatbots may lead to a therapeutic misconception (TM) where the user would underestimate the restrictions of such technologies and overestimate their ability to provide actual therapeutic support and guidance. TM raises major ethical concerns that can exacerbate one's mental health contributing to the global mental health crisis. This paper will explore the various ways in which TM can occur particularly through inaccurate marketing of these chatbots, forming a digital therapeutic alliance with them, receiving harmful advice due to bias in the design and algorithm, and the chatbots inability to foster autonomy with patients."
Quad-Rotor Unmanned Aerial Vehicle Path Planning Based on the Target Bias Extension and Dynamic Step Size RRT* Algorithm,2024,Haitao Gao; Xiaozhu Hou; Jiangpeng Xu; Banggui Guan,World Electric Vehicle Journal,7,W4390912814,10.3390/wevj15010029,https://openalex.org/W4390912814,https://www.mdpi.com/2032-6653/15/1/29/pdf?version=1705393723,Motion planning; Random tree; Algorithm; Computer science; Mathematical optimization,article,True,"For the path planning of quad-rotor UAVs, the traditional RRT* algorithm has weak exploration ability, low planning efficiency, and a poor planning effect. A TD-RRT* algorithm based on target bias expansion and dynamic step size is proposed herein. First, random-tree expansion is combined with the target bias strategy to remove the blindness of the random tree, and we assign different weights to the sampling point and the target point so that the target point can be quickly approached and the search speed can be improved. Then, the dynamic step size is introduced to speed up the search speed, effectively solving the problem of invalid expansion in the process of trajectory generation. We then adjust the step length required for the expansion tree and obstacles in real time, solve the opposition between smoothness and real time in path planning, and improve the algorithm’s search efficiency. Finally, the cubic B-spline interpolation method is used to modify the local inflection point of the path of the improved RRT* algorithm to smooth the path. The simulation results show that compared with the traditional RRT* algorithm, the number of iterations of path planning of the TD-RRT* algorithm is reduced, the travel distance from the starting position to the end position is shortened, the time consumption is reduced, the path route is smoother, and the path optimization effect is better. The TD-RRT* algorithm based on target bias expansion and dynamic step size significantly improves the planning efficiency and planning effect of quad-rotor UAVs in a three-dimensional-space environment."
De-Selection Bias Recommendation Algorithm Based on Propensity Score Estimation,2023,Teng Ma; Yu Su,Applied Sciences,4,W4383877140,10.3390/app13148038,https://openalex.org/W4383877140,https://www.mdpi.com/2076-3417/13/14/8038/pdf?version=1688970249,Propensity score matching; MovieLens; Mean squared error; Selection (genetic algorithm); Computer science,article,True,"There are various biases present in recommendation systems, and recommendation results that do not consider these biases are unfair to users, items, and platforms. To address the problem of selection bias in recommendation systems, in this study, the propensity score was utilized to mitigate this bias. A selection bias propensity score estimation method (SPE) was developed, which takes into account both user and item information. This method accurately estimates the user’s choice tendency by calculating the degree of difference between the user’s selection rate and the selected preference of the item. Subsequently, the SPE method was combined with the traditional matrix decomposition-based recommendation algorithms, such as the latent semantic model (LFM) and the bias singular value model (BiasSVD). The propensity score was then inversely weighted into the loss function, creating a recommendation model that effectively eliminated selection bias. The experiments were carried out on the public dataset MovieLens, and root mean square error (RMSE) and mean absolute error (MAE) were selected as evaluation indicators and compared with two baseline models and three models with other propensity score estimation methods. Overall, the experimental results demonstrate that the model combined with SPE achieves a minimum increase of 2.00% in RMSE and 2.97% in MAE compared to its baseline model. Moreover, in comparison to other propensity score estimation methods, the SPE method effectively eliminates selection bias in the scoring data, thereby enhancing the performance of the recommendation model."
"Artificial intelligence, ChatGPT, and other large language models for social determinants of health: Current state and future directions",2024,Jasmine Chiat Ling Ong; Jun Jie Benjamin Seng; Jeren Zheng Feng Law; Lian Leng Low; Andrea Lay‐Hoon Kwa; Kathleen M. Giacomini; Daniel Shu Wei Ting,Cell Reports Medicine,64,W4390921272,10.1016/j.xcrm.2023.101356,https://openalex.org/W4390921272,https://www.cell.com/article/S2666379123005736/pdf,Disinformation; Transformative learning; Social determinants of health; Public relations; Health care,review,True,"This perspective highlights the importance of addressing social determinants of health (SDOH) in patient health outcomes and health inequity, a global problem exacerbated by the COVID-19 pandemic. We provide a broad discussion on current developments in digital health and artificial intelligence (AI), including large language models (LLMs), as transformative tools in addressing SDOH factors, offering new capabilities for disease surveillance and patient care. Simultaneously, we bring attention to challenges, such as data standardization, infrastructure limitations, digital literacy, and algorithmic bias, that could hinder equitable access to AI benefits. For LLMs, we highlight potential unique challenges and risks including environmental impact, unfair labor practices, inadvertent disinformation or ""hallucinations,"" proliferation of bias, and infringement of copyrights. We propose the need for a multitiered approach to digital inclusion as an SDOH and the development of ethical and responsible AI practice frameworks globally and provide suggestions on bridging the gap from development to implementation of equitable AI technologies."
Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models,2024,Feng Chen; Liqin Wang; Julie Hong; Jiaqi Jiang; Li Zhou,Journal of the American Medical Informatics Association,70,W4393119757,10.1093/jamia/ocae060,https://openalex.org/W4393119757,,Computer science; Systematic review; Data science; Artificial intelligence; Health records,review,False,"Abstract Objectives Leveraging artificial intelligence (AI) in conjunction with electronic health records (EHRs) holds transformative potential to improve healthcare. However, addressing bias in AI, which risks worsening healthcare disparities, cannot be overlooked. This study reviews methods to handle various biases in AI models developed using EHR data. Materials and Methods We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-analyses guidelines, analyzing articles from PubMed, Web of Science, and IEEE published between January 01, 2010 and December 17, 2023. The review identified key biases, outlined strategies for detecting and mitigating bias throughout the AI model development, and analyzed metrics for bias assessment. Results Of the 450 articles retrieved, 20 met our criteria, revealing 6 major bias types: algorithmic, confounding, implicit, measurement, selection, and temporal. The AI models were primarily developed for predictive tasks, yet none have been deployed in real-world healthcare settings. Five studies concentrated on the detection of implicit and algorithmic biases employing fairness metrics like statistical parity, equal opportunity, and predictive equity. Fifteen studies proposed strategies for mitigating biases, especially targeting implicit and selection biases. These strategies, evaluated through both performance and fairness metrics, predominantly involved data collection and preprocessing techniques like resampling and reweighting. Discussion This review highlights evolving strategies to mitigate bias in EHR-based AI models, emphasizing the urgent need for both standardized and detailed reporting of the methodologies and systematic real-world testing and evaluation. Such measures are essential for gauging models’ practical impact and fostering ethical AI that ensures fairness and equity in healthcare."
Bias in Machine Learning: A Literature Review,2024,Konstantinos Mavrogiorgos; Athanasios Kiourtis; Argyro Mavrogiorgou; Andreas Menychtas; Dimosthenis Kyriazis,Applied Sciences,30,W4403074418,10.3390/app14198860,https://openalex.org/W4403074418,https://doi.org/10.3390/app14198860,Computer science; Psychology,review,True,"Bias could be defined as the tendency to be in favor or against a person or a group, thus promoting unfairness. In computer science, bias is called algorithmic or artificial intelligence (i.e., AI) and can be described as the tendency to showcase recurrent errors in a computer system, which result in “unfair” outcomes. Bias in the “outside world” and algorithmic bias are interconnected since many types of algorithmic bias originate from external factors. The enormous variety of different types of AI biases that have been identified in diverse domains highlights the need for classifying the said types of AI bias and providing a detailed overview of ways to identify and mitigate them. The different types of algorithmic bias that exist could be divided into categories based on the origin of the bias, since bias can occur during the different stages of the Machine Learning (i.e., ML) lifecycle. This manuscript is a literature study that provides a detailed survey regarding the different categories of bias and the corresponding approaches that have been proposed to identify and mitigate them. This study not only provides ready-to-use algorithms for identifying and mitigating bias, but also enhances the empirical knowledge of ML engineers to identify bias based on the similarity that their use cases have to other approaches that are presented in this manuscript. Based on the findings of this study, it is observed that some types of AI bias are better covered in the literature, both in terms of identification and mitigation, whilst others need to be studied more. The overall contribution of this research work is to provide a useful guideline for the identification and mitigation of bias that can be utilized by ML engineers and everyone who is interested in developing, evaluating and/or utilizing ML models."
Innovation and challenges of artificial intelligence technology in personalized healthcare,2024,Yu-Hao Li; Yulin Li; Mu-Yang Wei; Guangyu Li,Scientific Reports,107,W4401642802,10.1038/s41598-024-70073-7,https://openalex.org/W4401642802,https://doi.org/10.1038/s41598-024-70073-7,Health care; Data science; Computer science; Personalized medicine; Knowledge management,review,True,"As the burgeoning field of Artificial Intelligence (AI) continues to permeate the fabric of healthcare, particularly in the realms of patient surveillance and telemedicine, a transformative era beckons. This manuscript endeavors to unravel the intricacies of recent AI advancements and their profound implications for reconceptualizing the delivery of medical care. Through the introduction of innovative instruments such as virtual assistant chatbots, wearable monitoring devices, predictive analytic models, personalized treatment regimens, and automated appointment systems, AI is not only amplifying the quality of care but also empowering patients and fostering a more interactive dynamic between the patient and the healthcare provider. Yet, this progressive infiltration of AI into the healthcare sphere grapples with a plethora of challenges hitherto unseen. The exigent issues of data security and privacy, the specter of algorithmic bias, the requisite adaptability of regulatory frameworks, and the matter of patient acceptance and trust in AI solutions demand immediate and thoughtful resolution .The importance of establishing stringent and far-reaching policies, ensuring technological impartiality, and cultivating patient confidence is paramount to ensure that AI-driven enhancements in healthcare service provision remain both ethically sound and efficient. In conclusion, we advocate for an expansion of research efforts aimed at navigating the ethical complexities inherent to a technology-evolving landscape, catalyzing policy innovation, and devising AI applications that are not only clinically effective but also earn the trust of the patient populace. By melding expertise across disciplines, we stand at the threshold of an era wherein AI's role in healthcare is both ethically unimpeachable and conducive to elevating the global health quotient."
Algorithmic Gender Bias: Investigating Perceptions of Discrimination in Automated Decision-Making,2024,Soojong Kim; Poong Oh; Joomi Lee,,2,W4390840428,10.31234/osf.io/zpqrt,https://openalex.org/W4390840428,https://osf.io/zpqrt/download,Injustice; Situational ethics; Social psychology; Perception; Psychology,preprint,True,"With the widespread use of artificial intelligence and automated decision-making (ADM), concerns are increasing about automated decisions biased against certain social groups, such as women and racial minorities. The public's skepticism and the danger of algorithmic discrimination are widely acknowledged, yet the role of key factors constituting the context of discriminatory situations is underexplored. This study examined people’s perceptions of gender bias in ADM, focusing on three factors influencing the responses to discriminatory automated decisions: the target of discrimination (subject vs. other), the gender identity of the subject, and situational contexts that engender biases. Based on a randomized experiment (N = 602), we found stronger negative reactions to automated decisions that discriminate against the gender group of the subject than those discriminating against other gender groups, evidenced by lower perceived fairness and trust in ADM, and greater negative emotion and tendency to question the outcome. The negative reactions were more pronounced among participants in underserved gender groups than men. Also, participants were more sensitive to biases in economic and occupational contexts than in other situations. These findings suggest that perceptions of algorithmic biases should be understood in relation to the public's lived experience of inequality and injustice in society."
"Bias and Unfairness in Machine Learning Models: A Systematic Review on Datasets, Tools, Fairness Metrics, and Identification and Mitigation Methods",2023,Tiago Palma Pagano; Rafael B. Loureiro; Fernanda V. N. Lisboa; Rodrigo M. Peixoto; Guilherme A. de Sousa Guimarães; Gustavo O. R. Cruz; Maira M. Araujo; Lucas Lisboa dos Santos; Marco A S Cruz; Ewerton L. S. Oliveira; Ingrid Winkler; Erick Giovani Sperandio Nascimento,Big Data and Cognitive Computing,166,W4316038168,10.3390/bdcc7010015,https://openalex.org/W4316038168,https://www.mdpi.com/2504-2289/7/1/15/pdf?version=1674136071,Computer science; Machine learning; Identification (biology); Systematic review; Scopus,review,True,"One of the difficulties of artificial intelligence is to ensure that model decisions are fair and free of bias. In research, datasets, metrics, techniques, and tools are applied to detect and mitigate algorithmic unfairness and bias. This study examines the current knowledge on bias and unfairness in machine learning models. The systematic review followed the PRISMA guidelines and is registered on OSF plataform. The search was carried out between 2021 and early 2022 in the Scopus, IEEE Xplore, Web of Science, and Google Scholar knowledge bases and found 128 articles published between 2017 and 2022, of which 45 were chosen based on search string optimization and inclusion and exclusion criteria. We discovered that the majority of retrieved works focus on bias and unfairness identification and mitigation techniques, offering tools, statistical approaches, important metrics, and datasets typically used for bias experiments. In terms of the primary forms of bias, data, algorithm, and user interaction were addressed in connection to the preprocessing, in-processing, and postprocessing mitigation methods. The use of Equalized Odds, Opportunity Equality, and Demographic Parity as primary fairness metrics emphasizes the crucial role of sensitive attributes in mitigating bias. The 25 datasets chosen span a wide range of areas, including criminal justice image enhancement, finance, education, product pricing, and health, with the majority including sensitive attributes. In terms of tools, Aequitas is the most often referenced, yet many of the tools were not employed in empirical experiments. A limitation of current research is the lack of multiclass and multimetric studies, which are found in just a few works and constrain the investigation to binary-focused method. Furthermore, the results indicate that different fairness metrics do not present uniform results for a given use case, and that more research with varied model architectures is necessary to standardize which ones are more appropriate for a given context. We also observed that all research addressed the transparency of the algorithm, or its capacity to explain how decisions are taken."
A biased random-key genetic algorithm for the two-level hub location routing problem with directed tours,2023,Caio César De Freitas; Dário José Aloise; Fábio Francisco da Costa Fontes; Andréa Cynthia Santos; Matheus da Silva Menezes,OR Spectrum,10,W4366420580,10.1007/s00291-023-00718-y,https://openalex.org/W4366420580,,Metaheuristic; Computer science; Key (lock); Genetic algorithm; Routing (electronic design automation),article,False,
BC-PINN: an adaptive physics informed neural network based on biased multiobjective coevolutionary algorithm,2023,Zhicheng Zhu; Hao Jia; Jin Huang; Baoling Huang,Neural Computing and Applications,9,W4385495295,10.1007/s00521-023-08876-4,https://openalex.org/W4385495295,,Penalty method; Mathematical optimization; Artificial neural network; Computer science; Benchmark (surveying),article,False,
Addressing bias in artificial intelligence for public health surveillance,2023,Lidia Flores; SeungJun Kim; Sean D. Young,Journal of Medical Ethics,32,W4367675464,10.1136/jme-2022-108875,https://openalex.org/W4367675464,,Computer science; Artificial intelligence; Social media; Data science; Health care,article,False,"Components of artificial intelligence (AI) for analysing social big data, such as natural language processing (NLP) algorithms, have improved the timeliness and robustness of health data. NLP techniques have been implemented to analyse large volumes of text from social media platforms to gain insights on disease symptoms, understand barriers to care and predict disease outbreaks. However, AI-based decisions may contain biases that could misrepresent populations, skew results or lead to errors. Bias, within the scope of this paper, is described as the difference between the predictive values and true values within the modelling of an algorithm. Bias within algorithms may lead to inaccurate healthcare outcomes and exacerbate health disparities when results derived from these biased algorithms are applied to health interventions. Researchers who implement these algorithms must consider when and how bias may arise. This paper explores algorithmic biases as a result of data collection, labelling and modelling of NLP algorithms. Researchers have a role in ensuring that efforts towards combating bias are enforced, especially when drawing health conclusions derived from social media posts that are linguistically diverse. Through the implementation of open collaboration, auditing processes and the development of guidelines, researchers may be able to reduce bias and improve NLP algorithms that improve health surveillance."
ChatGPT: friend or foe?,2023,The Lancet Digital Health,The Lancet Digital Health,238,W4319301633,10.1016/s2589-7500(23)00023-7,https://openalex.org/W4319301633,https://doi.org/10.1016/s2589-7500(23)00023-7,Harm; Realm; Computer science; Scripting language; Health care,editorial,True,"You would have been hard-pressed to miss the storm surrounding ChatGPT (Chat Generative Pre-trained Transformer) over the past few months. News outlets and social media have been abuzz with reports on the chatbot developed by OpenAI. In response to a written prompt, ChatGPT can compose emails, write computer code, and even craft movie scripts. Researchers have also demonstrated its competency to pass medical licensing exams. But excitement has been matched by a swathe of ethical concerns that could—and perhaps should—limit its adoption ChatGPT is powered by a refined version of the large language model (LLM) GPT-3.5. Its base model GPT-3 was trained on articles, websites, books, and written conversations, but a process of fine-tuning (including optimisation for dialogue) enables ChatGPT to respond to prompts in a conversational way. In the realm of health care, Sajan B Patel and Kyle Lam illustrated ChatGPT's ability to generate a patient discharge summary from a brief prompt. Automating this process could reduce delays in discharge from secondary care without compromising on detail, freeing up valuable time for doctors to invest in patient care and developmental training. A separate study also tested its ability to simplify radiology reports, with the generated reports being deemed overall factually correct, complete, and with low perceived risk of harm to patients. But in both cases, errors were evident. In the discharge summary example provided by Patel and Lam, ChatGPT added extra information to the summary that was not included in their prompt. Likewise, the radiology report study identified potentially harmful mistakes such as missing key medical findings. Such errors signal that if implemented in clinical practice, manual checks of automated outputs would be required. The limitations of ChatGPT are known. By OpenAI's own admission, ChatGPT's output can be incorrect or biased, such as citing article references that do not exist or perpetuating sexist stereotypes. It could also respond to harmful instructions, such as to generate malware. OpenAI set up guardrails to minimise the risks, but users have found ways around these, and as ChatGPT's outputs could be used to train future iterations of the model, these errors might be recycled and amplified. OpenAI have asked users to report inappropriate responses in order to help improve the model, but this has been met with criticism, as it's often people disproportionately affected by algorithmic bias (such as those from marginalised communities) who are expected to help find solutions. Michael Liebrenz and colleagues opine that although ChatGPT could serve to democratise knowledge sharing as it can receive and output text in multiple languages (beneficial for non-native speakers publishing in English), inaccuracies in generated text could fuel the spread of misinformation. These concerns have serious implications for the integrity of the scientific record, given the risk of introducing not only errors but also plagiarised content into publications. This could result in future research or health policy decisions being made on the basis of false information. Last month, the World Association of Medical Editors published its recommendations on the use of ChatGPT and other chatbots in scholarly publications, one of which is that journal editors need new tools to detect AI-generated or modified content. Indeed, an AI output detector was shown to be better at distinguishing between original and ChatGPT-generated research article abstracts than a plagiarism detector and human reviewers, but did falsely flag an original abstract as being “fake”. Technology is evolving, and editorial policies need to evolve too. Elsevier has introduced a new policy on the use of AI and AI-assisted technologies in scientific writing, stipulating that use should be limited to improving readability and language of the work, and should be declared in the manuscript; authors should do manual checks of any AI-generated output; and these tools should not be listed or cited as an author or co-author as they cannot take on the responsibilities that authorship entails (such as being accountable for the published work). Widespread use of ChatGPT is seemingly inevitable but in its current iteration careless, unchecked use could be a foe to both society and scholarly publishing. More forethought and oversight on model training are needed, as is investment in robust AI output detectors. ChatGPT is a game changer, but we're not quite ready to play."
Bias as Boundary Object: Unpacking The Politics Of An Austerity Algorithm Using Bias Frameworks,2023,Gabriel Grill; Fabian Fischer; Florian Cech,"2022 ACM Conference on Fairness, Accountability, and Transparency",4,W4380320344,10.1145/3593013.3594120,https://openalex.org/W4380320344,,Unpacking; Austerity; Politics; Boundary (topology); Computer science,article,False,
A biased random-key genetic algorithm for the minimum quasi-clique partitioning problem,2023,Rafael A. Melo; Celso C. Ribeiro; José A. Riveaux,Annals of Operations Research,7,W4387140368,10.1007/s10479-023-05609-7,https://openalex.org/W4387140368,,Theory of computation; Mathematics; Clique problem; Algorithm; Random graph,article,False,
Misclassifications of Contact Lens Iris PAD Algorithms: Is it Gender Bias or Environmental Conditions?,2023,Akshay Agarwal; Nalini Ratha; Afzel Noore; Richa Singh; Mayank Vatsa,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),4,W4319299996,10.1109/wacv56688.2023.00102,https://openalex.org/W4319299996,,Computer science; Biometrics; Adversary; Iris recognition; Classifier (UML),article,False,"One of the critical steps in biometrics pipeline is detection of presentation attacks, a physical adversary. Several presentation (adversary) attack detection (PAD) algorithms, including iris PAD, have been proposed and have shown superlative performance. However, a recent study, on a small-scale database, has highlighted that iris PAD may have gender biases. In this research, we present a rigorous study on gender bias in iris presentation attack detection algorithms using a large-scale and gender-balanced database. The paper provides several interesting observations which can help in building future presentation attack detection algorithms with aim of fair treatment of each demography. In addition, we also present a robust iris presentation attack detection algorithm by combining gender-covariate based classifiers. The proposed robust classifier not only reduces the difference in accuracy between different genders but also improves the overall performance of the PAD system."
Exploring patient perspectives on how they can and should be engaged in the development of artificial intelligence (AI) applications in health care,2023,Samira Adus; Jillian Macklin; Andrew D. Pinto,BMC Health Services Research,44,W4387957120,10.1186/s12913-023-10098-2,https://openalex.org/W4387957120,https://bmchealthservres.biomedcentral.com/counter/pdf/10.1186/s12913-023-10098-2,Health informatics; Health care; Modalities; Medicine; Focus group,article,True,"Artificial intelligence (AI) is a rapidly evolving field which will have implications on both individual patient care and the health care system. There are many benefits to the integration of AI into health care, such as predicting acute conditions and enhancing diagnostic capabilities. Despite these benefits potential harms include algorithmic bias, inadequate consent processes, and implications on the patient-provider relationship. One tool to address patients' needs and prevent the negative implications of AI is through patient engagement. As it currently stands, patients have infrequently been involved in AI application development for patient care delivery. Furthermore, we are unaware of any frameworks or recommendations specifically addressing patient engagement within the field of AI in health care."
FedUB: Federated Learning Algorithm Based on Update Bias,2024,Hesheng Zhang; Ping Zhang; M. Hu; Muhua Liu; Jiechang Wang,Mathematics,3,W4398145625,10.3390/math12101601,https://openalex.org/W4398145625,https://www.mdpi.com/2227-7390/12/10/1601/pdf?version=1716209561,Computer science; Artificial intelligence; Machine learning; Algorithm,article,True,"Federated learning, as a distributed machine learning framework, aims to protect data privacy while addressing the issue of data silos by collaboratively training models across multiple clients. However, a significant challenge to federated learning arises from the non-independent and identically distributed (non-iid) nature of data across different clients. non-iid data can lead to inconsistencies between the minimal loss experienced by individual clients and the global loss observed after the central server aggregates the local models, affecting the model’s convergence speed and generalization capability. To address this challenge, we propose a novel federated learning algorithm based on update bias (FedUB). Unlike traditional federated learning approaches such as FedAvg and FedProx, which independently update model parameters on each client before direct aggregation to form a global model, the FedUB algorithm incorporates an update bias in the loss function of local models—specifically, the difference between each round’s local model updates and the global model updates. This design aims to reduce discrepancies between local and global updates, thus aligning the parameters of locally updated models more closely with those of the globally aggregated model, thereby mitigating the fundamental conflict between local and global optima. Additionally, during the aggregation phase at the server side, we introduce a metric called the bias metric, which assesses the similarity between each client’s local model and the global model. This metric adaptively sets the weight of each client during aggregation after each training round to achieve a better global model. Extensive experiments conducted on multiple datasets have confirmed the effectiveness of the FedUB algorithm. The results indicate that FedUB generally outperforms methods such as FedDC, FedDyn, and Scaffold, especially in scenarios involving partial client participation and non-iid data distributions. It demonstrates superior performance and faster convergence in tasks such as image classification."
Enhancing Consumer Behavior and Experience Through AI-Driven Insights Optimization,2024,N. Naveeenkumar; Sreekanth Rallapalli; K. Sasikala; P. Vidhya Priya; Jakeer Husain; Sampath Boopathi,"Advances in marketing, customer relationship management, and e-services book series",52,W4392368823,10.4018/979-8-3693-1918-5.ch001,https://openalex.org/W4392368823,,Computer science; Cognitive science; Psychology,book-chapter,False,"This chapter delves into the role of AI-driven behavioral analytics in understanding, predicting, and enhancing consumer experiences. It highlights the integration of behavioral analytics, consumer experiences, and predictive modeling in reshaping market dynamics. The chapter explains the fundamental components of behavioral analytics, emphasizing its significance in understanding consumer preferences and decision-making processes. It also discusses the impact of AI-powered predictive analytics on consumer experiences, anticipating behaviors and fostering proactive strategies. It addresses ethical concerns like data privacy and algorithmic biases. The chapter provides a guide for practitioners, researchers, and businesses to harness AI's transformative potential in contemporary markets."
Reviewing the role of AI in environmental monitoring and conservation: A data-driven revolution for our planet,2024,Onyebuchi Nneamaka Chisom; Preye Winston Biu; Aniekan Akpan Umoh; Bartholomew Obehioye Obaedo; Abimbola Oluwatoyin Adegbite; Ayodeji Abatan,World Journal of Advanced Research and Reviews,60,W4390564739,10.30574/wjarr.2024.21.1.2720,https://openalex.org/W4390564739,https://wjarr.com/sites/default/files/WJARR-2023-2720.pdf,Environmental resource management; Biodiversity; Citizen science; Wildlife; Deforestation (computer science),article,True,"The rapid increase in human activities is causing significant damage to our planet's ecosystems, necessitating innovative solutions to preserve biodiversity and counteract ecological threats. Artificial Intelligence (AI) has emerged as a transformative force, providing unparalleled capabilities for environmental monitoring and conservation. This research paper explores the applications of AI in ecosystem management, including wildlife tracking, habitat assessment, biodiversity analysis, and natural disaster prediction. AI's role in environmental monitoring and conservation includes wildlife tracking, habitat assessment, resource conservation, biodiversity analysis, and species identification. AI algorithms analyze camera trap footage, drone imagery, and GPS data to identify and estimate population sizes, leading to improved anti-poaching efforts and enhanced protection of diverse species. Habitat assessment and resource conservation involve AI-powered image analysis, which aids in assessing forest health, detecting deforestation, and identifying areas in need of restoration. Biodiversity analysis and species identification are achieved through AI algorithms that analyze acoustic recordings, environmental DNA (eDNA), and camera trap footage. These innovations identify different species, assess biodiversity levels, and even discover new or endangered species. AI-powered flood prediction systems provide early warnings, empowering communities with better preparedness and evacuation efforts. Challenges, such as data quality and availability, algorithmic bias, and infrastructure limitations, are acknowledged as opportunities for growth and improvement. In policy and regulation, the paper advocates for clear frameworks prioritizing data privacy and security, algorithmic transparency, and equitable access. Responsible development and ethical use of AI are emphasized as foundational pillars, ensuring that the integration of AI into environmental conservation aligns with principles of fairness, transparency, and societal benefit."
Stability analysis of the bias compensated LMS algorithm,2024,Rodrigo Pimenta; Mariane R. Petraglia; Diego B. Haddad,Digital Signal Processing,4,W4391147781,10.1016/j.dsp.2024.104395,https://openalex.org/W4391147781,,Independence (probability theory); Stability (learning theory); Algorithm; Heuristic; Noise (video),article,False,
Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias,2023,Sanguk Lee; Tai‐Quan Peng; Matthew H. Goldberg; Seth A. Rosenthal; John Kotcher; Edward Maibach; Anthony Leiserowitz,arXiv (Cornell University),6,W4388275389,10.48550/arxiv.2311.00217,https://openalex.org/W4388275389,https://arxiv.org/abs/2311.00217,Fidelity; Global warming; Psychology; Covariate; Climate change,preprint,True,"Large language models (LLMs) have demonstrated their potential in social science research by emulating human perceptions and behaviors, a concept referred to as algorithmic fidelity. This study assesses the algorithmic fidelity and bias of LLMs by utilizing two nationally representative climate change surveys. The LLMs were conditioned on demographics and/or psychological covariates to simulate survey responses. The findings indicate that LLMs can effectively capture presidential voting behaviors but encounter challenges in accurately representing global warming perspectives when relevant covariates are not included. GPT-4 exhibits improved performance when conditioned on both demographics and covariates. However, disparities emerge in LLM estimations of the views of certain groups, with LLMs tending to underestimate worry about global warming among Black Americans. While highlighting the potential of LLMs to aid social science research, these results underscore the importance of meticulous conditioning, model selection, survey question format, and bias assessment when employing LLMs for survey simulation. Further investigation into prompt engineering and algorithm auditing is essential to harness the power of LLMs while addressing their inherent limitations."
30.3 A Bias-Flip Rectifier with a Duty-Cycle-Based MPPT Algorithm for Piezoelectric Energy Harvesting with 98% Peak MPPT Efficiency and 738% Energy-Extraction Enhancement,2023,Xinling Yue; Sundeep Javvaji; Zhong Tang; Kofi A. A. Makinwa; Sijun Du,2022 IEEE International Solid- State Circuits Conference (ISSCC),9,W4360605772,10.1109/isscc42615.2023.10067284,https://openalex.org/W4360605772,https://repository.tudelft.nl/islandora/object/uuid%3A6b0abf48-27db-465d-a7c2-af286c27a711/datastream/OBJ/download,Maximum power point tracking; Rectifier (neural networks); Energy harvesting; Maximum power principle; Topology (electrical circuits),article,True,"Synchronized bias-flip rectifiers, such as synchronized switch harvesting on inductor (SSHI) rectifiers, are widely used for piezoelectric energy harvesting (PEH) [1], which can replace the use of batteries in many loT applications, thus reducing both system volume and maintenance cost. However, the output power extracted by such rectifiers strongly depends on the impedance matching between the piezoelectric transducer (PT) and the circuit. To maximize this, two maximum power point tracking (MPPT) algorithms are often used. As shown in Fig. 30.3.1 (left), the Perturb & Observe (P&O) (a.k.a. hill-climbing) algorithm adjusts the rectified output power in a stepwise manner towards the maximum power point (MPP), thus establishing robust and continuous MPPT. However, accurately sensing the rectified output power often requires complex and power-hungry hardware [1], [2]. Another simpler algorithm is based on the fractional open-circuit voltage (FOCV) and involves periodically measuring the PT's open-circuit voltage amplitude <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$(\mathrm{V}_{\text{OC}})$</tex> and regulating the rectified voltage <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$(\mathrm{V}_{\text{REC}})$</tex> to a level <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$(\mathrm{V}_{\text{MPP}})$</tex> , which corresponds to the MPP [3–6]. However, the PT must be periodically disconnected from the rectifier to measure <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathrm{V}_{\text{OC}}$</tex> , resulting in wasted energy, while the inherent delay in sensing <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathrm{V}_{\text{OC}}$</tex> variations reduces the overall tracking efficiency. Furthermore, a calibration step is usually necessary to determine <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$\mathrm{V}_{\text{MPP}}$</tex> , since this depends on the actual PT voltage flip efficiency <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$(\eta_{\mathrm{F}})$</tex> of the bias-flip rectifier."
Algorithmic Fairness in Recruitment: Designing AI-Powered Hiring Tools to Identify and Reduce Biases in Candidate Selection,2025,Chinyere Linda Agbasiere; Goodness Rex Nze-Igwe,Path of Science,3,W4410201757,10.22178/pos.116-10,https://openalex.org/W4410201757,https://doi.org/10.22178/pos.116-10,Selection (genetic algorithm); Psychology; Computer science; Personnel selection; Applied psychology,article,True,"The study looks into how artificial intelligence (AI) affects hiring procedures, focusing on the fairness of the algorithms that drive these tools. AI has improved the efficiency of the hiring process, yet its use results in institutionalised discrimination. The AI systems used for recruitment, which base evaluations on past performance data, have the potential to discriminate against minority candidates as well as women through unintentional actions. The ability of AI systems to decrease human biases during recruitment encounters major challenges, as Amazon's discriminatory resume screening demonstrates the issues in systemic bias maintenance. This paper discusses the origins of algorithmic bias, including biased training records, defining labels, and choosing features, and suggests debiasing methods. Methods such as reweighting, adversarial debiasing, and fairness-aware algorithms are assessed for suitability in developing unbiased AI hiring systems. A quantitative approach is used in the research, web scraping data from extensive secondary sources to assess these biases and their mitigation measures. A Fair Machine Learning (FML) theoretical framework is utilised, which introduces fairness constraints into machine learning models so that hiring models do not perpetuate present discrimination. The ethical, legal, and organisational ramifications of using AI for recruitment are further examined under GDPR and Equal Employment Opportunity law provisions. By investigating HR practitioners' experiences and AI-based recruitment data, the study aims to develop guidelines for designing open, accountable, and equitable AI-based hiring processes. The findings emphasise the value of human oversight and the necessity of regular audits to guarantee equity in AI hiring software and, consequently, encourage diversity and equal opportunity during employment."
Three Satellites Dynamic Switching Range Integrated Navigation and Positioning Algorithm with Clock Bias Cancellation and Altimeter Assistance,2023,Lvyang Ye; Ning Gao; Yikang Yang; Lingyu Deng; Hengnian Li,Aerospace,9,W4367310835,10.3390/aerospace10050411,https://openalex.org/W4367310835,https://www.mdpi.com/2226-4310/10/5/411/pdf?version=1683196602,Computer science; Altimeter; Global Positioning System; Constellation; Elevation (ballistics),article,True,"Challenging environments such as cities, canyons, and forests have become key factors affecting navigation stability. When users pass through intricate overpasses and winding road sections, due to the fluctuation of the geoid, there will be a large fluctuation problem in the elevation measurement error of the user’s receiver. In addition, even if the low Earth orbit (LEO) constellation has thousands of satellites, there will be no technical problems in regard to destroying LEO satellites with existing technology in extreme situations such as warfare and in challenging environments such as dense forests, canyons, and ravines, where three or fewer visible satellites is a foreseeable scenario. To solve the problem of providing location services in such challenging environments, first, we analyze the relationship between temperature and atmospheric pressure and altitude; and then, based on this, we propose an initialization correction method for elevation measurements. Next, based on the broadband LEO constellation, we give an integrated navigation and positioning scheme with the assistance of both a clock bias elimination system and an altimeter. Finally, the proposed scheme is simulated and verified. The experimental results show that the dynamic switching of LEO satellites, combined with the assistance of the altimeter, can effectively improve the stability and positioning accuracy of navigation and positioning and can suppress the large navigation errors caused by the long switching time without the assistance of the altimeter. This allows the switching time to be extended; thus, it can be used as a technical reference solution for integrated communication and navigation (ICN) in the future."
A CRITICAL REVIEW OF AI-DRIVEN STRATEGIES FOR ENTREPRENEURIAL SUCCESS,2024,Favour Oluwadamilare Usman; Nsisong Louis Eyo-Udo; Emmanuel Augustine Etukudoh; Beryl Odonkor; Chidera Victoria Ibeh; Ayodeji Adegbola,International Journal of Management & Entrepreneurship Research,71,W4391579939,10.51594/ijmer.v6i1.748,https://openalex.org/W4391579939,https://fepbl.com/index.php/ijmer/article/download/748/939,Critical success factor; Psychology; Computer science; Knowledge management,review,True,"In the rapidly evolving landscape of entrepreneurship, the integration of Artificial Intelligence (AI) has emerged as a transformative force, reshaping traditional business paradigms and offering unprecedented opportunities for success. This paper provides a comprehensive and critical review of AI-driven strategies employed by entrepreneurs to enhance their ventures. The review encompasses a thorough analysis of key AI applications, their impact on various aspects of entrepreneurship, and the potential benefits and challenges associated with their implementation. The first section explores the role of AI in market analysis, highlighting how advanced data analytics and predictive modelling contribute to informed decision-making and market forecasting. The discussion then extends to AI-driven innovations in product development, emphasizing the acceleration of ideation, prototyping, and customization through machine learning algorithms. Next, the paper scrutinizes the influence of AI on customer engagement and relationship management. It delves into the personalized customer experiences facilitated by chatbots, recommendation systems, and sentiment analysis, while also addressing ethical considerations surrounding data privacy and algorithmic biases. Entrepreneurial operations and efficiency gains are examined in the subsequent section, emphasizing AI's impact on supply chain management, logistics, and resource optimization. The review underscores the potential for increased productivity and cost-effectiveness through the implementation of AI-powered automation and smart systems. Despite the myriad advantages, the paper critically examines challenges such as ethical concerns, job displacement, and the digital divide. It emphasizes the need for a balanced approach that addresses the societal impact of AI adoption while fostering inclusive entrepreneurial ecosystems. In conclusion, this critical review not only provides a comprehensive overview of the current landscape of AI-driven strategies in entrepreneurship but also offers insights into the potential future developments and challenges. Entrepreneurs, policymakers, and researchers can leverage this analysis to navigate the evolving intersection of AI and entrepreneurship, fostering a sustainable and ethically sound environment for entrepreneurial success in the digital era.&#x0D; Keywords: Artificial Intelligence (AI), Entrepreneurship, Strategic Implementation, Innovation, Market Analysis, Predictive Modelling."
Double-Layer RRT* Objective Bias Anytime Motion Planning Algorithm,2024,Hamada Esmaiel; Zhao Guolin; Zeyad A. H. Qasem; Jie Qi; Haixin Sun,Robotics,3,W4392358192,10.3390/robotics13030041,https://openalex.org/W4392358192,https://www.mdpi.com/2218-6581/13/3/41/pdf?version=1709261572,Motion planning; Computer science; Motion (physics); Algorithm; Artificial intelligence,article,True,"This paper proposes a double-layer structure RRT* algorithm based on objective bias called DOB-RRT*. The algorithm adopts an initial path with an online optimization structure for motion planning. The first layer of RRT* introduces a feedback-based objective bias strategy with segment forward pruning processing to quickly obtain a smooth initial path. The second layer of RRT* uses the heuristics of the initial tree structure to optimize the path by using reverse maintenance strategies. Compared with conventional RRT and RRT* algorithms, the proposed algorithm can obtain the initial path with high quality, and it can quickly converge to the progressive optimal path during the optimization process. The performance of the proposed algorithm is effectively evaluated and tested in real experiments on an actual wheeled robotic vehicle running ROS Kinetic in a real environment."
Investigating Biases in COVID-19 Diagnostic Systems Processed with Automated Speech Anonymization Algorithms,2023,Yi Zhu; Mohamed Imoussaïne-Aïkous; Carolyn Côté‐Lussier; Tiago H. Falk,,4,W4386712553,10.21437/spsc.2023-8,https://openalex.org/W4386712553,https://www.isca-archive.org/spsc_2023/zhu23_spsc.pdf,Metadata; Computer science; Socioeconomic status; Coronavirus disease 2019 (COVID-19); Speech recognition,article,True,"Automated voice anonymization algorithms are used to obfuscate speaker identity while leaving other vocal attributes untouched; they have been used for e.g., speech recognition, speech emotion detection, and most recently, remote speechbased health diagnostics.However, speech data is commonly collected in an uncontrolled manner in various environments, potentially compromising its quality, and frequently omits key metadata that could improve model performance.In this study, we employed the Cambridge COVID-19 sound database and used COVID-19 detection as a case study.We first present descriptive statistics on sample composition (i.e., COVID-19 status, age, gender).We also present a measure of signal-tonoise ratio (SNR), a feature of speech that can denote individuals' socioeconomic status.Next, we assess how age and SNR, the two most unbalanced features of the dataset, are associated with model performance and the impact of automated anonymization algorithms performance.Our findings suggest the existence of diagnostic biases related to age and SNR of the recording, which become more prominent after anonymization.To tackle these biases, we explore the usefulness of two data augmentation methods.We show that although data augmentation helps to recover some loss in overall performance, it can lead to a larger discrepancy in performance for over-represented and under-represented groups.We conclude with a discussion of the limitations associated with using SNR as an indicator of socioeconomic status, and of the potential effects of diagnostic biases associated with socioeconomic status."
Investigating anatomical bias in clinical machine learning algorithms,2023,Jannik Skyttegaard Pedersen; Martin Sundahl Laursen; Pernille Just Vinholt; Anne Alnor; Thiusius Rajeeth Savarimuthu,,2,W4386566592,10.18653/v1/2023.findings-eacl.103,https://openalex.org/W4386566592,https://aclanthology.org/2023.findings-eacl.103.pdf,Computer science; Artificial intelligence; Machine learning; Algorithm,article,True,"Clinical machine learning algorithms have shown promising results and could potentially be implemented in clinical practice to provide diagnosis support and improve patient treatment. Barriers for realisation of the algorithms’ full potential include bias which is systematic and unfair discrimination against certain individuals in favor of others. The objective of this work is to measure anatomical bias in clinical text algorithms. We define anatomical bias as unfair algorithmic outcomes against patients with medical conditions in specific anatomical locations. We measure the degree of anatomical bias across two machine learning models and two Danish clinical text classification tasks, and find that clinical text algorithms are highly prone to anatomical bias. We argue that datasets for creating clinical text algorithms should be curated carefully to isolate the effect of anatomical location in order to avoid bias against patient subgroups."
TRANSFORMING FINTECH FRAUD DETECTION WITH ADVANCED ARTIFICIAL INTELLIGENCE ALGORITHMS,2024,Philip Olaseni Shoetan; Babajide Tolulope Familoni,Finance & Accounting Research Journal,60,W4394884079,10.51594/farj.v6i4.1036,https://openalex.org/W4394884079,https://fepbl.com/index.php/farj/article/download/1036/1259,Computer science; Artificial intelligence; Machine learning; Scalability; Transformative learning,article,True,"The rapid evolution of financial technology (fintech) platforms has exponentially increased the volume and sophistication of financial transactions, concurrently elevating the risk and complexity of fraudulent activities. This necessitates a paradigm shift in fraud detection methodologies towards more agile, accurate, and predictive solutions. This paper presents a comprehensive study on the transformative potential of advanced Artificial Intelligence (AI) algorithms in enhancing fintech fraud detection mechanisms. By leveraging cutting-edge AI techniques including deep learning, machine learning, and natural language processing, this research aims to develop a robust fraud detection framework capable of identifying, analyzing, and preventing fraudulent transactions in real-time.&#x0D; Our methodology encompasses the deployment of several AI algorithms on extensive datasets comprising genuine and fraudulent financial transactions. Through a comparative analysis, we identify the most effective algorithms in terms of accuracy, efficiency, and scalability. Key findings reveal that deep learning models, particularly those employing neural networks, outperform traditional machine learning models in detecting complex and nuanced fraudulent activities. Furthermore, the integration of natural language processing enables the extraction and analysis of unstructured data, significantly enhancing the detection capabilities.&#x0D; Conclusively, this paper underscores the critical role of advanced AI algorithms in revolutionizing fintech fraud detection. It highlights the superior performance of AI-based models over conventional methods, offering fintech platforms a more dynamic and predictive approach to fraud prevention. This research not only contributes to the academic discourse on financial security but also provides practical insights for fintech companies striving to safeguard their operations against fraud.&#x0D; Keywords: Artificial Intelligence, Fintech, Fraud Detection, Ethical Ai, Regulatory Compliance, Data Privacy, Algorithmic Bias, Predictive Analytics, Blockchain Technology, Quantum Computing, Interdisciplinary Collaboration, Innovation, Transparency, Accountability, Continuous Learning, Ethical Principles, Real-Time Processing, Financial Sector."
An Improved Goal-bias RRT algorithm for Unmanned Aerial Vehicle Path Planning,2024,Hongyu Zhang; Xiaomei Xie; Mingzhu Wei; Xinhua Wang; Dongjing Song; Jin Luo,2022 IEEE International Conference on Mechatronics and Automation (ICMA),6,W4401687143,10.1109/icma61710.2024.10633102,https://openalex.org/W4401687143,,Motion planning; Computer science; Path (computing); Artificial intelligence; Algorithm,article,False,
Contribution of Artificial Intelligence in Improving Accessibility for Individuals with Disabilities,2023,Jeff Shuford,Journal of Knowledge Learning and Science Technology ISSN 2959-6386 (online),28,W4394586626,10.60087/jklst.vol2.n2.p433,https://openalex.org/W4394586626,https://jklst.org/index.php/home/article/download/178/151,Psychology; Computer science; Artificial intelligence,article,True,"Artificial intelligence (AI) stands as a revolutionary force with profound implications for society, offering considerable advantages for individuals with disabilities. Yet, alongside its promise, AI brings inherent risks, including ethical dilemmas that could heighten discrimination against marginalized communities. This article conducts a thorough analysis of the benefits and drawbacks of AI for people with disabilities, with a specific focus on algorithmic biases. These biases, capable of molding societal frameworks and influencing decision-making, hold the potential to perpetuate unfair treatment and bias. Given these challenges, the article delves into potential remedies to mitigate these concerns and ensure that AI effectively caters to the needs of all individuals, irrespective of disability status."
The Power of Artificial Intelligence in Recruitment: An Analytical Review of Current AI-Based Recruitment Strategies,2023,Wael Abdulrahman Albassam,International Journal of Professional Business Review,49,W4381684347,10.26668/businessreview/2023.v8i6.2089,https://openalex.org/W4381684347,https://openaccessojs.com/JBReview/article/download/2089/969,Originality; Value (mathematics); Quality (philosophy); Ethical issues; Applications of artificial intelligence,article,True,"Purpose: The aim of this study is to contribute to the understanding of the power of artificial intelligence (AI) in recruitment and to highlight the opportunities and challenges associated with its use. Theoretical framework: This paper provides a comprehensive analytical review of current AI-based recruitment strategies, drawing on both academic research and industry reports. Design/methodology/approach: The paper critically evaluates the potential benefits and drawbacks of using AI in recruitment and assesses the effectiveness of various AI-based recruitment strategies. Findings: The results indicate that AI-based recruitment strategies such as resume screening, candidate matching, video interviewing, chatbots, predictive analytics, gamification, virtual reality assessments, and social media screening offer significant potential benefits for organizations, including improved efficiency, cost savings, and better-quality hires. However, the use of AI in recruitment also raises ethical and legal concerns, including the potential for algorithmic bias and discrimination. Research, Practical &amp; Social implications: The study concludes by emphasizing the need for further research and development to ensure that AI-based recruitment strategies are effective, unbiased, and aligned with ethical and legal standards. Originality/value: The value of the study lies in its comprehensive exploration of AI in recruitment, synthesizing insights from academic and industry perspectives, and assessing the balance of potential benefits against ethical and legal concerns."
CYBERSECURITY CHALLENGES IN THE AGE OF AI: THEORETICAL APPROACHES AND PRACTICAL SOLUTIONS,2024,Babajide Tolulope Familoni,Computer Science & IT Research Journal,73,W4393077032,10.51594/csitrj.v5i3.930,https://openalex.org/W4393077032,https://fepbl.com/index.php/csitrj/article/download/930/1144,Computer science; Computer security; Data science,article,True,"In the ever-evolving landscape of cybersecurity, the proliferation of artificial intelligence (AI) technologies introduces both promising advancements and daunting challenges. This paper explores the theoretical underpinnings and practical implications of addressing cybersecurity challenges in the age of AI. With the integration of AI into various facets of digital infrastructure, including threat detection, authentication, and response mechanisms, cyber threats have become increasingly sophisticated and difficult to mitigate. Theoretical approaches delve into understanding the intricate interplay between AI algorithms, human behavior, and adversarial tactics, elucidating the underlying mechanisms of cyber attacks and defense strategies. However, this complexity also engenders novel vulnerabilities, as AI-driven attacks leverage machine learning algorithms to evade traditional security measures, posing formidable challenges to organizations across sectors. As such, practical solutions necessitate a multifaceted approach, encompassing robust threat intelligence, adaptive defense mechanisms, and ethical considerations to safeguard against AI-driven cyber threats effectively. Leveraging AI for cybersecurity defense holds promise in enhancing detection capabilities, automating response actions, and augmenting human analysts' capabilities. Yet, inherent limitations, such as algorithmic biases, data privacy concerns, and the potential for AI-enabled attacks, underscore the need for a comprehensive risk management framework. Regulatory frameworks and industry standards play a crucial role in shaping the development and deployment of AI-powered cybersecurity solutions, ensuring accountability, transparency, and compliance with ethical principles. Moreover, fostering interdisciplinary collaboration and investing in cybersecurity education and training are vital for cultivating a skilled workforce equipped to navigate the evolving threat landscape. By integrating theoretical insights with practical strategies, this paper elucidates key challenges and opportunities in securing AI-driven systems, offering insights for policymakers, researchers, and practitioners alike.&#x0D; Keywords: Cybersecurity; Artificial Intelligence; Threat Detection; Defense Strategies; Ethical Considerations; Regulatory Frameworks."
Artificial Intelligence in Point-of-Care Biosensing: Challenges and Opportunities,2024,Connor D. Flynn; Dingran Chang,Diagnostics,57,W4399054302,10.3390/diagnostics14111100,https://openalex.org/W4399054302,https://www.mdpi.com/2075-4418/14/11/1100/pdf?version=1716636804,Computer science; Biosensor; Field (mathematics); Health care; Transformative learning,article,True,"The integration of artificial intelligence (AI) into point-of-care (POC) biosensing has the potential to revolutionize diagnostic methodologies by offering rapid, accurate, and accessible health assessment directly at the patient level. This review paper explores the transformative impact of AI technologies on POC biosensing, emphasizing recent computational advancements, ongoing challenges, and future prospects in the field. We provide an overview of core biosensing technologies and their use at the POC, highlighting ongoing issues and challenges that may be solved with AI. We follow with an overview of AI methodologies that can be applied to biosensing, including machine learning algorithms, neural networks, and data processing frameworks that facilitate real-time analytical decision-making. We explore the applications of AI at each stage of the biosensor development process, highlighting the diverse opportunities beyond simple data analysis procedures. We include a thorough analysis of outstanding challenges in the field of AI-assisted biosensing, focusing on the technical and ethical challenges regarding the widespread adoption of these technologies, such as data security, algorithmic bias, and regulatory compliance. Through this review, we aim to emphasize the role of AI in advancing POC biosensing and inform researchers, clinicians, and policymakers about the potential of these technologies in reshaping global healthcare landscapes."
“Inventor’s Bias” at Work: When Low-Performing Algorithms Seem Fair,2023,Maya J. Cratsley; Nathanael J. Fast,International Journal of Human-Computer Interaction,3,W4381857282,10.1080/10447318.2023.2224954,https://openalex.org/W4381857282,,Perception; Context (archaeology); Product (mathematics); Phenomenon; Politics,article,False,"This article introduces the ""Inventor's Bias Effect,"" the propensity for inventors to be over-optimistic about the positive features and uses of the products they create. We explore this phenomenon in the context of decision-making algorithms by conducting two online studies (N = 1001) where subjects were asked to either create or evaluate an AI-based tool that can automate human resource decisions in an organization. Study 1 revealed that individuals in the role of inventor perceived a low-performing algorithm they created as fairer relative to the ratings of other stakeholders (CEOs, employees, and the general public). The tendency for these ""inventors"" to personally identify with the products they created mediated this effect. Study 2 showed that inventors' perceptions of fairness of the algorithms they created translated into an increased desire for the organization to continue using their product, even though it was inaccurate for a third of all decisions. This research demonstrates how stakeholders' relations to algorithms may encourage biased decision making and highlights the need for caution in organizational and political decision-making processes."
Should ChatGPT be biased? Challenges and risks of bias in large language models,2023,Emilio Ferrara,First Monday,158,W4388488349,10.5210/fm.v28i11.13346,https://openalex.org/W4388488349,https://firstmonday.org/ojs/index.php/fm/article/download/13346/11369,Unintended consequences; Transparency (behavior); Computer science; Generative grammar; Language model,article,True,"As generative language models, exemplified by ChatGPT, continue to advance in their capabilities, the spotlight on biases inherent in these models intensifies. This paper delves into the distinctive challenges and risks associated with biases specifically in large-scale language models. We explore the origins of biases, stemming from factors such as training data, model specifications, algorithmic constraints, product design, and policy decisions. Our examination extends to the ethical implications arising from the unintended consequences of biased model outputs. In addition, we analyze the intricacies of mitigating biases, acknowledging the inevitable persistence of some biases, and consider the consequences of deploying these models across diverse applications, including virtual assistants, content generation, and chatbots. Finally, we provide an overview of current approaches for identifying, quantifying, and mitigating biases in language models, underscoring the need for a collaborative, multidisciplinary effort to craft AI systems that embody equity, transparency, and responsibility. This article aims to catalyze a thoughtful discourse within the AI community, prompting researchers and developers to consider the unique role of biases in the domain of generative language models and the ongoing quest for ethical AI."
On the Potential of Algorithm Fusion for Demographic Bias Mitigation in Face Recognition,2024,Jascha Kolberg; Yannik Schäfer; Christian Rathgeb; Christoph Busch,IET Biometrics,4,W4392122172,10.1049/2024/1808587,https://openalex.org/W4392122172,https://downloads.hindawi.com/journals/ietbm/2024/1808587.pdf,Biometrics; Computer science; Facial recognition system; Demographics; Face (sociological concept),article,True,"With the rise of deep neural networks, the performance of biometric systems has increased tremendously. Biometric systems for face recognition are now used in everyday life, e.g., border control, crime prevention, or personal device access control. Although the accuracy of face recognition systems is generally high, they are not without flaws. Many biometric systems have been found to exhibit demographic bias, resulting in different demographic groups being not recognized with the same accuracy. This is especially true for facial recognition due to demographic factors, e.g., gender and skin color. While many previous works already reported demographic bias, this work aims to reduce demographic bias for biometric face recognition applications. In this regard, 12 face recognition systems are benchmarked regarding biometric recognition performance as well as demographic differentials, i.e., fairness. Subsequently, multiple fusion techniques are applied with the goal to improve the fairness in contrast to single systems. The experimental results show that it is possible to improve the fairness regarding single demographics, e.g., skin color or gender, while improving fairness for demographic subgroups turns out to be more challenging."
A Bias-Compensated NMMCC Algorithm Against Noisy Input and Non-Gaussian Interference,2023,Xiaoqiang Long; Haiquan Zhao; Xinyan Hou,IEEE Transactions on Circuits & Systems II Express Briefs,3,W4367663187,10.1109/tcsii.2023.3271634,https://openalex.org/W4367663187,,Robustness (evolution); Algorithm; Computer science; Gaussian; Gaussian function,article,False,"This brief proposes a bias-compensated normalized adaptive filtering algorithm (BC-NMMCC) under the mixture maximum correntropy (MMCC), which can increase the flexibility of the correntropy by changing the weight of the mixture parameter, as well as change the mixture kernel function to a single kernel function when the mixture parameter takes endpoint value. In order to deal with the case of noisy input and non-Gaussian noise interference, a bias-compensated term is obtained by utilizing an unbiased criterion and some assumptions to increase the robustness of the NMMCC algorithm. To further improve the performance of the proposed algorithm, a modified parameter is added to the bias-compensated term. Then, the computational complexity of the proposed algorithm is analyzed. Finally, the numerical simulation results also show that the BC-NMMCC algorithm performs better in terms of convergence speed and steady-state performance than several other existing algorithms with better robustness under different noise disturbances."
"Exploring Nexus of Social Media Algorithms, Content Creators, and Gender Bias: A Systematic Literature Review",2024,,Asian Journal of Research in Education and Social Sciences,3,W4393417718,10.55057/ajress.2024.6.1.39,https://openalex.org/W4393417718,https://myjms.mohe.gov.my/index.php/ajress/article/download/25798/14431,Nexus (standard); Social media; Content (measure theory); Computer science; Data science,article,True,"Drawing on the PRISMA framework, this study systematically investigates the dynamics between social media algorithms, content creators, and gender bias. An analysis of 18 quantitative and mixed-method studies from the Web of Science and Scopus databases, spanning 2019 to 2023, uncovers three main research trajectories: algorithms' influence on gender bias, their role in shaping content, and the interactions between algorithms, gender bias, and content creators. The review synthesizes diverse theoretical approaches and models, offering comprehensive insights into the complex nexus of algorithms, gender bias, and content creators. The application of varied research methodologies, including experiments, surveys, and content analyses, facilitates a thorough examination of algorithmic impacts. The chosen studies, focusing on different social media platforms and algorithmic features, reflect the varied interests of researchers. The findings reveal that algorithms perpetuate gender stereotypes by processing and learning content imbued with gender biases and further marginalizing gender minorities, reinforcing binary gender norms. The algorithmic curation of popular content also introduces inequities among content creators. Highlighting the need for equitable and inclusive digital environments, this review advocates for ethical content creation and algorithmic practices to mitigate gender bias and foster equality on social media platforms."
"A critical review towards artificial general intelligence: Challenges, ethical considerations, and the path forward",2024,Sedat Sonko; Adebunmi Okechukwu Adewusi; Ogugua Chimezie; Shedrack Onwusinkwue; Akoh Atadoga,World Journal of Advanced Research and Reviews,63,W4392894584,10.30574/wjarr.2024.21.3.0817,https://openalex.org/W4392894584,https://wjarr.com/sites/default/files/WJARR-2024-0817.pdf,Transparency (behavior); Accountability; Engineering ethics; Computer science; Scrutiny,review,True,"The pursuit of Artificial General Intelligence (AGI) has captivated researchers and industry leaders alike, promising a future where machines possess human-like cognitive abilities. However, this ambitious endeavor is fraught with multifaceted challenges and ethical dilemmas that necessitate careful examination. This critical review surveys the landscape of AGI research, identifying key hurdles and ethical considerations while outlining potential pathways forward. Firstly, technical challenges loom large on the path to AGI. These encompass fundamental problems such as developing robust learning algorithms capable of generalizing across diverse domains, as well as engineering systems that can exhibit adaptive and autonomous behavior akin to human intelligence. Additionally, ensuring the safety and reliability of AGI systems presents a formidable obstacle, with concerns ranging from algorithmic bias to the potential for catastrophic outcomes in unanticipated scenarios. Ethical considerations permeate every facet of AGI development and deployment. Questions of accountability, transparency, and control surface as central concerns, as the implications of relinquishing decision-making authority to autonomous systems raise profound ethical dilemmas. Moreover, the socio-economic ramifications of widespread AGI adoption, including job displacement and inequality, demand careful scrutiny and proactive mitigation strategies. Navigating these challenges requires a concerted effort from interdisciplinary stakeholders. Collaboration between computer scientists, ethicists, policymakers, and the public is essential to establish robust frameworks for the responsible development and deployment of AGI. Moreover, fostering an inclusive dialogue that prioritizes ethical principles and societal values is paramount in shaping a future where AGI augments human capabilities while safeguarding against potential risks. While the pursuit of AGI holds immense promise, its realization demands a holistic approach that addresses technical challenges alongside ethical considerations. By charting a path forward that prioritizes safety, transparency, and ethical governance, we can harness the transformative potential of AGI while ensuring its alignment with human values and interests."
"Human Resources Analytics for Public Personnel Management: Concepts, Cases, and Caveats",2023,Wonhyuk Cho; Seeyoung Choi; Hemin Choi,Administrative Sciences,47,W4318777251,10.3390/admsci13020041,https://openalex.org/W4318777251,https://www.mdpi.com/2076-3387/13/2/41/pdf?version=1675156791,Analytics; Public sector; Thematic analysis; Scope (computer science); Business intelligence,article,True,"The advancement of data technology such as machine learning and artificial intelligence has broadened the scope of human resources (HR) analytics, commonly referred to as “people analytics.” This field has seen significant growth in recent years as organizations increasingly rely on algorithm-based predictive tools for HR-related decision making. However, its application in the public sector is not yet fully understood. This study examined the concepts and practices of HR analytics through a thematic review, and proposed a five-step process (define, collect, analyze, share, and reflect) for implementation in the public sector—the process aims to assist with the integration of HR analytics in public personnel management practices. By analyzing cases in both the public and private sectors, this study identified key lessons for functional areas such as workforce planning, recruitment, HR development, and performance management. This research also identified the necessary conditions for introducing HR analytics in public organizations, including data management, staff capabilities, and acceptance, and discussed the potential challenges of privacy, integrity, algorithmic bias, and publicness."
A Systematic Review of Synthetic Data Generation Techniques Using Generative AI,2024,Mandeep Goyal; Qusay H. Mahmoud,Electronics,63,W4402230126,10.3390/electronics13173509,https://openalex.org/W4402230126,https://doi.org/10.3390/electronics13173509,Generative grammar; Computer science; Synthetic data; Artificial intelligence; Data science,review,True,"Synthetic data are increasingly being recognized for their potential to address serious real-world challenges in various domains. They provide innovative solutions to combat the data scarcity, privacy concerns, and algorithmic biases commonly used in machine learning applications. Synthetic data preserve all underlying patterns and behaviors of the original dataset while altering the actual content. The methods proposed in the literature to generate synthetic data vary from large language models (LLMs), which are pre-trained on gigantic datasets, to generative adversarial networks (GANs) and variational autoencoders (VAEs). This study provides a systematic review of the various techniques proposed in the literature that can be used to generate synthetic data to identify their limitations and suggest potential future research areas. The findings indicate that while these technologies generate synthetic data of specific data types, they still have some drawbacks, such as computational requirements, training stability, and privacy-preserving measures which limit their real-world usability. Addressing these issues will facilitate the broader adoption of synthetic data generation techniques across various disciplines, thereby advancing machine learning and data-driven solutions."
Shapley value: from cooperative game to explainable artificial intelligence,2024,Meng Li; Hengyang Sun; Yanjun Huang; Hong Chen,Autonomous Intelligent Systems,47,W4391685200,10.1007/s43684-023-00060-8,https://openalex.org/W4391685200,https://link.springer.com/content/pdf/10.1007/s43684-023-00060-8.pdf,Shapley value; Mathematical economics; Value (mathematics); Computer science; Artificial intelligence,article,True,"Abstract With the tremendous success of machine learning (ML), concerns about their black-box nature have grown. The issue of interpretability affects trust in ML systems and raises ethical concerns such as algorithmic bias. In recent years, the feature attribution explanation method based on Shapley value has become the mainstream explainable artificial intelligence approach for explaining ML models. This paper provides a comprehensive overview of Shapley value-based attribution methods. We begin by outlining the foundational theory of Shapley value rooted in cooperative game theory and discussing its desirable properties. To enhance comprehension and aid in identifying relevant algorithms, we propose a comprehensive classification framework for existing Shapley value-based feature attribution methods from three dimensions: Shapley value type, feature replacement method, and approximation method. Furthermore, we emphasize the practical application of the Shapley value at different stages of ML model development, encompassing pre-modeling, modeling, and post-modeling phases. Finally, this work summarizes the limitations associated with the Shapley value and discusses potential directions for future research."
Unveiling the Relationship Between News Recommendation Algorithms and Media Bias: A Simulation-Based Analysis of the Evolution of Bias Prevalence,2023,Qin Ruan; Brian Mac Namee; Ruihai Dong,Lecture notes in computer science,4,W4388468543,10.1007/978-3-031-47994-6_17,https://openalex.org/W4388468543,,Computer science; Recommender system; Media bias; Algorithm; News media,book-chapter,False,
Stress-Testing Bias Mitigation Algorithms to Understand Fairness Vulnerabilities,2023,Karan Bhanot; Ioana Baldini; Dennis Wei; Jiaming Zeng; Kristin P. Bennett,,2,W4386242375,10.1145/3600211.3604713,https://openalex.org/W4386242375,https://dl.acm.org/doi/pdf/10.1145/3600211.3604713,Computer science; Metric (unit); Audit; Algorithm; Fairness measure,article,True,"To address the growing concern of unfairness in Artificial Intelligence (AI), several bias mitigation algorithms have been introduced in prior research. Their capabilities are often evaluated on certain overly-used datasets without rigorously stress-testing them under simultaneous train and test distribution shifts. To address this, we investigate the fairness vulnerabilities of these algorithms across several distribution shift scenarios using synthetic data, to highlight scenarios where these algorithms do and don't work to encourage their trustworthy use. The paper makes three important contributions. Firstly, we propose a flexible pipeline called the Fairness Auditor to systematically stress-test bias mitigation algorithms using multiple synthetic datasets with shifts. Secondly, we introduce the Deviation Metric for measuring the fairness and utility performance of these algorithms under such shifts. Thirdly, we propose an interactive reporting tool for comparing algorithmic performance across various synthetic datasets, mitigation algorithms and metrics called the Fairness Report."
Human visual explanations mitigate bias in AI-based assessment of surgeon skills,2023,Dani Kiyasseh; Jasper Laca; Taseen F. Haque; Maxwell Otiato; Brian J. Miles; Christian von Wagner; Daniel A. Donoho; Quoc‐Dien Trinh; Animashree Anandkumar; Andrew J. Hung,npj Digital Medicine,32,W4362506590,10.1038/s41746-023-00766-2,https://openalex.org/W4362506590,https://www.nature.com/articles/s41746-023-00766-2.pdf,Credentialing; Credential; Leverage (statistics); Debiasing; Privilege (computing),article,True,"Artificial intelligence (AI) systems can now reliably assess surgeon skills through videos of intraoperative surgical activity. With such systems informing future high-stakes decisions such as whether to credential surgeons and grant them the privilege to operate on patients, it is critical that they treat all surgeons fairly. However, it remains an open question whether surgical AI systems exhibit bias against surgeon sub-cohorts, and, if so, whether such bias can be mitigated. Here, we examine and mitigate the bias exhibited by a family of surgical AI systems-SAIS-deployed on videos of robotic surgeries from three geographically-diverse hospitals (USA and EU). We show that SAIS exhibits an underskilling bias, erroneously downgrading surgical performance, and an overskilling bias, erroneously upgrading surgical performance, at different rates across surgeon sub-cohorts. To mitigate such bias, we leverage a strategy -TWIX-which teaches an AI system to provide a visual explanation for its skill assessment that otherwise would have been provided by human experts. We show that whereas baseline strategies inconsistently mitigate algorithmic bias, TWIX can effectively mitigate the underskilling and overskilling bias while simultaneously improving the performance of these AI systems across hospitals. We discovered that these findings carry over to the training environment where we assess medical students' skills today. Our study is a critical prerequisite to the eventual implementation of AI-augmented global surgeon credentialing programs, ensuring that all surgeons are treated fairly."
Human intelligence and artificial intelligence and the challenges of biases in ai algorithms,2024,Erika Ribeiro Fernandes; Marcelo Augusto Vieira Graglia,Journal on Innovation and Sustainability RISUS,1,W4398186076,10.23925/2179-3565.2023v15i1p133-142,https://openalex.org/W4398186076,https://revistas.pucsp.br/index.php/risus/article/download/66296/44877,Human intelligence; Artificial intelligence; Computer science; Cognitive science; Machine learning,article,True,"This article acknowledges the profound transformations that Artificial Intelligence imposes on society. A descriptive-exploratory study aims to discuss algorithmic biases and understand their impacts on society. The article starts from the understanding of human intelligence and learning from a pluralistic perspective, based on the analysis of literary works and scientific articles. This approach provides a context in which AI and machine learning can be conceived from an innovation perspective for the common good. The critical analysis emphasizes the need for ethical approaches in the development of these systems. The topics discussed highlight the importance of a multidimensional approach in mitigating algorithmic biases. From data selection to audits and accountability, diversity of perspectives, both in datasets and development teams, is crucial. The implementation of continuous training and human supervision reflects a continuous commitment to transparency and fairness in artificial intelligence. These integrated strategies are essential for the ethical, transparent, and equitable development of AI. This holistic approach, involving diverse skills and people, continuous training, and vigilant oversight, is vital to ensure the ethical use of AI for the collective well-being."
Application of deep learning algorithms to correct bias in <scp>CMIP6</scp> simulations of surface air temperature over the Indian monsoon core region,2023,A. Sabarinath; A. Naga Rajesh; Sachin S. Gunthe; T. V. Lakshmi Kumar,International Journal of Climatology,5,W4387617059,10.1002/joc.8276,https://openalex.org/W4387617059,,Climatology; Mean squared error; Climate model; GCM transcription factors; Environmental science,article,False,"Abstract Indian subcontinent witnessed a rise in surface air temperature (SAT) in recent decades, during the summer months of March, April and May. The monsoon core region (MCR) of India experiences a hot and humid climate, with temperatures typically highest in May and June before the onset of the monsoon. Global climate model (GCM) simulations of SAT are very much essential to understand the future climate of Indian MCR. Biases in GCMs simulations are due to insufficient knowledge of parameterizations and various assumptions that are made to simulate the complex interactions between land, ocean and atmosphere. The objective of this study is to correct the bias in the Coupled Model Intercomparison Project Phase 6 (CMIP6)–GCM simulations of SAT during March, April and May months over MCR for the historical period 1985–2014 and shared socio‐economic pathways (SSPs) SSP2‐4.5 and SSP5‐8.5 for the period 2015–2100. SAT dataset of fifth‐generation reanalysis (ERA5) of the European Centre for Medium‐Range Weather Forecasts (ECMWF) is used as reference dataset to perform bias correction for the historical period. Preliminary investigation of both SAT datasets has shown that there exists considerable warm bias (1.47°C) over the MCR. Bias correction is performed using a one‐dimensional convolutional neural network (CNN‐1D) and a convolutional long short‐term memory network (CNN‐LSTM) deep learning algorithm. The performance of these algorithms is evaluated with the statistical metrics such as root‐mean‐square error (RMSE), normalized root‐mean‐square error, Nash–Sutcliffe efficiency, mean absolute error, percent bias, correlation coefficient and dynamic time warping. RMSE and percent bias were decreased to 0.35°C and 0.8% with CNN‐LSTM algorithm. The CNN‐LSTM algorithm also preserves the year‐to‐year variability of SAT. Hence, CNN‐LSTM algorithm is found to be suitable for the bias correction of GCM simulations of SAT with encouraging results."
Calibration of the SMAP Soil Moisture Retrieval Algorithm to Reduce Bias Over the Amazon Rainforest,2024,Kyeungwoo Cho; Robinson Negrón‐Juárez; Andreas Colliander; Eric G. Cosio; Norma Salinas; A LETCIA PONTES DE ARAUJO; Jeffrey Q. Chambers; Jingfeng Wang,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,5,W4394828130,10.1109/jstars.2024.3388914,https://openalex.org/W4394828130,https://ieeexplore.ieee.org/ielx7/4609443/4609444/10499827.pdf,Amazon rainforest; Rainforest; Vegetation (pathology); Environmental science; Remote sensing,article,True,"Soil moisture (SM) is crucial for the Earth's ecosystem, impacting climate and vegetation health. Obtaining in situ observations of SM is labor-intensive and complex, particularly in remote and densely vegetated regions like the Amazon rainforest. NASA's Soil Moisture Active and Passive (SMAP) mission, utilizing an L-band radiometer, aims to monitor global SM. While it has been validated in areas with low Vegetation Water Content (VWC) (< 5 kgm <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-2</sup> ), its efficiency in the Amazon, with dense canopies and high VWC (> 10 kgm <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-2</sup> ), is limitedly investigated due to scarce in situ measurements. This study assessed and analyzed the SMAP SM retrievals in the Amazon, employing the single-channel algorithm (SCA) and adjusting vegetation optical depth (τ) and single scattering albedo (ω), two key vegetation parameters. It incorporated in situ SM observations from three old-growth rainforest locations: Tambopata (Southwest Amazon), Manaus (Central Amazon), and Caxiuana (Eastern Amazon). The SMAP SM deviated substantially from the in situ SM. However, calibrating τ and ω values, characterized by a lower τ, resulted in better agreement with the in situ measurements. The study emphasizes the pressing need for innovative methodologies to accurately retrieve SM in high-VWC regions like the Amazon rainforest using SMAP data."
Heuristics and biases in human–algorithm interaction and hotel revenue management override decision-making,2024,Ibrahim Mohammed; Basak Denizci Guillet,International Journal of Contemporary Hospitality Management,3,W4401924202,10.1108/ijchm-02-2024-0288,https://openalex.org/W4401924202,,Heuristics; Revenue management; Revenue; Computer science; Yield management,article,False,"Purpose This study aims to provide insights into human–algorithm interaction in revenue management (RM) decision-making and to uncover the underlying heuristics and biases of overriding systems’ recommendations. Design/methodology/approach Following constructivist traditions, 20 in-depth interviews were conducted with revenue optimisers, analysts, managers and directors with vast experience in over 25 markets and working with different RM systems (RMSs) at the property and corporate levels. The hermeneutics approach was used to interpret and make meaning of the participants’ lived experiences and interactions with RMSs. Findings The findings explain the nature of the interaction between RM professionals and RMSs, the cognitive mechanism by which the system users judgementally adjust or override its recommendations and the heuristics and biases behind override decisions. Additionally, the findings reveal the individual decision-maker characteristics and organisational factors influencing human–algorithm interactions. Research limitations/implications Although the study focused on human–system interaction in hotel RM, it has larger implications for integrating human judgement into computerised systems for optimal decision-making. Practical implications The study findings expose human biases in working with RMSs and highlight the influencing factors that can be addressed to achieve effective human–algorithm interactions. Originality/value The study offers a holistic framework underpinned by the organisational role and expectation confirmation theories to explain the cognitive mechanisms of human–system interaction in managerial decision-making."
Mitigating Age-related Bias in Predictive Policing Algorithms,2023,Ahmed S. Almasoud,,1,W4388960667,10.20944/preprints202311.1534.v1,https://openalex.org/W4388960667,https://doi.org/10.20944/preprints202311.1534.v1,Odds; Computer science; Parity (physics); Psychology; Econometrics,preprint,True,"This study addressed algorithmic bias in predictive policing, focusing on the Chicago Police Department&amp;#039;s Strategic Subject List (SSL) dataset. We specifically focused on identifying and mitigating age-related biases, a notably underexplored area in prior research. Our research introduced Conditional Score Recalibration as a bias mitigation strategy alongside the well-established Class Balancing technique. Conditional Score Recalibration involved reassessing and adjusting risk scores for individuals initially assigned moderately high-risk scores in the dataset. This recalibration marked such individuals as low risk if they met three conditions, namely: no prior arrests for violent offenses, no previous arrests for narcotic offenses, and having never been involved in shooting incidents. These fairness strategies were implemented on the Random Forest model, and the fairness metrics employed included Equality of Opportunity Difference, Average Odds Difference, and Demographic Parity. The results showed a significant improvement in model fairness, particularly for age biases, without compromising the model&amp;#039;s accuracy. These findings challenged the often-assumed trade-off between fairness and accuracy, underscoring the feasibility of achieving fairness without compromising accuracy."
Improved Multilayer Perceptron Neural Networks Weights and Biases Based on The Grasshopper optimization Algorithm to Predict Student Performance on Ambient Learning,2023,Mercy K. Michira; Richard Rimiru; Waweru Mwangi,,4,W4379352342,10.1145/3583788.3583797,https://openalex.org/W4379352342,https://dl.acm.org/doi/pdf/10.1145/3583788.3583797,Perceptron; Artificial neural network; Computer science; Algorithm; Backpropagation,article,True,The classification accuracy of a multi-layer Perceptron Neural Networks depends on the selection of its parameters such the connection weights and biases. Generating an optimal value of these parameters requires a suitable algorithm to train the multilayer perceptron neural networks. This paper presents swam based Grasshopper optimization algorithm that optimizes the connection weights and biases of Multilayer Perceptron Neural Network. Grasshopper optimization algorithm is a swarm-based metaheuristic algorithm applied for accurate learning of Multilayer Perceptron Neural Networks. The proposed Multilayer Layer Perceptron Neural Networks based on the Grasshopper Optimization Algorithm was validated using a Genetic algorithm and Backpropagation algorithm this algorithm has proved to perform satisfactorily performance by escaping local optimal and its fast convergence.
How Much Does Racial Bias Affect Mortgage Lending? Evidence from Human and Algorithmic Credit Decisions,2025,Neil Bhutta; Aurel Hizmo; Daniel Ringo,The Journal of Finance,7,W4409344392,10.1111/jofi.13444,https://openalex.org/W4409344392,,Affect (linguistics); Economics; Business; Monetary economics; Actuarial science,article,False,"ABSTRACT We assess racial discrimination in mortgage approvals using confidential data on mortgage applications. Minority applicants tend to have lower credit scores and higher leverage, and are less likely to receive algorithmic approval from race‐blind automated underwriting systems (AUS). Observable applicant‐risk factors explain most of the racial disparities in lender denials. Further, exploiting the AUS data, we show there are risk factors we do not observe, and these factors at least partially explain the residual 1 to 2 percentage point denial gaps. We conclude that differential treatment plays a more limited role in generating denial disparities than previous research suggests."
Correction: Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms,2023,Benedetta Giovanola; Simona Tiribelli,AI & Society,6,W4384934767,10.1007/s00146-023-01722-0,https://openalex.org/W4384934767,https://link.springer.com/content/pdf/10.1007/s00146-023-01722-0.pdf,Health care; Performing arts; Artificial intelligence; Computer science; Machine learning,article,True,
A bias-compensated NLMS algorithm based on arctangent framework for system identification,2024,Rosalin Rosalin; Ansuman Patnaik; Sarita Nanda; Deepak Kumar Rout,Signal Image and Video Processing,4,W4391896268,10.1007/s11760-024-03024-4,https://openalex.org/W4391896268,,Identification (biology); Inverse trigonometric functions; Algorithm; Computer science; System identification,article,False,
Bias and Fairness Addressing Discrimination in AI Systems,2024,Padmaja Pulivarthy; Pawan Whig,Advances in human and social aspects of technology book series,32,W4403508773,10.4018/979-8-3693-4147-6.ch005,https://openalex.org/W4403508773,,Transparency (behavior); Accountability; Dignity; Equity (law); Diversity (politics),book-chapter,False,"As artificial intelligence (AI) becomes increasingly pervasive in decision-making processes across various sectors, concerns about bias and fairness have risen to the forefront of ethical discussions. This chapter delves into the complex landscape of bias in AI systems, exploring its origins, manifestations, and implications for societal equity. We examine how biases can inadvertently infiltrate algorithms through data collection, preprocessing, and model training phases, leading to discriminatory outcomes against certain demographic groups. Moreover, we explore methodologies and frameworks aimed at mitigating bias, such as fairness-aware algorithms, bias detection techniques, and diversity-enhancing approaches. Ethical considerations and regulatory efforts are also scrutinized, highlighting the urgent need for transparency and accountability in AI development. By addressing these issues comprehensively, this chapter aims to contribute to the ongoing dialogue on fostering inclusive and equitable AI systems that uphold fundamental human rights and dignity."
From 'black box' to 'glass box': using Explainable Artificial Intelligence (XAI) to reduce opacity and address bias in algorithmic models,2024,Otávio Morato de Andrade; Marco Antônio Sousa Alves,Revista Thesis Juris,4,W4400111128,10.5585/13.2024.26510,https://openalex.org/W4400111128,https://doi.org/10.5585/13.2024.26510,Transparency (behavior); Computer science; Black box; Artificial intelligence; Identification (biology),article,True,"Artificial intelligence (AI) has been extensively employed across various domains, with increasing social, ethical, and privacy implications. As their potential and applications expand, concerns arise about the reliability of AI systems, particularly those that use deep learning techniques that can make them true “black boxes”. Explainable artificial intelligence (XAI) aims to offer information that helps explain the predictive process of a given algorithmic model. This article examines the potential of XAI in elucidating algorithmic decisions and mitigating bias in AI systems. In the first stage of the work, the issue of AI fallibility and bias is discussed, emphasizing how opacity exacerbates these issues. The second part explores how XAI can enhance transparency, helping to combat algorithmic errors and biases. The article concludes that XAI can contribute to the identification of biases in algorithmic models, then it is suggested that the ability to “explain” should be a requirement for adopting AI systems in sensitive areas such as court decisions."
Funds of Knowledge used by Adolescents of Color in Scaffolded Sensemaking around Algorithmic Fairness,2023,Jean Salac; Alannah Oleson; Lena Armstrong; Audrey Le Meur; Amy J. Ko,,12,W4386588485,10.1145/3568813.3600110,https://openalex.org/W4386588485,https://dl.acm.org/doi/pdf/10.1145/3568813.3600110,Sensemaking; Computer science; Knowledge management,article,True,"With the ubiquity of computing technologies, adolescents are increasingly affected by algorithmic biases. While previous work provides insight into adolescents' perceptions of algorithmic bias, few provide guidance on how to engage adolescents in discourse on algorithmic bias that prioritizes both their agency and safety. To address this, we developed and conducted group discussions and design activities based on three scenarios of algorithmic bias with 15 adolescents of color (ages 15-17) in a summer academic program in the United States targeted at students from families with low-income backgrounds or who would be the first in their family to pursue post-secondary education. When sensemaking, all participants considered factors beyond the scenarios, using their situated knowledge to contextualize perceptions of unfairness. They also considered sources of bias and impacts of unfairness at different levels of individuals, communities, and society. However, when designing solutions, they tended to design for hypothetical ""average users"" instead of considering nuances of user populations. We offer insights for algorithmic fairness learning experiences that support situated reasoning in adolescents."
Investigating Structural Bias in Real-Coded Genetic Algorithms,2024,Kanchan Rajwar; Yogesh Kumar; Kusum Deep,Proceedings of the Genetic and Evolutionary Computation Conference Companion,3,W4401212855,10.1145/3638530.3654312,https://openalex.org/W4401212855,,Computer science; Algorithm,article,False,
Mitigating Representation Bias in Action Recognition: Algorithms and Benchmarks,2023,Haodong Duan; Yue Zhao; Kai Chen; Yuanjun Xiong; Dahua Lin,Lecture notes in computer science,3,W4320502918,10.1007/978-3-031-25069-9_36,https://openalex.org/W4320502918,https://arxiv.org/pdf/2209.09393,Debiasing; Computer science; Leverage (statistics); Machine learning; Artificial intelligence,book-chapter,False,
Intelligent classification and personalized recommendation of E-commerce products based on machine learning,2024,Kangming Xu; Huiming Zhou; Haotian Zheng; Mingwei Zhu; Xin Qi,Applied and Computational Engineering,34,W4396903457,10.54254/2755-2721/64/20241365,https://openalex.org/W4396903457,https://www.ewadirect.com/proceedings/ace/article/view/12391/pdf,Recommender system; Computer science; Scalability; Operability; Information overload,article,True,"With the rapid evolution of the Internet and the exponential proliferation of information, users encounter information overload and the conundrum of choice. Personalized recommendation systems play a pivotal role in alleviating this burden by aiding users in filtering and selecting information tailored to their preferences and requirements. This paper undertakes a comparative analysis between the operational mechanisms of traditional e-commerce commodity classification systems and personalized recommendation systems. It delineates the significance and application of personalized recommendation systems across e-commerce, content information, and media domains. Furthermore, it delves into the challenges confronting personalized recommendation systems in e-commerce, including data privacy, algorithmic bias, scalability, and the cold start problem. Strategies to address these challenges are elucidated. Subsequently, the paper outlines a personalized recommendation system leveraging the BERT model and nearest neighbor algorithm, specifically tailored to address the exigencies of the eBay e-commerce platform. The efficacy of this recommendation system is substantiated through manual evaluation, and a practical application operational guide and structured output recommendation results are furnished to ensure the system's operability and scalability."
Uses of Generative AI in the Newsroom: Mapping Journalists’ Perceptions of Perils and Possibilities,2024,Hannes Cools; Nicholas Diakopoulos,Journalism Practice,44,W4401926094,10.1080/17512786.2024.2394558,https://openalex.org/W4401926094,https://doi.org/10.1080/17512786.2024.2394558,Generative grammar; Journalism; Perception; Political science; Sociology,article,True,"This study delves into journalists' perspectives on the perils and possibilities of using generative AI-tools like ChatGPT, Bard, and DALL-E in the newsroom. Semi-structured interviews with journalists from The Netherlands, and Denmark, who self-identify as early adopters of generative AI-tools, were conducted. Results reveal 16 different specific uses of generative-AI tools across the news reporting process, mostly situated in the news production and distribution phase. The rationale for specific uses (or non-uses) of generative AI were grounded in journalistic intuitions and gut feeling. While journalists appreciate the advantages of these tools, such as improved efficiency and data handling capabilities, respondents also voice concerns about the potential for harm to journalism's accuracy and credibility, as well as ethical considerations like algorithmic bias. The study further emphasizes the necessity of providing journalists with sufficient education and algorithmic literacy in using generative AI tools, as well as the significance of ongoing monitoring and assessment to guarantee their ethical and responsible usage in journalism."
The smart future for sustainable development: Artificial intelligence solutions for sustainable urbanization,2024,Marwan Al‐Raeei,Sustainable Development,64,W4400957436,10.1002/sd.3131,https://openalex.org/W4400957436,,Urbanization; Sustainable development; Sustainable city; Smart city; Big data,article,False,"Abstract Future tools for supporting collaborations between technology and sustainable development include artificial intelligence (AI) applications in sustainable Urbanization roles. This article highlights the various applications of AI in advancing sustainable urbanization. From urban planning to disaster management, AI technology is revolutionizing the way cities are designed and managed. By leveraging data analytics, machine learning, and predictive modeling, AI is helping city officials make informed decisions, optimize resource usage, and improve quality of life for urban residents. Despite the immense potential of AI in sustainable urban development, there are still challenges and limitations to overcome. We show some of the most significant problems related to these issues. These include issues related to data privacy, algorithm bias, and ethical considerations. Continued research and innovation are needed to address these challenges and ensure that AI technology is used responsibly and effectively in shaping sustainable cities. As a result, AI has the power to transform urban environments and create more sustainable, resilient communities. By harnessing the capabilities of AI, cities can become more efficient, environmentally‐friendly, and prepared for the challenges of the future. It is essential for policymakers, urban planners, and technology developers to work together to harness the full potential of AI in sustainable urbanization and create a better future for all. Proactively addressing these challenges can unlock the full potential of AI in combating sustainable cities and building a sustainable future for all."
Advancing Community Engaged Approaches to Identifying Structural Drivers of Racial Bias in Health Diagnostic Algorithms,2023,Jill Kuhlberg; Irene Headen; Ellis Ballard; Donald Martin,arXiv (Cornell University),3,W4378100319,10.48550/arxiv.2305.13485,https://openalex.org/W4378100319,https://arxiv.org/abs/2305.13485,Health care; Context (archaeology); Process (computing); Health equity; Computer science,preprint,True,"Much attention and concern has been raised recently about bias and the use of machine learning algorithms in healthcare, especially as it relates to perpetuating racial discrimination and health disparities. Following an initial system dynamics workshop at the Data for Black Lives II conference hosted at MIT in January of 2019, a group of conference participants interested in building capabilities to use system dynamics to understand complex societal issues convened monthly to explore issues related to racial bias in AI and implications for health disparities through qualitative and simulation modeling. In this paper we present results and insights from the modeling process and highlight the importance of centering the discussion of data and healthcare on people and their experiences with healthcare and science, and recognizing the societal context where the algorithm is operating. Collective memory of community trauma, through deaths attributed to poor healthcare, and negative experiences with healthcare are endogenous drivers of seeking treatment and experiencing effective care, which impact the availability and quality of data for algorithms. These drivers have drastically disparate initial conditions for different racial groups and point to limited impact of focusing solely on improving diagnostic algorithms for achieving better health outcomes for some groups."
Biased thermodynamics can explain the behaviour of smart optimization algorithms that work above the dynamical threshold,2023,Angelo Giorgio Cavaliere; Federico Ricci‐Tersenghi,arXiv (Cornell University),3,W4361193629,10.48550/arxiv.2303.14879,https://openalex.org/W4361193629,https://arxiv.org/abs/2303.14879,Ergodicity; Measure (data warehouse); Constraint satisfaction problem; Dynamical systems theory; Mathematics,preprint,True,"Random constraint satisfaction problems can display a very rich structure in the space of solutions, with often an ergodicity breaking -- also known as clustering or dynamical -- transition preceding the satisfiability threshold when the constraint-to-variables ratio $\alpha$ is increased. However, smart algorithms start to fail finding solutions in polynomial time at some threshold $\alpha_{\rm alg}$ which is algorithmic dependent and generally bigger than the dynamical one $\alpha_d$. The reason for this discrepancy is due to the fact that $\alpha_d$ is traditionally computed according to the uniform measure over all the solutions. Thus, while bounding the region where a uniform sampling of the solutions is easy, it cannot predict the performance of off-equilibrium processes, that are still able of finding atypical solutions even beyond $\alpha_d$. Here we show that a reconciliation between algorithmic behaviour and thermodynamic prediction is nonetheless possible at least up to some threshold $\alpha_d^{\rm opt}\geq\alpha_d$, which is defined as the maximum value of the dynamical threshold computed on all possible probability measures over the solutions. We consider a simple Monte Carlo-based optimization algorithm, which is restricted to the solution space, and we demonstrate that sampling the equilibrium distribution of a biased measure improving on $\alpha_d$ is still possible even beyond the ergodicity breaking point for the uniform measure, where other algorithms hopelessly enter the out-of-equilibrium regime. The conjecture we put forward is that many smart algorithms sample the solution space according to a biased measure: once this measure is identified, the algorithmic threshold is given by the corresponding ergodicity-breaking transition."
Social Bias in AI: Re-coding Innovation through Algorithmic Political Capitalism,2025,S. Carter; John G. Dale,AI & Society,1,W4413135319,10.1007/s00146-025-02540-2,https://openalex.org/W4413135319,https://doi.org/10.1007/s00146-025-02540-2,Capitalism; Transparency (behavior); Accountability; Sociotechnical system; Corporate governance,article,True,"Abstract This research examines the social dynamics underpinning algorithmic bias, proposing a framework for addressing these issues through the lens of algorithmic political capitalism. We explore how socio-technical-ecological relations of power often reproduce harmful algorithmic effects, including social bias, data exploitation in the knowledge economy, prejudiced predictions, and unexamined user biases that obscure power asymmetries and harm society. Building on complexity theory, particularly Morçöl’s definition of public policy as a dynamic system with co-evolving relationships between actors and systems, we analyze the challenges and opportunities to mitigate these harms within a multilayered framework. Our framework extends Keller and Block’s concept of ‘technology-dependent political capitalism’, incorporating mechanisms to ensure government assistance is conditional, allowing bicameral governance in supported corporations, and empowering local and state authorities to hold organizations accountable. Finally, we highlight the crucial roles of transparency, accountability, and democratization in fostering meaningful innovation, and argue that addressing algorithmic bias and the inequities of the knowledge economy requires a nuanced understanding of the interplay between public policy, technological systems, and societal structures. Our proposals aim to reshape the socio-technical-ecological landscape, creating conditions for algorithmic innovation that align with democratic values and equitable societal progress, while mitigating systemic violence."
Biased Bi-Population Evolutionary Algorithm for Energy-Efficient Fuzzy Flexible Job Shop Scheduling with Deteriorating Jobs,2024,Libao Deng; Yingjian Zhu; Yuanzhu Di; Lili Zhang,Complex System Modeling and Simulation,4,W4396752094,10.23919/csms.2023.0021,https://openalex.org/W4396752094,https://ieeexplore.ieee.org/ielx7/9420428/10525230/10525671.pdf,Fuzzy logic; Computer science; Mathematical optimization; Evolutionary algorithm; Flow shop scheduling,article,True,"There are many studies about flexible job shop scheduling problem with fuzzy processing time and deteriorating scheduling, but most scholars neglect the connection between them, which means the purpose of both models is to simulate a more realistic factory environment. From this perspective, the solutions can be more precise and practical if both issues are considered simultaneously. Therefore, the deterioration effect is treated as a part of the fuzzy job shop scheduling problem in this paper, which means the linear increase of a certain processing time is transformed into an internal linear shift of a triangle fuzzy processing time. Apart from that, many other contributions can be stated as follows. A new algorithm called reinforcement learning based biased bi-population evolutionary algorithm (RB <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> EA) is proposed, which utilizes Q-learning algorithm to adjust the size of the two populations and the interaction frequency according to the quality of population. A local enhancement method which combimes multiple local search stratgies is presented. An interaction mechanism is designed to promote the convergence of the bi-population. Extensive experiments are designed to evaluate the efficacy of RB <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> EA, and the conclusion can be drew that RB <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> EA is able to solve energy-efficient fuzzy flexible job shop scheduling problem with deteriorating jobs (EFFJSPD) efficiently."
Unveiling the shadows: Beyond the hype of AI in education,2024,Abdulrahman M. Al-Zahrani,Heliyon,57,W4396622564,10.1016/j.heliyon.2024.e30696,https://openalex.org/W4396622564,https://www.cell.com/article/S2405844024067276/pdf,Transparency (behavior); Transformative learning; Safeguarding; Creativity; Engineering ethics,article,True,"Despite the wave of enthusiasm for the role of Artificial Intelligence (AI) in reshaping education, critical voices urge a more tempered approach. This study investigates the less-discussed 'shadows' of AI implementation in educational settings, focusing on potential negatives that may accompany its integration. Through a multi-phased exploration consisting of content analysis and survey research, the study develops and validates a theoretical model that pinpoints several areas of concern. The initial phase, a systematic literature review, yielded 56 relevant studies from which the model was crafted. The subsequent survey with 260 participants from a Saudi Arabian university aimed to validate the model. Findings confirm concerns about human connection, data privacy and security, algorithmic bias, transparency, critical thinking, access equity, ethical issues, teacher development, reliability, and the consequences of AI-generated content. They also highlight correlations between various AI-associated concerns, suggesting intertwined consequences rather than isolated issues. For instance, enhancements in AI transparency could simultaneously support teacher professional development and foster better student outcomes. Furthermore, the study acknowledges the transformative potential of AI but cautions against its unexamined adoption in education. It advocates for comprehensive strategies to maintain human connections, ensure data privacy and security, mitigate biases, enhance system transparency, foster creativity, reduce access disparities, emphasize ethics, prepare teachers, ensure system reliability, and regulate AI-generated content. Such strategies underscore the need for holistic policymaking to leverage AI's benefits while safeguarding against its disadvantages."
Role and Challenges of ChatGPT and Similar Generative Artificial Intelligence in Business Management,2023,Nitin Liladhar Rane,SSRN Electronic Journal,61,W4388152605,10.2139/ssrn.4603227,https://openalex.org/W4388152605,,Generative grammar; Business management; Artificial intelligence; Business; Knowledge management,article,False,"The incorporation of ChatGPT and other Large Language Models (LLMs) has transformed the landscape of Business Management across various sectors. This research delves into the multifaceted roles these advanced AI technologies play in optimizing different aspects of business management. ChatGPT facilitates smooth communication, automates customer support, generates leads, and offers personalized customer experiences, enhancing customer relationship management from customer relations to market analysis. Additionally, in financial analysis and forecasting, these LLMs assist businesses in interpreting data, enabling data-driven decision-making and precise financial planning. The study explores the challenges faced by businesses when integrating ChatGPT and LLMs into different areas of business management. Ethical concerns, such as data privacy and algorithmic biases, necessitate careful consideration, urging businesses to maintain transparency and fairness in their AI-driven interactions. Moreover, in human resource management, these technologies aid in talent acquisition, employee training, and performance analysis, revolutionizing HR processes. Furthermore, the research investigates the transformative potential of ChatGPT and similar LLMs in fostering innovation and creativity within business processes. By automating routine tasks in supply chain management and logistics, employees can concentrate on strategic planning and innovation, thereby enhancing operational efficiency and competitiveness. This study highlights the significant impact of ChatGPT and similar LLMs across diverse fields of business management. While emphasizing their benefits, it underscores the importance for businesses to navigate challenges, promote ethical practices, and empower their workforce to fully utilize the potential of these technologies in an ever-evolving business landscape."
The Rise of Artificial Intelligence in Educational Measurement: Opportunities and Ethical Challenges,2024,Okan Bulut; Maggie Beiting-Parrish,Chinese/English Journal of Educational Measurement and Evaluation,34,W4405276448,10.59863/miql7785,https://openalex.org/W4405276448,https://doi.org/10.59863/miql7785,Transparency (behavior); Equity (law); Educational assessment; Engineering ethics; Computer science,article,True,"The integration of artificial intelligence (AI) in educational measurement has transformed assessment methods, allowing for automated scoring, swift content analysis, and personalized feedback through machine learning and natural language processing. These advancements provide valuable insights into student performance while also enhancing the overall assessment experience. However, the implementation of AI in education also raises significant ethical concerns regarding validity, reliability, transparency, fairness, and equity. Issues such as algorithmic bias and the opacity of AI decision-making processes risk perpetuating inequalities and affecting assessment outcomes. In response, various stakeholders, including educators, policymakers, and testing organizations, have developed guidelines to ensure the ethical use of AI in education. The National Council of Measurement in Education’s Special Interest Group on AI in Measurement and Education (AIME) is dedicated to establishing ethical standards and advancing research in this area. In this paper, a diverse group of AIME members examines the ethical implications of AI-powered tools in educational measurement, explores significant challenges such as automation bias and environmental impact, and proposes solutions to ensure AI’s responsible and effective use in education."
Smart Smile: Revolutionizing Dentistry With Artificial Intelligence,2023,Ashwini Dhopte; Hiroj Bagde,Cureus,59,W4382651942,10.7759/cureus.41227,https://openalex.org/W4382651942,https://assets.cureus.com/uploads/review_article/pdf/167748/20230701-8816-tv79o7.pdf,Medicine; Transformative learning; Virtual reality; Robotics; Artificial intelligence,review,True,"Artificial intelligence (AI) has emerged as a transformative technology in various industries, and its potential in dentistry is gaining significant attention. This abstract explores the future prospects of AI in dentistry, highlighting its potential to revolutionize clinical practice, improve patient outcomes, and enhance the overall efficiency of dental care. The application of AI in dentistry encompasses several key areas, including diagnosis, treatment planning, image analysis, patient management, and personalized care. AI algorithms have shown promising results in the automated detection and diagnosis of dental conditions, such as caries, periodontal diseases, and oral cancers, aiding clinicians in early intervention and improving treatment outcomes. Furthermore, AI-powered treatment planning systems leverage machine learning techniques to analyze vast amounts of patient data, considering factors like medical history, anatomical variations, and treatment success rates. These systems provide dentists with valuable insights and support in making evidence-based treatment decisions, ultimately leading to more predictable and tailored treatment approaches. While the potential of AI in dentistry is immense, it is essential to address certain challenges, including data privacy, algorithm bias, and regulatory considerations. Collaborative efforts between dental professionals, AI experts, and policymakers are crucial to developing robust frameworks that ensure the responsible and ethical implementation of AI in dentistry. Moreover, AI-driven robotics has introduced innovative approaches to dental surgery, enabling precise and minimally invasive procedures, and ultimately reducing patient discomfort and recovery time. Virtual reality (VR) and augmented reality (AR) applications further enhance dental education and training, allowing dental professionals to refine their skills in a realistic and immersive environment. AI holds tremendous promise in shaping the future of dentistry. Through its ability to analyze vast amounts of data, provide accurate diagnoses, facilitate treatment planning, improve image analysis, streamline patient management, and enable personalized care, AI has the potential to enhance dental practice and significantly improve patient outcomes. Embracing this technology and its future development will undoubtedly revolutionize the field of dentistry, fostering a more efficient, precise, and patient-centric approach to oral healthcare. Overall, AI represents a powerful tool that has the potential to revolutionize various aspects of society, from improving healthcare outcomes to optimizing business operations. Continued research, development, and responsible implementation of AI technologies will shape our future, unlocking new possibilities and transforming the way we live and work."
A Fairness Approach to Mitigating Racial Bias of Credit Scoring Models by Decision Tree and the Reweighing Fairness Algorithm,2023,Jen–Ying Shih; Ze-Han Chin,,3,W4383503216,10.1109/iceib57887.2023.10170339,https://openalex.org/W4383503216,,Computer science; Decision tree; Credit score; Algorithm; Fairness measure,article,False,"Credit scoring models have been widely applied by financial institutions, Peer to Peer (P2P) lending service providers, and Buy Now Pay Later (BNPL) service providers to evaluate their customers' financial status. Therefore, it has a large impact on consumer financing activities. However, unfair evaluation may occur as the development of credit scoring models contains biased judgments (e.g., racial bias), which deteriorates users' credit access ability. Thus, we study the feasibility of mitigating racial bias in developing a credit scoring model. By using a data set provided by a P2P lending platform, LendingClub, we integrated the C5.0 decision tree algorithm and the reweighing fairness algorithm to develop credit scoring models with cost-sensitive modeling concepts. Multi-class fair credit scoring evaluation was also studied in terms of performance indices, including accuracy, average cost, and unfairness metrics. The results demonstrated that the reweighing fairness algorithm reduced the unfairness and average cost of models. In addition, combining the fairness algorithm and cost-sensitive modeling minimized the average cost of models while maintaining the functionality of the fairness algorithm."
"Fairness in Recommendation: Foundations, Methods, and Applications",2023,Yunqi Li; Hanxiong Chen; Shuyuan Xu; Yingqiang Ge; Juntao Tan; Shuchang Liu; Yongfeng Zhang,ACM Transactions on Intelligent Systems and Technology,48,W4385302663,10.1145/3610302,https://openalex.org/W4385302663,https://doi.org/10.1145/3610302,Recommender system; Computer science; Ranking (information retrieval); Quality (philosophy); Domain (mathematical analysis),article,True,"As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision-making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This survey focuses on the foundations for fairness in recommendation literature. It first presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking to provide a general overview of fairness research, as well as introduce the more complex situations and challenges that need to be considered when studying fairness in recommender systems. After that, the survey will introduce fairness in recommendation with a focus on the taxonomies of current fairness definitions, the typical techniques for improving fairness, as well as the datasets for fairness studies in recommendation. The survey also talks about the challenges and opportunities in fairness research with the hope of promoting the fair recommendation research area and beyond."
Biased regression algorithms in the quaternion domain,2024,Rosa M. Fernández-Alcalá; J.D. Jiménez-López; Jesús Navarro-Moreno; Juan Carlos Ruiz-Molina,Journal of the Franklin Institute,2,W4392906913,10.1016/j.jfranklin.2024.106785,https://openalex.org/W4392906913,https://doi.org/10.1016/j.jfranklin.2024.106785,Quaternion; Algorithm; Regression; Domain (mathematical analysis); Computer science,article,True,"The ill-conditioned matrix problem in quaternion linear regression models is addressed in this paper and several dimension-reduction based regression methods for circumventing this problem are suggested. The algorithms are formulated in a general way and can be easily adapted to different scenarios: widely linear, semi-widely linear and strictly linear processing, in accordance with the properness properties presented by quaternion random vectors. A comparison with existing solutions is carried out by using both laboratory data and a color face database."
"Province of Origin, Decision‐Making Bias, and Responses to Bureaucratic Versus Algorithmic Decision‐Making",2025,Ge Wang; Zhejun Zhang; Shenghua Xie; Yue Guo,Public Administration Review,1,W4406869107,10.1111/puar.13928,https://openalex.org/W4406869107,https://doi.org/10.1111/puar.13928,Bureaucracy; Political science; Law; Politics,article,False,"ABSTRACT As algorithmic decision‐making (ADM) becomes prevalent in certain public sectors, its interaction with traditional bureaucratic decision‐making (BDM) evolves, especially in contexts shaped by regional identities and decision‐making biases. To explore these dynamics, we conducted two survey experiments within traffic enforcement scenarios, involving 4816 participants across multiple provinces. Results indicate that non‐native residents perceived ADM as fairer and more acceptable than BDM when they did not share a province of origin with local bureaucrats. Both native and non‐native residents showed a preference for ADM in the presence of bureaucratic and algorithmic biases but preferred BDM when such biases were absent. When bureaucratic and algorithmic biases coexisted, the lack of a shared province of origin further reinforced non‐native residents' perception of ADM as fairer and more acceptable than BDM. Our findings reveal the complex interplay among province of origin, decision‐making biases, and responses to different decision‐making approaches."
Innovative recruitment strategies in the IT sector: A review of successes and failures,2024,Funmilayo Aribidesi Ajayi; Chioma Ann Udeh,Magna Scientia Advanced Research and Reviews,37,W4394064896,10.30574/msarr.2024.10.2.0057,https://openalex.org/W4394064896,https://magnascientiapub.com/journals/msarr/sites/default/files/MSARR-2024-0057.pdf,Business; Engineering ethics; Engineering,review,True,"This study systematically reviews and analyzes the impact of innovative recruitment strategies on diversity and inclusion within the Information Technology (IT) sector. Given the rapid technological advancements and the increasing demand for skilled professionals, IT firms are at the forefront of adopting novel recruitment practices. The main objective of this research is to explore how these innovations influence the diversity of the IT workforce and to identify best practices for promoting inclusivity. Employing a systematic literature review and content analysis, the study examines peer-reviewed articles, industry reports, and relevant grey literature published within the last decade. Key insights reveal a significant shift towards the use of artificial intelligence (AI) and data analytics in recruitment processes, aiming to enhance efficiency and reduce biases. However, challenges such as potential algorithmic bias, privacy concerns, and the need for human oversight in automated recruitment are identified. The study concludes with actionable recommendations for IT firms, recruiters, and policymakers to ensure that recruitment innovations contribute positively to workforce diversity and inclusivity. It advocates for a balanced approach that leverages technology while addressing ethical considerations and promoting equal opportunities for underrepresented groups. Future research directions include exploring the long-term effects of these recruitment strategies on organizational performance and further examining the ethical implications of AI in recruitment. This study contributes to a deeper understanding of the evolving landscape of IT recruitment and its implications for diversity and inclusion initiatives."
Bias-compensated based Diffusion Affine Projection Like Maximum Correntropy Algorithm,2024,Chengjin Li; Haiquan Zhao; Xiang Wang,Digital Signal Processing,4,W4401043211,10.1016/j.dsp.2024.104702,https://openalex.org/W4401043211,,Algorithm; Affine transformation; Computer science; Diffusion; Projection (relational algebra),article,False,
Ethical Considerations in Artificial Intelligence: Addressing Bias and Fairness in Algorithmic Decision-Making,2024,Yogesh Morchhale,INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT,2,W4395669504,10.55041/ijsrem31693,https://openalex.org/W4395669504,,Prejudice (legal term); Transparency (behavior); Ethical decision; Economic Justice; Equity (law),article,False,"The expanding use of artificial intelligence (AI) in decision-making across a range of industries has given rise to serious ethical questions about prejudice and justice. This study looks at the moral ramifications of using AI algorithms in decision-making and looks at methods to combat prejudice and advance justice. The study investigates the underlying causes of prejudice in AI systems, the effects of biased algorithms on people and society, and the moral obligations of stakeholders in reducing bias, drawing on prior research and real-world examples. The study also addresses new frameworks and strategies for advancing justice in algorithmic decision-making, emphasizing the value of openness, responsibility, and diversity in dataset gathering and algorithm development. The study concludes with suggestions for further investigation and legislative actions to guarantee that AI systems respect moral standards and advance justice and equity in the processes of making decisions. Keywords Ethical considerations, Artificial intelligence, Bias, Fairness, Algorithmic decision-making, Ethical implications, Ethical responsibilities, Stakeholders, Bias in AI systems, Impact of biased algorithms, Strategies for addressing bias, Promoting fairness, Algorithmic transparency."
How can we manage biases in artificial intelligence systems – A systematic literature review,2023,P. S. Varsha,International Journal of Information Management Data Insights,160,W4323041949,10.1016/j.jjimei.2023.100165,https://openalex.org/W4323041949,https://doi.org/10.1016/j.jjimei.2023.100165,Variety (cybernetics); Ambiguity; Process (computing); Automation; Order (exchange),article,True,"Artificial intelligence is similar to human intelligence, and robots in organisations always perform human tasks. However, AI encounters a variety of biases during its operational process in the online economy. The coded algorithms helps in decision-making in firms with a variety of biases and ambiguity. The study is qualitative in nature and asserts that AI biases and vulnerabilities experienced by people across industries lead to gender biases and racial discrimination. Furthermore, the study describes the different types of biases and emphasises the importance of responsible AI in firms in order to reduce the risk from AI. The implications discuss how policymakers, managers, and employees must understand biases to improve corporate fairness and societal well-being. Future research can be carryout on consumer bias, bias in job automation and bias in societal data."
Adaptive bias-variance trade-off in advantage estimator for actor–critic algorithms,2023,Yurou Chen; Fengyi Zhang; Zhiyong Liu,Neural Networks,4,W4388049652,10.1016/j.neunet.2023.10.023,https://openalex.org/W4388049652,,Estimator; Variance (accounting); Bootstrapping (finance); Computer science; Algorithm,article,False,
An overview of the effects of algorithm use on judgmental biases affecting forecasting,2024,Alvaro Chacon; Esther Kaufmann,International Journal of Forecasting,3,W4404592879,10.1016/j.ijforecast.2024.09.007,https://openalex.org/W4404592879,https://doi.org/10.1016/j.ijforecast.2024.09.007,Econometrics; Computer science; Algorithm; Economics,article,True,
A Biased-Randomized Discrete Event Algorithm to Improve the Productivity of Automated Storage and Retrieval Systems in the Steel Industry,2024,Mattia Neroni; Massimo Bertolini; Àngel A. Juan,Algorithms,3,W4391023723,10.3390/a17010046,https://openalex.org/W4391023723,https://www.mdpi.com/1999-4893/17/1/46/pdf?version=1705678616,Computer science; Simulated annealing; Heuristics; RSS; Job shop scheduling,article,True,"In automated storage and retrieval systems (AS/RSs), the utilization of intelligent algorithms can reduce the makespan required to complete a series of input/output operations. This paper introduces a simulation optimization algorithm designed to minimize the makespan in a realistic AS/RS commonly found in the steel sector. This system includes weight and quality constraints for the selected items. Our hybrid approach combines discrete event simulation with biased-randomized heuristics. This combination enables us to efficiently address the complex time dependencies inherent in such dynamic scenarios. Simultaneously, it allows for intelligent decision making, resulting in feasible and high-quality solutions within seconds. A series of computational experiments illustrates the potential of our approach, which surpasses an alternative method based on traditional simulated annealing."
Microsoft Copilot and Anthropic Claude AI in education and library service,2024,Adebowale Jeremy Adetayo; Mariam Oyinda Aborisade; Basheer Abiodun Sanni,Library Hi Tech News,24,W4390912975,10.1108/lhtn-01-2024-0002,https://openalex.org/W4390912975,,Computer science; Transparency (behavior); Context (archaeology); Special education; Corporate governance,article,False,"Purpose This study aims to explore the collaborative potential of Microsoft Copilot and Anthropic Claude AI as an assistive technology in education and library services. The research delves into technical architectures and various use cases for both tools, proposing integration strategies within educational and library environments. The paper also addresses challenges such as algorithmic bias, hallucination and data rights. Design/methodology/approach The study used a literature review approach combined with the proposal of integration strategies across education and library settings. Findings The collaborative framework between Copilot and Claude AI offers a comprehensive solution for transforming education and library services. The study identifies the seamless combination of real-time internet access, information retrieval and advanced comprehension features as key findings. In addition, challenges such as algorithmic bias and data rights are addressed, emphasizing the need for responsible AI governance, transparency and continuous improvement. Originality/value Contribute to the field by exploring the unique collaborative framework of Copilot and Claude AI in a specific context, emphasizing responsible AI governance and addressing existing gaps."
Equity and Bias in AI Educational Tools: A Critical Examination of Algorithmic Decision-Making in Classrooms,2025,Shumaila Farheen; Azhar Abbas Cheema; Rooh Ullah; Marium Minhas Bandeali,The critical review of social sciences studies,1,W4412011869,10.59075/zqmnpa62,https://openalex.org/W4412011869,,Equity (law); Computer science; Mathematics education; Actuarial science; Psychology,article,False,"The integration of artificial intelligence (AI) into education raises significant concerns about algorithmic bias and its impact on equity. This study examines the linkage between teachers' perceptions of AI bias and their concerns for fairness in the classroom while also assessing the influence of teaching level and AI familiarity. Through a cross-sectional survey of 270 educators in Punjab, Pakistan, we utilized quantitative methods, including correlation and regression analysis, to test our hypotheses. Our findings reveal a strong positive association between perceived algorithmic bias and threats to educational equity. We found that school teachers were significantly more aware of AI bias than university faculty. Furthermore, greater teacher familiarity with AI tools correlated with a more nuanced understanding of their potential biases. These results underscore the urgent need to address the ""black box"" nature of educational AI. The study provides empirical evidence that, without careful regulation and comprehensive teacher training, AI-driven tools risk perpetuating and even exacerbating existing educational inequalities, aligning with global concerns about the ethical deployment of AI in sensitive sectors."
Revolutionizing Healthcare: How Machine Learning is Transforming Patient Diagnoses - a Comprehensive Review of AI's Impact on Medical Diagnosis,2023,Ahmad Yousaf Gill; Ayesha Saeed; Saad Rasool; Ali Husnain; Hafiz Khawar Hussain,Journal Of World Science,34,W4388001743,10.58344/jws.v2i10.449,https://openalex.org/W4388001743,https://jws.rivierapublishing.id/index.php/jws/article/download/449/1018,Health care; Transformative learning; Compassion; Autonomy; Engineering ethics,review,True,"The integration of machine learning into healthcare heralds a new era where the convergence of technology and human compassion reshapes the very essence of healing. This monumental shift transcends mere technological advancement; it represents a profound evolution in patient care. By unraveling intricate patterns within medical data, machine learning empowers healthcare professionals with early disease detection and precise risk assessment, augmenting human intuition rather than replacing it. This synergy between AI-driven insights and human expertise has led to remarkable achievements, from redefining radiological interpretations to foreseeing infectious disease outbreaks, painting a future where healthcare is not only precise but profoundly patient-centered. Yet, amidst these groundbreaking advancements, ethical considerations stand as pillars guiding responsible innovation. Upholding patient autonomy, ensuring data privacy, and addressing algorithmic bias are essential to maintain trust and integrity. As we navigate this transformative path, the promise of a healthcare landscape where healing becomes a symphony of technology and tradition becomes evident. It is a future where the well-being and hopes of millions are at the core, promising a brighter, more compassionate tomorrow for healthcare, where every diagnosis, treatment, and act of care resonates with the harmony of human expertise and technological marvels."
When Mitigating Bias is Unfair: A Comprehensive Study on the Impact of Bias Mitigation Algorithms,2023,Nataša Krčo; Thibault Laugel; Jean‐Michel Loubes; Marcin Detyniecki,arXiv (Cornell University),2,W4321015313,10.48550/arxiv.2302.07185,https://openalex.org/W4321015313,https://arxiv.org/abs/2302.07185,Debiasing; Odds; Arbitrariness; Computer science; Selection bias,preprint,True,"Most works on the fairness of machine learning systems focus on the blind optimization of common fairness metrics, such as Demographic Parity and Equalized Odds. In this paper, we conduct a comparative study of several bias mitigation approaches to investigate their behaviors at a fine grain, the prediction level. Our objective is to characterize the differences between fair models obtained with different approaches. With comparable performances in fairness and accuracy, are the different bias mitigation approaches impacting a similar number of individuals? Do they mitigate bias in a similar way? Do they affect the same individuals when debiasing a model? Our findings show that bias mitigation approaches differ a lot in their strategies, both in the number of impacted individuals and the populations targeted. More surprisingly, we show these results even apply for several runs of the same mitigation approach. These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process."
Early years of biased random-key genetic algorithms: a systematic review,2024,Mariana A. Londe; Luciana Fontes Pessôa; Carlos E. Andrade; Maurício G. C. Resende,Journal of Global Optimization,3,W4404062777,10.1007/s10898-024-01446-5,https://openalex.org/W4404062777,,Mathematics; Key (lock); Algorithm; Computer science; Computer security,review,False,
How Public Officials Perceive Algorithmic Discretion: A Study of Status Quo Bias in Policing,2025,Muhammad Afzal; Panos Panagiotopoulos,Public Administration Review,2,W4409372777,10.1111/puar.13957,https://openalex.org/W4409372777,https://doi.org/10.1111/puar.13957,Status quo; Discretion; Status quo bias; Political science; Public administration,article,True,"ABSTRACT Algorithms are disrupting established decision‐making practices in public administration. A key area of interest lies in algorithmic discretion or how public officials use algorithms to exercise discretion. The article develops a framework to explain algorithmic discretion by drawing on status quo bias theory and bureaucratic discretion. A study with police officers in the UK shows that—while officers still value their discretion—it is resistance via the aspects of status quo bias that accounts for a more substantial explanation. Transition costs, loss aversion, and performance uncertainty determine resistance and, in turn, reluctance to delegate discretion to algorithms. The study contributes to public administration research that demonstrates the influence of cognitive biases in the increasing use of algorithms in areas like policing. The article concludes with recommendations for embedding algorithmic discretion into the professional development of public officials to mitigate sources of status quo bias."
Are Emily and Greg Still More Employable than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of ChatGPT,2023,Akshaj Kumar Veldanda; Fabian Grob; Shailja Thakur; Hammond Pearce; Benjamin Tan; Ramesh Karri; Siddharth Garg,arXiv (Cornell University),3,W4387559711,10.48550/arxiv.2310.05135,https://openalex.org/W4387559711,https://arxiv.org/abs/2310.05135,Race (biology); Matching (statistics); Set (abstract data type); Politics; Psychology,preprint,True,"Large Language Models (LLMs) such as GPT-3.5, Bard, and Claude exhibit applicability across numerous tasks. One domain of interest is their use in algorithmic hiring, specifically in matching resumes with job categories. Yet, this introduces issues of bias on protected attributes like gender, race and maternity status. The seminal work of Bertrand & Mullainathan (2003) set the gold-standard for identifying hiring bias via field experiments where the response rate for identical resumes that differ only in protected attributes, e.g., racially suggestive names such as Emily or Lakisha, is compared. We replicate this experiment on state-of-art LLMs (GPT-3.5, Bard, Claude and Llama) to evaluate bias (or lack thereof) on gender, race, maternity status, pregnancy status, and political affiliation. We evaluate LLMs on two tasks: (1) matching resumes to job categories; and (2) summarizing resumes with employment relevant information. Overall, LLMs are robust across race and gender. They differ in their performance on pregnancy status and political affiliation. We use contrastive input decoding on open-source LLMs to uncover potential sources of bias."
Fairness and Biases in AI Algorithms and Interfaces,2024,Seul Lee,Proceedings of the ALISE Annual Conference,1,W4403529711,10.21900/j.alise.2024.1701,https://openalex.org/W4403529711,https://iopn.library.illinois.edu/journals/aliseacp/article/download/1701/1442,Computer science; Algorithm,article,True,"This workshop will explore various biases embedded in AI algorithms and interfaces across online platforms, including digital archives and libraries, social media, search engines, and AI-powered services like ChatGPT. Participants will critically engage with issues surrounding information credibility, transparency in content selection and appraisal, the accuracy and integrity in representation, and the intricate dynamics of information authority, format, and editorial oversight. The workshop aims to equip participants with practical strategies to identify, assess, and address biases inherent in various media, empowering them to navigate and evaluate online information across platforms with informed judgment. The workshop will begin with a 10-minute introduction, welcoming participants and outlining the goals of the session. The workshop will kick off with an interactive introductory lecture focused on various potential biases in AI algorithms. The first part, which will last about 10 to 20 minutes, will address recognizing biases in social media. Participants will analyze these biases by identifying fake news in provided articles and reflecting on their own everyday information practices on social media. After this informative segment, participants will engage in hands-on workshops aimed at applying their knowledge to recognize the types of biases they may encounter during their online information-seeking activities. They will participate in hands-on activities lasting 20 to 30 minutes, using different online tools to help identify such biases. Following the interactive hands-on workshops, participants will have the opportunity to share their insights and findings regarding the biases they discovered. The second section, which will explore biases in digital archives, digital libraries, and search engines through case studies, will last approximately one hour. It will include a 40-minute lecture that covers different types of biases in digital archives, libraries, and search engines, illustrated through case studies. After the lecture, participants will reflect on the biases presented for 10 minutes in a group discussion, sharing their thoughts and insights. Following participants' reflections on these biases, the last part of the session will focus on potential biases in ChatGPT. It will begin with a 30-minute lecture on how ChatGPT generates responses, emphasizing potential biases that could arise at each layer or step of the process. By examining a step-by-step analysis of how ChatGPT generates its answers, participants will deepen their understanding of the biases and errors inherent in the information generated by ChatGPT and its complex operational dynamics. Following this lecture, participants will engage in discussion for 20 minutes, working in small groups to identify biases and discussing necessary educational changes for themselves and their students. The workshop will conclude with a final reflection, where participants will share insights gained throughout the session and discuss how to apply their learning moving forward. The workshop will be wrapped up in a brief 10-minute summary of key points, encouraging participants to implement the strategies discussed in their own information evaluation practices. This workshop is designed for beginner-level students and researchers, as well as those interested in incorporating critical digital literacy principles into their teaching. It is open to people with diverse backgrounds and levels of expertise. No prior knowledge is required, allowing participants to embark on their learning journey with a fresh perspective and receptive attitude. The goal of this workshop is to enhance critical thinking skills related to the credibility of information while unpacking the complexities of information curation, representation, and editorial control in today's intricate digital landscape. Participants will gain a more nuanced understanding of how information is selected, represented, and potentially biased across various platforms, including social media, digital archives and libraries, search engines, and AI-powered services like ChatGPT. Through this workshop, participants will learn to effectively navigate the complexities of their information practices and develop the ability to critically assess the credibility of the online information they consume. Through engaging lectures and hands-on activities, participants will learn to identify and evaluate the inherent biases present in various media channels. They will explore practical strategies for assessing the credibility of information, recognizing potential mis/disinformation, and critically analyzing the content they encounter in their everyday information-seeking activities. By the end of the workshop, participants will be equipped with the necessary tools to navigate the digital information landscape safely and effectively, empowering them to make informed decisions not only for themselves but also in guiding others, such as children, students, or peers, in developing their own critical digital media literacy skills. This comprehensive approach aims to enhance participants’ ability to engage thoughtfully with information, ultimately fostering a more informed and critically aware community."
Bias-based Denoising Causal Recommendation Algorithm,2023,Yang Xu,Frontiers in Computing and Intelligent Systems,1,W4366783237,10.54097/fcis.v3i2.6909,https://openalex.org/W4366783237,https://drpress.org/ojs/index.php/fcis/article/download/6909/6698,Computer science; Causal inference; Collaborative filtering; Inference; Noise (video),article,True,"Traditional recommendation algorithms, such as collaborative filtering, make recommendations by learning the relevant relationships between users and items. However, considering only the relationships without considering the underlying causal mechanisms would be unfair, uninterpretable, and would lead to bias. In this paper, we propose bias-based denoising causal recommendation algorithm (BDCR) . First, the method dynamically transforms the explicit user-item feedback into implicit feedback with an embedded representation. Then, a truncation function based on causal inference is constructed to remove false positive noise. In addition, traditional recommendations and denoised causal recommendations are aggregated to obtain predictive scores. Finally, experimental results on two real datasets show that the BDCR algorithm outperforms the classical algorithm in terms of recall and NDCG metrics."
Definition drives design: Disability models and mechanisms of bias in AI technologies,2023,Denis Newman-Griffis; Jessica Sage Rauchberg; Rahaf Alharbi; Louise Hickman; Harry Hochheiser,First Monday,28,W4319845028,10.5210/fm.v28i1.12903,https://openalex.org/W4319845028,https://firstmonday.org/ojs/index.php/fm/article/download/12903/10797,Computer science; Transparency (behavior); Data science; Implementation; Software deployment,article,True,"The increasing deployment of artificial intelligence (AI) tools to inform decision-making across diverse areas including healthcare, employment, social benefits, and government policy, presents a serious risk for disabled people, who have been shown to face bias in AI implementations. While there has been significant work on analysing and mitigating algorithmic bias, the broader mechanisms of how bias emerges in AI applications are not well understood, hampering efforts to address bias where it begins. In this article, we illustrate how bias in AI-assisted decision-making can arise from a range of specific design decisions, each of which may seem self-contained and non-biasing when considered separately. These design decisions include basic problem formulation, the data chosen for analysis, the use the AI technology is put to, and operational design elements in addition to the core algorithmic design. We draw on three historical models of disability common to different decision-making settings to demonstrate how differences in the definition of disability can lead to highly distinct decisions on each of these aspects of design, leading in turn to AI technologies with a variety of biases and downstream effects. We further show that the potential harms arising from inappropriate definitions of disability in fundamental design stages are further amplified by a lack of transparency and disabled participation throughout the AI design process. Our analysis provides a framework for critically examining AI technologies in decision-making contexts and guiding the development of a design praxis for disability-related AI analytics. We put forth this article to provide key questions to facilitate disability-led design and participatory development to produce more fair and equitable AI technologies in disability-related contexts."
Research on the Bias Sampling RRT Algorithm for Supermarket Chain Distribution Routes under the O2O Model,2023,Zheng Shi,Frontiers in Computing and Intelligent Systems,2,W4388399257,10.54097/fcis.v5i2.12444,https://openalex.org/W4388399257,https://drpress.org/ojs/index.php/fcis/article/download/12444/12113,Sampling (signal processing); Terrain; Path (computing); Computer science; Transformation (genetics),article,True,"The purpose of this article is to propose the Bias Sampling RRT algorithm and use it as an optimization algorithm for supermarket chain distribution routes under the O2O model. As a retail store in the transformation and upgrading of chain stores, the actual terrain factors in distribution directly affect the timely delivery of goods from online to offline. The Bias Sampling RRT algorithm, as a path search method for time window vehicle routing problems, can find the optimal path that meets time window constraints. The applicability and effectiveness of the Bias Sampling RRT algorithm have been demonstrated through map simulation calculations. The simulation results show that compared with the RRT method, the Bias Sampling RRT algorithm has a shorter distribution path and shorter distribution time. This method is very suitable for the distribution activities of chain supermarkets or single store retail enterprises in the complex transformation and upgrading of actual terrain."
Navigating the Ethical Challenges of Artificial Intelligence in Higher Education: An Analysis of Seven Global AI Ethics Policies,2023,Zouhaier Slimi; Beatriz Villarejo-Carballido,TEM Journal,98,W4381570295,10.18421/tem122-02,https://openalex.org/W4381570295,https://www.temjournal.com/content/122/TEMJournalMay2023_590_602.pdf,Accountability; Transparency (behavior); Software deployment; Openness to experience; Documentation,article,True,"AI use in higher education raises ethical concerns that must be addressed. Biased algorithms pose a significant threat, especially if used in admission or grading processes, as they could have devastating effects on students. Another issue is the displacement of human educators by AI systems, and there are concerns about transparency and accountability as AI becomes more integrated into decision-making processes. This paper examined three AI objectives related higher education: biased algorithms, AI and decision-making, and human displacement. Discourse analysis of seven AI ethics policies was conducted, including those from UNESCO, China, the European Commission, Google, MIT, Sanford HAI, and Carnegie Mellon. The findings indicate that stakeholders must work together to address these challenges and ensure responsible AI deployment in higher education while maximizing its benefits. Fair use and protecting individuals, especially those with vulnerable characteristics, are crucial. Gender bias must be avoided in algorithm development, learning data sets, and AI decision-making. Data collection, labeling, and algorithm documentation must be of the highest quality to ensure traceability and openness. Universities must study the ethical, social, and policy implications of AI to ensure responsible development and deployment. The AI ethics policies stress responsible AI development and deployment, with a focus on transparency and accountability. Making AI systems more transparent and answerable may reduce the adverse effects of displacement. In conclusion, AI must be considered ethically in higher education, and stakeholders must ensure that AI is used responsibly, fairly, and in a way that maximizes its benefits while minimizing its risks."
REVIEWING THE ETHICAL IMPLICATIONS OF AI IN DECISION MAKING PROCESSES,2024,Femi Osasona; Olukunle Oladipupo Amoo; Akoh Atadoga; Temitayo Oluwaseun Abrahams; Oluwatoyin Ajoke Farayola; Benjamin Samson Ayinla,International Journal of Management & Entrepreneurship Research,102,W4391898528,10.51594/ijmer.v6i2.773,https://openalex.org/W4391898528,https://fepbl.com/index.php/ijmer/article/download/773/967,Engineering ethics; Ethical decision; Management science; Political science; Psychology,article,True,"Artificial Intelligence (AI) has rapidly become an integral part of decision-making processes across various industries, revolutionizing the way choices are made. This Review delves into the ethical considerations associated with the use of AI in decision-making, exploring the implications of algorithms, automation, and machine learning. The incorporation of AI in decision-making introduces a myriad of ethical concerns that demand careful scrutiny. The opacity of algorithms raises questions about transparency, accountability, and bias. Decision-making processes driven by AI can be complex and difficult to interpret, leading to challenges in understanding how and why specific choices are made. As a result, ethical concerns emerge regarding the potential lack of transparency and accountability, especially when these decisions impact individuals or societal groups. Bias in AI algorithms poses a critical ethical challenge. Machine learning models learn from historical data, and if that data is biased, the AI system may perpetuate and even exacerbate existing biases. Addressing this challenge requires meticulous examination of training data, algorithmic design, and ongoing monitoring to ensure fairness and mitigate discrimination. The increased reliance on AI in decision-making processes also raises concerns about accountability and responsibility. When AI systems make decisions, determining who is ultimately responsible for those decisions becomes a complex ethical issue. Establishing a framework for accountability is crucial to ensure that individuals, organizations, and developers share responsibility for the outcomes of AI-driven decisions. Moreover, ethical considerations extend to the broader societal impact of AI in decision-making. Issues such as job displacement, economic inequality, and the potential concentration of power in the hands of a few require careful ethical examination. Striking a balance between technological advancement and social responsibility is paramount to ensuring that AI benefits society as a whole. In conclusion, this review highlights the ethical implications of integrating AI into decision-making processes. It underscores the need for transparency, fairness, and accountability to address concerns related to bias, responsibility, and the broader societal impact of AI-driven decisions. Ethical frameworks must evolve alongside technological advancements to foster a responsible and equitable integration of AI in decision-making processes.&#x0D; Keywords: Ethical, Implications, AI, Decision Making, Process."
A diffusion bias-compensated LMS algorithm for distributed estimation with ARMAX models,2023,Jiale Zeng; Wen Mi; Wei Xing Zheng,Digital Signal Processing,3,W4386227081,10.1016/j.dsp.2023.104202,https://openalex.org/W4386227081,,Colors of noise; Algorithm; Least mean squares filter; Noise (video); Stability (learning theory),article,False,
"Structural bias in metaheuristic algorithms: Insights, open problems, and future prospects",2024,Kanchan Rajwar; Kusum Deep,Swarm and Evolutionary Computation,3,W4405303904,10.1016/j.swevo.2024.101812,https://openalex.org/W4405303904,,Computer science; Metaheuristic; Algorithm,article,False,
Ethical Considerations of AI Implementation in the Library Era,2024,N. Rajkumar; C. Viji; A. Mohanraj; K. R. Senthilkumar; R. Jagajeevan; Judeson Antony Kovilpillai,Advances in library and information science (ALIS) book series,26,W4397003176,10.4018/979-8-3693-5593-0.ch007,https://openalex.org/W4397003176,,Engineering ethics; Library science; Computer science; Engineering,book-chapter,False,"As the mixture of artificial intelligence (AI) continues to permeate several sectors, ethical considerations have ended up a focus in ensuring responsible and sustainable AI deployment. This virtual library explores the multifaceted moral dimensions related to AI implementation. The gathering of scholarly articles and studies papers delves into key moral problems, spanning troubles which includes bias and fairness, transparency, responsibility, privacy, and societal impact. The number one section of the virtual library addresses the undertaking of algorithmic bias and fairness, reading how biases in AI systems can perpetuate societal inequalities. Various methods to mitigating bias and selling fairness in AI algorithms are explored, providing insights into the improvement of more equitable AI programs. Transparency and duty are the focal factors of the second one segment, emphasizing the need for clean conversation of AI decision-making techniques and mechanisms for holding AI systems answerable for their movements."
Bias and ethics of AI systems applied in auditing - A systematic review,2024,Wilberforce Murikah; Jeff Kimanga Nthenge; Faith Mueni Musyoka,Scientific African,43,W4399619293,10.1016/j.sciaf.2024.e02281,https://openalex.org/W4399619293,https://doi.org/10.1016/j.sciaf.2024.e02281,Accountability; Audit; Computer science; Due diligence; Transparency (behavior),review,True,"The integration of artificial intelligence into auditing shows great potential in enhancing automation and gaining insights from complex data. However, it also presents significant ethical challenges, including algorithmic biases, transparency, accountability, and fairness. This study aimed to investigate the sources of bias and risks posed by AI systems applied in auditing and the complex downstream interactions and effects they have. The study also explored the technical and ethical guardrails proposed and recommendations for translating principles into auditing practice. A systematic methodology was employed to acquire relevant studies across scientific databases. This involved a three-step process, including a targeted search query using Boolean operators and snowballing to yield 310 preliminary publications. A systematic review process was then conducted to identify 123 relevant articles focused on AI's implications for auditing, accounting, finance, or assurance contexts. Finally, screening and filtering on research quality distilled 83 high-quality publications from the year 2018 to 2023 spanning computer science, accounting, management science, and ethics disciplines. The analysis revealed five primary sources driving technical and human biases: data deficiencies, demographic homogeneity, spurious correlations, improper comparators, and cognitive biases. It also highlighted wider issues, such as trade-offs between efficiency and diligence, erosion of human skills and judgement, data dependence risks, and privacy violations from uncontrolled personal data exploitation. The study found promising remedies, including causal modeling to enable auditors to uncover subtle biases, representative algorithmic testing to evaluate fairness, periodic auditing of AI systems, human oversight alongside automation, and embedding ethical values like fairness and accountability into system design. The study concludes that auditors play a crucial role in assessing and ensuring AI's reliable and socially beneficial integration. It recommends governance, risk assessment before deployment, ongoing performance monitoring, and policies fostering trust and collaboration to responsibly translate principles into auditing practice."
Reducing the incidence of biased algorithmic decisions through feature importance transparency: an empirical study,2024,Sepideh Ebrahimi; Esraa Abdelhalim; Khaled Hassanein; Milena Head,European Journal of Information Systems,2,W4402130453,10.1080/0960085x.2024.2395531,https://openalex.org/W4402130453,https://www.tandfonline.com/doi/pdf/10.1080/0960085X.2024.2395531?needAccess=true,Transparency (behavior); Computer science; Strategic information system; Empirical research; Feature (linguistics),article,True,"As firms move towards data-driven decision-making using algorithmic systems, concerns are raised regarding the lack of transparency of these systems which could have ramifications related to users' trust and the potential for provoking discriminatory decisions. Although previous research has developed methods to improve algorithmic transparency, little empirical evidence exists regarding the extent of the effectiveness of these approaches. Drawing upon Rest's theory of ethical decision-making and the literature on algorithmic transparency and bias, we investigate the effectiveness of feature importance (FI), a common transparency-enhancing approach, which illustrates the nature and the weights of the features utilised by an algorithm. Through an online experiment employing a fictitious tool that provided recommendations for selecting employees for a promotion-related training programme, we find that FI is effective when biased recommendations include direct discrimination (i.e. when individuals are treated less favourably on protected grounds such as gender); but is of little assistance when discrimination is indirect (i.e. when a criterion or practice that is apparently neutral, disadvantages a group of individuals who are of a protected class). Additionally, we propose a new transparency approach, using aggregated demographic information, to accompany FI in indirect discrimination circumstances and report the results of testing its effects."
Beyond traditional interviews: Psychometric analysis of asynchronous video interviews for personality and interview performance evaluation using machine learning,2024,Antonis Koutsoumpis; Sina Ghassemi; Janneke K. Oostrom; Djurre Holtrop; Ward van Breda; Tianyi Zhang; Reinout E. de Vries,Computers in Human Behavior,26,W4390479873,10.1016/j.chb.2023.108128,https://openalex.org/W4390479873,https://doi.org/10.1016/j.chb.2023.108128,Psychology; Asynchronous communication; Applied psychology; Personality; Social psychology,article,True,"With the advent of new technology, traditional job interviews have been supplemented by asynchronous video interviews (AVIs). However, research on psychometric properties of AVIs is limited. In this study, 710 participants completed a mock AVI responding to eight personality questions (Extraversion, Conscientiousness). We collected self- and observer reports of personality, interview performance ratings, attractiveness, and AVI meta-information (e.g., professional attire, audio quality). Then, we automatically extracted the words, facial expressions, and voice characteristics from the videos and trained machine learning models to predict the personality traits and interview performance. Our algorithm explained substantially more variance in observer reports of Extraversion and Conscientiousness (average R2 = 0.32) and interview performance (R2 = 0.44), than self-reported Extraversion and Conscientiousness (average R2 = 0.12). Consistent with Trait Activation Theory, the explained variance in personality traits increased when participants responded to trait-relevant, compared to trait-irrelevant, questions. The test-retest reliability of our algorithm was somewhat stable over a time period of seven months, but lower than desired reliability standards in personnel selection. We examined potential sources of bias, including age, gender, and attractiveness, and found some instances of algorithmic bias (e.g., gender differences were often amplified in favor of women)."
Artificial intelligence in environmental health and public safety: A comprehensive review of USA strategies,2023,Adedayo Adefemi; Emmanuel Adikwu Ukpoju; Oladipo Olugbenga Adekoya; Ayodeji Abatan; Abimbola Oluwatoyin Adegbite,World Journal of Advanced Research and Reviews,41,W4390155689,10.30574/wjarr.2023.20.3.2591,https://openalex.org/W4390155689,https://wjarr.com/sites/default/files/WJARR-2023-2591.pdf,Public health; Resilience (materials science); Emergency management; Environmental monitoring; Big data,review,True,"This study explores the transformative role of artificial intelligence (AI) in environmental health and public safety within the USA, focusing on pollution monitoring, emergency response, and sustainable practices for public. With the growing challenges posed by climate change, pollution, and emerging public health threats, the integration of Artificial Intelligence (AI) in environmental health and public safety strategies has become imperative. This comprehensive review explores the diverse array of AI applications implemented in the United States to address environmental issues and enhance public safety measures. The paper analyzes the multifaceted role of AI across various domains, including air and water quality monitoring, disease surveillance, disaster response, and infrastructure resilience. The advancements in AI technologies that have revolutionized data collection, analysis, and prediction in environmental health are examined. Machine learning algorithms, sensor networks, and satellite imagery are examined as tools for real-time monitoring and early detection of environmental hazards. Additionally, the paper investigates the integration of AI in public health surveillance systems, showcasing how predictive analytics and data-driven models contribute to the identification and containment of infectious diseases. Furthermore, the study sheds light on the incorporation of AI in disaster management, emphasizing the role of predictive modeling and risk assessment in optimizing emergency response strategies. The implementation of smart city technologies and intelligent infrastructure systems is discussed, highlighting how AI contributes to enhancing public safety and minimizing the impact of natural disasters. The review also critically evaluates the ethical, legal, and privacy considerations associated with the widespread adoption of AI in environmental health and public safety initiatives. It addresses concerns related to data security, algorithmic biases, and the need for transparent and accountable governance frameworks. Through an in-depth analysis of case studies, policies, and initiatives, this review provides insights into the successes and challenges of AI implementation in the USA. It concludes with recommendations for future research directions and policy considerations to ensure the responsible and effective integration of AI technologies in safeguarding environmental health and public safety. The findings presented in this review contribute to the broader discourse on leveraging AI for sustainable and resilient communities in the face of evolving environmental and public health challenges."
AI revolution in healthcare and medicine and the (re-)emergence of inequalities and disadvantages for ageing population,2023,Justyna Stypińska; Annette Franke,Frontiers in Sociology,27,W4317717912,10.3389/fsoc.2022.1038854,https://openalex.org/W4317717912,https://www.frontiersin.org/articles/10.3389/fsoc.2022.1038854/pdf,Health care; Harm; Inequality; Population; Population ageing,article,True,"AI systems in medicine and healthcare are being extensively explored in prevention, diagnosis, novel drug designs and after-care. The application of AI technology in healthcare systems promises impressive outcomes such as equalising healthcare, reducing mortality rate and human error, reducing medical costs, as well as reducing reliance on social services. In the light of the WHO ""Decade of Healthy Ageing"", AI applications are designed as digital innovations to support the quality of life for older persons. However, the emergence of evidence of different types of algorithmic bias in AI applications, ageism in the use of digital devices and platforms, as well as age bias in digital data suggests that the use of AI might have discriminatory effects on older population or even cause harm. This paper addresses the issue of age biases and age discrimination in AI applications in medicine and healthcare systems and try to identify main challenges in this area. It will reflect on the potential of AI applications to amplify the already existing health inequalities by discussing two levels where potential negative impact of AI on age inequalities might be observed. Firstly, we will address the technical level of age bias in algorithms and digital datasets (especially health data). Secondly, we will discuss the potential disparate outcomes of automatic decision-making systems (ADMs) used in healthcare on the older population. These examples will demonstrate, although only partially, how AI systems may create new structures of age inequalities and novel dimensions of exclusion in healthcare and medicine."
"Artificial intelligence in nursing: Current trends, possibilities and pitfalls",2024,Sirwan Khalid Ahmed,Journal of Medicine Surgery and Public Health,51,W4391848861,10.1016/j.glmedi.2024.100072,https://openalex.org/W4391848861,https://doi.org/10.1016/j.glmedi.2024.100072,Transformative learning; Transparency (behavior); Health care; Workflow; Accountability,article,True,"This paper explores the integration of artificial intelligence (AI) in nursing and its implications for healthcare research and academic writing. The use of AI in healthcare has become increasingly prevalent across various industries and holds great promise for optimizing clinical workflows, enhancing diagnostic accuracy, and improving patient engagement in nursing. Moreover, AI has the potential to expedite research cycles and foster collaboration in academic writing, thereby making significant contributions to the field. Nevertheless, there are challenges associated with this paradigm shift, such as concerns about the loss of the human touch in patient care, ethical dilemmas concerning algorithmic bias and data privacy, and the risk of excessive reliance on AI systems. Addressing these challenges requires a balanced approach that places patient-centered care at the forefront and upholds ethical standards. To achieve this, nurses and researchers must actively participate in the design, implementation, and regulation of AI technologies, ensuring that they align with clinical expertise and patient-centered values. Furthermore, the establishment of transparent guidelines and regulations is essential to govern the responsible use of AI. Additionally, training programs should equip professionals with the necessary skills to effectively collaborate with AI systems. By fostering collaboration, transparency, and accountability, the complexities of integrating AI can be effectively managed, thereby unlocking its transformative potential to revolutionize patient care and advance knowledge discovery in the field of nursing and healthcare research."
Effective Strategies for Mitigating Bias in Hiring Algorithms: A Comparative Analysis,2023,Yusuf Jazakallah,Journal of Artificial Intelligence Machine Learning and Data Science,1,W4388298463,10.51219/jaimld/yusuf-jazakallah/16,https://openalex.org/W4388298463,https://doi.org/10.51219/jaimld/yusuf-jazakallah/16,Algorithm; Computer science,article,True,"Bias in hiring algorithms is a critical issue that has been widely recognized in recent years.As more companies rely on automated candidate selection processes, it is essential to develop fair and equitable recruitment practices that ensure equal opportunities for all candidates.The objective of this research paper is to propose a comprehensive framework for mitigating bias in hiring algorithms.By utilizing a combination of machine learning techniques, statistical analysis, and ethical considerations, the study aims to identify, measure, and mitigate both overt and subtle forms of bias present in these algorithms.This paper's findings underscore the significance of employing de-biasing strategies to ensure diversity and inclusion in the workplace.In this introduction, we will discuss the critical issue of bias mitigation in hiring algorithms, the importance of fair and equitable recruitment practices, and the objective of the study.We will also provide an overview of the research methodology, the measurement of bias, and the proposed mitigation strategies.Finally, we will summarize the key findings and the proposed framework for reducing bias in hiring algorithms."
Mass media impact on opinion evolution in biased digital environments: a bounded confidence model,2023,Valentina Pansanella; Alina Sîrbu; János Kertész; Giulio Rossetti,Scientific Reports,13,W4386457915,10.1038/s41598-023-39725-y,https://openalex.org/W4386457915,https://www.nature.com/articles/s41598-023-39725-y.pdf,Mainstream; Computer science; Social media; Population; Radicalization,article,True,"People increasingly shape their opinions by accessing and discussing content shared on social networking websites. These platforms contain a mixture of other users' shared opinions and content from mainstream media sources. While online social networks have fostered information access and diffusion, they also represent optimal environments for the proliferation of polluted information and contents, which are argued to be among the co-causes of polarization/radicalization phenomena. Moreover, recommendation algorithms - intended to enhance platform usage - likely augment such phenomena, generating the so-called Algorithmic Bias. In this work, we study the effects of the combination of social influence and mass media influence on the dynamics of opinion evolution in a biased online environment, using a recent bounded confidence opinion dynamics model with algorithmic bias as a baseline and adding the possibility to interact with one or more media outlets, modeled as stubborn agents. We analyzed four different media landscapes and found that an open-minded population is more easily manipulated by external propaganda - moderate or extremist - while remaining undecided in a more balanced information environment. By reinforcing users' biases, recommender systems appear to help avoid the complete manipulation of the population by external propaganda."
Ethical Considerations and Challenges in the Integration of Artificial Intelligence in Education: A Systematic Review,2024,Muhammad Tahir Khan Farooqi; Ishaq Amanat; Sher Muhammad Awan,Journal of excellence in management sciences.,21,W4402841691,10.69565/jems.v3i4.314,https://openalex.org/W4402841691,,Engineering ethics; Psychology; Management science; Engineering,review,False,"This systematic review examines those challenges in light of data privacy, algorithmic bias, ethical implications, technological hurdles, and acceptance of AI by educators and students. First, data privacy should be a primary concern, as AI systems require extensive data, bringing up the potential for breach and misuse. Secondly, there must be a robust mechanism concerning data protection and against the application of GDPR. Another critical point is algorithm bias: biased training data sets may lead to discriminative decisions that will increase inequalities in education. It talks about AI's impact on teachers and classroom dynamics because the takeover of responsibilities may lower the intensity of necessary human contact. From a technical perspective, there is so much infrastructure and expertise required that too many educational institutions lack, especially in developing countries. In addition, educators themselves may feel that the change resists and fears job loss and therefore acts as a deterrent to AI integration. The review underscores the imperative for extensive training of teachers to support enabling the integration of AI. It now demands a collaborative effort on the part of all stakeholders to maximize the gains and reduce the drawbacks of AI in educational aspects. Continuous research in, policy-making for, and ethical guidelines on AI are required to benefit all aspects of education equitably and effectively."
Modulator Bias Control Based on Bi-PID Algorithm for Optical Time and Frequency Transmission,2023,Zhuoze Zhao; Zhaohui Wang; Hao Gao; Jiameng Dong; Jiahui Cheng; Jie Zhang; Ziyang Chen; Song Yu; Bin Luo; Hong Guo,IEEE Photonics Technology Letters,2,W4388854561,10.1109/lpt.2023.3335376,https://openalex.org/W4388854561,,Algorithm; Stability (learning theory); PID controller; Dither; Computer science,article,False,"To resist the bias drift of the Mach–Zehnder modulator (MZM), we proposed and demonstrated a dither-free bias control method based on the bidirectional proportion integration differentiation (Bi-PID) algorithm. We theoretically analyzed the characteristics of time and frequency signals and made the bias point stable at the null point and quadrature point through optical power feedback and harmonic component feedback, respectively. 726.6 km transmission experiments were conducted to measure the effectiveness of the proposed modulator bias control (MBC) method. The results demonstrated that the stability of the time-sync system with the proposed MBC was better than that of the traditional method in the long-term measurement, whose time deviation (TDEV) was 2.8×10 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-12</sup> @10 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">4</sup> s. The frequency stability characterized by Allan deviation (ADEV), reaching 3.1×10 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-17</sup> @10 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">4</sup> s, outperformed traditional methods as well."
A Simple Bias Reduction Algorithm for RNA Sequencing Datasets,2023,Christopher Thron; Hannah E. Bergom; Ella Boytim; Mienie Roberts; Justin H. Hwang; Farhad Jafari,bioRxiv (Cold Spring Harbor Laboratory),2,W4388141697,10.1101/2023.10.31.564992,https://openalex.org/W4388141697,https://www.biorxiv.org/content/biorxiv/early/2023/11/01/2023.10.31.564992.full.pdf,Computational biology; Gene; Biology; RNA; Computer science,preprint,True,"Abstract RNA sequencing (RNA-seq) is the conventional genome-scale approach used to capture the expression levels of all detectable genes in a biological sample. This is now regularly used in the clinical diagnostic space for cancer patients. While the information gained is intended to impact treatment decisions, numerous technical and quality issues remain. This includes inaccuracies in the dissemination of gene-gene relationships. For such reasons, clinical decisions are still mostly driven by DNA biomarkers, such as gene mutations or fusions. In this study, we aimed to correct for systemic bias based on RNA-sequencing platforms in order to improve our understanding of the gene-gene relationships. To do so, we examined standard pre-processed RNA-seq datasets obtained from three studies conducted by two consortium efforts including The Cancer Genome Atlas (TCGA) and Stand Up 2 Cancer (SU2C). We particularly examined the TCGA Bladder Cancer (n = 408) and Prostate Cancer (n = 498) studies as well as the SU2C Prostate Cancer study (n = 208). Using various statistical tests, we detected expression-level dependent, per-sample biases in all datasets. Using simulations, we show that these biases corrupt the results of t -tests designed to identify expression level differences between subpopulations. Importantly, these biases introduce large errors into estimates of gene-gene correlations. To mitigate these biases, we introduce Local Leveling as a novel mathematical approach that transforms count level data and corrects these observed biases. Local Leveling specifically corrects for the bias due to the inherent differential detection of transcripts that is driven by differential expression levels. Based on standard forms of count data (Raw counts, transcripts per million, fragments per kilobase of exon per million), we demonstrate that local leveling effectively removes the observed per-sample biases, and improves the accuracy in simulated statistical tests. Importantly, this led to systemic changes of gene-gene relationships when examining the correlation of key oncogenes, such as the Androgen Receptor, with all other detectable genes. Altogether, Local Leveling improves our capacity towards understanding gene-gene relationships, which may lead to novel ways to utilize the information derived from clinical tests."
Analytical quaternion-based bias estimation algorithm for fast and accurate stationary gyro-compassing,2024,Hamed Mohammadkarimi; Saadat Pour Mozafari; Mohammad Alizadeh,Scientific Reports,2,W4400463011,10.1038/s41598-024-66282-9,https://openalex.org/W4400463011,https://www.nature.com/articles/s41598-024-66282-9.pdf,Quaternion; Azimuth; Kalman filter; Computer science; Inertial navigation system,article,True,"Abstract This work introduces a novel approach to Strapdown Inertial Navigation System (SINS) alignment, distinct from recursive methods like Kalman filtering. The proposed methodology expedites bias error calculations by utilizing quaternion-based analytical relationships, which bypasses the slow convergence behavior associated with recursive algorithms, particularly in azimuth angle error estimation. In addition, the proposed approach demonstrates comparable accuracy to traditional fine alignment methods. Simulations and experiments validate that in contrast to the 10-min time requirement of traditional fine alignment methods (for azimuth angle estimation in stationary conditions), the proposed approach achieves the same accuracy within 20 s. However, limitations exist as the algorithm is applicable only in stationary conditions, and necessitating a high-grade IMU capable of measuring the earth’s rotation rate."
Biased random-key genetic algorithms: A tutorial with applications,2024,Thiago F. Noronha; Celso C. Ribeiro,,2,W4401285165,10.1145/3665065.3665083,https://openalex.org/W4401285165,,Key (lock); Computer science; Algorithm; Theoretical computer science; Computer security,article,False,
Time–Frequency Signal Integrity Monitoring Algorithm Based on Temperature Compensation Frequency Bias Combination Model,2024,Yu Guo; Zongnan Li; Hang Gong; Jing Peng; Gang Ou,Remote Sensing,2,W4394963276,10.3390/rs16081453,https://openalex.org/W4394963276,https://www.mdpi.com/2072-4292/16/8/1453/pdf?version=1713537964,Computer science; Robustness (evolution); Signal integrity; Time–frequency analysis; Adaptability,article,True,"To ensure the long-term stable and uninterrupted service of satellite navigation systems, the robustness and reliability of time–frequency systems are crucial. Integrity monitoring is an effective method to enhance the robustness and reliability of time–frequency systems. Time–frequency signals are fundamental for integrity monitoring, with their time differences and frequency biases serving as essential indicators. These indicators are influenced by the inherent characteristics of the time–frequency signals, as well as the links and equipment they traverse. Meanwhile, existing research primarily focuses on only monitoring the integrity of the time–frequency signals’ output by the atomic clock group, neglecting the integrity monitoring of the time–frequency signals generated and distributed by the time–frequency signal generation and distribution subsystem. This paper introduces a time–frequency signal integrity monitoring algorithm based on the temperature compensation frequency bias combination model. By analyzing the characteristics of time difference measurements, constructing the temperature compensation frequency bias combination model, and extracting and monitoring noise and frequency bias features from the time difference measurements, the algorithm achieves comprehensive time–frequency signal integrity monitoring. Experimental results demonstrate that the algorithm can effectively detect, identify, and alert users to time–frequency signal faults. Additionally, the model and the integrity monitoring parameters developed in this paper exhibit high adaptability, making them directly applicable to the integrity monitoring of time–frequency signals across various links. Compared with traditional monitoring algorithms, the algorithm proposed in this paper greatly improves the effectiveness, adaptability, and real-time performance of time–frequency signal integrity monitoring."
Collaborative filtering algorithms are prone to mainstream-taste bias,2023,Pantelis P. Analytis; Philipp Hager,,1,W4386729270,10.1145/3604915.3608825,https://openalex.org/W4386729270,https://doi.org/10.1145/3604915.3608825,Collaborative filtering; Computer science; Recommender system; Mainstream; Variation (astronomy),article,False,"Collaborative filtering has been a dominant approach in the recommender systems community since the early 1990s. Collaborative filtering (and other) algorithms, however, have been predominantly evaluated by aggregating results across users or user groups. These performance averages hide large disparities: an algorithm may perform very well for some users (or groups) and poorly for others. We show that performance variation is large and systematic. In experiments on three large-scale datasets and using an array of collaborative filtering algorithms, we demonstrate large performance disparities across algorithms, datasets and metrics for different users. We then show that two key features that characterize users, their mean taste similarity and dispersion in taste similarity with other users, can systematically explain performance variation better than previously identified features. We use these two features to visualize algorithm performance for different users and we point out that this mapping can capture different categories of users that have been proposed before. Our results demonstrate an extensive mainstream-taste bias in collaborative filtering algorithms, which implies a fundamental fairness limitation that needs to be mitigated."
"ChatGPT's contributions to the evolution of neurosurgical practice and education: a systematic review of benefits, concerns and limitations",2023,Hakija Bečulić; Emir Begagić; Rasim Skomorac; Anes Mašović; Edin Selimović; Mirza Pojskić,Medicinski Glasnik,30,W4388615529,10.17392/1661-23,https://openalex.org/W4388615529,https://doi.org/10.17392/1661-23,Medicine; Neurosurgery; Systematic review; Management science; Clinical Practice,review,True,"Aim This study provides a comprehensive review of the current literature on the use of ChatGPT, a generative Artificial Intelligence (AI) tool, in neurosurgery. The study examines potential benefits and limitations of ChatGPT in neurosurgical practice and education. Methods The study involved a systematic review of the current literature on the use of AI in neurosurgery, with a focus on ChatGPT. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed to ensure a comprehensive and transparent review process. Thirteen studies met the inclusion criteria and were included in the final analysis. The data extracted from the included studies were analysed and synthesized to provide an overview of the current state of research on the use of ChatGPT in neurosurgery. Results The ChatGPT showed a potential to complement and enhance neurosurgical practice. However, there are risks and limitations associated with its use, including question format limitations, validation challenges, and algorithmic bias. The study highlights the importance of validating machine-generated content for accuracy and addressing ethical concerns associated with AI technologies. The study also identifies potential benefits of ChatGPT, such as providing personalized treatment plans, supporting surgical planning and navigation, and enhancing large data processing efficiency and accuracy. Conclusion The integration of AI technologies into neurosurgery should be approached with caution and careful consideration of ethical and validation issues. Continued research and development of AI tools in neurosurgery can help us further understand their potential benefits and limitations."
A Study of Symbiosis Bias in A/B Tests of Recommendation Algorithms,2023,David Holtz; Jennifer Brennan; Jean Pouget-Abadie,arXiv (Cornell University),1,W4386755597,10.48550/arxiv.2309.07107,https://openalex.org/W4386755597,https://arxiv.org/abs/2309.07107,Computer science; Randomized experiment; Algorithm; Value (mathematics); Cluster (spacecraft),preprint,True,"One assumption underlying the unbiasedness of global treatment effect estimates from randomized experiments is the stable unit treatment value assumption (SUTVA). Many experiments that compare the efficacy of different recommendation algorithms violate SUTVA, because each algorithm is trained on a pool of shared data, often coming from a mixture of recommendation algorithms in the experiment. We explore, through simulation, cluster randomized and data-diverted solutions to mitigating this bias, which we call ""symbiosis bias."""
Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm,2024,Weiran Wang; Zelin Wu; Diamantino Caseiro; Tsendsuren Munkhdalai; Khe Chai Sim; Pat Rondon; Golan Pundak; G. Hugh Song; Rohit Prabhavalkar; Zhong Meng; Ding Zhao; Tara N. Sainath; Yanzhang He; Pedro Moreno Mengibar,Interspeech 2022,1,W4402112258,10.21437/interspeech.2024-1349,https://openalex.org/W4402112258,https://arxiv.org/pdf/2310.00178,Computer science; Matching (statistics); Algorithm; Artificial intelligence; Mathematics,article,True,"We propose a GPU/TPU-friendly implementation for contextual biasing based on the Knuth-Morris-Pratt (KMP) pattern matching algorithm. Our algorithms simulate classical search-based biasing approaches which are often implemented in the weighted finite state transducer (WFST) framework, with careful considerations on memory footprint and efficiency by vectorization. We design scoring mechanisms such that, during beam search, a token extension receives a bonus if it extends matching into a biasing phrase, and receives a penalty to cancel previously received bonus otherwise. Our methods could be incorporated in either the shallow fusion or on-the-fly rescoring manner, to trade off accuracy with efficiency. On a large-scale voice search dataset, our method achieves significant word error rate (WER) reductions on biasing test sets without introducing additional model parameters, and yields further performance gain when combined with a model-based biasing method."
Exploring the potential of AI-driven optimization in enhancing network performance and efficiency,2024,Uchenna Joseph Umoga; Enoch Oluwademilade Sodiya; Ejike David Ugwuanyi; Boma Sonimitiem Jacks; Oluwaseun Augustine Lottu; Obinna Donald Daraojimba; Alexander Obaigbena,Magna Scientia Advanced Research and Reviews,37,W4392356715,10.30574/msarr.2024.10.1.0028,https://openalex.org/W4392356715,https://magnascientiapub.com/journals/msarr/sites/default/files/MSARR-2024-0028.pdf,Computer science,article,True,"The exponential growth of network complexity and data volume in modern digital ecosystems has underscored the need for innovative approaches to optimize network performance and efficiency. This paper delves into the potential of AI-driven optimization techniques in addressing this imperative. Leveraging artificial intelligence (AI) algorithms such as machine learning and deep learning, the study investigates how AI can revolutionize network management and operation to achieve higher levels of performance and reliability. Through a comprehensive review of existing literature and case studies, this paper elucidates the fundamental principles, methodologies, and applications of AI-driven optimization in diverse network environments. It examines how AI algorithms can analyze vast amounts of network data, identify patterns, and make data-driven decisions to optimize network configurations, routing protocols, and resource allocation strategies. Moreover, the study explores how AI-driven optimization can enhance network security, fault tolerance, and scalability by autonomously detecting and mitigating potential threats and vulnerabilities. The Review succinctly encapsulates the main findings and insights derived from the analysis, emphasizing the transformative potential of AI-driven optimization for network performance and efficiency enhancement. It underscores the benefits of AI-driven approaches in automating complex optimization tasks, reducing operational overhead, and adapting dynamically to changing network conditions and user demands. Additionally, the Review discusses the challenges and considerations associated with the implementation of AI-driven optimization techniques, including algorithmic bias, data privacy concerns, and ethical implications. In conclusion, the Review underscores the critical role of AI-driven optimization in addressing the evolving challenges of network management and operation. It advocates for continued research and development efforts aimed at harnessing the full potential of AI-driven optimization to unlock new levels of performance and efficiency in network infrastructures. By embracing AI-driven approaches, organizations can streamline network operations, improve user experience, and drive innovation in the digital era."
"Transforming Healthcare with AI: Promises, Pitfalls, and Pathways Forward",2024,Ali Shuaib,International Journal of General Medicine,37,W4396560157,10.2147/ijgm.s449598,https://openalex.org/W4396560157,https://www.dovepress.com/getfile.php?fileID=98729,Medicine; Health care; Critical pathways; Data science; Computational biology,article,True,"Abstract: This perspective paper provides a comprehensive examination of artificial intelligence (AI) in healthcare, focusing on its transformative impact on clinical practices, decision-making, and physician-patient relationships. By integrating insights from evidence, research, and real-world examples, it offers a balanced analysis of AI's capabilities and limitations, emphasizing its role in streamlining administrative processes, enhancing patient care, and reducing physician burnout while maintaining a human-centric approach in medicine. The research underscores AI's capacity to augment clinical decision-making and improve patient interactions, but it also highlights the variable impact of AI in different healthcare settings. The need for context-specific adaptations and careful integration of AI technologies into existing healthcare workflows is emphasized to maximize benefits and minimize unintended consequences. Significant attention is given to the implications of AI on the roles and competencies of healthcare professionals. The emergence of AI necessitates new skills in data literacy and technology use, prompting a shift in educational curricula towards digital health and AI training. Ethical considerations are a pivotal aspect of the discussion. The paper explores the challenges posed by data privacy concerns, algorithmic biases, and ensuring equitable access to AI-driven healthcare. It advocates for the development of comprehensive ethical frameworks and ongoing research to guide the responsible use of AI in healthcare. Conclusively, the paper advocates for a balanced approach to AI adoption in healthcare, highlighting the importance of ongoing research, strategic implementation, and the synergistic combination of human expertise with AI technologies for optimal patient care. Keywords: artificial intelligence, healthcare transformation, AI integration strategies"
"MiniMed 780G System Outperforms Other Automated Insulin Systems Due to Algorithm Design, Not Bias: Response to Inaccurate Allegations",2024,Tim van den Heuvel; Javier Castañeda; Isabeau Thijs; Arcelia Arrieta; Lou Lintereur; John Shin; Ohad Cohen,Diabetes Technology & Therapeutics,5,W4393526032,10.1089/dia.2024.0121,https://openalex.org/W4393526032,,Medicine; Algorithm; Diabetes mellitus; Computer science; Endocrinology,letter,False,Not applicable (letter to the editor)
Predictive policing and algorithmic fairness,2023,Tzu-Wei Hung; Chun-Ping Yen,Synthese,18,W4379473793,10.1007/s11229-023-04189-0,https://openalex.org/W4379473793,https://link.springer.com/content/pdf/10.1007/s11229-023-04189-0.pdf,Philosophy of language; Causation; Context (archaeology); Corporate governance; Predictive power,article,True,"Abstract This paper examines racial discrimination and algorithmic bias in predictive policing algorithms (PPAs), an emerging technology designed to predict threats and suggest solutions in law enforcement. We first describe what discrimination is in a case study of Chicago’s PPA. We then explain their causes with Broadbent’s contrastive model of causation and causal diagrams. Based on the cognitive science literature, we also explain why fairness is not an objective truth discoverable in laboratories but has context-sensitive social meanings that need to be negotiated through democratic processes. With the above analysis, we next predict why some recommendations given in the bias reduction literature are not as effective as expected. Unlike the cliché highlighting equal participation for all stakeholders in predictive policing, we emphasize power structures to avoid hermeneutical lacunae. Finally, we aim to control PPA discrimination by proposing a governance solution—a framework of a social safety net."
Examining bias perpetuation in academic search engines: An algorithm audit of Google and Semantic Scholar,2024,Celina Kacperski; Mona Bielig; Mykola Makhortykh; Maryna Sydorova; Roberto Ulloa,First Monday,3,W4404038469,10.5210/fm.v29i11.13730,https://openalex.org/W4404038469,https://firstmonday.org/ojs/index.php/fm/article/download/13730/11713,Audit; Computer science; Search engine; Information retrieval; World Wide Web,article,True,"Researchers rely on academic Web search engines to find scientific sources, but search engine mechanisms may selectively present content that aligns with biases embedded in queries. This study examines whether confirmation biased queries prompted into Google Scholar and Semantic Scholar will yield results aligned with a query’s bias. Six queries (topics across health and technology domains such as ‘vaccines’, ‘Internet use’) were analyzed for disparities in search results. We confirm that biased queries (targeting ‘benefits’ or ‘risks’) affect search results in line with bias, with technology-related queries displaying more significant disparities. Overall, Semantic Scholar exhibited fewer disparities than Google Scholar. Topics rated as more polarizing did not consistently show more disparate results. Academic search results that perpetuate confirmation bias have strong implications for both researchers and citizens searching for evidence. More research is needed to explore how scientific inquiry and academic search engines interact."
Survey and Evaluation of Hypertension Machine Learning Research,2023,Clea du Toit; Tran Tran; Neha Deo; Sachin Aryal; Stefanie Lip; Robert Sykes; Ishan Manandhar; Aristeidis Sionakidis; Leah Stevenson; Harsha Pattnaik; Safaa Alsanosi; Maria Kassi; Ngọc Minh Lê; Maggie Rostron; Sarah Nichol; Alisha Aman; Faisal A. Nawaz; Dhruven Mehta; Ramakumar Tummala; Linsay McCallum; Sandeep Reddy; Shyam Visweswaran; Rahul Kashyap; Bina Joe; Sandosh Padmanabhan,Journal of the American Heart Association,16,W4367368062,10.1161/jaha.122.027896,https://openalex.org/W4367368062,https://www.ahajournals.org/doi/pdf/10.1161/JAHA.122.027896,Medicine; Blood pressure; Artificial intelligence; Big data; Machine learning,article,True,"Background Machine learning (ML) is pervasive in all fields of research, from automating tasks to complex decision-making. However, applications in different specialities are variable and generally limited. Like other conditions, the number of studies employing ML in hypertension research is growing rapidly. In this study, we aimed to survey hypertension research using ML, evaluate the reporting quality, and identify barriers to ML's potential to transform hypertension care. Methods and Results The Harmonious Understanding of Machine Learning Analytics Network survey questionnaire was applied to 63 hypertension-related ML research articles published between January 2019 and September 2021. The most common research topics were blood pressure prediction (38%), hypertension (22%), cardiovascular outcomes (6%), blood pressure variability (5%), treatment response (5%), and real-time blood pressure estimation (5%). The reporting quality of the articles was variable. Only 46% of articles described the study population or derivation cohort. Most articles (81%) reported at least 1 performance measure, but only 40% presented any measures of calibration. Compliance with ethics, patient privacy, and data security regulations were mentioned in 30 (48%) of the articles. Only 14% used geographically or temporally distinct validation data sets. Algorithmic bias was not addressed in any of the articles, with only 6 of them acknowledging risk of bias. Conclusions Recent ML research on hypertension is limited to exploratory research and has significant shortcomings in reporting quality, model validation, and algorithmic bias. Our analysis identifies areas for improvement that will help pave the way for the realization of the potential of ML in hypertension and facilitate its adoption."
AI SOLUTIONS FOR DEVELOPMENTAL ECONOMICS: OPPORTUNITIES AND CHALLENGES IN FINANCIAL INCLUSION AND POVERTY ALLEVIATION,2024,Temitayo Oluwaseun Jejeniwa; Noluthando Zamanjomane Mhlongo; Titilola Olaide Jejeniwa,International Journal of Advanced Economics,31,W4395669997,10.51594/ijae.v6i4.1073,https://openalex.org/W4395669997,https://fepbl.com/index.php/ijae/article/download/1073/1297,Financial inclusion; Poverty; Inclusion (mineral); Economics; Development economics,article,True,"AI presents immense potential in addressing the complex challenges of developmental economics, particularly in the realms of financial inclusion and poverty alleviation. This abstract explores the opportunities and challenges associated with integrating AI solutions in these critical areas. Financial inclusion, essential for sustainable development, remains hampered by barriers such as limited access to banking services, socioeconomic disparities, and regulatory constraints. AI offers innovative approaches through data analytics and prediction models, enabling tailored financial services, risk assessment, and personalized interventions. However, the implementation of AI solutions poses significant challenges, including concerns regarding data privacy, ethical implications such as algorithmic bias, and accessibility issues in underserved regions. Through case studies and best practices, lessons can be gleaned to inform future initiatives, emphasizing the importance of adaptable policy frameworks, collaboration, and impact assessment. Looking ahead, emerging AI technologies like blockchain and enhanced regulatory measures hold promise, necessitating cross-sector partnerships and a concerted effort to harness AI's transformative potential for sustainable development and inclusive growth. Keywords: Developmental Economics, Financial Inclusion, Poverty Alleviation, AI Solutions, Challenges, Opportunities."
Investigating the Impact of Bias in Web Search Algorithms: Implications for Digital Inequality,2023,Haffaz Aladeen,,1,W4360616945,10.31219/osf.io/dmkar,https://openalex.org/W4360616945,https://osf.io/dmkar/download,Personalization; Computer science; Inequality; Diversity (politics); Search engine,preprint,True,"Web search algorithms are essential tools for informationretrieval in the digital age. However, concerns about the pres-ence and impact of bias in these algorithms have grown sig-nificantly in recent years. This study aims to investigate theextent to which web search algorithms exhibit biases and toexplore the implications of these biases on digital inequality.To achieve this, we conducted a comparative analysis of thesearch results from major search engines, considering factorssuch as personalization, localization, and content diversity.Our findings reveal that biases in web search algorithmsare prevalent, leading to filter bubbles, echo chambers, andunequal access to information. Furthermore, we observed astrong correlation between biased search results and the rein-forcement of stereotypes, discrimination, and digital divide.This research highlights the importance of addressing algo-rithmic bias in order to promote equal access to informationand digital equity. Future work should focus on developingdebiasing techniques and ethical guidelines for web searchalgorithms to ensure fair and unbiased information retrieval,"
Artificial Intelligence and Inequality: Challenges and Opportunities,2024,Milad Shahvaroughi Farahani; Ghazal Ghasemi,Qeios,44,W4391997180,10.32388/7hwuz2,https://openalex.org/W4391997180,https://doi.org/10.32388/7hwuz2,Inequality; Computer science; Mathematics; Mathematical analysis,preprint,True,"Integrating artificial intelligence (AI) technologies into various aspects of society has sparked both excitement and concern regarding its potential impact on inequality. This abstract provides an overview of the key issues surrounding AI and inequality, exploring the challenges and opportunities arising from the widespread adoption of AI systems. Firstly, we examine how AI technologies have the potential to exacerbate existing inequalities across various domains, including labor markets, education, healthcare, and access to services. AI-driven automation may lead to job displacement and wage polarization, widening the gap between high-skilled and low-skilled workers. Moreover, algorithmic biases embedded in AI systems can perpetuate discrimination and inequity, particularly against marginalized communities. However, alongside these challenges, AI also presents opportunities to address inequality and promote inclusivity. AI-powered innovations have the potential to enhance efficiency, accessibility, and affordability in sectors such as healthcare, education, and financial services, thereby reducing disparities in access to essential resources and opportunities. Additionally, initiatives focused on ethical AI development and responsible AI governance can mitigate the negative impacts of AI on inequality by promoting fairness, transparency, and accountability in algorithmic decision-making processes. In conclusion, while AI has the potential to both exacerbate and mitigate inequality, its ultimate impact depends on the choices we make in designing, deploying, and governing AI systems. By prioritizing equity, social justice, and human welfare in AI development and implementation, we can harness the transformative power of AI to create a more equitable and inclusive society."
Bias-compensated sign subband adaptive filtering algorithm with individual-weighting-factors: Performance analysis and improvement,2023,Dongxu Liu; Haiquan Zhao,Digital Signal Processing,4,W4323041532,10.1016/j.dsp.2023.103981,https://openalex.org/W4323041532,,Weighting; Algorithm; Convergence (economics); Sign (mathematics); Computer science,article,False,
A scoping review of fair machine learning techniques when using real-world data,2024,Yu Huang; Jingchuan Guo; Wei‐Han Chen; Hsin-Yueh Lin; Huilin Tang; Fei Wang; Hua Xu; Jiang Bian,Journal of Biomedical Informatics,22,W4392515337,10.1016/j.jbi.2024.104622,https://openalex.org/W4392515337,https://doi.org/10.1016/j.jbi.2024.104622,Health care; Computer science; Artificial intelligence; Machine learning; Field (mathematics),review,True,"The integration of artificial intelligence (AI) and machine learning (ML) in health care to aid clinical decisions is widespread. However, as AI and ML take important roles in health care, there are concerns about AI and ML associated fairness and bias. That is, an AI tool may have a disparate impact, with its benefits and drawbacks unevenly distributed across societal strata and subpopulations, potentially exacerbating existing health inequities. Thus, the objectives of this scoping review were to summarize existing literature and identify gaps in the topic of tackling algorithmic bias and optimizing fairness in AI/ML models using real-world data (RWD) in health care domains. We conducted a thorough review of techniques for assessing and optimizing AI/ML model fairness in health care when using RWD in health care domains. The focus lies on appraising different quantification metrics for accessing fairness, publicly accessible datasets for ML fairness research, and bias mitigation approaches. We identified 11 papers that are focused on optimizing model fairness in health care applications. The current research on mitigating bias issues in RWD is limited, both in terms of disease variety and health care applications, as well as the accessibility of public datasets for ML fairness research. Existing studies often indicate positive outcomes when using pre-processing techniques to address algorithmic bias. There remain unresolved questions within the field that require further research, which includes pinpointing the root causes of bias in ML models, broadening fairness research in AI/ML with the use of RWD and exploring its implications in healthcare settings, and evaluating and addressing bias in multi-modal data. This paper provides useful reference material and insights to researchers regarding AI/ML fairness in real-world health care data and reveals the gaps in the field. Fair AI/ML in health care is a burgeoning field that requires a heightened research focus to cover diverse applications and different types of RWD."
A linear least squares algorithm for PolInSAR forest height inversion that considers the influence of introduced biases,2025,Dongfang Lin; Yibin Yao; Yongsheng Liu; Haiqiang Fu; Shaoning Li,Remote Sensing Letters,2,W4406122224,10.1080/2150704x.2024.2442112,https://openalex.org/W4406122224,,Inversion (geology); Algorithm; Computer science; Singular value decomposition; Remote sensing,article,False,"Polarimetric interferometric synthetic aperture radar has the capability to invert forest height by acquiring multi-polarization observations in forested areas. However, forest height inversion using the random volume over ground model often encounters ill-posed problems, resulting in significant uncertainties in forest height estimation. Commonly used algorithms mitigate the ill-conditioned effects by supplementing empirical information, but inaccurate empirical information can introduce excessive uncontrollable biases in parameter estimation, thus limiting the accuracy of parameter inversion. In this context, this study introduces a linear least squares estimation method that integrates a novel truncated singular value decomposition approach to mitigate ill-posed problems and minimize the introduction of biases, thereby enhancing parameter inversion accuracy. Finally, forest height inversion experiments are conducted to assess the efficacy of the proposed method. The results indicate that, compared to the nonlinear least squares method, the forest height retrieval accuracy improved by 44% and the inversion robustness was significantly enhanced, effectively demonstrating the practicality of the new method."
Scaffolding Children’s Sensemaking around Algorithmic Fairness,2023,Jean Salac; Rotem Landesman; Stefania Druga; Amy J. Ko,,9,W4380479537,10.1145/3585088.3589379,https://openalex.org/W4380479537,https://dl.acm.org/doi/pdf/10.1145/3585088.3589379,Situated; Sensemaking; Distrust; Agency (philosophy); Computer science,article,True,"Prior research has investigated children's perceptions of algorithmic bias, but provides little guidance on engaging children in conversations on algorithmic bias that center their agency and well-being. To address this, we developed discussions and design activities based on three scenarios of algorithmic (un)fairness. We conducted these discussions and activities with 16 children (ages 8-12) in the US, and examined our data using qualitative thematic analysis. Grounded in lived experiences and situated knowledge, participants were capable of reasoning around both explicit and implicit effects of algorithmic bias. Participants also expressed distrust of technology, doubting technology's abilities and preferring human approaches to resolve unfairness. This work contributes (1) a more nuanced understanding of children's situated reasoning of technology, suggesting their potential for critical engagement and (2) a blueprint for engaging children in scaffolded yet open-ended sensemaking around algorithmic fairness, informing the design of tools, curricula, and other learning experiences for children."
Assessment of the Support Vector Regression and Random Forest Algorithms in the Bias Correction Process on Temperatures,2024,Brina Miftahurrohmah; Heri Kuswanto; Doni Setio Pambudi; Fatkhurokhman Fauzi; Felix Atmaja,Procedia Computer Science,3,W4396220438,10.1016/j.procs.2024.03.049,https://openalex.org/W4396220438,https://doi.org/10.1016/j.procs.2024.03.049,Computer science; Random forest; Support vector machine; Process (computing); Algorithm,article,True,"Climate information can be obtained from General circulation models (GCMs). However, this model has poor resolution, so it is necessary to do bias correction to overcome this problem. This study carried out a bias correction process using the Support Vector Regression (SVR) and Random Forest (RF) approaches. Bias correction is carried out for temperature in Indonesia using the BNU-ESM and MERRA-2 climate models, which act as observational data. The results show that the RF method (RMSE: 0.334; Correlation: 0.694; Standard Deviation: 0.582) is better than SVR (RMSE: 0.341; Correlation: 0.675; Standard Deviation: 0.588) in performing bias correction."
Using conventional framing to offset bias against algorithmic errors,2025,Hamza Tariq; Jonathan A. Fugelsang; Derek J. Koehler,Judgment and Decision Making,1,W4409516416,10.1017/jdm.2025.8,https://openalex.org/W4409516416,https://doi.org/10.1017/jdm.2025.8,Status quo bias; Framing (construction); Offset (computer science); Decision maker; Psychology,article,True,"Abstract Prior research has shown that people judge algorithmic errors more harshly than identical mistakes made by humans—a bias known as algorithm aversion. We explored this phenomenon across two studies ( N = 1199), focusing on the often-overlooked role of conventionality when comparing human versus algorithmic errors by introducing a simple conventionality intervention. Our findings revealed significant algorithm aversion when participants were informed that the decisions described in the experimental scenarios were conventionally made by humans. However, when participants were told that the same decisions were conventionally made by algorithms, the bias was significantly reduced—or even completely offset. This intervention had a particularly strong influence on participants’ recommendations of which decision-maker should be used in the future—even revealing a bias against human error makers when algorithms were framed as the conventional choice. These results suggest that the existing status quo plays an important role in shaping people’s judgments of mistakes in human–algorithm comparisons."
AdaGC: A Novel Adaptive Optimization Algorithm with Gradient Bias Correction,2024,Qi Wang; Feng Su; Shipeng Dai; Xiaojun Lu; Yang Liu,Expert Systems with Applications,2,W4401261565,10.1016/j.eswa.2024.124956,https://openalex.org/W4401261565,,Convergence (economics); Moment (physics); Algorithm; Rate of convergence; Mathematical optimization,article,False,
Popularity Bias Analysis of Recommendation Algorithm Based on ABM Simulation,2023,Cizhou Yu; Dongsheng Li; Tun Lu; Yichuan Jiang,Communications in computer and information science,2,W4376485417,10.1007/978-981-99-2356-4_35,https://openalex.org/W4376485417,,Popularity; Computer science; Recommender system; Interactivity; Visualization,book-chapter,False,
Based on the Heuristic Bias Method of Efficient Algorithm of RRT - Connect,2023,Fan Yang; Jinyang Fan,,1,W4387698303,10.1109/aicit59054.2023.10278084,https://openalex.org/W4387698303,,Sampling (signal processing); Convergence (economics); Heuristic; Computer science; Algorithm,article,False,"Aiming at the problems of Rapidly Exploring Random Tree (RRT) algorithm with a large number of redundant points and slow convergence in the sampling stage, An efficient RRT-Connect algorithm based on Heuristic Bias RRT-Connect (HB-RRT-Connect) is proposed. The algorithm divides the sampling process into global random sampling and biased target sampling, and determines the next sampling method by judging the environment of the previous sampling point, so as to realize the heuristic biased sampling. At the same time, the concept of double optimized dynamic step is also introduced to improve the expansion efficiency. The simulation results show that the proposed algorithm has less convergence time and less number of nodes than other algorithms, and can effectively improve the convergence speed."
Active learning with human heuristics: an algorithm robust to labeling bias,2024,Sriram Ravichandran; Nandan Sudarsanam; Balaraman Ravindran; Konstantinos V. Katsikopoulos,Frontiers in Artificial Intelligence,1,W4404509705,10.3389/frai.2024.1491932,https://openalex.org/W4404509705,https://doi.org/10.3389/frai.2024.1491932,Heuristics; Computer science; Oracle; Machine learning; Artificial intelligence,article,True,"Active learning enables prediction models to achieve better performance faster by adaptively querying an oracle for the labels of data points. Sometimes the oracle is a human, for example when a medical diagnosis is provided by a doctor. According to the behavioral sciences, people, because they employ heuristics, might sometimes exhibit biases in labeling. How does modeling the oracle as a human heuristic affect the performance of active learning algorithms? If there is a drop in performance, can one design active learning algorithms robust to labeling bias? The present article provides answers. We investigate two established human heuristics (fast-and-frugal tree, tallying model) combined with four active learning algorithms (entropy sampling, multi-view learning, conventional information density, and, our proposal, inverse information density) and three standard classifiers (logistic regression, random forests, support vector machines), and apply their combinations to 15 datasets where people routinely provide labels, such as health and other domains like marketing and transportation. There are two main results. First, we show that if a heuristic provides labels, the performance of active learning algorithms significantly drops, sometimes below random. Hence, it is key to design active learning algorithms that are robust to labeling bias. Our second contribution is to provide such a robust algorithm. The proposed inverse information density algorithm, which is inspired by human psychology, achieves an overall improvement of 87% over the best of the other algorithms. In conclusion, designing and benchmarking active learning algorithms can benefit from incorporating the modeling of human heuristics."
The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field,2023,Hossein Hassani; Emmanuel Sirimal Silva,Big Data and Cognitive Computing,310,W4361002760,10.3390/bdcc7020062,https://openalex.org/W4361002760,https://www.mdpi.com/2504-2289/7/2/62/pdf?version=1679985158,Computer science; Data science; Workflow; Field (mathematics); Artificial intelligence,article,True,"ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s architecture, including its ability to be fine-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the benefits outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workflows and is likely to become an increasingly important tool for intelligence augmentation in the field of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classification. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be fine-tuned for specific use cases, it may not perform well on certain tasks if it has not been specifically trained for them. Additionally, the output of ChatGPT may be difficult to interpret, which could pose challenges for decision-making in data science applications."
Causal effect of racial bias in machine learning algorithms affecting user persuasiveness &amp;amp; decision-making: An Empirical Study,2023,Kinshuk Sengupta; Praveen Ranjan Srivast,Research Square (Research Square),5,W4318540816,10.21203/rs.3.rs-2509731/v1,https://openalex.org/W4318540816,https://www.researchsquare.com/article/rs-2509731/latest.pdf,Counterfactual thinking; Artificial intelligence; Computer science; Harm; Outcome (game theory),preprint,True,"Abstract Language data and models demonstrate various types of bias, be it ethnic, religious, gender, or socioeconomic. AI/NLP models, when trained on the racially biased dataset, AI/NLP models instigate poor model explainability, influence user experience during decision making, and thus further magnify societal biases, raising profound ethical implications for society. The motivation of the study is to investigate how AI systems imbibe bias introduced in data and further produce unexplainable discriminatory outcomes. This implication of the study will aid in understanding how machine learning models imbibe bias from data and try to influence individuals' articulateness of system outcomes due to bias features. The design of the experiment involves studying the counterfactual impact of racial bias features present in language datasets and its associated effect on the model outcome. A mixed research methodology is adopted to investigate the cross implication of biased model outcome on user experience effect on decision-making through controlled lab experimentation. The findings provide foundation support for correlating the importance of carry-over an artificial intelligence model solving NLP task due to biased concept presented in the dataset. Further, the research outcomes justify the negative influence on users' persuasiveness 1 that leads to altering the decision-making quotient of an individual when trying to rely on the model outcome to act. The paper bridges the gap across the harm caused in establishing poor user trustworthiness due to an inequitable system design and provides strong support for researchers, policymakers, and data scientists to build responsible AI frameworks within organizations. 1 Refer to abbreviation in appendix for more explanation"
IoT Service Composition — An Estimation of Distribution Algorithm with Adaptive Bias,2023,Fengyang Sun; Hui Ma; Gang Chen; Sven Hartmann,2022 IEEE Congress on Evolutionary Computation (CEC),1,W4387005615,10.1109/cec53210.2023.10254066,https://openalex.org/W4387005615,,Estimation of distribution algorithm; Computer science; Probabilistic logic; Heuristic; Stability (learning theory),article,False,"Service composition in Internet of Things (SCIoT), as an emerging topic in service computing, aims to select optimal services to complete user requests according to various user requirements such as minimizing energy consumption and response time. To solve this NP-hard problem, numerous heuristic methods, e.g., local search and population-based algorithms, have been proposed, wherein Estimation of Distribution Algorithm (EDA) gains increasing attention because of its explicit global probabilistic nature. However, existing EDAs increase solution diversity by using fixed bias, yet interfere the stability of the learned distribution in the later stage of optimization. Therefore, this paper proposes an EDA with an adaptive bias strategy (EDA-AdaBias) to solve the service composition in IoT problem. The decreasing bias value is added onto the probability values for all choices of each solution variable over generations, which improves diversity of sampled solutions and avoids dramatic change of constructed distribution. Experiments indicate that EDA-AdaBias presents promising performance compared to other competitive methods on this problem."
Harnessing the Power of Large Language Models for Cybersecurity,2024,Hewa Majeed Zangana; Marwan Omar,"Advances in information security, privacy, and ethics book series",26,W4403969640,10.4018/979-8-3693-9311-6.ch001,https://openalex.org/W4403969640,,Computer security; Computer science; Power (physics); Physics; Quantum mechanics,book-chapter,False,"The LLMs not only have changed the overall nature of NPL but have also helped a lot in setting standards in cyber security. Within the confines of this review, the authors discuss the benefits, progressions, difficulties, as well as the future paths aimed to be taken in the cybersecurity field of LLMs. They delve into how LLMs help companies process unstructured textual data for text dangers detections, vulnerability assessments, and incident responses. In addition, they investigate the ethical and societal consequences of using LLMs for cybersecurity, facing challenges like algorithmic bias, privacy, and data safety. Besides that, they find that critical research questions in the crossroads of LLMs and cybersecurity language include unique assessing techniques and the improvement of algorithms to clarify the information. Through the development of many-faceted interdisciplinary cooperation and ethics-based considerations, we can maximize the opportunities LLMs present in the cyber world and build a more resilient and secure environment for everyone."
ETHICAL IMPLICATIONS OF AI IN FINANCIAL DECISION – MAKING: A REVIEW WITH REAL WORLD APPLICATIONS,2024,Oluwatobi Opeyemi Adeyelu; Chinonye Esther Ugochukwu; Mutiu Alade Shonibare,International Journal of Applied Research in Social Sciences,15,W4394883974,10.51594/ijarss.v6i4.1033,https://openalex.org/W4394883974,https://fepbl.com/index.php/ijarss/article/download/1033/1256,Business; Finance,review,True,"This study delves into the ethical implications of Artificial Intelligence (AI) in financial decision-making, exploring the transformative impact of AI technologies on the financial services sector. Through a comprehensive literature review, the research highlights the dual nature of AI's integration into finance, showcasing both its potential to enhance operational efficiency and decision accuracy and the ethical challenges it introduces. These challenges include concerns over data privacy, algorithmic bias, and the potential for systemic risks, underscoring the need for robust ethical frameworks and regulatory standards. The study emphasizes the importance of a multidisciplinary approach to AI development and deployment, advocating for collaboration among technologists, ethicists, policymakers, and end-users to ensure that AI technologies are aligned with societal values and ethical principles. Future directions for research are identified, focusing on the development of adaptive ethical guidelines, methodologies for embedding ethical principles into AI systems, and the investigation of AI's long-term impact on market dynamics and consumer behaviour. This research contributes valuable insights into the ethical integration of AI in finance, offering recommendations for ensuring that AI technologies are utilized in a manner that is both ethically sound and conducive to the advancement of the financial services industry.&#x0D; Keywords: Artificial Intelligence, Financial Decision-Making, Ethical Implications, Algorithmic Bias, Data Privacy, Regulatory Standards, Multidisciplinary Approach."
Adaptive Learning through Artificial Intelligence,2023,Meet Joshi,SSRN Electronic Journal,21,W4385326031,10.2139/ssrn.4514887,https://openalex.org/W4385326031,https://zenodo.org/records/10731719/files/Adaptive%20Learning%20through%20Artificial%20Intelligence.pdf,Artificial intelligence; Computer science; Adaptive learning; Cognitive science; Psychology,article,True,"This article explores the integration of artificial intelligence (AI) into adaptive learning systems for the purpose of individualizing education through machine learning and predictive analytics. It examines the benefits, challenges and implications of this merger and highlights its potential to revolutionize education by providing a customized and streamlined learning experience. It discusses the role of AI in learner modeling, content customization, and feedback mechanisms, along with considerations such as privacy, data security, and algorithmic bias. AI-powered adaptive learning promises to shape the future of education in the digital age by enabling learners and educators to achieve optimal outcomes."
RRT Autonomous Detection Algorithm Based on Multiple Pilot Point Bias Strategy and Karto SLAM Algorithm,2024,Lieping Zhang; Xiaoxu Shi; Liu Tang; Yilin Wang; Jiansheng Peng; Jianchu Zou,"Computers, materials & continua/Computers, materials & continua (Print)",2,W4391480431,10.32604/cmc.2024.047235,https://openalex.org/W4391480431,https://www.techscience.com/cmc/online/detail/19880/pdf,Algorithm; Computer science; Point (geometry); Artificial intelligence; Mathematics,article,True,"A Rapid-exploration Random Tree (RRT) autonomous detection algorithm based on the multi-guide-node deflection strategy and Karto Simultaneous Localization and Mapping (SLAM) algorithm was proposed to solve the problems of low efficiency of detecting frontier boundary points and drift distortion in the process of map building in the traditional RRT algorithm in the autonomous detection strategy of mobile robot.Firstly, an RRT global frontier boundary point detection algorithm based on the multi-guide-node deflection strategy was put forward, which introduces the reference value of guide nodes' deflection probability into the random sampling function so that the global search tree can detect frontier boundary points towards the guide nodes according to random probability.After that, a new autonomous detection algorithm for mobile robots was proposed by combining the graph optimization-based Karto SLAM algorithm with the previously improved RRT algorithm.The algorithm simulation platform based on the Gazebo platform was built.The simulation results show that compared with the traditional RRT algorithm, the proposed RRT autonomous detection algorithm can effectively reduce the time of autonomous detection, plan the length of detection trajectory under the condition of high average detection coverage, and complete the task of autonomous detection mapping more efficiently.Finally, with the help of the ROS-based mobile robot experimental platform, the performance of the proposed algorithm was verified in the real environment of different obstacles.The experimental results show that in the actual environment of simple and complex obstacles, the proposed RRT autonomous detection algorithm was superior to the traditional RRT autonomous detection algorithm in the time of detection, length of detection trajectory, and average coverage, thus improving the efficiency and accuracy of autonomous detection."
The Impact of NBA Implementation Across Engineering Disciplines,2024,S. Saravanan; J. Chandrasekar; S. Satheesh Kumar; Pavan Patel; J. Maria Shanthi; Sampath Boopathi,Advances in higher education and professional development book series,23,W4399452311,10.4018/979-8-3693-1666-5.ch010,https://openalex.org/W4399452311,,Transformative learning; Commodification; Entertainment; Equity (law); Engineering,book-chapter,False,"NBA principles and technologies have significantly impacted engineering disciplines, leading to social changes and reshaping industries. NBA-inspired innovations have influenced materials science, data analytics, public health, urban design, and entertainment experiences. The study examines equity, access, and ethical dimensions of NBA-driven engineering innovations, focusing on data privacy, algorithmic bias, and commodification of athlete performance. The goal is to foster inclusivity, equity, and sustainable development, fostering collaboration between the sports industry and engineering disciplines. This approach emphasizes the transformative power of sports-driven innovation in engineering for societal betterment."
Stock Market Directional Bias Prediction Using ML Algorithms,2023,Ryan Chipwanya,arXiv (Cornell University),1,W4387994863,10.48550/arxiv.2310.16855,https://openalex.org/W4387994863,https://arxiv.org/abs/2310.16855,Stock market; Machine learning; Computer science; Random forest; Artificial intelligence,preprint,True,"The stock market has been established since the 13th century, but in the current epoch of time, it is substantially more practicable to anticipate the stock market than it was at any other point in time due to the tools and data that are available for both traditional and algorithmic trading. There are many different machine learning models that can do time-series forecasting in the context of machine learning. These models can be used to anticipate the future prices of assets and/or the directional bias of assets. In this study, we examine and contrast the effectiveness of three different machine learning algorithms, namely, logistic regression, decision tree, and random forest to forecast the movement of the assets traded on the Japanese stock market. In addition, the models are compared to a feed forward deep neural network, and it is found that all of the models consistently reach above 50% in directional bias forecasting for the stock market. The results of our study contribute to a better understanding of the complexity involved in stock market forecasting and give insight on the possible role that machine learning could play in this context."
Impact on bias mitigation algorithms to variations in inferred sensitive attribute uncertainty,2025,Yanchen Wang; Lisa Singh,Frontiers in Artificial Intelligence,1,W4408191332,10.3389/frai.2025.1520330,https://openalex.org/W4408191332,https://doi.org/10.3389/frai.2025.1520330,Computer science; Debiasing; Inference; Trustworthiness; Data mining,article,True,"Concerns about the trustworthiness, fairness, and privacy of AI systems are growing, and strategies for mitigating these concerns are still in their infancy. One approach to improve trustworthiness and fairness in AI systems is to use bias mitigation algorithms. However, most bias mitigation algorithms require data sets that contain sensitive attribute values to assess the fairness of the algorithm. A growing number of real world data sets do not make sensitive attribute information readily available to researchers. One solution is to infer the missing sensitive attribute information and apply an existing bias mitigation algorithm using this inferred knowledge. While researchers are beginning to explore this question, it is still unclear how robust existing bias mitigation algorithms are to different levels of inference accuracy. This paper explores this question by investigating the impact of different levels of accuracy of the inferred sensitive attribute on the performance of different bias mitigation strategies. We generate variation in sensitive attribute accuracy using both simulation and construction of neural models for the inference task. We then assess the quality of six bias mitigation algorithms that are deployed across different parts of our learning life cycle: pre-processing, in-processing, and post-processing. We find that the disparate impact remover is the least sensitive bias mitigation strategy and that if we apply the bias mitigation algorithms using an inferred sensitive attribute with reasonable accuracy, the fairness scores are higher than the best standard model and the balanced accuracy is similar to that of the standard model. These findings open the door for improving fairness of black box AI systems using some bias mitigation strategies."
Individual bias and fluctuations in collective decision making: from algorithms to Hamiltonians,2023,Petro Sarkanych; Mariana Krasnytska; Luis Gómez-Nava; Pawel Romanczuk; Yurij Holovatch,Physical Biology,2,W4377013935,10.1088/1478-3975/acd6ce,https://openalex.org/W4377013935,https://iopscience.iop.org/article/10.1088/1478-3975/acd6ce/pdf,Statistical physics; Probabilistic logic; Hamiltonian (control theory); Analogy; Mathematics,article,True,"In this paper, we reconsider the spin model suggested recently to understand some features of collective decision making among higher organisms (Hartnettet al2016Phys. Rev. Lett.116038701). Within the model, the state of an agentiis described by the pair of variables corresponding to its opinionSi=±1and a biasωitoward any of the opposing values ofSi. Collective decision making is interpreted as an approach to the equilibrium state within the nonlinear voter model subject to a social pressure and a probabilistic algorithm. Here, we push such a physical analogy further and give the statistical physics interpretation of the model, describing it in terms of the Hamiltonian of interaction and looking for the equilibrium state via explicit calculation of its partition function. We show that, depending on the assumptions about the nature of social interactions, two different Hamiltonians can be formulated, which can be solved using different methods. In such an interpretation the temperature serves as a measure of fluctuations, not considered before in the original model. We find exact solutions for the thermodynamics of the model on the complete graph. The general analytical predictions are confirmed using individual-based simulations. The simulations also allow us to study the impact of system size and initial conditions on the collective decision making in finite-sized systems, in particular, with respect to convergence to metastable states."
Sampling lattice points in a polytope: a Bayesian biased algorithm with random updates,2024,Miles Bakenhus; Sonja Petrović,Algebraic Statistics,1,W4397005953,10.2140/astat.2024.15.61,https://openalex.org/W4397005953,https://doi.org/10.2140/astat.2024.15.61,Polytope; Bayesian probability; Algorithm; Lattice (music); Gibbs sampling,article,True,"The set of nonnegative integer lattice points in a polytope, also known as the fiber of a linear map, makes an appearance in several applications including optimization and statistics.We address the problem of sampling from this set using three ingredients: an easy-to-compute lattice basis of the constraint matrix, a biased sampling algorithm with a Bayesian framework, and a step-wise selection method.The bias embedded in our algorithm updates sampler parameters to improve fiber discovery rate at each step chosen from previously discovered elements.We showcase the performance of the algorithm on several examples, including fibers that are out of reach for the state-of-the-art Markov bases samplers."
Epistemically violent biases in artificial intelligence design: the case of DALLE-E 2 and Starry AI,2023,Blessing Mbalaka,Digital Transformation and Society,15,W4382403160,10.1108/dts-01-2023-0003,https://openalex.org/W4382403160,https://www.emerald.com/insight/content/doi/10.1108/DTS-01-2023-0003/full/pdf?title=epistemically-violent-biases-in-artificial-intelligence-design-the-case-of-dalle-e-2-and-starry-ai,Offensive; Generator (circuit theory); Diversity (politics); Inclusion (mineral); Computer science,article,True,"Purpose The paper aims to expand on the works well documented by Joy Boulamwini and Ruha Benjamin by expanding their critique to the African continent. The research aims to assess if algorithmic biases are prevalent in DALL-E 2 and Starry AI. The aim is to help inform better artificial intelligence (AI) systems for future use. Design/methodology/approach The paper utilised a desktop study for literature and gathered data from Open AI’s DALL-E 2 text-to-image generator and StarryAI text-to-image generator. Findings The DALL-E 2 significantly underperformed when it was tasked with generating images of “An African Family” as opposed to images of a “Family”. The pictures lacked any conceivable detail as compared to the latter of this comparison. The StarryAI significantly outperformed the DALL-E 2 and rendered visible faces. However, the accuracy of the culture portrayed was poor. Research limitations/implications Because of the chosen research approach, the research results may lack generalisability. Therefore, researchers are encouraged to test the proposed propositions further. The implications, however, are that more inclusion is warranted to help address the issue of cultural inaccuracies noted in a few of the paper’s experiments. Practical implications The paper is useful for advocates who advocate for algorithmic equality and fairness by highlighting evidence of the implications of systemic-induced algorithmic bias. Social implications The reduction in offensive racism and more socially appropriate AI can be a better product for commercialisation and general use. If AI is trained on diversity, it can lead to better applications in contemporary society. Originality/value The paper’s use of DALL-E 2 and Starry AI is an under-researched area, and future studies on this matter are welcome."
Biased Target-tree <sup>*</sup> Algorithm with RRT <sup>*</sup> for Reducing Parking Path Planning Time,2023,Joonwoo Ahn; Minsoo Kim; Jaeheung Park,2022 IEEE Intelligent Vehicles Symposium (IV),2,W4385312479,10.1109/iv55152.2023.10186712,https://openalex.org/W4385312479,,Tree (set theory); Path (computing); Algorithm; Computer science; Combinatorics,article,False,"The target-tree <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> algorithm, which is a variant of the optimal rapidly-exploring random tree (RRT <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> ) has been proposed to reduce the parking path planning time. This algorithm pre-generates a set of backward paths (target-tree) around a parking spot and extends an RRT <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> from the initial pose until it is connected to a random sample of the target-tree. However, it is difficult to obtain the shortest (optimal) parking path within a short planning time because connected samples between the tree and the target-tree are randomly searched. To deal with this problem, this paper proposes a biased target-tree <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> algorithm with RRT <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> that searches connected random samples in a biased range near the target-tree. This range has a Gaussian distribution centered on the optimal connected sample where the shortest parking path can be obtained quickly and is obtained through supervised learning. In actual parking situations, the biased target-tree <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> algorithm obtained a shorter path with less length deviation than the original target-tree <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">*</sup> algorithm within a shorter planning time."
Shaping the Future of Healthcare: Ethical Clinical Challenges and Pathways to Trustworthy AI,2025,Polat Göktaş; Andrzej Grzybowski,Journal of Clinical Medicine,48,W4407998253,10.3390/jcm14051605,https://openalex.org/W4407998253,https://doi.org/10.3390/jcm14051605,Transparency (behavior); Health care; Accountability; Corporate governance; Sustainability,article,True,"Background/Objectives: Artificial intelligence (AI) is transforming healthcare, enabling advances in diagnostics, treatment optimization, and patient care. Yet, its integration raises ethical, regulatory, and societal challenges. Key concerns include data privacy risks, algorithmic bias, and regulatory gaps that struggle to keep pace with AI advancements. This study aims to synthesize a multidisciplinary framework for trustworthy AI in healthcare, focusing on transparency, accountability, fairness, sustainability, and global collaboration. It moves beyond high-level ethical discussions to provide actionable strategies for implementing trustworthy AI in clinical contexts. Methods: A structured literature review was conducted using PubMed, Scopus, and Web of Science. Studies were selected based on relevance to AI ethics, governance, and policy in healthcare, prioritizing peer-reviewed articles, policy analyses, case studies, and ethical guidelines from authoritative sources published within the last decade. The conceptual approach integrates perspectives from clinicians, ethicists, policymakers, and technologists, offering a holistic “ecosystem” view of AI. No clinical trials or patient-level interventions were conducted. Results: The analysis identifies key gaps in current AI governance and introduces the Regulatory Genome—an adaptive AI oversight framework aligned with global policy trends and Sustainable Development Goals. It introduces quantifiable trustworthiness metrics, a comparative analysis of AI categories for clinical applications, and bias mitigation strategies. Additionally, it presents interdisciplinary policy recommendations for aligning AI deployment with ethical, regulatory, and environmental sustainability goals. This study emphasizes measurable standards, multi-stakeholder engagement strategies, and global partnerships to ensure that future AI innovations meet ethical and practical healthcare needs. Conclusions: Trustworthy AI in healthcare requires more than technical advancements—it demands robust ethical safeguards, proactive regulation, and continuous collaboration. By adopting the recommended roadmap, stakeholders can foster responsible innovation, improve patient outcomes, and maintain public trust in AI-driven healthcare."
A multistart biased‐randomized algorithm for solving a three‐dimensional case picking problem with real‐life constraints,2023,Mattia Neroni; Ángel A. Juan; Massimo Bertolini,International Transactions in Operational Research,2,W4390345618,10.1111/itor.13421,https://openalex.org/W4390345618,,Pallet; Vehicle routing problem; Mathematical optimization; Heuristic; Computer science,article,False,"Abstract This paper introduces the three‐dimensional case picking problem (3D‐CPP) and proposes a multistart biased‐randomized algorithm (BRA) to solve it. The 3D‐CPP combines two important topics in modern warehouse logistics: the pallet loading problem and the routing of pickers in manual warehouses. The proposed optimization procedure aims at minimizing the overall distance traveled by the pickers, and is achieved by combining a routing problem (i.e., the order in which picking positions are visited) with a loading problem (i.e., the way in which cases are placed onto the pallet). We also consider additional constraints regarding the weight, vertical support, and strength of the cases. In order to solve this problem, we first propose a constructive heuristic which combines routing and packing procedures. This initial heuristic is then extended into a multistart BRA by employing a skewed probability distribution to introduce a certain degree of randomness during the solution‐construction process. A series of computational experiments allow us to assess the quality of the proposed approach, through a comparison with other algorithms as well as using real‐life data provided by an industrial partner."
Harnessing the Power of AI: A Comprehensive Review of Its Impact and Challenges in Nursing Science and Healthcare,2023,Seema Yelne; Minakshi Chaudhary; Karishma Dod; Akhtaribano Sayyad; Ranjana Sharma,Cureus,124,W4388895516,10.7759/cureus.49252,https://openalex.org/W4388895516,https://assets.cureus.com/uploads/review_article/pdf/206741/20231122-10807-1n3hf8c.pdf,Transformative learning; Health care; Medicine; Engineering ethics; Applications of artificial intelligence,review,True,"This comprehensive review delves into the impact and challenges of Artificial Intelligence (AI) in nursing science and healthcare. AI has already demonstrated its transformative potential in these fields, with applications spanning from personalized care and diagnostic accuracy to predictive analytics and telemedicine. However, the integration of AI has its complexities, including concerns related to data privacy, ethical considerations, and biases in algorithms and datasets. The future of healthcare appears promising, with AI poised to advance diagnostics, treatment, and healthcare practices. Nevertheless, it is crucial to remember that AI should complement, not replace, healthcare professionals, preserving the essential human element of care. To maximize AI's potential in healthcare, interdisciplinary collaboration, ethical guidelines, and the protection of patient rights are essential. This review concludes with a call to action, emphasizing the need for ongoing research and collective efforts to ensure that AI contributes to improved healthcare outcomes while upholding the highest standards of ethics and patient-centered care."
"What's fair is… fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in the integration of clinical machine learning",2023,Melissa D. McCradden; Oluwadara Odusi; Shalmali Joshi; Ismail Akrout; Kagiso Ndlovu; Ben Glocker; Gabriel Maicas; Xiaoxuan Liu; Mjaye Mazwi; Tee Garnett; Lauren Oakden‐Rayner; Myrtede Alfred; Irvine Sihlahla; Oswa Shafei; Anna Goldenberg,"2022 ACM Conference on Fairness, Accountability, and Transparency",19,W4380318724,10.1145/3593013.3594096,https://openalex.org/W4380318724,https://dl.acm.org/doi/pdf/10.1145/3593013.3594096,Operationalization; Context (archaeology); Engineering ethics; Health care; Economic Justice,article,True,"The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools."
Opportunities and Challenges of Integrating Generative Artificial Intelligence in Education,2024,Rommel AlAli; Yousef Wardat,International Journal of Religion,34,W4396872845,10.61707/8y29gv34,https://openalex.org/W4396872845,https://ijor.co.uk/ijor/article/download/4397/2263,Generative grammar; Artificial intelligence; Psychology; Cognitive science; Computer science,article,True,"This paper thoroughly examines both the opportunities and obstacles associated with integrating Generative Artificial Intelligence (AI) into educational settings. It explores how Generative AI has the potential to enrich learning experiences, customize education for individuals, and foster creativity. However, it also confronts several challenges including ethical dilemmas, safeguarding data privacy, mitigating algorithmic biases, and reshaping the role of educators. Through a synthesis of theoretical frameworks and empirical research, the paper offers valuable insights into effective strategies for navigating these challenges. It emphasizes the importance of establishing ethical guidelines, ensuring transparency in algorithms, and adopting inclusive design principles during AI integration. Furthermore, the paper underscores the importance of providing educators with adequate training and professional development opportunities to effectively utilize AI tools. Additionally, it advocates for ongoing dialogue among stakeholders—such as educators, policymakers, technologists, and students—to steer responsible AI integration in education. Ultimately, the paper advocates for a collaborative approach that prioritizes human-centric values, equity, and diversity. While Generative AI holds promise for revolutionizing educational practices, its integration requires thoughtful consideration of ethical, social, and pedagogical implications. Through proactive collaboration and partnership, educators can leverage AI's potential to create more immersive, tailored, and equitable learning environments."
Evaluating the Potential of Artificial Intelligence in Orthopedic Surgery for Value-based Healthcare,2023,Aftab Tariq; Ahmad Yousaf Gill; Hafiz Khawar Hussain,International Journal of Multidisciplinary Sciences and Arts,25,W4382792584,10.47709/ijmdsa.v2i1.2394,https://openalex.org/W4382792584,https://jurnal.itscience.org/index.php/ijmdsa/article/download/2394/1851,Orthopedic surgery; Health care; Medicine; Analytics; Workforce,article,True,"&#x0D; &#x0D; &#x0D; &#x0D; The potential of artificial intelligence (AI) to transform value-based healthcare in the area of orthopedic surgery is examined in this research. Orthopedic surgeons and healthcare systems may improve patient outcomes, increase efficiency, and alter care delivery by combining AI algorithms, cutting-edge data analytics, and novel technology. Through case studies and success stories, the article provides a thorough study of the advantages and prospects provided by AI in orthopedic surgery. These instances demonstrate how AI has been successfully applied to several facets of orthopedic surgery, including as diagnosis, planning of the surgical course, surgical navigation, postoperative care, and resource allocation. The ethical and legal ramifications of using AI are also discussed in the study, with a focus on patient autonomy, privacy, accountability, and any potential effects on the healthcare workforce. The potential applications of AI in orthopedic surgery are examined, together with developments in preoperative planning, surgical robotics, remote monitoring, predictive analytics, personalised medicine, research, and innovation. The promise of AI in orthopedic surgery is obvious, despite issues with data quality, privacy, algorithm biases, and legal constraints. The ethical and appropriate application of AI technology in orthopedic surgery has the potential to significantly enhance patient outcomes, lower complications, boost efficiency, and change the way healthcare is provided. This study lays the groundwork for future study and application in the field of orthopedic surgery by offering insightful information on the role of AI in delivering value-based healthcare.&#x0D; &#x0D; &#x0D; &#x0D; &#x0D; &#x0D; &#x0D; &#x0D; &#x0D; &#x0D;"
Algorithmic Unfairness through the Lens of EU Non-Discrimination Law,2023,Hilde Weerts; Raphaële Xenidis; Fabien Tarissan; Henrik Palmer Olsen; Mykola Pechenizkiy,"2022 ACM Conference on Fairness, Accountability, and Transparency",16,W4378446601,10.1145/3593013.3594044,https://openalex.org/W4378446601,https://dl.acm.org/doi/pdf/10.1145/3593013.3594044,Lens (geology); Computer science; Law; Political science; Optics,article,True,"Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. Ultimately, we show that metaphors depicting the law as a decision tree are misguiding. We suggest moving away from asking what should be equal, and towards asking why a particular distribution of burdens and benefits is right in a given context."
Bias and Non-Diversity of Big Data in Artificial Intelligence: Focus on Retinal Diseases,2023,Cris Martin P. Jacoba; Leo Anthony Celi; Anja Lorch; Ward Fickweiler; Lucia Sobrin; Judy Wawira Gichoya; Lloyd Paul Aiello; Paolo S. Silva,Seminars in Ophthalmology,24,W4317359848,10.1080/08820538.2023.2168486,https://openalex.org/W4317359848,,Diversity (politics); Health care; Big data; Medicine; Inequality,article,False,"Artificial intelligence (AI) applications in healthcare will have a potentially far-reaching impact on patient care, however issues regarding algorithmic bias and fairness have recently surfaced. There is a recognized lack of diversity in the available ophthalmic datasets, with 45% of the global population having no readily accessible representative images, leading to potential misrepresentations of their unique anatomic features and ocular pathology. AI applications in retinal disease may show less accuracy with underrepresented populations that may further widen the gap of health inequality if left unaddressed. Beyond disease symptomatology, social determinants of health must be integrated into our current paradigms of disease understanding, with the goal of more personalized care. AI has the potential to decrease global healthcare inequality, but it will need to be based on a more diverse, transparent and responsible use of healthcare data."
Transforming Healthcare through AI: Unleashing the Power of Personalized Medicine,2023,Abdullah Khan,International Journal of Multidisciplinary Sciences and Arts,16,W4382792558,10.47709/ijmdsa.v2i1.2424,https://openalex.org/W4382792558,https://jurnal.itscience.org/index.php/ijmdsa/article/download/2424/1849,Health care; Workflow; Analytics; Big data; Openness to experience,article,True,"The healthcare sector now places a lot of emphasis on providing patients with personalized care that is catered to their unique requirements and features. A significant force behind the development and use of customized healthcare is artificial intelligence (AI). This essay examines the use of AI in personalized healthcare and how it can affect several facets of healthcare provision. The study starts out by talking about how AI is being used in diagnostics, emphasizing how machine learning algorithms and the examination of various datasets can improve diagnostic accuracy. It goes into more detail about how AI can be used to create tailored treatment plans, utilizing patient-specific data and predictive analytics to maximize therapeutic results and reduce side effects. Examined are the ethical issues surrounding AI in customized healthcare, such as data privacy, algorithmic bias, and the value of responsibility and openness. The integration of AI into clinical practice is also covered in the study, along with prospects for improving decision-making, streamlining workflow, and overall healthcare delivery effectiveness. The healthcare sector now places a lot of emphasis on providing patients with personalized care that is catered to their unique requirements and features. A significant force behind the development and use of customized healthcare is artificial intelligence (AI). This essay examines the use of AI in personalized healthcare and how it can affect several facets of healthcare provision. The study starts out by talking about how AI is being used in diagnostics, emphasizing how machine learning algorithms and the examination of various datasets can improve diagnostic accuracy. It goes into more detail about how AI can be used to create tailored treatment plans, utilizing patient-specific data and predictive analytics to maximize therapeutic results and reduce side effects. Examined are the ethical issues surrounding AI in customized healthcare, such as data privacy, algorithmic bias, and the value of responsibility and openness. The integration of AI into clinical practice is also covered in the study, along with prospects for improving decision-making, streamlining workflow, and overall healthcare delivery effectiveness.&#x0D; &#x0D;"
Fairness in Deep Learning: A Survey on Vision and Language Research,2023,Otávio Parraga; Martin D. Móre; Christian Mattjie; Nathan Gavenski; Lucas Silveira Kupssinskü; Adilson Medronha; Luis V. Moura; Gabriel S. Simões; Rodrigo C. Barros,ACM Computing Surveys,20,W4390002645,10.1145/3637549,https://openalex.org/W4390002645,https://dl.acm.org/doi/pdf/10.1145/3637549,Debiasing; Computer science; Artificial intelligence; Artificial neural network; Taxonomy (biology),review,True,"Despite being responsible for state-of-the-art results in several computer vision and natural language processing tasks, neural networks have faced harsh criticism due to some of their current shortcomings. One of them is that neural networks are correlation machines prone to model biases within the data instead of focusing on actual useful causal relationships. This problem is particularly serious in application domains affected by aspects such as race, gender, and age. To prevent models from incurring unfair decision-making, the AI community has concentrated efforts on correcting algorithmic biases, giving rise to the research area now widely known as fairness in AI . In this survey paper, we provide an in-depth overview of the main debiasing methods for fairness-aware neural networks in the context of vision and language research. We propose a novel taxonomy that builds upon previous proposals but is tailored for deep learning research to better organize the literature on debiasing methods for fairness. We review all important neural-based methods and evaluation metrics while discussing the current challenges, trends, and important future work directions for the interested researcher and practitioner."
Applying artificial intelligence in healthcare: lessons from the COVID-19 pandemic,2023,Sreejith Balasubramanian; Vinaya Shukla; Nazrul Islam; Arvind Upadhyay; Linh Duong,International Journal of Production Research,26,W4387307469,10.1080/00207543.2023.2263102,https://openalex.org/W4387307469,https://www.tandfonline.com/doi/pdf/10.1080/00207543.2023.2263102?needAccess=true,Health care; Pandemic; Stakeholder; Big data; Knowledge management,article,True,"The COVID-19 pandemic exposed vulnerabilities in global healthcare systems and highlighted the need for innovative, technology-driven solutions like Artificial Intelligence (AI). However, previous research on the topic has been limited and fragmented, leading to an incomplete understanding of the ‘what’, ‘where’ and ‘how’ of its application, as well as its associated benefits and challenges. This study proposes a comprehensive AI framework for healthcare and assesses its effectiveness within the UAE's healthcare sector. It provides valuable insights into AI applications for healthcare stakeholders that range from the molecular to the population level. The study covers the different computational techniques employed, from machine learning to computer vision, and the various types of data inputs fed into these techniques, including clinical, epidemiological, locational, behavioural and genomic data. Additionally, the research highlights AI's capacity to enhance healthcare's operational, quality-related and social outcomes, and recognises regulatory policies, technological infrastructure, stakeholder cooperation and innovation readiness as key facilitators of AI adoption. Lastly, we stress the importance of addressing challenges such as data privacy, security, generalisability and algorithmic bias. Our findings are relevant beyond the pandemic in facilitating the development of AI-related policy interventions and support mechanisms for building resilient healthcare sector that can withstand future challenges."
Ethical Considerations and Fairness in the Use of Artificial Intelligence for Neuroradiology,2023,Christopher G. Filippi; Joel M. Stein; Zihao Wang; Spyridon Bakas; Yichuan Liu; Peter Chang; Yvonne W. Lui; Christopher P. Hess; Daniel P. Barboriak; Adam E. Flanders; Max Wintermark; Greg Zaharchuk; Ona Wu,American Journal of Neuroradiology,13,W4386324990,10.3174/ajnr.a7963,https://openalex.org/W4386324990,https://www.ajnr.org/content/ajnr/early/2023/08/31/ajnr.A7963.full.pdf,Neuroradiology; Software deployment; Workflow; Accountability; Harm,review,True,"<h3>SUMMARY:</h3> In this review, concepts of algorithmic bias and fairness are defined qualitatively and mathematically. Illustrative examples are given of what can go wrong when unintended bias or unfairness in algorithmic development occurs. The importance of explainability, accountability, and transparency with respect to artificial intelligence algorithm development and clinical deployment is discussed. These are grounded in the concept of ""primum no nocere"" (first, do no harm). Steps to mitigate unfairness and bias in task definition, data collection, model definition, training, testing, deployment, and feedback are provided. Discussions on the implementation of fairness criteria that maximize benefit and minimize unfairness and harm to neuroradiology patients will be provided, including suggestions for neuroradiologists to consider as artificial intelligence algorithms gain acceptance into neuroradiology practice and become incorporated into routine clinical workflow."
Real Estate Insights: Is the AI revolution a real estate boon or bane?,2023,Philip Seagraves,Journal of Property Investment and Finance,20,W4383105045,10.1108/jpif-05-2023-0045,https://openalex.org/W4383105045,,Real estate; Valuation (finance); Automation; Computer science; Artificial intelligence,article,False,"Purpose The paper aims to provide a comprehensive analysis of artificial intelligence’s (AI) transformative impact on the real estate industry. By examining various AI applications, from property recommendations to compliance automation, this study highlights potential benefits such as increased accuracy and efficiency. At the same time, this study critically discusses potential drawbacks, like privacy concerns and job displacement. The paper's goal is to offer valuable insights to industry professionals and policy makers, aiding strategic decision-making as AI continues to reshape the landscape of the real estate sector. Design/methodology/approach This paper employs an extensive literature review, combined with a qualitative analysis of case studies. Various AI applications in the real estate industry are examined, including machine learning for property recommendations and valuation, VR/AR property tours, AI automation for contract and regulatory compliance, and chatbots for customer service. The study also delves into the optimisation potential of AI in building management, lead generation, and risk assessment, whilst critically discussing potential challenges such as data privacy, algorithmic bias, and job displacement. The outcomes aim to inform strategic decisions for industry professionals and policy makers. Findings The study finds that AI has significant potential to revolutionise the real estate industry through enhanced accuracy in property valuation, efficient automation and immersive AR/VR experiences. AI-driven chatbots and optimisation in building management also hold promise. However, this study also uncovers potential challenges, including data privacy issues, algorithmic biases, and possible job displacement due to increased automation. The insights gleaned from this study underscore the importance of strategic decision-making in harnessing the benefits of AI while mitigating potential drawbacks in the real estate sector. Practical implications The paper's practical implications extend to industry professionals, policy makers, and technology developers. Professionals gain insights into how AI can enhance efficiency and accuracy in the real estate sector, guiding strategic decision-making. For policy makers, understanding potential challenges like data privacy and job displacement informs regulatory measures. Technology developers can also benefit from understanding the sector-specific applications and concerns raised. Additionally, highlighting the need for addressing algorithmic bias and privacy concerns in AI systems may foster better design practices. Therefore, the paper's findings could significantly shape the future trajectory of AI integration in real estate. Originality/value The paper provides original value by offering a comprehensive analysis of the transformative impact of AI in the real estate industry. Its multi-faceted examination of AI applications, coupled with a critical discussion on potential challenges, provides a balanced perspective. The paper's focus on informing strategic decisions for professionals and policy makers makes it a valuable resource. Moreover, by considering both benefits and drawbacks, this study contributes to the discourse on AI's broader societal implications. In the context of rapid technological change, such comprehensive studies are rare, adding to the paper's originality."
"The Transformative Role of Artificial Intelligence in Dentistry: A Comprehensive Overview Part 2: The Promise and Perils, and the International Dental Federation Communique",2025,Nozimjon Tuygunov; Lakshman P. Samaranayake; Zohaib Khurshid; Paak Rewthamrongsris; Falk Schwendicke; Thanaphum Osathanon; Noor Azlin Yahya,International Dental Journal,33,W4407932918,10.1016/j.identj.2025.02.006,https://openalex.org/W4407932918,https://doi.org/10.1016/j.identj.2025.02.006,Transformative learning; Dentistry; Political science; Medicine; Engineering ethics,review,True,"In the final part of this two part article on artificial intelligence (AI) in dentistry we review its transformative role, focusing on AI in dental education, patient communications, challenges of integration, strategies to overcome barriers, ethical considerations, and finally, the recently released International Dental Federation (FDI) Communique (white paper) on AI in Dentistry. AI in dental education is highlighted for its potential in enhancing theoretical and practical dimensions, including patient telemonitoring and virtual training ecosystems. Challenges of AI integration in dentistry are outlined, such as data availability, bias, and human accountability. Strategies to overcome these challenges include promoting AI literacy, establishing regulations, and focusing on specific AI implementations. Ethical considerations in AI integration within dentistry, such as patient privacy and algorithm bias, are emphasized. The need for clear guidelines and ongoing evaluation of AI systems is crucial. The FDI White Paper on AI in Dentistry provides insights into the significance of AI in oral care, dental education, and research, along with standards for governance. It discusses AI's impact on individual patients, community health, dental education, and research. The paper addresses biases, limited generalizability, accessibility, and regulatory requirements for AI in dental practice. In conclusion, AI plays a significant role in modern dental care, offering benefits in diagnosis, treatment planning, and decision-making. While facing challenges, strategic initiatives focusing on AI literacy, regulations, and targeted implementations can help overcome barriers and maximize the potential of AI in dentistry. Ethical considerations and ongoing evaluation are essential for ensuring responsible, effective and efficacious deployment of AI technologies in dental ecosystem."
Mitigating the risk of artificial intelligence bias in cardiovascular care,2024,Ariana Mihan; Ambarish Pandey; Harriette G.C. Van Spall,The Lancet Digital Health,18,W4401983318,10.1016/s2589-7500(24)00155-9,https://openalex.org/W4401983318,https://doi.org/10.1016/s2589-7500(24)00155-9,Computer science; Artificial intelligence; Risk analysis (engineering); Medicine,review,True,"Digital health technologies can generate data that can be used to train artificial intelligence (AI) algorithms, which have been particularly transformative in cardiovascular health-care delivery. However, digital and health-care data repositories that are used to train AI algorithms can introduce bias when data are homogeneous and health-care processes are inequitable. AI bias can also be introduced during algorithm development, testing, implementation, and post-implementation processes. The consequences of AI algorithmic bias can be considerable, including missed diagnoses, misclassification of disease, incorrect risk prediction, and inappropriate treatment recommendations. This bias can disproportionately affect marginalised demographic groups. In this Series paper, we provide a brief overview of AI applications in cardiovascular health care, discuss stages of algorithm development and associated sources of bias, and provide examples of harm from biased algorithms. We propose strategies that can be applied during the training, testing, and implementation of AI algorithms to mitigate bias so that all those at risk for or living with cardiovascular disease might benefit equally from AI."
Tutorials at The Web Conference 2023,2023,Valeria Fionda; Olaf Hartig; Reyhaneh Abdolazimi; Sihem Amer-Yahia; Hongzhi Chen; Xiao Chen; Peng Cui; Jeff Dalton; Xin Luna Dong; Lisette Espín-Noboa; Wenqi Fan; Manuela Fritz; Quan Gan; Jingtong Gao; Xiaojie Guo; Torsten Hahmann; Jiawei Han; Soyeon Caren Han; Estevam R. Hruschka; Liang Hu; Jiaxin Huang; Utkarshani Jaimini; Olivier Jeunen; Yushan Jiang; Fariba Karimi; George Karypis; Krishnaram Kenthapadi; Himabindu Lakkaraju; Hady W. Lauw; Thai Le; Trung-Hoang Le; Dongwon Lee; Geon Lee; Liat Levontin; Cheng–Te Li; Haoyang Li; Ying Li; Jay Chiehen Liao; Qidong Liu; Usha Lokala; Ben London; Siqu Long; Hande Küçük Mcginty; Meng Yu; Seungwhan Moon; Usman Naseem; Pradeep Natarajan; Behrooz Omidvar-Tehrani; Zijie Pan; Devesh Parekh; Jian Pei; Tiago P. Peixoto; Steven Pemberton; Josiah Poon; Filip Radlinski; Federico Rossetto; Kaushik Roy; Aghiles Salah; Mehrnoosh Sameki; Amit Sheth; Cogan Shimizu; Kijung Shin; Dongjin Song; Julia Stoyanovich; Dacheng Tao; Johanne R. Trippas; Quoc Truong; Yu-Che Tsai; Adaku Uchendu; Bram van den Akker; Lin Wang; Minjie Wang; Shoujin Wang; Xin Wang; Ingmar Weber; Henry Weld; Lingfei Wu; Da Xu; Yifan Ethan Xu; Shuyuan Xu; Bo Yang; Ke Yang; Elad Yom‐Tov; Jaemin Yoo; Zhou Yu; Reza Zafarani; Hamed Zamani; Meike Zehlike; Qi Zhang; Xikun Zhang; Yongfeng Zhang; Yu Zhang; Zheng Zhang; Liang Zhao; Xiangyu Zhao; Wenwu Zhu,,23,W4367310414,10.1145/3543873.3587713,https://openalex.org/W4367310414,https://dl.acm.org/doi/pdf/10.1145/3543873.3587713,Computer science; World Wide Web,preprint,True,"Social networks have been widely studied over the last century from multiple disciplines to understand societal issues such as inequality in employment rates, managerial performance, and epidemic spread. Today, these and many more issues can be studied at global scale thanks to the digital footprints that we generate when browsing the Web or using social media platforms. Unfortunately, scientists often struggle to access to such data primarily because it is proprietary, and even when it is shared with privacy guarantees, such data is either no representative or too big. In this tutorial, we will discuss recent advances and future directions in network modeling. In particular, we focus on how to exploit synthetic networks to study real-world problems such as data privacy, spreading dynamics, algorithmic bias, and ranking inequalities. We start by reviewing different types of generative models for social networks including node-attributed and scale-free networks. Then, we showcase how to perform a network selection analysis to characterize the mechanisms of edge formation of any given real-world network."
A Novel Bias-TSP Algorithm for Maritime Patrol,2023,Geraldo Mulato de Lima Filho; Angelo Pássaro; Guilherme Moura Delfino; Herman Monsuur,IEEE Access,1,W4323065993,10.1109/access.2023.3252013,https://openalex.org/W4323065993,https://ieeexplore.ieee.org/ielx7/6287639/6514899/10058498.pdf,Computer science; Algorithm,article,True,"This work aims to develop a search planning strategy to be used by a drone equipped with an inverse synthetic-aperture radar (ISAR) and an electro-optical sensor. After describing the specifics of our maritime scenario, we discuss four methodologies that can be used to find vessels involved in illegal fishing activities as quickly as possible. In addition to the clustering of the vessels, determined by the drone's electro-optical sensor range, we introduce a novel technique to bias a traveling salesman problem (TSP) tour. This bias is based on deliberately increasing distances to vessels that are classified as probable fishing vessels. This increase in distance is meant to prioritize visits to probable fishing vessels. Vessels are classified based on length using the. The classification result and the vessel clustering are available before the actual planning of the tour. Simulations of scenarios in which we have a few vessels fishing illegally show that the novel technique, the bias-TSP, combined with a tour orientation based on operational considerations, outperforms the classic TSP: the mean distance traveled to find all the vessels involved in illegal fishing activities is reduced by at least 35–50%. We also show that different drone take-off locations significantly impact the results."
Ortho-Heterodox Biases and the Economist Algorithms of ChatGPT,2023,Oz Iazdi,Iberian Journal of the History of Economic Thought,1,W4390782957,10.5209/ijhe.91545,https://openalex.org/W4390782957,https://revistas.ucm.es/index.php/IJHE/article/download/91545/4564456567998,Heterodoxy; Impartiality; Orthodoxy; Positive economics; Perception,article,True,"Recommendations for economic policies can be based on different theoretical perspectives and may present hidden biases. Identifying these biases is challenging when they are embedded in recommendations from sources with high technological and social disruptive potential, where a good level of impartiality is expected, such as contemporary large language models. Thus, a questionnaire was administered to economists affiliated with the Brazilian academic community to assess their perception of orthodox/heterodox biases in economic policy recommendations derived from interactions with ChatGPT. The results showed that: i) there is still no consensus on the concepts of orthodoxy and heterodoxy in Brazil; ii) there are indications of a positive relationship between how self-proclaimed heterodox (orthodox) an economist is and how heterodox (orthodox) the perceived bias in an economic policy is; iii) it was not possible to identify a consistently orthodox or heterodox bias in ChatGPT's recommendations, which exhibited a good degree of impartiality."
"Justice, trust, and moral judgements when personnel selection is supported by algorithms",2023,Tina Feldkamp; Markus Langer; Leo Wies; Cornelius J. König,European Journal of Work and Organizational Psychology,18,W4321458265,10.1080/1359432x.2023.2169140,https://openalex.org/W4321458265,,Judgement; Psychology; Perception; Social psychology; Selection (genetic algorithm),article,False,"Although algorithm-based systems are increasingly used as a decision-support for managers, there is still a lack of research on the effects of algorithm use and more specifically on potential algorithmic bias on decision-makers. To investigate how potential social bias in a recommendation outcome influences trust, fairness perceptions, and moral judgement, we used a moral dilemma scenario. Participants (N = 215) imagined being human resource managers responsible for personnel selection and receiving decision-support from either human colleagues or an algorithm-based system. They received an applicant preselection that was either gender-balanced or predominantly male. Although participants perceived algorithm-based support as less biased, they also perceived it as generally less fair and had less trust in it. This could be related to the finding that participants perceived algorithm-based systems as more consistent but also as less likely to uphold moral standards. Moreover, participants tended to reject algorithm-based preselection more often than human-based and were more likely to use utilitarian judgements when accepting it, which may indicate different underlying moral judgement processes."
A biased random-key genetic algorithm for the knapsack problem with forfeit sets,2024,Raffaele Cerulli; Ciriaco D’Ambrosio; Andrea Raiconi,Soft Computing,1,W4401166730,10.1007/s00500-024-09948-w,https://openalex.org/W4401166730,https://doi.org/10.1007/s00500-024-09948-w,Knapsack problem; Continuous knapsack problem; Key (lock); Genetic algorithm; Computer science,article,True,"Abstract This work addresses the Knapsack Problem with Forfeit Sets, a recently introduced variant of the 0/1 Knapsack Problem considering subsets of items associated with contrasting choices. Some penalty costs need to be paid whenever the number of items in the solution belonging to a forfeit set exceeds a predefined allowance threshold. We propose an effective metaheuristic to solve the problem, based on the Biased Random-Key Genetic Algorithm paradigm. An appropriately designed decoder function assigns a feasible solution to each chromosome, and improves it using some additional heuristic procedures. We show experimentally that the algorithm outperforms significantly a previously introduced metaheuristic for the problem."
Emotion-aware music tower blocks (EmoMTB ): an intelligent audiovisual interface for music discovery and recommendation,2023,Alessandro B. Melchiorre; David Penz; Christian Ganhör; Oleg Lesota; Vasco Fragoso; Florian Fritzl; Emilia Parada‐Cabaleiro; Franz Schubert; Markus Schedl,International Journal of Multimedia Information Retrieval,30,W4379230707,10.1007/s13735-023-00275-8,https://openalex.org/W4379230707,https://link.springer.com/content/pdf/10.1007/s13735-023-00275-8.pdf,Computer science; Recommender system; Active listening; Popularity; Set (abstract data type),article,True,"Abstract Music listening has experienced a sharp increase during the last decade thanks to music streaming and recommendation services. While they offer text-based search functionality and provide recommendation lists of remarkable utility, their typical mode of interaction is unidimensional, i.e., they provide lists of consecutive tracks, which are commonly inspected in sequential order by the user. The user experience with such systems is heavily affected by cognition biases (e.g., position bias, human tendency to pay more attention to first positions of ordered lists) as well as algorithmic biases (e.g., popularity bias, the tendency of recommender systems to overrepresent popular items). This may cause dissatisfaction among the users by disabling them to find novel music to enjoy. In light of such systems and biases, we propose an intelligent audiovisual music exploration system named EmoMTB . It allows the user to browse the entirety of a given collection in a free nonlinear fashion. The navigation is assisted by a set of personalized emotion-aware recommendations, which serve as starting points for the exploration experience. EmoMTB adopts the metaphor of a city, in which each track (visualized as a colored cube) represents one floor of a building. Highly similar tracks are located in the same building; moderately similar ones form neighborhoods that mostly correspond to genres. Tracks situated between distinct neighborhoods create a gradual transition between genres. Users can navigate this music city using their smartphones as control devices. They can explore districts of well-known music or decide to leave their comfort zone. In addition, EmoMTB integrates an emotion-aware music recommendation system that re-ranks the list of suggested starting points for exploration according to the user’s self-identified emotion or the collective emotion expressed in EmoMTB ’s Twitter channel. Evaluation of EmoMTB has been carried out in a threefold way: by quantifying the homogeneity of the clustering underlying the construction of the city, by measuring the accuracy of the emotion predictor, and by carrying out a web-based survey composed of open questions to obtain qualitative feedback from users."
"Artificial Intelligence in Head and Neck Cancer: Innovations, Applications, and Future Directions",2024,Tuan D. Pham; Muy‐Teck Teh; Domniki Chatzopoulou; Simon Holmes; Paul Coulthard,Current Oncology,27,W4402314224,10.3390/curroncol31090389,https://openalex.org/W4402314224,https://doi.org/10.3390/curroncol31090389,Artificial intelligence; Applications of artificial intelligence; Medicine; Precision medicine; Deep learning,review,True,"Artificial intelligence (AI) is revolutionizing head and neck cancer (HNC) care by providing innovative tools that enhance diagnostic accuracy and personalize treatment strategies. This review highlights the advancements in AI technologies, including deep learning and natural language processing, and their applications in HNC. The integration of AI with imaging techniques, genomics, and electronic health records is explored, emphasizing its role in early detection, biomarker discovery, and treatment planning. Despite noticeable progress, challenges such as data quality, algorithmic bias, and the need for interdisciplinary collaboration remain. Emerging innovations like explainable AI, AI-powered robotics, and real-time monitoring systems are poised to further advance the field. Addressing these challenges and fostering collaboration among AI experts, clinicians, and researchers is crucial for developing equitable and effective AI applications. The future of AI in HNC holds significant promise, offering potential breakthroughs in diagnostics, personalized therapies, and improved patient outcomes."
Using Regularization to Identify Measurement Bias Across Multiple Background Characteristics: A Penalized Expectation–Maximization Algorithm,2024,William C. M. Belzak; Daniel J. Bauer,Journal of Educational and Behavioral Statistics,3,W4391532052,10.3102/10769986231226439,https://openalex.org/W4391532052,,Expectation–maximization algorithm; Algorithm; Regularization (linguistics); Computer science; Mathematics,article,False,"Testing for differential item functioning (DIF) has undergone rapid statistical developments recently. Moderated nonlinear factor analysis (MNLFA) allows for simultaneous testing of DIF among multiple categorical and continuous covariates (e.g., sex, age, ethnicity, etc.), and regularization has shown promising results for identifying DIF among many covariates. However, computationally inefficient estimation methods have hampered practical use of the regularized MNFLA method. We develop a penalized expectation–maximization (EM) algorithm with soft- and firm-thresholding to more efficiently estimate regularized MNLFA parameters. Simulation and empirical results show that, compared to previous implementations of regularized MNFLA, the penalized EM algorithm is faster, more flexible, and more statistically principled. This method also yields similar recovery of DIF relative to previous implementations, suggesting that regularized DIF detection remains a preferred approach over traditional methods of identifying DIF."
Ethical and regulatory considerations in the use of AI and machine learning in nursing: A systematic review,2025,Suheb Mohammed; Yasmine M Osman; Ateya Megahed Ibrahim; Mostafa Shaban,International Nursing Review,13,W4408244061,10.1111/inr.70010,https://openalex.org/W4408244061,https://doi.org/10.1111/inr.70010,Transparency (behavior); Safeguarding; Autonomy; Accountability; Nursing,review,False,"This study systematically explores the ethical and regulatory considerations surrounding the integration of artificial intelligence (AI) and machine learning (ML) in nursing practice, with a focus on patient autonomy, data privacy, algorithmic bias, and accountability. AI and ML are transforming nursing practice by enhancing clinical decision-making and operational efficiency. However, these technologies present significant ethical challenges related to ensuring patient autonomy, safeguarding data privacy, mitigating algorithmic bias, and ensuring transparency in decision-making processes. Current frameworks are not sufficiently tailored to nursing-specific contexts. A systematic review was conducted, adhering to PRISMA guidelines. Six major databases were searched for studies published between 2000 and 2024. Seventeen studies met the inclusion criteria and were included in the final analysis. Five key themes emerged from the review: enhancement of clinical decision-making, promotion of ethical awareness, support for routine nursing tasks, challenges in algorithmic bias, and the importance of public engagement in regulatory frameworks. The review identified critical gaps in nursing-specific ethical guidelines and regulatory oversight for AI integration in practice. AI technologies offer substantial benefits for nursing, particularly in decision-making and task efficiency. However, these advantages must be balanced against ethical concerns, including the protection of patient rights, algorithmic transparency, and bias mitigation. Current regulatory frameworks require adaptation to meet the ethical needs of nursing. The findings emphasize the need for the development of nursing-specific ethical guidelines and robust regulatory frameworks to ensure the responsible integration of AI technologies into nursing practice. AI integration must uphold ethical principles while enhancing the quality of care."
Machine learning software for optimizing SME social media marketing campaigns,2024,Wagobera Edgar Kedi; Chibundom Ejimuda; Courage Idemudia; Tochukwu Ignatius Ijomah,Computer Science & IT Research Journal,24,W4400999562,10.51594/csitrj.v5i7.1349,https://openalex.org/W4400999562,https://fepbl.com/index.php/csitrj/article/download/1349/1581,Social media; Business; Social media marketing; Software; Marketing,article,True,"This review paper explores the transformative role of machine learning in optimizing social media marketing strategies for small and medium-sized enterprises (SMEs). It begins by highlighting the significance of social media marketing for SMEs, outlining the historical context of traditional marketing strategies, and examining current trends and emerging machine learning applications. The paper delves into the technical challenges of implementing machine learning, such as data quality, algorithm complexity, and system integration, as well as ethical concerns surrounding data privacy and algorithmic bias. SME-specific limitations are also discussed, including budget constraints and lack of technical expertise. Future directions focus on emerging technologies like deep learning and reinforcement learning, offering practical recommendations for SMEs to leverage these advancements effectively. The conclusion emphasizes the importance of embracing machine learning to achieve sustainable growth and competitive advantage in the digital marketplace. Keywords: Machine Learning, Social Media Marketing, SMEs, Data Privacy, Audience Targeting."
Targeting Machine Learning and Artificial Intelligence Algorithms in Health Care to Reduce Bias and Improve Population Health,2024,Thelma C. Hurd; Fay Cobb Payton; Darryl B. Hood,Milbank Quarterly,3,W4401417168,10.1111/1468-0009.12712,https://openalex.org/W4401417168,https://doi.org/10.1111/1468-0009.12712,Population health; Health care; Software deployment; Artificial intelligence; Health policy,article,True,"Policy Points Artificial intelligence (AI) is disruptively innovating health care and surpassing our ability to define its boundaries and roles in health care and regulate its application in legal and ethical ways. Significant progress has been made in governance in the United States and the European Union. It is incumbent on developers, end users, the public, providers, health care systems, and policymakers to collaboratively ensure that we adopt a national AI health strategy that realizes the Quintuple Aim; minimizes race‐based medicine; prioritizes transparency, equity, and algorithmic vigilance; and integrates the patient and community voices throughout all aspects of AI development and deployment."
"<scp>ChatGPT</scp> and consumers: Benefits, Pitfalls and Future Research Agenda",2023,Justin Paul; Akiko Ueno; Charles Dennis,International Journal of Consumer Studies,255,W4360945746,10.1111/ijcs.12928,https://openalex.org/W4360945746,,Novelty; Misinformation; Personalization; Marketing; Context (archaeology),article,False,"Abstract The need of the hour is to encourage research on topics with newness and novelty. In this context, this article discusses multidimensional benefits and potential pitfalls of using artificial intelligence‐based Chat Generative Pre‐trained Transformer (ChatGPT), and provides numerous ideas for future research in consumer studies and marketing in the context of ChatGPT. ChatGPT provides algorithm‐generated conversational responses to text‐based prompts. Since its launch in the late 2022, ChatGPT has generated significant debate surrounding its hallmarks, benefits and potential pitfalls. On the one hand, ChatGPT can offer enhanced consumer engagement, improved customer service, personalization and shopping, social interaction and communication practice, cost‐effectiveness, insights into consumer behaviour and improved marketing campaigns. On the other hand, potential pitfalls include concerns about consumer well‐being, bias, misinformation, lack of context, privacy concerns, ethical considerations and security. The article concludes by outlining a potential future research agenda in the area of ChatGPT and consumer studies. Overall, this article provides valuable insights into the benefits and challenges associated with ChatGPT, shedding light on its potential applications and the need for further research."
Gender bias perpetuation and mitigation in AI technologies: challenges and opportunities,2023,Sinead O’Connor; Helen K. Liu,AI & Society,72,W4382293074,10.1007/s00146-023-01675-4,https://openalex.org/W4382293074,https://link.springer.com/content/pdf/10.1007/s00146-023-01675-4.pdf,Software deployment; Accountability; Emerging technologies; Categorization; Neutrality,article,True,"Abstract Across the world, artificial intelligence (AI) technologies are being more widely employed in public sector decision-making and processes as a supposedly neutral and an efficient method for optimizing delivery of services. However, the deployment of these technologies has also prompted investigation into the potentially unanticipated consequences of their introduction, to both positive and negative ends. This paper chooses to focus specifically on the relationship between gender bias and AI, exploring claims of the neutrality of such technologies and how its understanding of bias could influence policy and outcomes. Building on a rich seam of literature from both technological and sociological fields, this article constructs an original framework through which to analyse both the perpetuation and mitigation of gender biases, choosing to categorize AI technologies based on whether their input is text or images. Through the close analysis and pairing of four case studies, the paper thus unites two often disparate approaches to the investigation of bias in technology, revealing the large and varied potential for AI to echo and even amplify existing human bias, while acknowledging the important role AI itself can play in reducing or reversing these effects. The conclusion calls for further collaboration between scholars from the worlds of technology, gender studies and public policy in fully exploring algorithmic accountability as well as in accurately and transparently exploring the potential consequences of the introduction of AI technologies."
Metrics and Algorithms for Identifying and Mitigating Bias in AI Design: A Counterfactual Fairness Approach,2025,D. H. Moon; Seongjin Ahn,IEEE Access,2,W4409014323,10.1109/access.2025.3556082,https://openalex.org/W4409014323,https://doi.org/10.1109/access.2025.3556082,Counterfactual thinking; Computer science; Algorithm; Artificial intelligence; Machine learning,article,True,
Unraveling the Ethical Enigma: Artificial Intelligence in Healthcare,2023,Madhan Jeyaraman; Sangeetha Balaji; Naveen Jeyaraman; Sankalp Yadav,Cureus,159,W4385724150,10.7759/cureus.43262,https://openalex.org/W4385724150,https://assets.cureus.com/uploads/review_article/pdf/178557/20230810-7795-1otiewn.pdf,Health care; Transparency (behavior); Accountability; Transformative learning; Big data,review,True,"The integration of artificial intelligence (AI) into healthcare promises groundbreaking advancements in patient care, revolutionizing clinical diagnosis, predictive medicine, and decision-making. This transformative technology uses machine learning, natural language processing, and large language models (LLMs) to process and reason like human intelligence. OpenAI's ChatGPT, a sophisticated LLM, holds immense potential in medical practice, research, and education. However, as AI in healthcare gains momentum, it brings forth profound ethical challenges that demand careful consideration. This comprehensive review explores key ethical concerns in the domain, including privacy, transparency, trust, responsibility, bias, and data quality. Protecting patient privacy in data-driven healthcare is crucial, with potential implications for psychological well-being and data sharing. Strategies like homomorphic encryption (HE) and secure multiparty computation (SMPC) are vital to preserving confidentiality. Transparency and trustworthiness of AI systems are essential, particularly in high-risk decision-making scenarios. Explainable AI (XAI) emerges as a critical aspect, ensuring a clear understanding of AI-generated predictions. Cybersecurity becomes a pressing concern as AI's complexity creates vulnerabilities for potential breaches. Determining responsibility in AI-driven outcomes raises important questions, with debates on AI's moral agency and human accountability. Shifting from data ownership to data stewardship enables responsible data management in compliance with regulations. Addressing bias in healthcare data is crucial to avoid AI-driven inequities. Biases present in data collection and algorithm development can perpetuate healthcare disparities. A public-health approach is advocated to address inequalities and promote diversity in AI research and the workforce. Maintaining data quality is imperative in AI applications, with convolutional neural networks showing promise in multi-input/mixed data models, offering a comprehensive patient perspective. In this ever-evolving landscape, it is imperative to adopt a multidimensional approach involving policymakers, developers, healthcare practitioners, and patients to mitigate ethical concerns. By understanding and addressing these challenges, we can harness the full potential of AI in healthcare while ensuring ethical and equitable outcomes."
BrkgaCuda 2.0: a framework for fast biased random-key genetic algorithms on GPUs,2024,Bruno Oliveira; Eduardo C. Xavier; Edson Borin,Soft Computing,2,W4404695137,10.1007/s00500-024-10336-7,https://openalex.org/W4404695137,,Computer science; Key (lock); Parallel computing; Genetic algorithm; Algorithm,article,False,
Hybrid-biased genetic algorithm for packing unequal rectangles into a fixed-size circle,2024,Qiang Luo; Yunqing Rao; Piaoruo Yang; Xusheng Zhao,Computers & Operations Research,2,W4399207519,10.1016/j.cor.2024.106716,https://openalex.org/W4399207519,,Circle packing; Combinatorics; Mathematics; Algorithm; Genetic algorithm,article,False,
Distributed localization algorithm for wireless sensor networks under minuscule bias injection attacks,2024,Xinming Chen; Ya Wang; Lei Shi; Jinliang Shao,,1,W4392382798,10.1117/12.3014916,https://openalex.org/W4392382798,,Wireless sensor network; Computer science; Wireless; Algorithm; Distributed algorithm,article,False,"In this paper, the distributed localization problem of wireless sensor networks under minuscule bias injection attacks is studied. It is assumed that the attacker launches minuscule bias injection attacks on the distance measurement between sensors during the localization process of wireless sensor network, and the accumulated bias value has a great impact on the localization accuracy. To solve the above problems, a distributed localization algorithm based on adjacent detection and numerical fitting is proposed in this paper. The proposed algorithm realizes the detection of minuscule bias injection attacks by adjacent detection and improves the localization accuracy of the algorithm by numerical fitting. Finally, the correctness and superiority of the proposed distributed localization algorithm are verified by simulation experiment."
Social bias in artificial intelligence algorithms designed to improve cardiovascular risk assessment relative to the Framingham Risk Score: a protocol for a systematic review,2023,Ivneet Garcha; Susan P. Phillips,BMJ Open,2,W4378908275,10.1136/bmjopen-2022-067638,https://openalex.org/W4378908275,https://bmjopen.bmj.com/content/bmjopen/13/5/e067638.full.pdf,Medicine; Framingham Risk Score; Risk assessment; MEDLINE; Coronary artery disease,review,True,"Introduction Cardiovascular disease (CVD) prevention relies on timely identification of and intervention for individuals at risk. Risk assessment models such as the Framingham Risk Score (FRS) have been shown to over-estimate or under-estimate risk in certain groups, such as socioeconomically disadvantaged populations. Artificial intelligence (AI) and machine learning (ML) could be used to address such equity gaps to improve risk assessment; however, critical appraisal is warranted before ML-informed clinical decision-making is implemented. Methods and analysis This study will employ an equity-lens to identify sources of bias (ie, race/ethnicity, gender and social stratum) in ML algorithms designed to improve CVD risk assessment relative to the FRS. A comprehensive literature search will be completed using MEDLINE, Embase and IEEE to answer the research question: do AI algorithms that are designed for the estimation of CVD risk and that compare performance with the FRS address the sources of bias inherent in the FRS? No study date filters will be imposed on the search, but English language filters will be applied. Studies describing a specific algorithm or ML approach that provided a risk assessment output for coronary artery disease, heart failure, cardiac arrhythmias (ie, atrial fibrillation), stroke or a global CVD risk score, and that compared performance with the FRS are eligible for inclusion. Papers describing algorithms for the diagnosis rather than the prevention of CVD will be excluded. A structured narrative review analysis of included studies will be completed. Ethics and dissemination Ethics approval was not required. Ethics exemption was formally received from the General Research Ethics Board at Queen’s University. The completed systematic review will be submitted to a peer-reviewed journal and parts of the work will be presented at relevant conferences."
"AM–GM Algorithm for Evaluating, Analyzing, and Correcting the Spatial Scaling Bias of the Leaf Area Index",2023,Jingyu Zhang; Rui Sun; Zhiqiang Xiao; Liang Zhao; Donghui Xie,Remote Sensing,1,W4380633039,10.3390/rs15123068,https://openalex.org/W4380633039,https://www.mdpi.com/2072-4292/15/12/3068/pdf?version=1686566171,Scaling; Leaf area index; Logarithm; Algorithm; Scale (ratio),article,True,"The leaf area index (LAI) is a crucial variable in climate, ecological, and land surface modeling. However, the estimation of the LAI from coarse-resolution remote sensing data can be affected by the spatial scaling bias, which arises from the nonlinearity of retrieval models and the heterogeneity of the land surface. This study provides an algorithm named Arithmetic Mean and Geometric Mean (AM–GM) to correct the spatial scaling bias. It is established based on negative logarithmic functions and avoids second-order stationarity. In this algorithm, relationships are derived between the scaling bias of LAI and the arithmetic and geometric means of directional gap probability for two commonly used remote sensing models, the Beer–Lambert law and a semi-empirical transfer function, respectively. According to the AM–GM algorithm, the expression representing the model nonlinearity is derived and utilized for the analysis of LAI scaling bias. Furthermore, the AM–GM algorithm is simplified by a linear relationship, which is constructed between two quantities related to the directional gap probability between two specific resolutions. Two scenes simulated by the LargE-Scale remote sensing data and image Simulation framework (LESS) model and three sites are used to evaluate the proposed algorithm and analyze the scaling bias of LAI. The validation results show that the AM–GM algorithm provides accurate correction of LAI scaling bias. The analyses based on the AM–GM algorithm demonstrate that the scaling bias of LAI increases with the increase in the LAI value, with stronger surface heterogeneity and coarser spatial resolution. The validation results of the simplified AM–GM algorithm demonstrate that at the Sud-Ouest site, the absolute value of the bias for the estimated LAI decreases from 0.10, 0.22, 0.29, and 0.31 to 0.04, 0.01, 0.04, and 0.05 at 200 m, 500 m, 1000 m, and 1500 m resolutions, respectively. In conclusion, the proposed algorithm is effective in the analysis and correction of the scaling bias for coarse-resolution LAI."
Drop the shortcuts: image augmentation improves fairness and decreases AI detection of race and other demographics from medical images,2024,Ryan Wang; Po‐Chih Kuo; Li-Ching Chen; Kenneth P. Seastedt; Judy Wawira Gichoya; Leo Anthony Celi,EBioMedicine,10,W4392672760,10.1016/j.ebiom.2024.105047,https://openalex.org/W4392672760,http://www.thelancet.com/article/S2352396424000823/pdf,Demographics; Medicine; Magnetic resonance imaging; Artificial intelligence; Computer science,article,True,"It has been shown that AI models can learn race on medical images, leading to algorithmic bias. Our aim in this study was to enhance the fairness of medical image models by eliminating bias related to race, age, and sex. We hypothesise models may be learning demographics via shortcut learning and combat this using image augmentation."
An Improved Doppler-Aided Smoothing Code Algorithm for Bds-2/Bds-3 Un-geo Satellites in Consideration of Satellite Code Bias,2023,Gao Xiao; Zongfang Ma; Luxiao Jia; Lin Pan,,2,W4379387128,10.20944/preprints202306.0160.v1,https://openalex.org/W4379387128,https://doi.org/10.20944/preprints202306.0160.v1,Algorithm; Smoothing; GNSS applications; Code (set theory); Computer science,preprint,True,"The extensive use of carrier-aided smoothing code (CSC) filter has led to reduce the noise level of raw code measurements in GNSS positioning and navigation applications. However, the existing CSC technique is sensitive to the changes of the integer ambiguity and then the smoothing proce-dure needs to be restarted in the presence of cycle-slips. As the Doppler shift is instantaneous ob-servation and immune to cycle-slips, Doppler-aided smoothing code (DSC) algorithm would be more promising in challenged environment. Based on the Hatch-filter, an optimal DSC approach is proposed with the principle of minimum variance. Meanwhile, to inhibit the effect of integral cumulative error of Doppler, a balance factor is adopted to adjust the contributions of raw code and DSC. The noise level of code observable is not only affected by thermal noise, but also limited by systematic bias. Satellite code bias (SCB) has been identified in the raw code observable on each frequency for each BDS-2 satellite. By minimizing the sum of absolute value of residuals, polyno-mial segment fitting algorithm as a function of elevation-angles is applied to establish the SCB cor-rection model based on epoch-differenced Multipath (MP) deviations. Finally, numerical experi-ments demonstrate the validity and efficiency of the refined DSC filter with SCB corrections on each available frequency for BDS-2 un-GEO satellites."
"Harnessing the power of synthetic data in healthcare: innovation, application, and privacy",2023,Mauro Giuffré; Dennis Shung,npj Digital Medicine,195,W4387457231,10.1038/s41746-023-00927-3,https://openalex.org/W4387457231,https://www.nature.com/articles/s41746-023-00927-3.pdf,Big data; Distrust; Accountability; Context (archaeology); Health care,review,True,"Abstract Data-driven decision-making in modern healthcare underpins innovation and predictive analytics in public health and clinical research. Synthetic data has shown promise in finance and economics to improve risk assessment, portfolio optimization, and algorithmic trading. However, higher stakes, potential liabilities, and healthcare practitioner distrust make clinical use of synthetic data difficult. This paper explores the potential benefits and limitations of synthetic data in the healthcare analytics context. We begin with real-world healthcare applications of synthetic data that informs government policy, enhance data privacy, and augment datasets for predictive analytics. We then preview future applications of synthetic data in the emergent field of digital twin technology. We explore the issues of data quality and data bias in synthetic data, which can limit applicability across different applications in the clinical context, and privacy concerns stemming from data misuse and risk of re-identification. Finally, we evaluate the role of regulatory agencies in promoting transparency and accountability and propose strategies for risk mitigation such as Differential Privacy (DP) and a dataset chain of custody to maintain data integrity, traceability, and accountability. Synthetic data can improve healthcare, but measures to protect patient well-being and maintain ethical standards are key to promote responsible use."
Navigating Ethical Complexities Through Epistemological Analysis of ChatGPT,2023,Siraprapa Chavanayarn,Bulletin of Science Technology & Society,12,W4388911567,10.1177/02704676231216355,https://openalex.org/W4388911567,,Epistemology; Comprehension; Trustworthiness; Ethical issues; Ethical theories,article,False,"This article undertakes an epistemological analysis to explore the ethical complexities of ChatGPT, an AI system. While ethical concerns regarding AI have received considerable attention, the epistemological dimension has been largely neglected. By integrating epistemology, the study aims to deepen our understanding of the ethical issues associated with ChatGPT. Four specific issues are examined: ChatGPT's role in testimony, its potential designation as an expert, the influence of user epistemic limitations and vices, and the impact of algorithmic bias on ChatGPT. The study's findings contribute to an inclusive comprehension of the ethical implications arising from the epistemological complexities of ChatGPT. They reveal the limitations of ChatGPT as a trustworthy source of testimony, attributable to its lack of genuine understanding. The blurring of boundaries between AI-generated information and authentic expertise is identified as a significant concern. Furthermore, the study underscores the necessity of addressing the epistemic limitations and biases of users to foster responsible decision-making and prevent the perpetuation of flawed knowledge. Finally, the ethical ramifications of algorithmic bias in ChatGPT are explored, emphasizing its impact on societal fairness and justice."
Scheduling two-stage healthcare appointment systems via a knowledge-based biased random-key genetic algorithm,2025,Fajun Yang; Chao Li; Feng Wang; Zhi Yang; Kaizhou Gao,Swarm and Evolutionary Computation,3,W4407273792,10.1016/j.swevo.2025.101864,https://openalex.org/W4407273792,,Computer science; Key (lock); Scheduling (production processes); Algorithm; Genetic algorithm,article,False,
Artificial intelligence in intelligent tutoring systems toward sustainable education: a systematic review,2023,Chien-Chang Lin; Anna Y.Q. Huang; Owen H.T. Lu,Smart Learning Environments,220,W4386223224,10.1186/s40561-023-00260-y,https://openalex.org/W4386223224,https://slejournal.springeropen.com/counter/pdf/10.1186/s40561-023-00260-y,Software deployment; Computer science; Knowledge management; Sustainable development; Political science,review,True,"Abstract Sustainable education is a crucial aspect of creating a sustainable future, yet it faces several key challenges, including inadequate infrastructure, limited resources, and a lack of awareness and engagement. Artificial intelligence (AI) has the potential to address these challenges and enhance sustainable education by improving access to quality education, creating personalized learning experiences, and supporting data-driven decision-making. One outcome of using AI and Information Technology (IT) systems in sustainable education is the ability to provide students with personalized learning experiences that cater to their unique learning styles and preferences. Additionally, AI systems can provide teachers with data-driven insights into student performance, emotions, and engagement levels, enabling them to tailor their teaching methods and approaches or provide assistance or intervention accordingly. However, the use of AI and IT systems in sustainable education also presents challenges, including issues related to privacy and data security, as well as potential biases in algorithms and machine learning models. Moreover, the deployment of these systems requires significant investments in technology and infrastructure, which can be a challenge for educators. In this review paper, we will provide different perspectives from educators and information technology solution architects to connect education and AI technology. The discussion areas include sustainable education concepts and challenges, technology coverage and outcomes, as well as future research directions. By addressing these challenges and pursuing further research, we can unlock the full potential of these technologies and support a more equitable and sustainable education system."
How Much Does Racial Bias Affect Mortgage Lending? Evidence from Human and Algorithmic Credit Decisions,2024,Neil Bhutta; Aurel Hizmo; Daniel Ringo,Working paper,3,W4392954719,10.21799/frbp.wp.2024.09,https://openalex.org/W4392954719,https://doi.org/10.21799/frbp.wp.2024.09,Affect (linguistics); Business; Psychology; Communication,report,True,
Considering How Machine‐Learning Algorithms (Re)produce Social Biases in Generated Faces,2024,Matthew Gusdorff; Alvin Grissom; Jeová Farias Sales Rocha Neto; Yupeng Lin; Ryan Trotter; Ryan F. Lei,Social and Personality Psychology Compass,1,W4404059775,10.1111/spc3.70021,https://openalex.org/W4404059775,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/spc3.70021,Adversarial system; Generative grammar; Computer science; Face (sociological concept); Class (philosophy),article,True,"ABSTRACT Advances in computer science–specifically in the development and use of generative machine learning–have provided powerful new tools for psychologists to create synthetic human faces as stimuli, which ultimately provide high‐quality photorealistic face images that have many advantages, including reducing typical ethical and privacy concerns and generating face images from minoritized communities that are typically underrepresented in existing face databases. However, there are a number of ways that using machine learning‐based face generation and manipulation software can introduce bias into the research process, thus threatening the validity of studies. The present article provides a summary of how one class of recently popular algorithms for generating faces–generative adversarial networks (GANs)—works, how we control GANs, and where biases (with a particular focus on racial biases) emerge throughout these processes. We discuss recommendations for mitigating these biases, as well as how these concepts manifest in similar modern text‐to‐image algorithms."
AI-DRIVEN PREDICTIVE ANALYTICS IN RETAIL: A REVIEW OF EMERGING TRENDS AND CUSTOMER ENGAGEMENT STRATEGIES,2024,David Iyanuoluwa Ajiga; Ndubuisi Leonard Ndubuisi; Onyeka Franca Asuzu; Oluwaseyi Rita Owolabi; Tula Sunday Tubokirifuruar; Rhoda Adura Adeleye,International Journal of Management & Entrepreneurship Research,39,W4391898540,10.51594/ijmer.v6i2.772,https://openalex.org/W4391898540,,Customer engagement; Analytics; Predictive analytics; Data science; Business,review,False,"As the retail landscape undergoes a profound transformation in the era of digitalization, the integration of Artificial Intelligence (AI) and predictive analytics has emerged as a pivotal force reshaping the industry. This paper provides a comprehensive review of the latest trends in AI-driven predictive analytics within the retail sector and explores innovative customer engagement strategies that leverage these advanced technologies. The review begins by elucidating the foundational concepts of AI and predictive analytics, highlighting their synergistic role in forecasting consumer behavior, demand patterns, and market trends. The paper then delves into the emerging trends, such as machine learning algorithms, natural language processing, and computer vision, that are revolutionizing the way retailers harness data for strategic decision-making. In addition to outlining technological advancements, the paper emphasizes the crucial role of data quality and ethical considerations in the implementation of AI-driven predictive analytics. It examines the challenges associated with privacy concerns, algorithmic bias, and the need for transparent AI models to ensure responsible and fair use of customer data. Furthermore, the paper explores a spectrum of customer engagement strategies enabled by AI-driven predictive analytics. From personalized shopping experiences and targeted marketing campaigns to dynamic pricing and inventory optimization, retailers are deploying innovative approaches to enhance customer satisfaction and loyalty. The review also discusses case studies of successful AI implementations in leading retail enterprises, showcasing tangible benefits such as improved operational efficiency, increased sales, and enhanced customer retention. These real-world examples illustrate the transformative impact of AI-driven predictive analytics on diverse aspects of the retail value chain. By examining emerging trends and customer engagement strategies, it serves as a valuable resource for industry professionals, researchers, and policymakers seeking to navigate the evolving landscape of AI in the retail sector.&#x0D; Keywords: AI-driven Predictive Analytics, Retail Industry, Customer Engagement Strategies, Machine Learning Algorithms, Natural Language Processing."
A LiDAR-Assisted Reference Feedforward Bias Control Algorithm for Wind Turbines in the Transition Region,2023,Yanqin Huang; Guorong Zhu; Xiangtian Deng; Jianghua Lu; Chengzhen Jia; Hua Geng,,1,W4391216602,10.1109/peas58692.2023.10395638,https://openalex.org/W4391216602,,Control theory (sociology); Wind speed; Feed forward; Lidar; Aerodynamics,article,False,"The traditional reference bias control (RBC) algorithm has the problem that the rotor speed cannot be effectively controlled in the case of sudden increase of aerodynamic power, for which is caused by the delay of the reference signal. In this paper, the wind speed is obtained by LiDAR, and the accurate pitch angle command is obtained by using the inverse system method, which is used for the feedforward control of the reference bias signal. In the case of large changes in wind speed, the generator speed is well suppressed. Compared with the traditional RBC algorithm, this method accelerates the switching speed of torque control, and then improves the dynamic characteristics of the rotor speed in the transition region."
Role and Challenges of ChatGPT and Similar Generative Artificial Intelligence in Human Resource Management,2023,Nitin Rane,SSRN Electronic Journal,32,W4388152688,10.2139/ssrn.4603230,https://openalex.org/W4388152688,,Generative grammar; Human resource management; Artificial intelligence; Knowledge management; Computer science,article,False,"The integration of generative artificial intelligence (AI) systems, such as ChatGPT, into Human Resource Management (HRM) has marked the beginning of a groundbreaking era in innovative workforce management and employee engagement. This research investigates the pivotal role played by ChatGPT and analogous generative AI technologies in HRM, underscoring their significance in the realms of recruitment, employee training, and organizational communication. Leveraging their natural language processing abilities, these AI systems streamline the recruitment process, ensuring unbiased candidate selection and enhancing the overall efficiency of HR departments. Moreover, in training and development initiatives, ChatGPT facilitates tailored learning experiences, adjusting content to meet individual employee needs, thereby fostering skill enhancement and professional growth. Nevertheless, the widespread adoption of ChatGPT in HRM is not without its challenges. Ethical concerns, such as data privacy and algorithmic bias, necessitate thorough examination to prevent discriminatory practices and guarantee equitable treatment of employees. Additionally, the requirement for continuous monitoring and refinement of AI algorithms to align with evolving organizational cultures and goals presents a significant hurdle. Moreover, striking a harmonious balance between AI-driven automation and human intervention is imperative to preserve the human touch in HRM processes, safeguarding the empathetic and intuitive elements that are essential for effective employee management. This research delves into these complex dynamics, shedding light on the transformative potential of ChatGPT and akin generative AI technologies in HRM, while also emphasizing the need for vigilance and strategic planning to address the associated challenges. Through an exhaustive analysis of real-world case studies and ethical frameworks, this study offers valuable insights for HR professionals, policymakers, and researchers endeavoring to navigate the intricate landscape of AI-powered HRM."
Age-related bias and artificial intelligence: a scoping review,2023,Charlene H. Chu; Simon Donato‐Woodger; Shehroz S. Khan; Rune Nyrup; Kathleen Leslie; Alexandra Lyn; Tianyu Shi; Andria Bianchi; Samira Abbasgholizadeh Rahimi; Amanda Grenier,Humanities and Social Sciences Communications,50,W4385952733,10.1057/s41599-023-01999-y,https://openalex.org/W4385952733,https://www.nature.com/articles/s41599-023-01999-y.pdf,Artificial intelligence; Grey literature; Gender bias; Computer science; Machine learning,review,True,"Abstract There are widespread concerns about bias and discriminatory output related to artificial intelligence (AI), which may propagate social biases and disparities. Digital ageism refers to ageism reflected design, development, and implementation of AI systems and technologies and its resultant data. Currently, the prevalence of digital ageism and the sources of AI bias are unknown. A scoping review informed by the Arksey and O’Malley methodology was undertaken to explore age-related bias in AI systems, identify how AI systems encode, produce, or reinforce age-related bias, what is known about digital ageism, and the social, ethical and legal implications of age-related bias. A comprehensive search strategy that included five electronic bases and grey literature sources including legal sources was conducted. A framework of machine learning biases spanning from data to user by Mehrabi et al. is used to present the findings (Mehrabi et al. 2021). The academic search resulted in 7595 articles that were screened according to the inclusion criteria, of which 307 were included for full-text screening, and 49 were included in this review. The grey literature search resulted in 2639 documents screened, of which 235 were included for full text screening, and 25 were found to be relevant to the research questions pertaining to age and AI. As a result, a total of 74 documents were included in this review. The results show that the most common AI applications that intersected with age were age recognition and facial recognition systems. The most frequent machine learning algorithms used were convolutional neural networks and support vector machines. Bias was most frequently introduced in the early ‘data to algorithm’ phase in machine learning and the ‘algorithm to user’ phase specifically with representation bias ( n = 33) and evaluation bias ( n = 29), respectively (Mehrabi et al. 2021). The review concludes with a discussion of the ethical implications for the field of AI and recommendations for future research."
Algorithm Biases and Values,2023,Domenico Talia,ACM eBooks,0,W4387807783,10.1145/3603178.3603186,https://openalex.org/W4387807783,,Citation; Computer science; Information retrieval; Operations research; World Wide Web,book-chapter,False,"chapter Share on Algorithm Biases and Values Author: Domenico Talia University of Calabria University of CalabriaView Profile Authors Info & Claims From Algorithms to Thinking Machines: The New Digital PowerOctober 2023https://doi.org/10.1145/3603178.3603186Published:20 October 2023Publication History 0citation0DownloadsMetricsTotal Citations0Total Downloads0Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access"
Impact of Structural Bias on the Sine Cosine Algorithm: A Theoretical Investigation Using the Signature Test,2024,Kanchan Rajwar; Kusum Deep; M. Mathirajan,Communications in computer and information science,2,W4402543161,10.1007/978-3-031-69257-4_10,https://openalex.org/W4402543161,,Sine; Signature (topology); Trigonometric functions; Algorithm; Computer science,book-chapter,False,
Algorithmic Fairness in Financial Decision-Making: Detection and Mitigation of Bias in Credit Scoring Applications,2024,The-Chuong Trinh; Daiyang Zhang,,2,W4411762796,10.69987/jacs.2024.40204,https://openalex.org/W4411762796,,Computer science; Business; Finance; Actuarial science,article,False,
Exploring the Potential of Chatbots in Critical Care Nephrology,2023,Supawadee Suppadungsuk; Charat Thongprayoon; Jing Miao; Pajaree Krisanapan; Fawad Qureshi; Kianoush Kashani; Wisit Cheungpasitporn,Medicines,28,W4387843512,10.3390/medicines10100058,https://openalex.org/W4387843512,https://www.mdpi.com/2305-6320/10/10/58/pdf?version=1697820232,Health care; Nephrology; Chatbot; Medicine; Multidisciplinary approach,review,True,"The exponential growth of artificial intelligence (AI) has allowed for its integration into multiple sectors, including, notably, healthcare. Chatbots have emerged as a pivotal resource for improving patient outcomes and assisting healthcare practitioners through various AI-based technologies. In critical care, kidney-related conditions play a significant role in determining patient outcomes. This article examines the potential for integrating chatbots into the workflows of critical care nephrology to optimize patient care. We detail their specific applications in critical care nephrology, such as managing acute kidney injury, alert systems, and continuous renal replacement therapy (CRRT); facilitating discussions around palliative care; and bolstering collaboration within a multidisciplinary team. Chatbots have the potential to augment real-time data availability, evaluate renal health, identify potential risk factors, build predictive models, and monitor patient progress. Moreover, they provide a platform for enhancing communication and education for both patients and healthcare providers, paving the way for enriched knowledge and honed professional skills. However, it is vital to recognize the inherent challenges and limitations when using chatbots in this domain. Here, we provide an in-depth exploration of the concerns tied to chatbots’ accuracy, dependability, data protection and security, transparency, potential algorithmic biases, and ethical implications in critical care nephrology. While human discernment and intervention are indispensable, especially in complex medical scenarios or intricate situations, the sustained advancements in AI signal that the integration of precision-engineered chatbot algorithms within critical care nephrology has considerable potential to elevate patient care and pivotal outcome metrics in the future."
Legal accountability and ethical considerations of AI in financial services,2024,Ngozi Samuel Uzougbo; Chinonso Gladys Ikegwu; Adefolake Olachi Adewusi,GSC Advanced Research and Reviews,44,W4396845448,10.30574/gscarr.2024.19.2.0171,https://openalex.org/W4396845448,https://gsconlinepress.com/journals/gscarr/sites/default/files/GSCARR-2024-0171.pdf,Accountability; Business; Financial services; Accounting; Engineering ethics,article,True,"Artificial Intelligence (AI) is revolutionizing the financial services industry, offering unparalleled opportunities for efficiency, innovation, and personalized services. However, along with its benefits, AI in financial services raises significant legal and ethical concerns. This paper explores the legal accountability and ethical considerations surrounding the use of AI in financial services, aiming to provide insights into how these challenges can be addressed. The legal accountability of AI in financial services revolves around the allocation of responsibility for AI-related decisions and actions. As AI systems become more autonomous, questions arise about who should be held liable for AI errors, misconduct, or regulatory violations. This paper examines the existing legal frameworks, such as data protection laws, consumer protection regulations, and liability laws, and assesses their adequacy in addressing AI-related issues. Ethical considerations in AI implementation in financial services are paramount, as AI systems can impact individuals' financial well-being and access to services. Issues such as algorithmic bias, transparency, and fairness are critical in ensuring ethical AI practices. This paper discusses the importance of ethical guidelines and frameworks for AI development and deployment in financial services, emphasizing the need for transparency, accountability, and fairness. The paper also examines the role of regulatory bodies and industry standards in addressing legal and ethical challenges associated with AI in financial services. It proposes recommendations for policymakers, regulators, and industry stakeholders to promote responsible AI practices, including the development of clear guidelines, enhanced transparency measures, and mechanisms for accountability. Overall, this paper highlights the complex interplay between AI, legal accountability, and ethical considerations in the financial services industry. By addressing these challenges, stakeholders can harness the full potential of AI while ensuring that it is deployed in a responsible and ethical manner, benefiting both businesses and consumers."
Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices,2024,Xukang Wang; Ying Cheng Wu; Xueliang Ji; Hongpeng Fu,Frontiers in Artificial Intelligence,12,W4398174495,10.3389/frai.2024.1320277,https://openalex.org/W4398174495,https://www.frontiersin.org/articles/10.3389/frai.2024.1320277/pdf?isPublishedV2=False,Emphasis (telecommunications); Political science; Computer science; Telecommunications,article,True,"Introduction Algorithmic decision-making systems are widely used in various sectors, including criminal justice, employment, and education. While these systems are celebrated for their potential to enhance efficiency and objectivity, they also pose risks of perpetuating and amplifying societal biases and discrimination. This paper aims to provide an indepth analysis of the types of algorithmic discrimination, exploring both the challenges and potential solutions. Methods The methodology includes a systematic literature review, analysis of legal documents, and comparative case studies across different geographic regions and sectors. This multifaceted approach allows for a thorough exploration of the complexity of algorithmic bias and its regulation. Results We identify five primary types of algorithmic bias: bias by algorithmic agents, discrimination based on feature selection, proxy discrimination, disparate impact, and targeted advertising. The analysis of the U.S. legal and regulatory framework reveals a landscape of principled regulations, preventive controls, consequential liability, self-regulation, and heteronomy regulation. A comparative perspective is also provided by examining the status of algorithmic fairness in the EU, Canada, Australia, and Asia. Conclusion Real-world impacts are demonstrated through case studies focusing on criminal risk assessments and hiring algorithms, illustrating the tangible effects of algorithmic discrimination. The paper concludes with recommendations for interdisciplinary research, proactive policy development, public awareness, and ongoing monitoring to promote fairness and accountability in algorithmic decision-making. As the use of AI and automated systems expands globally, this work highlights the importance of developing comprehensive, adaptive approaches to combat algorithmic discrimination and ensure the socially responsible deployment of these powerful technologies."
Ethical Implication of Artificial Intelligence (AI) Adoption in Financial Decision Making,2024,Omoshola S. Owolabi; Prince C. Uche; Nathaniel T. Adeniken; Christopher Ihejirika; Riyad Bin Islam; Bishal Jung Thapa Chhetri,Computer and Information Science,28,W4396498564,10.5539/cis.v17n1p49,https://openalex.org/W4396498564,https://ccsenet.org/journal/index.php/cis/article/download/0/0/50148/54269,Computer science; Artificial intelligence; Ethical decision; Engineering ethics; Engineering,article,True,"The integration of artificial intelligence (AI) into the financial sector has raised ethical concerns that need to be addressed. This paper analyzes the ethical implications of using AI in financial decision-making and emphasizes the importance of an ethical framework to ensure its fair and trustworthy deployment. The study explores various ethical considerations, including the need to address algorithmic bias, promote transparency and explainability in AI systems, and adhere to regulations that protect equity, accountability, and public trust. By synthesizing research and empirical evidence, the paper highlights the complex relationship between AI innovation and ethical integrity in finance. To tackle this issue, the paper proposes a comprehensive and actionable ethical framework that advocates for clear guidelines, governance structures, regular audits, and collaboration among stakeholders. This framework aims to maximize the potential of AI while minimizing negative impacts and unintended consequences. The study serves as a valuable resource for policymakers, industry professionals, researchers, and other stakeholders, facilitating informed discussions, evidence-based decision-making, and the development of best practices for responsible AI integration in the financial sector. The ultimate goal is to ensure fairness, transparency, and accountability while reaping the benefits of AI for both the financial sector and society."
Addressing Algorithmic Bias,2025,Abhishek Benedict Kumar; Karun Sanjaya,Advances in computational intelligence and robotics book series,0,W4414697660,10.4018/979-8-3373-2387-9.ch011,https://openalex.org/W4414697660,,Computer science; Psychology,book-chapter,False,"As artificial intelligence (AI) systems increasingly make decisions in areas such as justice, health, and finance, issues related to algorithmic biases have risen to prominence when discussing equity and inclusion. This chapter investigates how prejudices are built into data and model specifications, and how they can contribute to further entrenching social disparities. From a legal and ethical perspective, it examines the shortcoming that current anti-discrimination laws face when confronted with AI-generated harm. The chapter provides examples of actual cases, such as COMPAS risk assessment tool, to illustrate the real-life implications of biased algorithms. It calls for holistic approaches that include legal remedies, algorithmic transparency, participatory governance, and anti-discrimination data practices. By integrating the development of AI into fair, accountable, and human rights respecting principles, the chapter emphasize the immediate necessity of ensuring that technological development is aligned with just and inclusive development."
Criminal Justice and Artificial Intelligence: How Should we Assess the Performance of Sentencing Algorithms?,2024,Jesper Ryberg,Philosophy & Technology,14,W4390795059,10.1007/s13347-024-00694-3,https://openalex.org/W4390795059,https://link.springer.com/content/pdf/10.1007/s13347-024-00694-3.pdf,Philosophy of technology; Criminal justice; Algorithm; Economic Justice; Criminology,article,True,"Artificial intelligence is increasingly permeating many types of high-stake societal decision-making such as the work at the criminal courts. Various types of algorithmic tools have already been introduced into sentencing. This article concerns the use of algorithms designed to deliver sentence recommendations. More precisely, it is considered how one should determine whether one type of sentencing algorithm (e.g., a model based on machine learning) would be ethically preferable to another type of sentencing algorithm (e.g., a model based on old-fashioned programming). Whether the implementation of sentencing algorithms is ethically desirable obviously depends upon various questions. For instance, some of the traditional issues that have received considerable attention are algorithmic biases and lack of transparency. However, the purpose of this article is to direct attention to a further challenge that has not yet been considered in the discussion of sentencing algorithms. That is, even if is assumed that the traditional challenges concerning biases, transparency, and cost-efficiency have all been solved or proven insubstantial, there will be a further serious challenge associated with the comparison of sentencing algorithms; namely, that we do not yet possess an ethically plausible and applicable criterion for assessing how well sentencing algorithms are performing."
Loss modeling with the size-biased lognormal mixture and the entropy regularized EM algorithm,2024,Taehan Bae; Tatjana Miljkovic,Insurance Mathematics and Economics,1,W4396888684,10.1016/j.insmatheco.2024.05.003,https://openalex.org/W4396888684,https://doi.org/10.1016/j.insmatheco.2024.05.003,Log-normal distribution; Mixture model; Erlang (programming language); Mathematics; Expectation–maximization algorithm,article,True,"The Erlang mixture with a common scale parameter is one of many popular models for modeling insurance losses. However, the actuarial literature recognizes and discusses some limitations of aforementioned model in approximate heavy-tailed distributions. In this paper, a size-biased left-truncated Lognormal (SB-ltLN) mixture is proposed as a robust alternative to the Erlang mixture for modeling left-truncated insurance losses with a heavy tail. The weak denseness property of the weighted Lognormal mixture is studied along with the tail behavior. Explicit analytical solutions are derived for moments and Tail Value at Risk based on the proposed model. An extension of the regularized expectation–maximization (REM) algorithm with Shannon's entropy weights (ewREM) is introduced for parameter estimation and variability assessment. The Operational Riskdata eXchange's left-truncated internal fraud loss data set is used to illustrate applications of the proposed model. Finally, the results of a simulation study show promising performance of the proposed SB-ltLN mixture in different simulation settings."
"Generative Visual AI in News Organizations: Challenges, Opportunities, Perceptions, and Policies",2024,T.J. Thomson; Ryan J. Thomas; Phoebe Matich,Digital Journalism,23,W4394571198,10.1080/21670811.2024.2331769,https://openalex.org/W4394571198,https://www.tandfonline.com/doi/pdf/10.1080/21670811.2024.2331769?needAccess=true,Perception; Generative grammar; Public relations; Political science; Knowledge management,article,True,"The use of AI-enabled text-to-image generators, such as Midjourney and DALL-E, raises profound questions about the purpose, meaning, and value of images generally, and the production, editing, and consumption of images in journalism specifically. This study explores how photo editors (or their equivalents) in seven countries perceive and/or use generative visual AI in their editorial operations and outlines the challenges and opportunities they see for the technology. It also identifies the extent to which these news organizations have policies governing how generative visual AI is used or, if not, the principles that they feel should inform their development. Participants identified mis/disinformation as the primary challenge of AI-generated images, also raising concerns about labor and copyright implications, the difficulty or impossibility of detecting AI-generated images, the potential for algorithmic bias, and the potential reputational risk of using AI-generated images. Conversely, participants saw potential for using AI for illustrations and brainstorming, while a minority saw it as an opportunity to increase efficiencies and cut costs."
"Finding the white male: The prevalence and consequences of algorithmic
  gender and race bias in political Google searches",2024,Tobias Rohrbach; Mykola Makhortykh; Maryna Sydorova,arXiv (Cornell University),1,W4396606341,10.48550/arxiv.2405.00335,https://openalex.org/W4396606341,https://arxiv.org/pdf/2405.00335,Race (biology); Politics; White (mutation); Gender bias; Political science,preprint,True,"Search engines like Google have become major information gatekeepers that use artificial intelligence (AI) to determine who and what voters find when searching for political information. This article proposes and tests a framework of algorithmic representation of minoritized groups in a series of four studies. First, two algorithm audits of political image searches delineate how search engines reflect and uphold structural inequalities by under- and misrepresenting women and non-white politicians. Second, two online experiments show that these biases in algorithmic representation in turn distort perceptions of the political reality and actively reinforce a white and masculinized view of politics. Together, the results have substantive implications for the scientific understanding of how AI technology amplifies biases in political perceptions and decision-making. The article contributes to ongoing public debates and cross-disciplinary research on algorithmic fairness and injustice."
The Impact of Algorithmic Bias on Consumers,2024,Xin Chen,Frontiers in Business Economics and Management,0,W4395957389,10.54097/7m8yka94,https://openalex.org/W4395957389,https://doi.org/10.54097/7m8yka94,Computer science,article,True,"Algorithmic bias has aroused people's attention to the ethical problems of intelligent services, and directly affects consumers' willingness to use intelligent services. The purpose of this study is to explore the effect of algorithmic bias on consumers' willingness to use intelligent services, which has positive significance for the future design of intelligent services."
The Promises and Pitfalls of Using Chat GPT for Self-Determined Learning in Higher Education: An Argumentative Review,2023,FX. Risang Baskara,Prosiding Seminar Nasional Fakultas Tarbiyah dan Ilmu Keguruan IAIM Sinjai,21,W4385781160,10.47435/sentikjar.v2i0.1825,https://openalex.org/W4385781160,https://journal.uiad.ac.id/index.php/SENTIKJAR/article/download/1825/858,Psychological intervention; Compromise; Psychology; Viewpoints; Self-regulated learning,article,True,"The potential of artificial intelligence language models, such as Chat GPT, to support self-determined learning in higher education has garnered increasing attention from educators, researchers, and policymakers. However, the promises and pitfalls of using Chat GPT for self-determined learning remain subject to debate and warrant further exploration. In this argumentative review, we examine the central questions and statements of the problem related to the use of Chat GPT for self-determined learning in higher education. We synthesise and critically evaluate the existing literature on the potential of Chat GPT to support self-directed and self-determined learning and highlight the main challenges and concerns associated with its use. According to our analysis, Chat GPT promises to improve self-determined learning by offering individualised feedback, resources, and assistance to learners that can foster their acquisition of knowledge and skills. However, using Chat GPT in self-determined learning also raises ethical and pragmatic concerns. These include issues about privacy, data security, and algorithmic bias, which could compromise the effectiveness and reliability of Chat GPT-based interventions. We posit that while the potential benefits of Chat GPT for self-determined learning are significant, they must be weighed against its potential drawbacks. As such, the design, implementation, and assessment of Chat GPT-based higher education interventions must be carefully considered. Our results indicate that the advancement of Chat GPT-based interventions for self-determined learning in higher education necessitates a nuanced and multidisciplinary approach that considers the viewpoints of educators, researchers, learners, and other interested stakeholders."
Measuring Bias in Job Recommender Systems: Auditing the Algorithms,2024,Shuo Zhang; Peter Kuhn,SSRN Electronic Journal,1,W4401981012,10.2139/ssrn.4939157,https://openalex.org/W4401981012,,Recommender system; Audit; Computer science; Algorithm; Information retrieval,article,False,
Bias Compensation for Kernel Least-Mean-Square Algorithms,2025,Ying‐Ren Chien; Liu Jin-ling; En-Ting Lin; Guobing Qian,IEEE Sensors Letters,1,W4408792212,10.1109/lsens.2025.3553594,https://openalex.org/W4408792212,,Algorithm; Square (algebra); Kernel (algebra); Computer science; Mathematics,article,False,
Bias in medical AI: Implications for clinical decision-making,2024,James M. Cross; Michael A. Choma; John A. Onofrey,PLOS Digital Health,98,W4404134492,10.1371/journal.pdig.0000651,https://openalex.org/W4404134492,https://doi.org/10.1371/journal.pdig.0000651,Medical decision making; Clinical decision making; Psychology; Computer science; Artificial intelligence,review,True,"Biases in medical artificial intelligence (AI) arise and compound throughout the AI lifecycle. These biases can have significant clinical consequences, especially in applications that involve clinical decision-making. Left unaddressed, biased medical AI can lead to substandard clinical decisions and the perpetuation and exacerbation of longstanding healthcare disparities. We discuss potential biases that can arise at different stages in the AI development pipeline and how they can affect AI algorithms and clinical decision-making. Bias can occur in data features and labels, model development and evaluation, deployment, and publication. Insufficient sample sizes for certain patient groups can result in suboptimal performance, algorithm underestimation, and clinically unmeaningful predictions. Missing patient findings can also produce biased model behavior, including capturable but nonrandomly missing data, such as diagnosis codes, and data that is not usually or not easily captured, such as social determinants of health. Expertly annotated labels used to train supervised learning models may reflect implicit cognitive biases or substandard care practices. Overreliance on performance metrics during model development may obscure bias and diminish a model's clinical utility. When applied to data outside the training cohort, model performance can deteriorate from previous validation and can do so differentially across subgroups. How end users interact with deployed solutions can introduce bias. Finally, where models are developed and published, and by whom, impacts the trajectories and priorities of future medical AI development. Solutions to mitigate bias must be implemented with care, which include the collection of large and diverse data sets, statistical debiasing methods, thorough model evaluation, emphasis on model interpretability, and standardized bias reporting and transparency requirements. Prior to real-world implementation in clinical settings, rigorous validation through clinical trials is critical to demonstrate unbiased application. Addressing biases across model development stages is crucial for ensuring all patients benefit equitably from the future of medical AI."
Measuring Bias in Job Recommender Systems: Auditing the Algorithms,2024,Shuo Zhang; Peter Kühn,,1,W4402121071,10.3386/w32889,https://openalex.org/W4402121071,,Recommender system; Audit; Computer science; Algorithm; Information retrieval,report,False,
Bias Detection and Correction Methods for Machine Learning Algorithms,2024,Genny Dimitrakopoulou; Nikolaos C. Kapsalis; George Kokkinis,,1,W4402288768,10.1109/eeite61750.2024.10654404,https://openalex.org/W4402288768,,Computer science; Artificial intelligence; Algorithm; Machine learning,article,False,
Fairness in Healthcare: Assessing Data Bias and Algorithmic Fairness,2024,Fariba Dehghani; Nikita Malik; Jenn-Wei Lin; Sayeh Bayat; Mariana Bento,,1,W4405304237,10.1109/sipaim62974.2024.10783630,https://openalex.org/W4405304237,,Fairness measure; Health care; Computer science; Max-min fairness; Economics,article,False,
"Application of deep learning and machine learning models to improve healthcare in sub-Saharan Africa: Emerging opportunities, trends and implications",2023,Elliot Mbunge; John Batani,Telematics and Informatics Reports,37,W4386374624,10.1016/j.teler.2023.100097,https://openalex.org/W4386374624,https://doi.org/10.1016/j.teler.2023.100097,Health care; Transparency (behavior); Artificial intelligence; Deep learning; Psychological intervention,article,True,"Deep learning and machine learning techniques present unmatched opportunities to improve healthcare in sub-Saharan Africa (SSA). However, there is a paucity of literature on AI-based applications deployed to improve care in SSA, which makes it challenging to organise the research contributions in the present and to highlight obstacles and emerging research areas that need to be explored in the future. This study applied the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analysis) model to conduct a comprehensive review of deep learning and machine learning models deployed in SSA to improve access to care while exploring emerging opportunities, trends and implications for integrating AI-based models in SSA healthcare. This study reveals that AI models can analyse and derive inferences from massive health data for early detection, diagnosis, monitoring for chronic disorders, prediction of diseases, monitoring large-scale public health patterns and help limit exposure in contagious environments. AI can facilitate the development of targeted health interventions and improve patient outcomes in all stages of diagnosis, treatment, drug development and monitoring, personalised medicine, patient control and care. Integrating AI models with health applications can tremendously assist health professionals and policymakers in disease diagnosis and making informed decisions. AI algorithms bias, poor access to health data and formats, and lack of policies and frameworks supporting the integration of data-driven AI-based solutions into health systems hinder the integration of AI-based models into health systems. There is a need for transparency and ethical use of AI and crafting policies that support the use of AI in SSA health systems. Utilising AI-based models in healthcare can also assist researchers and healthcare workers to move towards smart care and better comprehend future research needs of AI in smart care."
Participation and Division of Labor in User-Driven Algorithm Audits: How Do Everyday Users Work together to Surface Algorithmic Harms?,2023,Rushi Li; Sara Kingsley; Chelsea Fan; Proteeti Sinha; Nora Wai; J. Lee; Hong Shen; Motahhare Eslami; Jason Hong,,13,W4362679204,10.1145/3544548.3582074,https://openalex.org/W4362679204,https://dl.acm.org/doi/pdf/10.1145/3544548.3582074,Audit; Computer science; Work (physics); Division (mathematics); Human–computer interaction,preprint,True,"Recent years have witnessed an interesting phenomenon in which users come together to interrogate potentially harmful algorithmic behaviors they encounter in their everyday lives. Researchers have started to develop theoretical and empirical understandings of these user-driven audits, with a hope to harness the power of users in detecting harmful machine behaviors. However, little is known about users' participation and their division of labor in these audits, which are essential to support these collective efforts in the future. Through collecting and analyzing 17,984 tweets from four recent cases of user-driven audits, we shed light on patterns of users' participation and engagement, especially with the top contributors in each case. We also identified the various roles users' generated content played in these audits, including hypothesizing, data collection, amplification, contextualization, and escalation. We discuss implications for designing tools to support user-driven audits and users who labor to raise awareness of algorithm bias."
Toward a More Ethical Future of Artificial Intelligence and Data Science,2024,Wasswa Shafik,Advances in computational intelligence and robotics book series,21,W4392432224,10.4018/979-8-3693-2964-1.ch022,https://openalex.org/W4392432224,,Transparency (behavior); Engineering ethics; Equity (law); Ethical issues; Political science,book-chapter,False,"Examining the ethical aspects of artificial intelligence (AI) and data science (DS) recognizes their impressive progress in innovation while emphasizing the pressing necessity to tackle intricate ethical dilemmas. The chapter provides a detailed framework for navigating the changing environment, beginning with an examination of the increasing ethical challenges. The study highlights transparency, fairness, and responsibility as crucial for cultivating confidence in AI systems. The chapter emphasizes the urgent requirement to address problems such as algorithmic bias and privacy breaches with strong mitigation techniques. Furthermore, it promotes flexible policies that strike a balance between innovation and ethical safeguards. The examination of societal effects, particularly on various socioeconomic groups, economies, and cultures, is conducted thoroughly, with a focus on equity and the protection of individual rights. Finally, to proactively tackle future ethical challenges in technology, it is advisable to employ proactive solutions such as implementing AI ethics by design."
Improving social media use for disaster resilience: challenges and strategies,2023,Nina Lam; Michelle A. Meyer; Margaret Reams; Seungwon Yang; Kisung Lee; Lei Zou; Volodymyr Mihunov; Kejin Wang; Ryan Kirby; Heng Cai,International Journal of Digital Earth,24,W4385766542,10.1080/17538947.2023.2239768,https://openalex.org/W4385766542,https://www.tandfonline.com/doi/pdf/10.1080/17538947.2023.2239768?needAccess=true&role=button,Social media; Resilience (materials science); Emergency management; Data science; Big data,article,True,"This paper develops a social media-disaster resilience analysis framework by categorizing types of social media use and their challenges to better understand and assess its role in disaster resilience research and management. The framework is derived primarily from several case studies of Twitter use in three hurricane events in the United States – Hurricanes Isaac, Sandy, and Harvey. The paper first outlines four major contributions of social media data for disaster resilience research and management, which include serving as an effective communication platform, providing ground truth information for emergency response and rescue operations, providing information on people's sentiments, and allowing predictive modeling. However, there are four key challenges to its uses, which include easy spreading of false information, social and geographical disparities of Twitter use, technical issues on processing and analyzing big and noisy data, especially on improving the locational accuracy of the tweets, and algorithm bias in AI and other types of modeling. Then, the paper proposes twenty strategies that the four sectors of the social media community – organizations, individuals, social media companies, and researchers – could take to improve social media use to increase disaster resilience."
Moving Beyond the Stigma: Understanding and Overcoming the Resistance to the Acceptance and Adoption of Artificial Intelligence Chatbots,2023,Ismail Dergaa; Feten Fekih‐Romdhane; Jordan M. Glenn; Mohamed Saifeddin Fessi; Karim Chamari; Wissem Dhahbi; Makram Zghibi; Nicola Luigi Bragazzi; Mohamed Ben Aissa; Noomen Guelmami; Abdelfattah El Omri; Sarya Swed; Katja Weiss; Beat Knechtle; Helmi Ben Saad,New Asian Journal of Medicine,20,W4390901779,10.61838/kman.najm.1.2.4,https://openalex.org/W4390901779,https://nasianjmed.com/index.php/najm/article/download/12/21,Stigma (botany); Resistance (ecology); Psychology; Social psychology; Psychiatry,article,True,"Artificial intelligence chatbots may fundamentally transform academic research, automate mundane tasks, and enhance productivity. However, the integration of artificial intelligence chatbots (AIc) is impeded by a complex stigma deeply rooted in individuals' misconceptions and apprehension, including concerns about academic integrity, job displacement, data privacy, and algorithmic bias. The aim of this study was to scrutinize the origins and impacts of the stigma associated with artificial intelligence chatbots within the realm of academic research and to propose strategies to mitigate such stigmas. This study draws parallels between the reception of artificial intelligence chatbots and previous transformative technologies, presenting case studies illustrating the spectrum of responses to the integration of artificial intelligence chatbots into academic research. This study identifies the need for a shift in mindset from perceiving artificial intelligence chatbots as threats to recognizing them as facilitators of efficiency and innovation. It also underscores the importance of understanding these models as tools that aid researchers but do not replace the need for human expertise and judgment. We further highlighted the role of education, transparency, regulation, and ethical guidelines in overcoming the stigma associated with artificial intelligence chatbots. Given how adaptable people are, the surrounding stigma will likely fade with time. We support a cooperative strategy with continuing education and discussion to maximize the benefits of artificial intelligence chatbots while minimizing their drawbacks, hopefully paving the way for their ethical and successful application in scholarly research."
Toward fairness in artificial intelligence for medical image analysis: identification and mitigation of potential biases in the roadmap from data collection to model deployment,2023,Karen Drukker; Weijie Chen; Judy Wawira Gichoya; Nicholas P. Gruszauskas; Jayashree Kalpathy‐Cramer; Oluwasanmi Koyejo; Kyle J. Myers; Rui Carlos Sá; Berkman Sahiner; Heather M. Whitney; Zi Zhang; Maryellen L. Giger,Journal of Medical Imaging,83,W4366998893,10.1117/1.jmi.10.6.061104,https://openalex.org/W4366998893,https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-10/issue-6/061104/Toward-fairness-in-artificial-intelligence-for-medical-image-analysis/10.1117/1.JMI.10.6.061104.pdf,Software deployment; Artificial intelligence; Medical imaging; Machine learning; Identification (biology),article,True,"PurposeTo recognize and address various sources of bias essential for algorithmic fairness and trustworthiness and to contribute to a just and equitable deployment of AI in medical imaging, there is an increasing interest in developing medical imaging-based machine learning methods, also known as medical imaging artificial intelligence (AI), for the detection, diagnosis, prognosis, and risk assessment of disease with the goal of clinical implementation. These tools are intended to help improve traditional human decision-making in medical imaging. However, biases introduced in the steps toward clinical deployment may impede their intended function, potentially exacerbating inequities. Specifically, medical imaging AI can propagate or amplify biases introduced in the many steps from model inception to deployment, resulting in a systematic difference in the treatment of different groups.ApproachOur multi-institutional team included medical physicists, medical imaging artificial intelligence/machine learning (AI/ML) researchers, experts in AI/ML bias, statisticians, physicians, and scientists from regulatory bodies. We identified sources of bias in AI/ML, mitigation strategies for these biases, and developed recommendations for best practices in medical imaging AI/ML development.ResultsFive main steps along the roadmap of medical imaging AI/ML were identified: (1) data collection, (2) data preparation and annotation, (3) model development, (4) model evaluation, and (5) model deployment. Within these steps, or bias categories, we identified 29 sources of potential bias, many of which can impact multiple steps, as well as mitigation strategies.ConclusionsOur findings provide a valuable resource to researchers, clinicians, and the public at large."
A Novel Bias-Compensated Linear Constrained Least Mean Squares Algorithm Over Distributed Network,2023,Liru Wang; Lijuan Jia; Deshui Miao; Yinan Guo; Shunshoku Kanae,,1,W4386821599,10.23919/ccc58697.2023.10240407,https://openalex.org/W4386821599,,Robustness (evolution); Computer science; Algorithm; Variance (accounting); Interference (communication),article,False,"In this paper, we propose a Diffusion Bias-Compensated Constrained Least Mean Squares (D-BC-CLMS) algorithm based on the idea of distributed estimation for adaptive filtering in network containing input noises. To reduce the interference of input noises, we use a new cost function. The variance of the input noises is derived by a novel method that uses some reasonable assumptions without any prior knowledge. Then we combine the diffusion strategy with BC-CLMS to improve the performance of single agent and to obtain more robustness. Eventually, simulation results confirm the theory is correct and demonstrate the excellent performance of the novel algorithm by comparing it with other conventional algorithms."
Smiling women pitching down: auditing representational and presentational gender biases in image-generative AI,2023,Luhang Sun; Mian Wei; Yibing Sun; Yoo Ji Suh; Liwei Shen; Sijia Yang,Journal of Computer-Mediated Communication,59,W4391463878,10.1093/jcmc/zmad045,https://openalex.org/W4391463878,https://academic.oup.com/jcmc/article-pdf/29/1/zmad045/56546560/zmad045.pdf,Presentational and representational acting; Audit; Psychology; Generative grammar; Artificial intelligence,article,True,"Abstract Generative Artificial Intelligence (AI) models like DALL·E 2 can interpret prompts and generate high-quality images that exhibit human creativity. Though public enthusiasm is booming, systematic auditing of potential gender biases in AI-generated images remains scarce. We addressed this gap by examining the prevalence of two occupational gender biases (representational and presentational biases) in 15,300 DALL·E 2 images spanning 153 occupations. We assessed potential bias amplification by benchmarking against the 2021 U.S. census data and Google Images. Our findings reveal that DALL·E 2 underrepresents women in male-dominated fields while overrepresenting them in female-dominated occupations. Additionally, DALL·E 2 images tend to depict more women than men with smiles and downward-pitching heads, particularly in female-dominated (versus male-dominated) occupations. Our algorithm auditing study demonstrates more pronounced representational and presentational biases in DALL·E 2 compared to Google Images and calls for feminist interventions to curtail the potential impacts of such biased AI-generated images on the media ecology."
A Genealogical Approach to Algorithmic Bias,2024,Marta Ziosi; David Watson; Luciano Floridi,Minds and Machines,0,W4396581355,10.1007/s11023-024-09672-2,https://openalex.org/W4396581355,https://doi.org/10.1007/s11023-024-09672-2,Philosophy of science; Theory of computation; Philosophy of mind; Computer science; Philosophy of language,article,True,"Abstract The Fairness, Accountability, and Transparency (FAccT) literature tends to focus on bias as a problem that requires ex post solutions (e.g. fairness metrics), rather than addressing the underlying social and technical conditions that (re)produce it. In this article, we propose a complementary strategy that uses genealogy as a constructive, epistemic critique to explain algorithmic bias in terms of the conditions that enable it. We focus on XAI feature attributions (Shapley values) and counterfactual approaches as potential tools to gauge these conditions and offer two main contributions. One is constructive: we develop a theoretical framework to classify these approaches according to their relevance for bias as evidence of social disparities. We draw on Pearl’s ladder of causation (Causality: models, reasoning, and inference. Cambridge University Press, Cambridge, 2000, Causality, 2nd edn. Cambridge University Press, Cambridge, 2009. https://doi.org/10.1017/CBO9780511803161 ) to order these XAI approaches concerning their ability to answer fairness-relevant questions and identify fairness-relevant solutions. The other contribution is critical: we evaluate these approaches in terms of their assumptions about the role of protected characteristics in discriminatory outcomes. We achieve this by building on Kohler-Hausmann’s (Northwest Univ Law Rev 113(5):1163–1227, 2019) constructivist theory of discrimination. We derive three recommendations for XAI practitioners to develop and AI policymakers to regulate tools that address algorithmic bias in its conditions and hence mitigate its future occurrence."
Comparison of the sigma metrics using the total error allowable algorithm with variation of bias source,2024,Sonny Feisal Rinaldi; Anisa Agustia Ibadurrahmah; Surya Ridwanna; Harianto Harianto,INDONESIAN JOURNAL OF MEDICAL LABORATORY SCIENCE AND TECHNOLOGY,1,W4395447207,10.33086/ijmlst.v6i1.4930,https://openalex.org/W4395447207,https://journal2.unusa.ac.id/index.php/IJMLST/article/download/4930/2363,Variation (astronomy); Algorithm; Sigma; Statistics; Computer science,article,True,"Sigma Metrics, as a quality indicator, have been widely applied in clinical laboratories to assess the performance of analytical methods. Described in the document Clinical and Laboratory Standards Institute (CLSI) EP15- A3, the use of target values can be sourced from certified reference standards, survey materials from the Proficiency Testing (PT)/External Quality Assessment (EQA), materials used in inter-laboratory quality control programs and internal quality control materials with predetermined targets. This research aims to determine whether there is a difference in the sigma metrics between the bias derived from the manufacturer's target value and those from the peer group source in the External Quality Assurance Services (EQAS) program. The research methodology employed is descriptive comparative analysis, utilizing the results of material inspection data for 15 internal quality control parameters of Clinical Chemistry over a span of 2 years at the Pramita Laboratory in Bandung. The calculation of the sigma metrics commences with computing the coefficient of variation (CV), and the appropriate Total Error aalowable (Tea) sources for each parameter are determined beforehand using the TEa algorithm. The research findings indicate a difference between the sigma metrics derived from the manufacturer's target value and those from the EQAS-peer group target value, accounting for 33% or 10 parameters out of the total parameters with 2 levels of inspection are calculated on the sigma scale. However, in 67% or 20 parameters out of the total parameters, no such difference is observed. Bias associated with the target value from the manufacturer and the EQAS peer group shows no significant difference, suggesting that the laboratory can utilize pre-existing target values confidently."
The Transformative Potential of Artificial Intelligence in Medical Billing: A Global Perspective,2023,Victor Kilanko,International Journal Of Scientific Advances,26,W4378877690,10.51542/ijscia.v4i3.8,https://openalex.org/W4378877690,https://www.ijscia.com/wp-content/uploads/2023/05/Volume4-Issue3-May-Jun-No.438-345-353.pdf,Reimbursement; Workflow; Health care; Computer science; Payment,article,True,"This paper explores the transformative potential of Artificial Intelligence (AI) in revolutionizing medical billing processes worldwide. As healthcare systems face increasing complexities and challenges, AI offers innovative solutions to streamline billing operations, enhance accuracy, and improve financial outcomes. By automating the claims processing workflow, AI can significantly reduce the administrative burden on healthcare providers, allowing them to focus more on patient care. AI-powered coding accuracy systems can analyze medical records and suggest appropriate billing codes, reducing coding errors and claim rejections. AI can also optimize reimbursement strategies by analyzing historical data and identifying patterns to ensure optimal reimbursement rates for healthcare providers. To address the growing concern of healthcare fraud, AI algorithms can analyze vast amounts of data, detect suspicious patterns, and flag potentially fraudulent activities, thus preventing financial losses. Moreover, AI-powered chatbots and virtual assistants can enhance patient engagement by providing personalized support, answering billing-related queries, and guiding patients through the payment process. Adoption of AI in medical billing brings various benefits, it also presents challenges such as data privacy, algorithm bias, and the need for robust infrastructure and training. Successful case studies from various healthcare settings worldwide demonstrate the tangible advantages of AI implementation, such as reduced billing errors, accelerated reimbursement cycles, and improved patient satisfaction. By harnessing the power of AI, healthcare systems can achieve greater efficiency, financial sustainability, and improved patient experiences."
"Personalized Care in Eye Health: Exploring Opportunities, Challenges, and the Road Ahead for Chatbots",2023,Mantapond Ittarat; Wisit Cheungpasitporn; Sunee Chansangpetch,Journal of Personalized Medicine,22,W4389274564,10.3390/jpm13121679,https://openalex.org/W4389274564,https://www.mdpi.com/2075-4426/13/12/1679/pdf?version=1701516161,Bespoke; Triage; Transparency (behavior); Health care; Medicine,article,True,"In modern eye care, the adoption of ophthalmology chatbots stands out as a pivotal technological progression. These digital assistants present numerous benefits, such as better access to vital information, heightened patient interaction, and streamlined triaging. Recent evaluations have highlighted their performance in both the triage of ophthalmology conditions and ophthalmology knowledge assessment, underscoring their potential and areas for improvement. However, assimilating these chatbots into the prevailing healthcare infrastructures brings challenges. These encompass ethical dilemmas, legal compliance, seamless integration with electronic health records (EHR), and fostering effective dialogue with medical professionals. Addressing these challenges necessitates the creation of bespoke standards and protocols for ophthalmology chatbots. The horizon for these chatbots is illuminated by advancements and anticipated innovations, poised to redefine the delivery of eye care. The synergy of artificial intelligence (AI) and machine learning (ML) with chatbots amplifies their diagnostic prowess. Additionally, their capability to adapt linguistically and culturally ensures they can cater to a global patient demographic. In this article, we explore in detail the utilization of chatbots in ophthalmology, examining their accuracy, reliability, data protection, security, transparency, potential algorithmic biases, and ethical considerations. We provide a comprehensive review of their roles in the triage of ophthalmology conditions and knowledge assessment, emphasizing their significance and future potential in the field."
Designing an AI-Powered Mentorship Platform for Professional Development: Opportunities and Challenges,2023,Rahul Bagai; Vaishali Mane,International Journal of Computer Trends and Technology,18,W4377101895,10.14445/22312803/ijctt-v71i4p114,https://openalex.org/W4377101895,http://arxiv.org/pdf/2407.20233,Mentorship; Professional development; Engineering management; Engineering; Engineering ethics,article,False,"This article examines the promising prospects and potential hurdles associated with the development of MentorAI, a conceptual AI-driven mentorship platform for professional growth yet to be actualized.The article explores the essential characteristics and technological underpinnings required for the successful creation and efficacy of the MentorAI platform in providing tailored mentorship experiences.The article highlights the transformative potential of MentorAI on various dimensions of professional growth, such as boosting career progression, nurturing skill development, and supporting a balanced work-life environment for professionals.MentorAI, through its AI-based approach, aspires to offer real-time guidance, resources, and assistance customized to each individual's specific needs and goals.Furthermore, the article examines the core technologies crucial to MentorAI's operation, including artificial intelligence, machine learning, and natural language comprehension.These technologies will empower the platform to process user inputs, deliver contextsensitive responses, and dynamically adjust to user preferences and objectives.The deployment of MentorAI presents potential challenges and ethical concerns, as with any groundbreaking technology.The article outlines critical issues like data protection, security, algorithmic bias, and moral quandaries concerning substituting human mentors with AI systems.Addressing these challenges proactively and deliberately is vital to ensure a positive impact on users."
"The YOLO Framework: A Comprehensive Review of Evolution, Applications, and Benchmarks in Object Detection",2024,Momina Liaqat Ali; Zhou Zhang,,33,W4403712112,10.20944/preprints202410.1785.v1,https://openalex.org/W4403712112,https://doi.org/10.20944/preprints202410.1785.v1,Computer science; Object (grammar); Data science; Artificial intelligence; Systems engineering,review,True,"This paper presents a comprehensive review of the You Only Look Once (YOLO) framework, a transformative one-stage object detection algorithm renowned for its remarkable balance between speed and accuracy. Since its inception, YOLO has evolved significantly, with versions spanning from YOLOv1 to the most recent YOLOv11, each introducing pivotal innovations in feature extraction, bounding box prediction, and optimization techniques. These advancements, particularly in the backbone, neck, and head components, have positioned YOLO as a leading solution for real-time object detection across a variety of domains. In this review, we explore YOLO&amp;#039;s diverse applications, including its critical role in medical imaging for COVID-19 detection, breast cancer identification, and tumor localization, where it has significantly enhanced diagnostic efficiency. YOLO&amp;#039;s robust performance in autonomous vehicles is also highlighted, as it excels in challenging conditions like fog, rain, and low-light environments, thereby contributing to improved road safety and autonomous driving systems. In the agricultural sector, YOLO has transformed precision farming by enabling early detection of pests, diseases, and crop health issues, promoting more sustainable farming practices. Additionally, we provide an in-depth performance analysis of YOLO models—such as YOLOv9, YOLO-NAS, YOLOv10, and YOLOv11—across multiple benchmark datasets. This analysis compares their suitability for a range of applications, from lightweight embedded systems to high-resolution, complex object detection tasks. The paper also addresses YOLO&amp;#039;s challenges, such as occlusion, small object detection, and dataset biases, while discussing recent advancements that aim to mitigate these limitations. Moreover, we examine the ethical implications of YOLO&amp;#039;s deployment, particularly in surveillance and monitoring applications, raising concerns about privacy, algorithmic biases, and the potential to perpetuate societal inequities. These ethical considerations are critical in domains like law enforcement, where biased object detection models can have serious repercussions. Through this detailed review of YOLO&amp;#039;s technical advancements, applications, performance, and ethical challenges, this paper serves as a valuable resource for researchers, developers, and policymakers looking to understand YOLO’s current capabilities and future directions in the evolving field of object detection."
"Navigating and reviewing ethical dilemmas in AI development: Strategies for transparency, fairness, and accountability",2024,Olatunji Akinrinola; Chinwe Chinazo Okoye; Onyeka Chrisanctus Ofodile; Chinonye Esther Ugochukwu,GSC Advanced Research and Reviews,100,W4392621839,10.30574/gscarr.2024.18.3.0088,https://openalex.org/W4392621839,https://gsconlinepress.com/journals/gscarr/sites/default/files/GSCARR-2024-0088.pdf,Accountability; Transparency (behavior); Psychology; Engineering ethics; Political science,article,True,"As artificial intelligence (AI) continues to permeate various aspects of our lives, the ethical challenges associated with its development become increasingly apparent. This paper navigates and reviews the ethical dilemmas in AI development, focusing on strategies to promote transparency, fairness, and accountability. The rapid growth of AI technology has given rise to concerns related to bias, lack of transparency, and the need for clear accountability mechanisms. In this exploration, we delve into the intricate ethical landscape of AI, examining issues such as bias and fairness, lack of transparency, and the challenges associated with accountability. To address these concerns, we propose strategies for transparency, including the implementation of Explainable AI (XAI), advocating for open data sharing, and embracing ethical AI frameworks. Furthermore, we explore strategies to promote fairness in AI algorithms, emphasizing the importance of fairness metrics, diverse training data, and continuous monitoring for iterative improvement. Additionally, the paper delves into strategies to ensure accountability in AI development, considering regulatory measures, ethical AI governance, and the incorporation of human-in-the-loop approaches. To provide practical insights, case studies and real-world examples are analyzed to distill lessons learned and best practices. The paper concludes with a comprehensive overview of the proposed strategies, emphasizing the importance of balancing innovation with ethical responsibility in the evolving landscape of AI development. This work contributes to the ongoing discourse on AI ethics, offering a roadmap for navigating the challenges and fostering responsible AI development practices."
Algorithmic Biases in Computational Antitrust,2023,Vydhrithi Reddy Peesari,SSRN Electronic Journal,0,W4382201232,10.2139/ssrn.4468341,https://openalex.org/W4382201232,,Economics; Computer science; Mathematical economics; Econometrics; Law and economics,article,False,"Artificial intelligence (AI) is a key component in the field of antitrust, as it is utilized to analyze vast quantities of data with the aim of detecting indications of unjust business practices. This paper provides a comprehensive survey of the issue of algorithmic biases in computational antitrust with a focus in the United States. The paper will additionally examine the diverse solutions to address this issue, along with its ethical and legal implications. In conclusion, it is imperative for enterprises to adopt preventive measures to avert legal ramifications arising from partiality in algorithms utilized in antitrust."
Controlling Bias Between Categorical Attributes in Datasets: A Two-Step Optimization Algorithm Leveraging Structural Equation Modeling,2023,Enrico Barbierato; Andrea Pozzi; Daniele Tessera,IEEE Access,1,W4387717503,10.1109/access.2023.3325235,https://openalex.org/W4387717503,,Categorical variable; Computer science; Structural equation modeling; Algorithm; Data mining,article,False,"In the realm of data-driven systems, understanding and controlling biases in datasets emerges as a critical challenge.These biases, defined in this study as systematic discrepancies, have the potential to skew algorithmic outcomes and even compromise data privacy.Mutual information serves as a key tool in the analysis, discerning both direct and indirect relationships between variables.Utilizing structural equation modeling, this paper introduces a synthetic dataset generation method founded on a two-step optimization algorithm that aims to fine-tune variable relationships and achieve targeted mutual information levels between attribute pairs.The algorithm's first phase utilizes gradient-less optimization, focusing on individual variables.The subsequent phase harnesses gradient-based methods to unravel deeper variable interdependencies.The approach is dual-purpose: it refines existing datasets for bias mitigation and creates synthetic datasets with defined bias levels, addressing a crucial research gap.Two case studies showcase the methodology.One emphasizes the finesse of network parameter adjustments in a simulated setting.The other applies the methodology to a realistic job hiring dataset, effectively reducing bias while safeguarding key variable relationships.In summary, this paper offers a novel method for bias management, presents tools for quantitative bias adjustments, and provides evidence of the method's broad applicability through varied use cases."
Algorithmic Bias,2025,,The MIT Press eBooks,0,W4410988878,10.7551/mitpress/15834.003.0015,https://openalex.org/W4410988878,,Computer science,book-chapter,False,
Multi-Objective Reliability Optimization using Fuzzy Nonlinear Programming with Interval Membership Functions and Bias Functions: A Comparison of Particle Swarm Optimization and Genetic Algorithm,2023,Ahmed Abdulhussein Jabbar; Audi Sabri Abd ALRazaq,Wasit Journal of Pure sciences,2,W4382794339,10.31185/wjps.140,https://openalex.org/W4382794339,https://wjps.uowasit.edu.iq/index.php/wjps/article/download/140/103,Mathematical optimization; Particle swarm optimization; Interval (graph theory); Reliability (semiconductor); Mathematics,article,True,"Multi-objective reliability optimization is a complex problem that involves simultaneously optimizing multiple objectives while ensuring that the system meets certain reliability requirements. In this paper, we present a methodology for solving multi-objective reliability optimization problems using fuzzy nonlinear programming. The methodology involves representing the reliability of each component as a triangular interval number and each objective function as an interval membership function. Conflicts between objectives are resolved using linear and nonlinear membership functions, and exponential and quadratic membership functions are used to obtain definite biases towards the objective. The proposed methodology employs Particle Swarm Optimization (PSO) or Genetic Algorithm (GA) to solve the problem, and the approach is compared with GA for linear and nonlinear membership functions. The results indicate the effectiveness of the methodology in addressing multi-objective reliability optimization problems"
"Auditing Political Exposure Bias: Algorithmic Amplification on Twitter/X
  Approaching the 2024 U.S. Presidential Election",2024,Jinyi Ye; Luca Luceri; Emilio Ferrara,arXiv (Cornell University),1,W4404390517,10.48550/arxiv.2411.01852,https://openalex.org/W4404390517,http://arxiv.org/pdf/2411.01852,Presidential election; Audit; Presidential system; Politics; Political science,preprint,True,"Approximately 50% of tweets in X's user timelines are personalized recommendations from accounts they do not follow. This raises a critical question: what political content are users exposed to beyond their established networks, and how might this influence democratic discourse online? Due to the black-box nature and constant evolution of social media algorithms, much remains unknown about this aspect of users' content exposure, particularly as it pertains to potential biases in algorithmic curation. Prior research has shown that certain political groups and media sources are amplified within users' in-network tweets. However, the extent to which this amplification affects out-of-network recommendations remains unclear. As the 2024 U.S. Election approaches, addressing this question is essential for understanding the influence of algorithms on online political content consumption and its potential impact on users' perspectives. In this paper, we conduct a three-week audit of X's algorithmic content recommendations using a set of 120 sock-puppet monitoring accounts that capture tweets in their personalized ``For You'' timelines. Our objective is to quantify out-of-network content exposure for right- and left-leaning user profiles and to assess any potential biases in political exposure. Our findings indicate that X's algorithm skews exposure toward a few high-popularity accounts across all users, with right-leaning users experiencing the highest level of exposure inequality. Both left- and right-leaning users encounter amplified exposure to accounts aligned with their own political views and reduced exposure to opposing viewpoints. Additionally, we observe a right-leaning bias in exposure for new accounts within their default timelines."
Unbiased recursive least squares identification methods for a class of nonlinear systems with irregularly missing data,2023,Wenxuan Liu; Meihang Li,International Journal of Adaptive Control and Signal Processing,70,W4379615208,10.1002/acs.3637,https://openalex.org/W4379615208,,Missing data; Bilinear interpolation; Least-squares function approximation; Algorithm; Nonlinear system,article,False,"Summary Missing data often occur in industrial processes. In order to solve this problem, an auxiliary model and a particle filter are adopted to estimate the missing outputs, and two unbiased parameter estimation methods are developed for a class of nonlinear systems (e.g., bilinear systems) with irregularly missing data. Firstly, an auxiliary model is constructed to estimate the unknown output, and an auxiliary model‐based multi‐innovation recursive least squares algorithm is presented by expanding the scalar innovation to an innovation vector. Secondly, according to the bias compensation principle, an auxiliary model‐based bias compensation multi‐innovation recursive least squares algorithm is proposed to compensate the bias caused by the colored noise. Thirdly, for further improving the parameter estimation accuracy, the unknown true output is estimated by a particle filter, and a particle filtering‐based bias compensation multi‐innovation recursive least squares algorithm is developed. Finally, a numerical example is selected to validate the effectiveness of the proposed algorithms. The simulation results indicate that the proposed algorithms have good performance in identifying bilinear systems with irregularly missing data."
Should ChatGPT Be Biased?&amp;nbsp;Challenges and Risks of Bias in Large Language Models,2023,Emilio Ferrara,,91,W4388687124,10.2139/ssrn.4627814,https://openalex.org/W4388687124,,Psychology; Chemistry,preprint,False,"As the capabilities of generative language models continue to advance, the implications of biases ingrained within these models have garnered increasing attention from researchers, practitioners, and the broader public. This article investigates the challenges and risks associated with biases in large-scale language models like ChatGPT. We discuss the origins of biases, stemming from, among others, the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions. We explore the ethical concerns arising from the unintended consequences of biased model outputs. We further analyze the potential opportunities to mitigate biases, the inevitability of some biases, and the implications of deploying these models in various applications, such as virtual assistants, content generation, and chatbots. Finally, we review the current approaches to identify, quantify, and mitigate biases in language models, emphasizing the need for a multi-disciplinary, collaborative effort to develop more equitable, transparent, and responsible AI systems. This article aims to stimulate a thoughtful dialogue within the artificial intelligence community, encouraging researchers and developers to reflect on the role of biases in generative language models and the ongoing pursuit of ethical AI."
"Reviewing the Philippines Legal Landscape of Artificial Intelligence (AI) in Business: Addressing Bias, Explainability, and Algorithmic Accountability",2024,Michael T. Sacramed,International Journal of Research and Innovation in Social Science,1,W4400004471,10.47772/ijriss.2024.805181,https://openalex.org/W4400004471,,Accountability; Business intelligence; Business; Artificial intelligence; Computer science,article,False,"Pushing towards the almost universal adoption of Artificial Intelligence (AI) across the globe, the Philippines is not far behind. This tsunami has huge promise, but at the same time, under the present legal footing, it is likely to raise critical issues of ethics that have yet to be resolved. Against this background, the present paper reviews related literature on this emerging issue of AI bias, explainability, and algorithmic accountability. It comes down mainly to work done regarding bias in AI relative to the domain of recruitment and facial recognition technologies, in this case how it leads to discrimination. This asks to discuss the “black box problem” applied to nontransparent AI systems for which there is a need for the outcome to be explainable. It identifies the Data Privacy Act (DPA) of 2012 as the nearest framework that may be the firm foundation in the assurance of the right to understand AI decision-making. The other issue the article is concerned with is algorithmic accountability. Currently, guiding laws exist in the country, but these are narrow in scope and may not necessarily capture the many faces of AI behavior. In other words, the paper reviews the European Union’s General Data Protection Regulation (GDPR) as a model that can possibly find a solution for the biases. To summarize, this country needs a legal framework to overcome the challenges that have been brought about and reach an agreement on AI explainability enhancement, a clear definition of who is responsible and liable for what, and bias mitigation. The identified gaps in previous studies will form the basis for making recommendations on further research into AI bias within Philippine enterprises. All this underlines ever-necessary comparative research on the other rules concerning AI that has been put in place elsewhere. Still more importantly, it complements reasons for exporting such an idea to which the Philippines should develop an all-encompassing legal framework in demeanor to the rise of responsible and ethical research, development, and deployment of AI."
Algorithmic Trading and AI: A Review of Strategies and Market Impact,2024,Wilhelmina Afua Addy; Adeola Olusola Ajayi-Nifise; Binaebi Gloria Bello; Sunday Tubokirifuruar Tula; Olubusola Odeyemi; Titilola Falaiye,World Journal of Advanced Engineering Technology and Sciences,28,W4391914348,10.30574/wjaets.2024.11.1.0054,https://openalex.org/W4391914348,,Algorithmic trading; Computer science; Financial economics; Economics,review,False,"This review explores the dynamic intersection of algorithmic trading and artificial intelligence (AI) within financial markets. It delves into the evolution, strategies, and broader market impact of algorithmic trading fueled by AI technologies. Examining the symbiotic relationship between advanced algorithms and AI, the review navigates through the various strategies employed, shedding light on their implications for market efficiency, liquidity, and overall stability. From high-frequency trading to machine learning-driven predictive analytics, this review unveils the multifaceted landscape of algorithmic trading in the era of AI, presenting both opportunities and challenges for financial markets. The review begins by tracing the historical development of algorithmic trading, emphasizing the paradigm shift with the integration of AI. From traditional programmatic trading to the emergence of sophisticated algorithms driven by machine learning and deep learning, the evolution sets the stage for a comprehensive understanding of the subject. An in-depth analysis of diverse algorithmic trading strategies unfolds, covering areas such as trend following, statistical arbitrage, market making, and sentiment analysis. The incorporation of AI introduces adaptive learning capabilities, enabling algorithms to evolve and optimize strategies based on real-time market conditions. Exploring the impact of algorithmic trading on financial markets, the review examines how AI-driven strategies contribute to market efficiency, liquidity provision, and price discovery. It dissects the implications for traditional market structures, regulatory considerations, and the potential risks associated with algorithmic dominance. Acknowledging the transformative power of algorithmic trading with AI, the review critically assesses the challenges and ethical considerations. From algorithmic bias to systemic risks, the review delves into the darker corners of this technological advancement, prompting a reflection on the need for responsible and transparent practices. The review concludes by peering into the future trajectory of algorithmic trading fueled by AI. Anticipated innovations, regulatory responses, and the evolving landscape of financial markets are discussed, offering insights into the ongoing transformation and potential disruptions in the realm of algorithmic trading. In essence, this review provides a nuanced perspective on the intricate relationship between algorithmic trading and AI, offering a comprehensive understanding of their strategies and the transformative impact on financial markets."
A robust federated biased learning algorithm for time series forecasting,2025,Mingli Song; Xinyu Zhao; Witold Pedrycz,Cluster Computing,1,W4407910880,10.1007/s10586-024-05061-7,https://openalex.org/W4407910880,,Computer science; Series (stratigraphy); Algorithm; Time series; Artificial intelligence,article,False,
Algorithmic Bias and the New Chicago School,2025,J. Hannah Lee,arXiv (Cornell University),0,W4407122907,10.48550/arxiv.2502.00014,https://openalex.org/W4407122907,http://arxiv.org/pdf/2502.00014,Computer science; Mathematics education; Psychology,preprint,True,"AI systems are increasingly deployed in both public and private sectors to independently make complicated decisions with far-reaching impact on individuals and the society. However, many AI algorithms are biased in the collection or processing of data, resulting in prejudiced decisions based on demographic features. Algorithmic biases occur because of the training data fed into the AI system or the design of algorithmic models. While most legal scholars propose a direct-regulation approach associated with the right of explanation or transparency obligation, this article provides a different picture regarding how indirect regulation can be used to regulate algorithmic bias based on the New Chicago School framework developed by Lawrence Lessig. This article concludes that an effective regulatory approach toward algorithmic bias will be the right mixture of direct and indirect regulations through architecture, norms, market, and the law."
Seiðr: Efficient calculation of robust ensemble gene networks,2023,Bastian Schiffthaler; Elena van Zalen; Alonso R. Serrano; Nathaniel R. Street; Nicolas Delhomme,Heliyon,19,W4378902303,10.1016/j.heliyon.2023.e16811,https://openalex.org/W4378902303,https://www.cell.com/article/S2405844023040185/pdf,Inference; Computer science; Gene regulatory network; Computational biology; Data mining,article,True,"Gene regulatory and gene co-expression networks are powerful research tools for identifying biological signal within high-dimensional gene expression data. In recent years, research has focused on addressing shortcomings of these techniques with regard to the low signal-to-noise ratio, non-linear interactions and dataset dependent biases of published methods. Furthermore, it has been shown that aggregating networks from multiple methods provides improved results. Despite this, few useable and scalable software tools have been implemented to perform such best-practice analyses. Here, we present Seidr (stylized Seiðr), a software toolkit designed to assist scientists in gene regulatory and gene co-expression network inference. Seidr creates community networks to reduce algorithmic bias and utilizes noise corrected network backboning to prune noisy edges in the networks. Using benchmarks in real-world conditions across three eukaryotic model organisms, Saccharomyces cerevisiae, Drosophila melanogaster, and Arabidopsis thaliana, we show that individual algorithms are biased toward functional evidence for certain gene-gene interactions. We further demonstrate that the community network is less biased, providing robust performance across different standards and comparisons for the model organisms. Finally, we apply Seidr to a network of drought stress in Norway spruce (Picea abies (L.) H. Krast) as an example application in a non-model species. We demonstrate the use of a network inferred using Seidr for identifying key components, communities and suggesting gene function for non-annotated genes."
A robust federated biased learning algorithm for time series forecasting ,2024,Mingli Song; Xinyu Zhao; Witold Pedrycz,Research Square (Research Square),1,W4400886164,10.21203/rs.3.rs-4658479/v1,https://openalex.org/W4400886164,,Series (stratigraphy); Computer science; Time series; Artificial intelligence; Algorithm,preprint,False,"<title>Abstract</title> The federated averaging algorithm (FedAvg) is extensively used for multi-sensor data modeling but often overlooks the unique characteristics of local models when privacy and data security are not considered. This study introduces a novel federated learning algorithm built upon the FedAvg framework, which emphasizes the specificity of each local model to optimize global knowledge aggregation. The algorithm's effectiveness is demonstrated through an air quality index prediction problem, showcasing superior prediction performance and robustness in noisy data scenarios. Additionally, the study delves into the reliability and robustness of the proposed approach, addressing the prevalent notion that centralized learning methods often surpass federated learning when data security is not a concern. Our experiments affirm the necessity and superiority of federated learning methods, even in the absence of privacy considerations, by effectively managing real-world noisy data."
Mitigating Bias in Algorithmic Decision-making: Evaluating Claims and Practices,2023,Manish Raghavan,ACM eBooks,1,W4386558425,10.1145/3603195.3603212,https://openalex.org/W4386558425,,Computer science; Risk analysis (engineering); Business,book-chapter,False,No abstract available.
Artificial Intelligence in Healthcare,2023,C. V. Suresh Babu; N. S. Akshayah; P. Maclin Vinola,Advances in computational intelligence and robotics book series,16,W4388782329,10.4018/978-1-6684-9814-9.ch005,https://openalex.org/W4388782329,,Health care; Transformative learning; Context (archaeology); Perspective (graphical); Independence (probability theory),book-chapter,False,"This chapter critically examines the claim that “healthcare independence relies on total dependence on artificial intelligence” in the context of the integration of AI in healthcare. It explores the role of AI in improving diagnostic accuracy, treatment planning, and operational efficiency. However, it also acknowledges the limitations and ethical considerations associated with AI, such as algorithmic biases and patient privacy concerns. The chapter emphasizes the importance of maintaining a patient-centric approach and preserving the human element in healthcare, with AI serving as a supportive tool rather than a replacement for human expertise. Interdisciplinary collaboration is highlighted as crucial in fully harnessing AI's potential in healthcare. Overall, the chapter provides a nuanced perspective on the transformative potential of AI in achieving healthcare independence while acknowledging the need for responsible and ethical AI implementation."
Digital Humans to Combat Loneliness and Social Isolation: Ethics Concerns and Policy Recommendations,2024,Nancy S. Jecker; Robert Sparrow; Zohar Lederman; Anita Ho,The Hastings Center Report,14,W4392105809,10.1002/hast.1562,https://openalex.org/W4392105809,https://doi.org/10.1002/hast.1562,Loneliness; Isolation (microbiology); Social isolation; Internet privacy; Globe,article,True,"Abstract Social isolation and loneliness are growing concerns around the globe that put people at increased risk of disease and early death. One much‐touted approach to addressing them is deploying artificially intelligent agents to serve as companions for socially isolated and lonely people. Focusing on digital humans, we consider evidence and ethical arguments for and against this approach. We set forth and defend public health policies that respond to concerns about replacing humans, establishing inferior relationships, algorithmic bias, distributive justice, and data privacy ."
"A Practical Introduction to Generative AI, Synthetic Media, and the Messages Found in the Latest Medium",2023,Jon M. Garon,SSRN Electronic Journal,26,W4328024808,10.2139/ssrn.4388437,https://openalex.org/W4328024808,,Generative grammar; Computer science; Data science; Artificial intelligence,article,False,"OpenAI’s text generation program ChatGPT and the text-to-image generators Stable Diffusion and Dall-E have broken records for early public adoption, capital investment, and a technological shift potentially more far-reaching than even the internet itself. The broad category of generative AI has the potential to disrupt industry, art, and culture, both if done poorly and if done well. Despite significant problems with accuracy and deep concerns about the social and legal consequences of the premature adoption of these technologies, global multinational enterprises are moving these projects out of the test labs and into everyday use. This article provides a comprehensive, but introductory overview of the development of generative AI, the training methods used to produce artificially generated content, the industry opportunities for generative AI, and the legal considerations that enterprises adopting these technologies should consider. After discussing the development and implementation of the technology, the article emphasizes the key concerns regarding algorithmic bias, adherence to civil rights laws and community standards, concerns regarding defamation, and legal liability under intellectual property laws including copyright, trademark, and trade secret. The article also provides common sense steps that enterprises adopting early generative AI systems should incorporate into their agreements with the producers of generative AI networks."
Algorithmic individual fairness and healthcare: a scoping review,2024,J. W. Anderson; Shyam Visweswaran,JAMIA Open,7,W4405906634,10.1093/jamiaopen/ooae149,https://openalex.org/W4405906634,https://doi.org/10.1093/jamiaopen/ooae149,Health care; Computer science; Psychology; Management science; Data science,review,True,"Statistical and artificial intelligence algorithms are increasingly being developed for use in healthcare. These algorithms may reflect biases that magnify disparities in clinical care, and there is a growing need for understanding how algorithmic biases can be mitigated in pursuit of algorithmic fairness. We conducted a scoping review on algorithmic individual fairness (IF) to understand the current state of research in the metrics and methods developed to achieve IF and their applications in healthcare."
Auditing Political Exposure Bias: Algorithmic Amplification on Twitter/X Approaching the 2024 U.S. Presidential Election,2024,Jinyi Ye; Luca Luceri; Emilio Ferrara,,2,W4405359942,10.2139/ssrn.5018879,https://openalex.org/W4405359942,,Presidential election; Politics; Audit; Presidential system; Political science,preprint,False,
Algorithmic Bias: Causes and Effects on Marginalized Communities,2023,Katrina Baha,,0,W4377693683,10.22371/04.2023.001,https://openalex.org/W4377693683,https://digital.sandiego.edu/cgi/viewcontent.cgi?article=1109&context=honors_theses,Health care; Set (abstract data type); Order (exchange); Public relations; Computer science,dissertation,True,"Individuals from marginalized backgrounds face different healthcare outcomes due to algorithmic bias in the technological healthcare industry. Algorithmic biases, which are the biases that arise from the set of steps used to solve or analyze a problem, are evident when people from marginalized communities use healthcare technology. For example, many pulse oximeters, which are the medical devices used to measure oxygen saturation in the blood, are not able to accurately read people who have darker skin tones. Thus, people with darker skin tones are not able to receive proper health care due to their pulse oximetry data being inaccurate. This research aims to highlight the ethical implications of marginalized communities facing different healthcare outcomes and provide suggestions on how to prevent algorithmic bias from appearing in healthcare. In order to do this, this paper will first give examples of algorithmic bias, then discuss the ethical implications of those biases, and lastly provide solutions that may help prevent algorithmic bias. It is unethical that marginalized communities are being misread, misdiagnosed, and mistreated due to algorithmic biases. Additionally, the technological healthcare industry must be diversified in order to prevent algorithmic biases from arising in their medical technologies."
Implications of Algorithmic Bias in Financial Services,2024,Dushyant Sengar,"Advances in finance, accounting, and economics book series",0,W4394804229,10.4018/979-8-3693-1758-7.ch004,https://openalex.org/W4394804229,,Livelihood; Intervention (counseling); Benchmark (surveying); Financial services; Data science,book-chapter,False,"Algorithms are wonderful and need data, technology, and human ethical intervention to perform. But they can easily cause biases leading to severe implications due to an imbalance of these factors. It is evident that algorithms, specifically AI and ML, are revolutionary forces that have disrupted several sectors, but in finance and healthcare, they can have major implications on human lives given their direct impact on the livelihood and health of individuals. This makes them the most highly regulated industries. The research shows four benchmark social norms that algorithms can potentially defy causing major societal ramifications. These algorithms categorized by risk levels breach social benchmarks impacting stakeholders across the spectrum. Navigating through real-world sources and implications of algorithmic bias, this chapter underscores the importance of addressing algorithmic bias to inform industry stakeholders, policymakers, and researchers about the pros and cons of AI adoption. This is to fortify the ethical foundation of financial systems as it relates to the use of AI."
Democratising or disrupting diagnosis? Ethical issues raised by the use of AI tools for rare disease diagnosis,2023,Nina Hallowell; Shirlene Badger; Francis McKay; Angeliki Kerasidou; Christoffer Nellåker,SSM - Qualitative Research in Health,20,W4320912368,10.1016/j.ssmqr.2023.100240,https://openalex.org/W4320912368,https://doi.org/10.1016/j.ssmqr.2023.100240,Medical diagnosis; Context (archaeology); Workforce; Stakeholder; Artificial intelligence,article,True,"Computational phenotyping (CP) technology uses facial recognition algorithms to classify and potentially diagnose rare genetic disorders on the basis of digitised facial images. This AI technology has a number of research as well as clinical applications, such as supporting diagnostic decision-making. Using the example of CP, we examine stakeholders' views of the benefits and costs of using AI as a diagnostic tool within the clinic. Through a series of in-depth interviews (n ​= ​20) with: clinicians, clinical researchers, data scientists, industry and support group representatives, we report stakeholder views regarding the adoption of this technology in a clinical setting. While most interviewees were supportive of employing CP as a diagnostic tool in some capacity we observed ambivalence around the potential for artificial intelligence to overcome diagnostic uncertainty in a clinical context. Thus, while there was widespread agreement amongst interviewees concerning the public benefits of AI assisted diagnosis, namely, its potential to increase diagnostic yield and enable faster more objective and accurate diagnoses by up skilling non specialists and thereby enabling access to diagnosis that is potentially lacking, interviewees also raised concerns about ensuring algorithmic reliability, expunging algorithmic bias and that the use of AI could result in deskilling the specialist clinical workforce. We conclude that, prior to widespread clinical implementation, on-going reflection is needed regarding the trade-offs required to determine acceptable levels of bias and conclude that diagnostic AI tools should only be employed as an assistive technology within the dysmorphology clinic."
Artificial Intelligence (AI) Ethics in Accounting,2024,Brandon Schweitze,Journal of Accounting Ethics & Public Policy,15,W4394860663,10.60154/jaepp.2024.v25n1p67,https://openalex.org/W4394860663,https://www.jaepp.org/index.php/jaepp/article/download/349/329/445,Transparency (behavior); Accountability; Safeguarding; Engineering ethics; Ethical decision,article,True,"The rapid advancement of artificial intelligence (AI) has revolutionized the accounting profession, automating tasks, identifying patterns, and improving accuracy. However, the increasing reliance on AI raises ethical concerns regarding privacy, bias, transparency, and accountability. This research paper delves into the ethical considerations of AI implementation in accounting practices.Thepaper begins by examining the potential benefits of AI in accounting, highlighting its ability to streamline operations, enhance efficiency, and reduce errors. However, it also acknowledges the ethical risks associated with AI, including data privacy breaches, biased decision-making, lack of transparency, and accountability issues.The paper proposes a framework for responsible AI implementation in accounting to address these ethical concerns. The framework emphasizes establishing clear ethical guidelines,ensuring data privacy and security, mitigating AI algorithms' bias, promoting AI decisionmaking transparency, and establishing accountability mechanisms.The paper further explores the role of accountants in addressing AI ethics. Accountants are responsible for upholding ethical standards and ensuring that AI systems are used responsibly and ethically. They must be aware of the ethical implications of AI and have the knowledge and skills to mitigate ethical risks.In conclusion, the paper emphasizes the need for a proactive approach to AI ethics in accounting. By establishing clear ethical guidelines, promoting responsible AI implementation, and empowering accountants with ethical knowledge and skills, the accounting profession can harness the potential of AI while upholding ethical principles and safeguarding public trust."
Bias in context: What to do when complete bias removal is not an option,2023,Sune Holm; Eike Petersen; Melanie Ganz; Aasa Feragen,Proceedings of the National Academy of Sciences,5,W4378781538,10.1073/pnas.2304710120,https://openalex.org/W4378781538,https://doi.org/10.1073/pnas.2304710120,Context (archaeology); Computer science; Environmental science; Geology; Paleontology,letter,True,"It is widely recognized that machine learning algorithms may be biased in the sense that they perform worse on some demographic groups than others. This motivates algorithmic development to remove algorithmic bias, which in turn might lead to a hope—even an expectation—that algorithmic bias can be mitigated or removed (1). In this short comment, we make three points to qualify Wang et al.’s suggestion: 1) It may not be possible for algorithms to perform equally well across groups on all measures, 2) which inequalities count as morally unacceptable bias is an ethical question, and 3) the answer to the ethical question will vary across decision contexts."
Toward A Two-Sided Fairness Framework in Search and Recommendation,2023,Jiqun Liu,,14,W4327909690,10.1145/3576840.3578332,https://openalex.org/W4327909690,https://doi.org/10.1145/3576840.3578332,Debiasing; Computer science; Recommender system; Perspective (graphical); Bounded rationality,article,False,"As artificial intelligence (AI) assisted search and recommender systems have become ubiquitous in workplaces and everyday lives, understanding and accounting for fairness has gained increasing attention in the design and evaluation of such systems. While there is a growing body of computing research on measuring system fairness and biases associated with data and algorithms, the impact of human biases that go beyond traditional machine learning (ML) pipelines still remain understudied. In this Perspective Paper, we seek to develop a two-sided fairness framework that not only characterizes data and algorithmic biases, but also highlights the cognitive and perceptual biases that may exacerbate system biases and lead to unfair decisions. Within the framework, we also analyze the interactions between human and system biases in search and recommendation episodes. Built upon the two-sided framework, our research synthesizes intervention and intelligent nudging strategies applied in cognitive and algorithmic debiasing, and also proposes novel goals and measures for evaluating the performance of systems in addressing and proactively mitigating the risks associated with biases in data, algorithms, and bounded rationality. This paper uniquely integrates the insights regarding human biases and system biases into a cohesive framework and extends the concept of fairness from human-centered perspective. The extended fairness framework better reflects the challenges and opportunities in users' interactions with search and recommender systems of varying modalities. Adopting the two-sided approach in information system design has the potential to enhancing both the effectiveness in online debiasing and the usefulness to boundedly rational users engaging in information-intensive decision-making."
Legal Taxonomies of Machine Bias: Revisiting Direct Discrimination,2023,Reuben Binns; Jeremias Adams‐Prassl; Aislinn Kelly‐Lyth,"2022 ACM Conference on Fairness, Accountability, and Transparency",6,W4380322089,10.1145/3593013.3594121,https://openalex.org/W4380322089,https://dl.acm.org/doi/pdf/10.1145/3593013.3594121,Disparate impact; Computer science; Argument (complex analysis); Doctrine; Context (archaeology),article,True,"Previous literature on 'fair' machine learning has appealed to legal frameworks of discrimination law to motivate a variety of discrimination and fairness metrics and de-biasing measures. Such work typically applies the US doctrine of disparate impact rather than the alternative of disparate treatment, and scholars of EU law have largely followed along similar lines, addressing algorithmic bias as a form of indirect rather than direct discrimination. In recent work, we have argued that such focus is unduly narrow in the context of European law: certain forms of algorithmic bias will constitute direct discrimination [1]. In this paper, we explore the ramifications of this argument for existing taxonomies of machine bias and algorithmic fairness, how existing fairness metrics might need to be adapted, and potentially new measures may need to be introduced. We outline how the mappings between fairness measures and discrimination definitions implied hitherto may need to be revised and revisited."
"Development of stacking algorithm for bias-correcting the precipitation projections using a multi-model ensemble of CMIP6 GCMs in a semi-arid basin, India",2025,Hemanandhini Shanmugam; V. Lakshmanan,Theoretical and Applied Climatology,3,W4406778237,10.1007/s00704-024-05321-x,https://openalex.org/W4406778237,,Precipitation; Climatology; Arid; Algorithm; Environmental science,article,False,
Towards Understanding Fairness and its Composition in Ensemble Machine Learning,2023,Usman Gohar; Sumon Biswas; Hridesh Rajan,,24,W4384302771,10.1109/icse48619.2023.00133,https://openalex.org/W4384302771,http://arxiv.org/pdf/2212.04593,Computer science; Ensemble learning; Fairness measure; Composition (language); Artificial intelligence,preprint,True,"Machine Learning (ML) software has been widely adopted in modern society, with reported fairness implications for minority groups based on race, sex, age, etc. Many recent works have proposed methods to measure and mitigate algorithmic bias in ML models. The existing approaches focus on single classifier-based ML models. However, real-world ML models are often composed of multiple independent or dependent learners in an ensemble (e.g., Random Forest), where the fairness composes in a non-trivial way. How does fairness compose in ensembles? What are the fairness impacts of the learners on the ultimate fairness of the ensemble? Can fair learners result in an unfair ensemble? Furthermore, studies have shown that hyperparameters influence the fairness of ML models. Ensemble hyperparameters are more complex since they affect how learners are combined in different categories of ensembles. Understanding the impact of ensemble hyperparameters on fairness will help programmers design fair ensembles. Today, we do not understand these fully for different ensemble algorithms. In this paper, we comprehensively study popular real-world ensembles: bagging, boosting, stacking and voting. We have developed a benchmark of 168 ensemble models collected from Kaggle on four popular fairness datasets. We use existing fairness metrics to understand the composition of fairness. Our results show that ensembles can be designed to be fairer without using mitigation techniques. We also identify the interplay between fairness composition and data characteristics to guide fair ensemble design. Finally, our benchmark can be leveraged for further research on fair ensembles. To the best of our knowledge, this is one of the first and largest studies on fairness composition in ensembles yet presented in the literature."
Computer-Aided Drug Design and Drug Discovery: A Prospective Analysis,2023,Sarfaraz K. Niazi; Zamara Mariam,Pharmaceuticals,154,W4390114224,10.3390/ph17010022,https://openalex.org/W4390114224,https://www.mdpi.com/1424-8247/17/1/22/pdf?version=1703240078,Expediting; Computer science; Data science; Drug discovery; Transformative learning,review,True,"In the dynamic landscape of drug discovery, Computer-Aided Drug Design (CADD) emerges as a transformative force, bridging the realms of biology and technology. This paper overviews CADDs historical evolution, categorization into structure-based and ligand-based approaches, and its crucial role in rationalizing and expediting drug discovery. As CADD advances, incorporating diverse biological data and ensuring data privacy become paramount. Challenges persist, demanding the optimization of algorithms and robust ethical frameworks. Integrating Machine Learning and Artificial Intelligence amplifies CADDs predictive capabilities, yet ethical considerations and scalability challenges linger. Collaborative efforts and global initiatives, exemplified by platforms like Open-Source Malaria, underscore the democratization of drug discovery. The convergence of CADD with personalized medicine offers tailored therapeutic solutions, though ethical dilemmas and accessibility concerns must be navigated. Emerging technologies like quantum computing, immersive technologies, and green chemistry promise to redefine the future of CADD. The trajectory of CADD, marked by rapid advancements, anticipates challenges in ensuring accuracy, addressing biases in AI, and incorporating sustainability metrics. This paper concludes by highlighting the need for proactive measures in navigating the ethical, technological, and educational frontiers of CADD to shape a healthier, brighter future in drug discovery."
AI and Machine Learning in Smart Education,2024,Pawan Kumar Goel; Amit Singhal; Shailendra Singh Bhadoria; Birendra Kumar Saraswat; Arvind D. Patel,Advances in web technologies and engineering book series,16,W4391229972,10.4018/979-8-3693-0782-3.ch003,https://openalex.org/W4391229972,,Engineering ethics; Computer science; Personalized learning; Ethical issues; Knowledge management,book-chapter,False,"This chapter explores the applications, benefits, and challenges of integrating AI and ML in smart education, focusing on how these technologies can enhance learning experiences, personalize education, and improve learning outcomes. It also addresses ethical and privacy concerns, highlighting the need for robust policies and guidelines to mitigate them and protect students' rights. AI and ML can enable personalized learning experiences, tailor content, delivery, and assessment to individual needs, and support competency-based education. However, the chapter acknowledges the challenges of privacy, security, algorithmic biases, teacher training, and ethical implications. By embracing these recommendations, educators and policymakers can harness the full potential of AI and ML technologies in creating a smarter and more effective educational environment."
Personalized cancer vaccine design using AI-powered technologies,2024,Anant Kumar; Shriniket Dixit; Kathiravan Srinivasan; M. Dinakaran; P. M. Durai Raj Vincent,Frontiers in Immunology,27,W4404196756,10.3389/fimmu.2024.1357217,https://openalex.org/W4404196756,https://doi.org/10.3389/fimmu.2024.1357217,Cancer; Cancer vaccine; Cancer immunotherapy; Medicine; Precision medicine,review,True,"Immunotherapy has ushered in a new era of cancer treatment, yet cancer remains a leading cause of global mortality. Among various therapeutic strategies, cancer vaccines have shown promise by activating the immune system to specifically target cancer cells. While current cancer vaccines are primarily prophylactic, advancements in targeting tumor-associated antigens (TAAs) and neoantigens have paved the way for therapeutic vaccines. The integration of artificial intelligence (AI) into cancer vaccine development is revolutionizing the field by enhancing various aspect of design and delivery. This review explores how AI facilitates precise epitope design, optimizes mRNA and DNA vaccine instructions, and enables personalized vaccine strategies by predicting patient responses. By utilizing AI technologies, researchers can navigate complex biological datasets and uncover novel therapeutic targets, thereby improving the precision and efficacy of cancer vaccines. Despite the promise of AI-powered cancer vaccines, significant challenges remain, such as tumor heterogeneity and genetic variability, which can limit the effectiveness of neoantigen prediction. Moreover, ethical and regulatory concerns surrounding data privacy and algorithmic bias must be addressed to ensure responsible AI deployment. The future of cancer vaccine development lies in the seamless integration of AI to create personalized immunotherapies that offer targeted and effective cancer treatments. This review underscores the importance of interdisciplinary collaboration and innovation in overcoming these challenges and advancing cancer vaccine development."
"AI-powered revolution in plant sciences: advancements, applications, and challenges for sustainable agriculture and food security",2024,Deependra Kumar Gupta; Anselmo Pagani; Paolo Zamboni; Ajay Kumar Singh,,17,W4401385722,10.37349/eff.2024.00045,https://openalex.org/W4401385722,https://www.explorationpub.com/uploads/Article/A101045/101045.pdf,Food security; Sustainable agriculture; Agriculture; Interpretability; Robustness (evolution),article,True,"Artificial intelligence (AI) is revolutionizing plant sciences by enabling precise plant species identification, early disease diagnosis, crop yield prediction, and precision agriculture optimization. AI uses machine learning and image recognition to aid ecological research and biodiversity conservation. It plays a crucial role in plant breeding by accelerating the development of resilient, high-yielding crops with desirable traits. AI models using climate and soil data contribute to sustainable agriculture and food security. In plant phenotyping, AI automates the measurement and analysis of plant characteristics, enhancing our understanding of plant growth. Ongoing research aims to improve AI models’ robustness and interpretability while addressing data privacy and algorithmic biases. Interdisciplinary collaboration is essential to fully harness AI’s potential in plant sciences for a sustainable, food-secure future."
Machine Learning and Artificial Intelligence in Disease Prediction,2023,Kama Ramudu; V. Murali Mohan; D. Jyothirmai; D. V. S. S. S. V. Prasad; Ruchi Agrawal; Sampath Boopathi,Advances in healthcare information systems and administration book series,112,W4385464598,10.4018/978-1-6684-8913-0.ch013,https://openalex.org/W4385464598,,Artificial intelligence; Machine learning; Computer science; Field (mathematics); Identification (biology),book-chapter,False,"Artificial intelligence (AI) based disease identification has the potential to transform medicine by utilizing machine learning algorithms and techniques to analyze large volumes of medical data and identify patterns and features that may be difficult for human experts to detect. However, there are still challenges and limitations to overcome, such as the need for high-quality medical data and concerns around privacy and bias. This chapter explores the growing intersection of machine learning (ML) and AI techniques with disease prediction. The chapter begins by providing an overview of ML and AI methodologies commonly employed in disease prediction, including supervised and unsupervised learning algorithms, deep learning techniques, and ensemble methods. Lastly, the chapter outlines future directions and research opportunities in the field."
"Harnessing artificial intelligence in sepsis care: advances in early detection, personalized treatment, and real-time monitoring",2025,Fang Li; Shengguo Wang; Zhi Gao; Ma Qing; Shan L. Pan; Yingying Liu; Chengchen Hu,Frontiers in Medicine,18,W4406112882,10.3389/fmed.2024.1510792,https://openalex.org/W4406112882,https://doi.org/10.3389/fmed.2024.1510792,Sepsis; Intensive care medicine; Medicine; Computer science; Internal medicine,review,True,"Sepsis remains a leading cause of morbidity and mortality worldwide due to its rapid progression and heterogeneous nature. This review explores the potential of Artificial Intelligence (AI) to transform sepsis management, from early detection to personalized treatment and real-time monitoring. AI, particularly through machine learning (ML) techniques such as random forest models and deep learning algorithms, has shown promise in analyzing electronic health record (EHR) data to identify patterns that enable early sepsis detection. For instance, random forest models have demonstrated high accuracy in predicting sepsis onset in intensive care unit (ICU) patients, while deep learning approaches have been applied to recognize complications such as sepsis-associated acute respiratory distress syndrome (ARDS). Personalized treatment plans developed through AI algorithms predict patient-specific responses to therapies, optimizing therapeutic efficacy and minimizing adverse effects. AI-driven continuous monitoring systems, including wearable devices, provide real-time predictions of sepsis-related complications, enabling timely interventions. Beyond these advancements, AI enhances diagnostic accuracy, predicts long-term outcomes, and supports dynamic risk assessment in clinical settings. However, ethical challenges, including data privacy concerns and algorithmic biases, must be addressed to ensure fair and effective implementation. The significance of this review lies in addressing the current limitations in sepsis management and highlighting how AI can overcome these hurdles. By leveraging AI, healthcare providers can significantly enhance diagnostic accuracy, optimize treatment protocols, and improve overall patient outcomes. Future research should focus on refining AI algorithms with diverse datasets, integrating emerging technologies, and fostering interdisciplinary collaboration to address these challenges and realize AI's transformative potential in sepsis care."
Transformative AI in human resource management: enhancing workforce planning with topic modeling,2024,Murale Venugopal; Vandana Madhavan; Rajiv Prasad; Raghu Raman,Cogent Business & Management,12,W4404772152,10.1080/23311975.2024.2432550,https://openalex.org/W4404772152,https://doi.org/10.1080/23311975.2024.2432550,Transformative learning; Workforce; Knowledge management; Business; Workforce planning,article,True,"This study explores the transformative role of artificial intelligence (AI) in human resource management (HRM), focusing on key functions such as recruitment, retention, and performance management. A comprehensive review was carried out PRISMA framework and BERTopic model on AI and HRM‑related keywords. The resulting publications were analyzed to extract meaningful topics. AI‑driven tools streamline candidate screening and interview analysis, significantly enhancing hiring efficiency and decision‑making accuracy. Concerns about algorithmic bias highlight the need for robust governance frameworks to ensure transparency and fairness in AI‑driven processes. The study emphasizes the importance of aligning AI adoption with Organizational Development principles to foster inclusivity and organizational justice. The integration of AI in performance management facilitates real‑time, objective performance assessments, although overreliance on such technologies can affect employee trust and engagement. Despite these advances, the study highlights ethical concerns surrounding data privacy and the potential for algorithmic bias. Addressing these challenges requires the implementation of comprehensive ethical frameworks to promote fairness and inclusivity in AI‑HRM applications. Strategically, AI transforms HR from a reactive function to a proactive, data‑driven partner aligned with long‑term organizational goals. Successful AI integration depends on governance mechanisms that uphold ethical standards, foster employee trust, and ensure transparency, enabling organizations to fully leverage AI's potential in enhancing workforce management."
"Algorithmic Bias, Generalist Models,and Clinical Medicine",2023,Geoff Keeling,arXiv (Cornell University),0,W4375957568,10.48550/arxiv.2305.04008,https://openalex.org/W4375957568,https://arxiv.org/abs/2305.04008,Generalist and specialist species; Computer science; Machine learning; Artificial intelligence; Data science,preprint,True,"The technical landscape of clinical machine learning is shifting in ways that destabilize pervasive assumptions about the nature and causes of algorithmic bias. On one hand, the dominant paradigm in clinical machine learning is narrow in the sense that models are trained on biomedical datasets for particular clinical tasks such as diagnosis and treatment recommendation. On the other hand, the emerging paradigm is generalist in the sense that general-purpose language models such as Google's BERT and PaLM are increasingly being adapted for clinical use cases via prompting or fine-tuning on biomedical datasets. Many of these next-generation models provide substantial performance gains over prior clinical models, but at the same time introduce novel kinds of algorithmic bias and complicate the explanatory relationship between algorithmic biases and biases in training data. This paper articulates how and in what respects biases in generalist models differ from biases in prior clinical models, and draws out practical recommendations for algorithmic bias mitigation."
Performance assessment and exhaustive listing of 500+ nature-inspired metaheuristic algorithms,2023,Zhongqiang Ma; Guohua Wu; Ponnuthurai Nagaratnam Suganthan; Aijuan Song; Qizhang Luo,Swarm and Evolutionary Computation,145,W4316113584,10.1016/j.swevo.2023.101248,https://openalex.org/W4316113584,https://doi.org/10.1016/j.swevo.2023.101248,Metaheuristic; Computer science; Benchmark (surveying); Parallel metaheuristic; Algorithm,article,True,"Metaheuristics are popularly used in various fields, and they have attracted much attention in the scientific and industrial communities. In recent years, the number of new metaheuristic names has been continuously growing. Generally, the inventors attribute the novelties of these new algorithms to inspirations from either biology, human behaviors, physics, or other phenomena. In addition, these new algorithms, compared against basic versions of other metaheuristics using classical benchmark problems, show competitive performances. However, many new metaheuristics are not rigorously tested on challenging benchmark suites and are not compared with state-of-the-art metaheuristic variants. Therefore, in this study, we exhaustively tabulate more than 500 metaheuristics. In particular, several representative metaheuristics are introduced from two aspects, namely, the inspirational source and the essential operators for generating solutions. To comparatively evaluate the performance of the state-of-the-art and newly proposed metaheuristics, 11 newly proposed metaheuristics (generally with high numbers of citations) and 4 state-of-the-art metaheuristics are comprehensively compared on the CEC2017 benchmark suite. For fair comparisons, a parameter tuning tool named irace is used to automatically configure the parameters of all 15 algorithms. In addition, whether these algorithms have a search bias to the origin (i.e., the center of the search space) is investigated. All the experimental results are analyzed by several nonparametric statistical methods, including the Bayesian rank-sum test, Friedman test, Wilcoxon signed-rank test, critical difference plot and Bayesian signed-rank test. Moreover, the convergence, diversity, and the trade-off between exploration and exploitation of these 15 algorithms are also analyzed. The results show that the performance of the newly proposed EBCM algorithm performs similarly to the 4 compared algorithms and has the same properties and behaviors, such as convergence, diversity, exploration and exploitation trade-offs, in many aspects. However, the other 10 recent metaheuristics are less efficient and robust than the 4 state-of-the-art metaheuristics. The performance of all 15 of the algorithms is likely to deteriorate due to certain transformations, while the 4 state-of-the-art metaheuristics are less affected by transformations such as the shifting of the global optimal point away from the center of the search space. It should be noted that, except EBCM, the other 10 new algorithms are inferior to the 4 state-of-the-art algorithms in terms of convergence speed and global search ability on CEC 2017 functions. Moreover, the other 10 new algorithms are rougher (i.e., present in their behavior with high oscillations) in terms of the trade-off between exploitation and exploration and population diversity compared with the 4 state-of-the-art algorithms. Finally, several important issues relevant to the metaheuristic research area are discussed and some potential research directions are suggested."
AI Algorithmic Bias and Manipulation in Social Networks,2025,Rupa Rani; Harnit Saini,BENTHAM SCIENCE PUBLISHERS eBooks,0,W4414444161,10.2174/9798898810030125040009,https://openalex.org/W4414444161,,Computer science; Psychology; Cognitive science; Artificial intelligence,book-chapter,False,"Algorithms are increasingly being employed in our daily lives to make decisions, and the manipulation of algorithmic bias is becoming a serious worry. Because these decisions greatly impact people and society, they must be neutral and fair. The purpose of this study is to raise awareness of the potential drawbacks of algorithmic bias reduction. Algorithmic bias manipulation can have a wide range of negative social consequences. It has the potential to discriminate against or favor specific outcomes for certain groups of individuals. This method may result in inequity and social injustice. Biased algorithm manipulation can have serious repercussions for persons and society. To lessen the impact, steps need to be taken. Algorithmic bias modification has a wide range of applications. It can be used to target certain racial or ethnic groups, or it can be used to promote specific results, such as higher earnings. One disadvantage is that algorithmic bias manipulation might be difficult to identify and prevent. It might be difficult to determine whether or not algorithms are prejudiced because they are often highly complicated and advanced. The manipulation of algorithms to introduce bias has serious ramifications for people and society. It is critical to implement mitigation measures. Algorithm manipulation to introduce bias is a critical issue that can hurt both society and individuals. Understanding and protecting against the possibility of manipulating algorithmic bias are crucial."
Artificial Intelligence And Cancer Care in Africa,2024,Adewunmi Akingbola; Abiodun Adegbesan; Olajide Ojo; Jessica Urowoli Otumara; Uthman Hassan Alao,Journal of Medicine Surgery and Public Health,26,W4401487607,10.1016/j.glmedi.2024.100132,https://openalex.org/W4401487607,https://doi.org/10.1016/j.glmedi.2024.100132,Cancer; Medicine; Internal medicine,article,True,"AI's potential to revolutionize oncology through enhanced diagnostics, treatment planning, and patient monitoring is well-documented globally. However, in Africa, its adoption has been slower, albeit steadily progressing. This commentary explores the integration of artificial Intelligence in cancer care across Africa, assessing its current state, challenges and future directions. It highlights significant AI innovations in cancer diagnostics, such as DataPathology, PapsAI, MinoHealth, and Hurone AI, which utilize AI for tissue analysis, cervical cell imaging, disease forecasting, and remote patient monitoring. Despite these advancements, several challenges impede AI's full integration into African healthcare systems. Key issues include data privacy and security, algorithm bias, and insufficient regulatory frameworks. The review emphasizes the necessity of robust data protection policies, representative datasets to mitigate biases, and clear guidelines for AI deployment tailored to the African context. Emerging AI technologies in Africa, such as AI-enhanced telemedicine, mobile health applications, predictive analytics, and virtual tumor boards, show promise in overcoming geographic and resource limitations. These innovations can facilitate remote consultations, continuous patient monitoring, and multidisciplinary collaborations, thereby improving cancer care accessibility and outcomes. Conclusively, recommendations for enhancing AI integration in African cancer care, including investing in data infrastructure, capacity building for healthcare professionals, and fostering international collaborations are discussed. Addressing ethical and regulatory challenges is crucial to ensure responsible and effective use of AI technologies. By leveraging AI, Africa can significantly improve cancer care delivery, reduce mortality rates, and enhance patient quality of life."
Impact of Artificial Intelligence and Virtual Reality on Educational Inclusion: A Systematic Review of Technologies Supporting Students with Disabilities,2024,Angelos Chalkiadakis; Antonia Seremetaki; Athanasia Kanellou; Maria Kallishi; Anastasia Morfopoulou; Marina Moraitaki; Sofia Mastrokoukou,Education Sciences,22,W4404144786,10.3390/educsci14111223,https://openalex.org/W4404144786,https://doi.org/10.3390/educsci14111223,Inclusion (mineral); Virtual reality; Educational technology; Computer science; Assistive technology,review,True,"The emergence of Artificial Intelligence (AI) and Virtual Reality (VR) technologies offers transformative potential for the advancement of inclusive education, particularly for students with disabilities. This systematic review critically evaluates the current state of research to assess the impact of AI and VR on enhancing educational accessibility, personalisation and social inclusion in education. AI-driven adaptive systems can dynamically tailor learning experiences to individual needs, while VR offers immersive, multi-sensory environments that promote experiential learning. Despite these advances, the review also identifies significant challenges, including the high cost of implementation, technical barriers and limited teacher readiness, which hinder widespread adoption. Ethical concerns such as privacy and algorithmic bias are cited as key areas that need careful consideration. The findings underscore the urgent need for further empirical research to explore the long-term impact of these technologies and advocate for more equitable access to AI and VR tools in underserved educational settings. Ultimately, the review highlights the importance of integrating AI and VR as part of a broader strategy to foster genuinely inclusive learning environments that align with the goals of the Convention on the Rights of Persons with Disabilities (CRPD)."
Explainable spatially explicit geospatial artificial intelligence in urban analytics,2023,Pengyuan Liu; Yan Zhang; Filip Biljecki,Environment and Planning B Urban Analytics and City Science,26,W4387187512,10.1177/23998083231204689,https://openalex.org/W4387187512,,Geospatial analysis; Interpretability; Computer science; Analytics; Graph,article,False,"Geospatial artificial intelligence (GeoAI) is proliferating in urban analytics, where graph neural networks (GNNs) have become one of the most popular methods in recent years. However, along with the success of GNNs, the black box nature of AI models has led to various concerns (e.g. algorithmic bias and model misuse) regarding their adoption in urban analytics, particularly when studying socio-economics where high transparency is a crucial component of social justice. Therefore, the desire for increased model explainability and interpretability has attracted increasing research interest. This article proposes an explainable spatially explicit GeoAI-based analytical method that combines a graph convolutional network (GCN) and a graph-based explainable AI (XAI) method, called GNNExplainer. Here, we showcase the ability of our proposed method in two studies within urban analytics: traffic volume prediction and population estimation in the tasks of a node classification and a graph classification, respectively. For these tasks, we used Street View Imagery (SVI), a trending data source in urban analytics. We extracted semantic information from the images and assigned them as features of urban roads. The GCN first provided reasonable predictions related to these tasks by encoding roads as nodes and their connectivities and networks as graphs. The GNNExplainer then offered insights into how certain predictions are made. Through such a process, practical insights and conclusions can be derived from the urban phenomena studied here. In this paper we also set out a path for developing XAI in future urban studies."
A Hybrid Biased Random-Key Genetic Algorithm for the Container Relocation Problem,2024,Andresson da Silva Firmino; Valéria Cesário Times,Springer tracts in nature-inspired computing,1,W4391961568,10.1007/978-981-99-8107-6_4,https://openalex.org/W4391961568,,Relocation; Container (type theory); Key (lock); Genetic algorithm; Computer science,book-chapter,False,
Algorithmic Lending Bias: Evaluating the Fairness of Historical Redlining in Loan Approvals,2024,Kuber Sarwal; Sheikh Rabiul Islam,2021 IEEE International Conference on Big Data (Big Data),1,W4406458411,10.1109/bigdata62323.2024.10825978,https://openalex.org/W4406458411,,Loan; Actuarial science; Business; Computer science; Finance,article,False,
Reducing Symbiosis Bias through Better A/B Tests of Recommendation Algorithms,2025,Jennifer Brennan; Yahu Cong; Yiwei Yu; Lina Lin; Yajun Peng; Changping Meng; Ningren Han; Jean Pouget-Abadie; David Holtz,,1,W4410088881,10.1145/3696410.3714738,https://openalex.org/W4410088881,https://doi.org/10.1145/3696410.3714738,Computer science; Algorithm,article,True,
Avances y desafíos éticos en la integración de la IA en la producción científica,2023,Lourdes Amalia González Ciriaco; Aquiles José Medina Marín,JoSME :,15,W4399799927,10.69821/josme.v1ii.2,https://openalex.org/W4399799927,,Transparency (behavior); Multidisciplinary approach; Engineering ethics; Political science; Sociology,article,False,"In the dynamic landscape of contemporary research, the integration of artificial intelligence (AI) is underscored to enhance scientific production, simultaneously confronting new ethical challenges such as maintaining transparency and managing algorithmic biases. This article aims to clarify how researchers can balance ethical imperatives with the benefits and challenges arising from the use of AI in their work. A systematic review methodology was employed, allowing for critical analysis of relevant studies and the identification of key trends and perspectives on the ethical use of AI. The findings highlight a growing awareness of the importance of ethical frameworks and academic integrity, as well as the need to develop explainable AI. The most significant conclusion is that through multidisciplinary approaches that incorporate ongoing ethical guidelines and training in AI ethics, an ethical and effective integration of these technologies in research is promoted, benefiting both the scientific community and society at large."
Robust Bias-Compensated CR-NSAF Algorithm: Design and Performance Analysis,2024,Pengwei Wen; Bolin Wang; Boyang Qu; Sheng Zhang; Haiquan Zhao; Jing Liang,IEEE Transactions on Systems Man and Cybernetics Systems,1,W4404469651,10.1109/tsmc.2024.3491188,https://openalex.org/W4404469651,,Computer science; Algorithm; Control theory (sociology); Artificial intelligence; Control (management),article,False,
Algorithmic Bias and Trust,2025,Donghee Shin,Routledge eBooks,0,W4408519823,10.1201/9781003530244-7,https://openalex.org/W4408519823,,Computer science; Psychology,book-chapter,False,
Algorithmic Bias and Explainability in Insurance,2024,Jakob Walter; Martin Eling,SSRN Electronic Journal,0,W4396992916,10.2139/ssrn.4830403,https://openalex.org/W4396992916,,Actuarial science; Business; Econometrics; Economics,article,False,"We focus on the fundamental tradeoff between performance and explainability in machine learning algorithms. This tradeoff is well established in the computer science literature but has not yet been explored in the insurance literature. We consider a market where two biased machine learning algorithms, subject to a performance-explainability tradeoff, compete on price to sell an insurance contract with homogeneous coverage to a large number of buyers, who are differentiated by their loss probabilities. Our findings reveal that the realization of the equilibrium, and thus optimal regulatory policy, is determined by the interplay of the algorithm-specific bias and explainability markdown, as well as the data access range. Policymakers face a tradeoff between premium size and the degree of algorithmic explainability, which can be alleviated by favoring algorithms with decreasing marginal explainability costs."
Study on the Impact of Artificial Intelligence on Student Learning Outcomes,2024,P. Sasikala; R. Ravichandran,Journal of Digital Learning and Education,14,W4401886736,10.52562/jdle.v4i2.1234,https://openalex.org/W4401886736,https://doi.org/10.52562/jdle.v4i2.1234,Mathematics education; Psychology; Artificial intelligence; Computer science,article,True,"This study explores the transformative potential of Artificial Intelligence (AI) in education by analyzing its impact on student learning outcomes. Through a comprehensive literature review, the research synthesizes current findings on the integration of AI in educational settings, examining both the benefits and challenges it presents. The study explores into AI's role in personalizing learning experiences, enhancing student engagement, and improving academic performance. Ethical considerations such as data privacy and algorithmic bias are also assessed. This research also identifies existing gaps in the literature and suggests avenues for future inquiry, contributing to a deeper understanding of how AI can be effectively and responsibly integrated into education to optimize student success."
Parallel and bias-free RSA algorithm for maximal Poisson-sphere sampling,2024,Marc Josien; Raphaël Prat,Computer Physics Communications,1,W4401854267,10.1016/j.cpc.2024.109354,https://openalex.org/W4401854267,,Poisson distribution; Algorithm; Sampling (signal processing); Mathematics; Computer science,article,False,
Combining Human-in-the-Loop Systems and AI Fairness Toolkits to Reduce Age Bias in AI Job Hiring Algorithms,2024,Christopher G. Harris,,2,W4394713269,10.1109/bigcomp60711.2024.00019,https://openalex.org/W4394713269,,Computer science; Human-in-the-loop; Loop (graph theory); Artificial intelligence; Algorithm,article,False,"As artificial intelligence (AI) systems become more sophisticated, they are increasingly integrated into high-stakes decision-making processes, such as hiring, fraud detection, loan approvals, and medical diagnoses. However, this growing reliance on AI raises concerns about the potential for these systems to perpetuate and amplify societal biases. Researchers have developed two main approaches to bias mitigation in AI to address this issue: human-in-the-loop (HITL) systems and AI fairness toolkits. HITL systems involve human reviewers actively participating in the AI decision-making process, while AI fairness toolkits are software tools that can identify and mitigate bias. HITL systems are particularly effective in addressing biases tied to specific domains, while AI fairness toolkits can be useful in identifying and addressing bias proactively. This paper examines different combinations of HITL systems and AI fairness toolkits, conducts an experiment to evaluate biases in hiring decisions using each, and provides recommendations for organizations considering implementing one or both approaches."
A Biased Random-Key Genetic Algorithm for the Home Care Routing and Scheduling Problem: Exploring the Algorithm’s Configuration Process,2023,Ana Raquel Aguiar; Tânia Rodrigues Pereira Ramos; Maria Isabel Gomes,Springer proceedings in mathematics & statistics,2,W4319454262,10.1007/978-3-031-20788-4_1,https://openalex.org/W4319454262,,Robustness (evolution); Key (lock); Computer science; Solver; Genetic algorithm,book-chapter,False,
Biased random-key genetic algorithm for the job sequencing and tool switching problem with non-identical parallel machines,2023,Leonardo C.R. Soares; Marco Antonio Moreira Carvalho,Computers & Operations Research,2,W4389670605,10.1016/j.cor.2023.106509,https://openalex.org/W4389670605,,Benchmark (surveying); Minification; Computer science; Key (lock); Job shop scheduling,article,False,
Navigating ethical considerations in software development and deployment in technological giants,2024,Daniel Ajiga; Patrick Azuka Okeleke; Samuel Olaoluwa Folorunsho; Chinedu Ezeigweneme,International Journal of Engineering Research Updates,9,W4401829668,10.53430/ijeru.2024.7.1.0033,https://openalex.org/W4401829668,,Transparency (behavior); Accountability; Software deployment; Technological change; Process (computing),article,False,"The rapid evolution of software development and deployment in technological giants has brought unprecedented advancements and efficiencies, reshaping industries and societies. However, this rapid growth also presents significant ethical considerations that developers and organizations must navigate to ensure responsible and sustainable technology. This review explores the key ethical issues inherent in the software development lifecycle within large technology companies, focusing on data privacy, algorithmic bias, transparency, accountability, and the broader societal impact. Data privacy remains a paramount concern, with technological giants often possessing vast amounts of sensitive user information. Ensuring the ethical handling, storage, and use of this data is crucial to maintaining user trust and complying with regulatory frameworks. Additionally, algorithmic bias poses a significant challenge, as biased algorithms can perpetuate and even exacerbate social inequalities. Addressing this issue requires concerted efforts in diverse representation during the development process and rigorous testing for bias. Transparency and accountability are also essential in ethical software development. Technological giants must be transparent about their data practices and the functioning of their algorithms, providing users and stakeholders with clear information about how decisions are made. Moreover, establishing accountability mechanisms is vital to address potential harms and ensure that developers and organizations are held responsible for their technological outputs. The societal impact of software deployed by technological giants cannot be overlooked. The widespread adoption of new technologies can have far-reaching effects on employment, mental health, and social dynamics. Thus, ethical considerations must extend beyond technical aspects to encompass the broader implications of technology on society. In conclusion, navigating ethical considerations in software development and deployment within technological giants requires a multifaceted approach. By prioritizing data privacy, addressing algorithmic bias, ensuring transparency and accountability, and considering the societal impact, these companies can develop and deploy software that is not only innovative but also ethically responsible. This review underscores the importance of integrating ethical frameworks into the technological development process to foster trust, fairness, and societal well-being."
Algorithmic Bias and Fairness in Biomedical and Health Research,2025,Rebet Keith Jones,Advances in computational intelligence and robotics book series,0,W4410355209,10.4018/979-8-3373-4252-8.ch008,https://openalex.org/W4410355209,,Psychology; Computer science; Sociology; Political science; Internet privacy,book-chapter,False,"The rapid integration of artificial intelligence (AI) and machine learning (ML) into biomedical and health research has the potential to transform patient care, diagnosis, and treatment outcomes. However, as these technologies evolve, concerns surrounding algorithmic bias and fairness have emerged. In the context of healthcare, biased algorithms can exacerbate disparities in health outcomes, leading to inequality in care and undermining trust in AI-driven systems. This chapter explores the ethical implications of algorithmic bias in biomedical research, focusing on the factors contributing to bias in datasets, model design, and decision-making processes. Additionally, it examines various strategies and frameworks aimed at promoting fairness and equity in AI applications. Through a multidisciplinary lens, the chapter presents a critical analysis of how algorithmic fairness can be achieved, with particular emphasis on practical solutions and regulatory considerations to safeguard both the integrity of research and the well-being of diverse patient populations"
Artificial Intelligence in Personalized Learning with a Focus on Current Developments and Future Prospects,2024,Chaira Mahmoud; Jan Tind Sørensen,Research and Advances in Education,19,W4402026005,10.56397/rae.2024.08.04,https://openalex.org/W4402026005,,Focus (optics); Current (fluid); Computer science; Artificial intelligence; Data science,article,False,"This paper provides a comprehensive review of the role of Artificial Intelligence (AI) in personalized learning, exploring current developments and future prospects. The integration of AI into education has led to significant advancements in adaptive learning systems, intelligent tutoring systems, and learning analytics, all of which contribute to more customized and effective learning experiences. The paper examines the benefits of AI-powered personalized learning, including enhanced student engagement, improved learning outcomes, and scalability. It also addresses the challenges and ethical considerations associated with AI in education, such as data privacy, equity, algorithmic bias, and the evolving role of teachers. Looking ahead, the paper discusses the future prospects of AI in personalized learning, highlighting the potential for more advanced adaptive systems, AI-driven content creation, and the integration of immersive technologies like virtual and augmented reality. The paper concludes by emphasizing the need for ongoing innovation, collaboration, and ethical considerations to fully realize the potential of AI in creating a more personalized, inclusive, and effective educational landscape."
Challenging Cognitive Load Theory: The Role of Educational Neuroscience and Artificial Intelligence in Redefining Learning Efficacy,2025,Evgenia Gkintoni; Hera Antonopoulou; Andrew Sortwell; Constantinos Halkiopoulos,Brain Sciences,45,W4407657137,10.3390/brainsci15020203,https://openalex.org/W4407657137,https://doi.org/10.3390/brainsci15020203,Computer science; Artificial intelligence; Scalability; Deep learning; Machine learning,review,True,"Background/Objectives: This systematic review integrates Cognitive Load Theory (CLT), Educational Neuroscience (EdNeuro), Artificial Intelligence (AI), and Machine Learning (ML) to examine their combined impact on optimizing learning environments. It explores how AI-driven adaptive learning systems, informed by neurophysiological insights, enhance personalized education for K-12 students and adult learners. This study emphasizes the role of Electroencephalography (EEG), Functional Near-Infrared Spectroscopy (fNIRS), and other neurophysiological tools in assessing cognitive states and guiding AI-powered interventions to refine instructional strategies dynamically. Methods: This study reviews n = 103 papers related to the integration of principles of CLT with AI and ML in educational settings. It evaluates the progress made in neuroadaptive learning technologies, especially the real-time management of cognitive load, personalized feedback systems, and the multimodal applications of AI. Besides that, this research examines key hurdles such as data privacy, ethical concerns, algorithmic bias, and scalability issues while pinpointing best practices for robust and effective implementation. Results: The results show that AI and ML significantly improve Learning Efficacy due to managing cognitive load automatically, providing personalized instruction, and adapting learning pathways dynamically based on real-time neurophysiological data. Deep Learning models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Support Vector Machines (SVMs) improve classification accuracy, making AI-powered adaptive learning systems more efficient and scalable. Multimodal approaches enhance system robustness by mitigating signal variability and noise-related limitations by combining EEG with fMRI, Electrocardiography (ECG), and Galvanic Skin Response (GSR). Despite these advances, practical implementation challenges remain, including ethical considerations, data security risks, and accessibility disparities across learner demographics. Conclusions: AI and ML are epitomes of redefinition potentials that solid ethical frameworks, inclusive design, and scalable methodologies must inform. Future studies will be necessary for refining pre-processing techniques, expanding the variety of datasets, and advancing multimodal neuroadaptive learning for developing high-accuracy, affordable, and ethically responsible AI-driven educational systems. The future of AI-enhanced education should be inclusive, equitable, and effective across various learning populations that would surmount technological limitations and ethical dilemmas."
When facial recognition does not ‘recognise’: erroneous identifications and resulting liabilities,2023,Vera Lúcia Raposo,AI & Society,13,W4319460931,10.1007/s00146-023-01634-z,https://openalex.org/W4319460931,https://link.springer.com/content/pdf/10.1007/s00146-023-01634-z.pdf,Computer science; Identification (biology); Perspective (graphical); Probabilistic logic; Artificial intelligence,article,True,"Abstract Facial recognition is an artificial intelligence-based technology that, like many other forms of artificial intelligence, suffers from an accuracy deficit. This paper focuses on one particular use of facial recognition, namely identification, both as authentication and as recognition. Despite technological advances, facial recognition technology can still produce erroneous identifications. This paper addresses algorithmic identification failures from an upstream perspective by identifying the main causes of misidentifications (in particular, the probabilistic character of this technology, its ‘black box’ nature and its algorithmic bias) and from a downstream perspective, highlighting the possible legal consequences of such failures in various scenarios (namely liability lawsuits). In addition to presenting the causes and effects of such errors, the paper also presents measures that can be deployed to reduce errors and avoid liabilities."
Mitigating Algorithmic Bias in Predictive Models,2025,Tamanno Maripova,The American Journal of Engineering And Technology,0,W4410926455,10.37547/tajet/volume07issue05-19,https://openalex.org/W4410926455,https://doi.org/10.37547/tajet/volume07issue05-19,Computer science,article,True,"This article considers the issue of systematic errors in predictive machine-learning models generating disparate outcomes for different social groups and proposes a holistic approach to its mitigation. The risks and increasing legal requirements, along with corporate commitments to ethical AIs, drive the relevance of this study. The work herewith attempts to develop a bias-source taxonomy at data collection and annotation, proxy-feature selection, model training, and deployment stages; also, it tries to compare pre-, in-, and post-processing methods' effectiveness on representative datasets measured by demographic parity, equalized error rates, and disparate impact. This article is unprecedented in undertaking a two-level approach: first, a systematic review of regulatory definitions (NIST, IBM) and case studies (COMPAS, healthcare-service prediction, face recognition) that identified key bias factors from sample imbalance to feedback loops; second, an empirical comparison of Reweighing, adversarial debiasing, threshold post-processing techniques alongside flexible multi-objective strategies—YODO (via AI Fairness 360 and Fairlearn libraries)—considering acceptable accuracy losses. The root source of unfairness remains data bias; hence, pre-processing must be undertaken (rebalancing, synthetic oversampling), while in- and post-processing can essentially harmonize group metrics at some cost in accuracy reduction Furthermore, without continuous online monitoring and documentation (datasheets, model cards), the balanced model risks losing fairness due to dynamic feedback effects. Bringing together technical fixes with rules and making the audit process official ensures the ability to copy and openness, which is key for long-term faith in AI systems. This article will help machine-learning builders, AI-responsibility experts, and checkers find ways to find, gauge, and lessen algorithmic bias in live models."
Leveraging AI in E-Learning: Personalized Learning and Adaptive Assessment through Cognitive Neuropsychology—A Systematic Analysis,2024,Constantinos Halkiopoulos; Evgenia Gkintoni,Electronics,120,W4402762189,10.3390/electronics13183762,https://openalex.org/W4402762189,https://doi.org/10.3390/electronics13183762,Neuropsychology; Computer science; Cognition; Personalized learning; Adaptive learning,article,True,"This paper reviews the literature on integrating AI in e-learning, from the viewpoint of cognitive neuropsychology, for Personalized Learning (PL) and Adaptive Assessment (AA). This review follows the PRISMA systematic review methodology and synthesizes the results of 85 studies that were selected from an initial pool of 818 records across several databases. The results indicate that AI can improve students’ performance, engagement, and motivation; at the same time, some challenges like bias and discrimination should be noted. The review covers the historic development of AI in education, its theoretical grounding, and its practical applications within PL and AA with high promise and ethical issues of AI-powered educational systems. Future directions are empirical validation of effectiveness and equity, development of algorithms that reduce bias, and exploration of ethical implications regarding data privacy. The review identifies the transformative potential of AI in developing personalized and adaptive learning (AL) environments, thus, it advocates continued development and exploration as a means to improve educational outcomes."
A review of AI-driven pedagogical strategies for equitable access to science education,2024,Chima Abimbola Eden; Olabisi Oluwakemi Adeleye; Idowu Sulaimon Adeniyi,Magna Scientia Advanced Research and Reviews,22,W4392750151,10.30574/msarr.2024.10.2.0043,https://openalex.org/W4392750151,https://magnascientiapub.com/journals/msarr/sites/default/files/MSARR-2024-0043.pdf,Pace; Equity (law); Computer science; Personalized learning; Psychology,review,True,"Access to quality science education is essential for equitable development and advancement in society. However, disparities in access to science education persist, particularly among marginalized and underserved populations. Artificial intelligence (AI) offers innovative solutions to address these disparities by enhancing pedagogical strategies that promote equitable access to science education. This review examines AI-driven pedagogical strategies aimed at improving equitable access to science education. The review explores how AI technologies, such as machine learning, natural language processing, and computer vision, can be leveraged to personalize learning experiences, provide real-time feedback, and enhance engagement among students from diverse backgrounds.AI-driven personalized learning platforms can adapt to individual learning styles and pace, ensuring that each student receives tailored instruction. These platforms can also provide additional support to students facing learning challenges, thus promoting inclusivity and equity in science education. Furthermore, AI-driven assessment tools can provide educators with insights into student performance and comprehension, enabling them to identify areas for improvement and provide targeted interventions. Additionally, AI can facilitate collaborative learning environments, allowing students to work together irrespective of their physical location, thus breaking down geographical barriers to access. However, the implementation of AI-driven pedagogical strategies raises ethical considerations, such as data privacy and algorithmic bias, which must be carefully addressed to ensure equitable access to science education for all students. In conclusion, AI-driven pedagogical strategies have the potential to revolutionize science education by enhancing personalized learning, providing real-time feedback, and fostering inclusive learning environments. However, careful consideration must be given to the ethical implications of AI implementation to ensure that these technologies are used responsibly and equitably."
Algorithmic inclusion: Shaping the predictive algorithms of artificial intelligence in hiring,2023,Elisabeth Kelan,Human Resource Management Journal,49,W4366989571,10.1111/1748-8583.12511,https://openalex.org/W4366989571,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/1748-8583.12511,Inclusion (mineral); Replicate; Audit; Computer science; Selection (genetic algorithm),article,True,"Abstract Despite frequent claims that increased use of artificial intelligence (AI) in hiring will reduce the human bias that has long plagued recruitment and selection, AI may equally replicate and amplify such bias and embed it in technology. This article explores exclusion and inclusion in AI‐supported hiring, focusing on three interrelated areas: data, design and decisions. It is suggested that in terms of data, organisational fit, categorisations and intersectionality require consideration in relation to exclusion. As various stakeholders collaborate to create AI, it is essential to explore which groups are dominant and how subjective assessments are encoded in technology. Although AI‐supported hiring should enhance recruitment decisions, evidence is lacking on how humans and machines interact in decision‐making, and how algorithms can be audited and regulated effectively for inclusion. This article recommends areas for interrogation through further research, and contributes to understanding how algorithmic inclusion can be achieved in AI‐supported hiring."
Expanding on the Frames: Making a Case for Algorithmic Literacy,2023,Susan Gardner Archambault,Communications in Information Literacy,8,W4389729208,10.15760/comminfolit.2023.17.2.11,https://openalex.org/W4389729208,https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=1656&context=comminfolit,Information literacy; Computer science; Frame (networking); Dominance (genetics); Literacy,article,True,"Traditional information literacy skills (e.g., effectively finding and evaluating information) need to be updated due to the rapidly changing information ecosystem and the growing dominance of online platforms that use algorithms to control and shape information. This article proposes additions to the current ACRL Framework for Information Literacy for Higher Education that relate to algorithmic literacy. The ""Authority is Constructed and Contextual"" frame can be applied to recognizing the need to question algorithmic authority (including algorithmic bias), the Information Has Value"" frame can be used to acknowledge online platforms' use of proprietary algorithms allowing third parties to access personal data, and the ""Searching as Strategic Exploration"" frame can draw attention to search results in online platforms are mediated through algorithms. Classroom activities to teach the new knowledge practices and dispositions are also included."
Artificial Intelligence Prediction Program in Criminal Justice System: focused on its Biased Algorithm in relation to the Racial Discrimination,2023,Gina S. Rhee,Wonkwang University Legal Research Institute,1,W4385325387,10.22397/wlri.2023.39.2.57,https://openalex.org/W4385325387,,Sanctions; Recidivism; Criminal justice; Criminology; Relation (database),article,False,"In recent period, crime prediction programs have been newly introduced and utilized internationally in the field of criminal justice. COMPAS (“Corrective Offender Management Profiling for Alternative Sanctions”), as a representative example, is a recidivism prediction program used in several States in the United States. COMPAS is the most widely used risk assessment tools in the United States. The U.S. company Northpointe has developed an artificial intelligence algorithm that predicts the possibility of recidivism by analyzing the accumulated data such as criminal records, family relationships, educational history, drug abuse, etc. However, as it has been controversially argued that the results of these algorithms violate the defendants' constitutional rights, fundamental questions arise on how the results of the algorithm are produced, and what factors are calculated in judging a specific decision. In the era of A.I., ‘artificial intelligence’ is a concept that encompasses both technology development, utilization, and operation systems, normative judgment and policy design related to the use of the system in the judicial system. Furthermore, ethical guidelines for preventing individual risks in the use of artificial intelligence and other legal restraints, including criminal sanctions, should be established. Based on the crime prediction, this study will discuss the bias and racism of algorithms based on crime prediction technologies. This paper further aims to scrutinize the crime prediction and artificial intelligence algorithms in relation to the racial discrimination and social inequality against specific groups in criminal justice. Though not as much as in the U.S., often referred to as a ‘salad bowl’ society, South Korea has also entered a multicultural society due to recent surge in immigration, labor market, and international marriage. Lastly, the author emphasizes the importance of further research on the utilization of crime prediction tool in South Korea, as it requires careful deliberation and thorough comparative legal research prior the adoption of the new technology in the criminal justice system."
How Optimal Transport Can Tackle Gender Biases in Multi-Class Neural Network Classifiers for Job Recommendations,2023,Fanny Jourdan; Titon Tshiongo Kaninku; Nicholas Asher; Jean‐Michel Loubes; Laurent Risser,Algorithms,9,W4353075229,10.3390/a16030174,https://openalex.org/W4353075229,https://www.mdpi.com/1999-4893/16/3/174/pdf?version=1679630201,Computer science; Artificial neural network; Artificial intelligence; Machine learning; Class (philosophy),article,True,"Automatic recommendation systems based on deep neural networks have become extremely popular during the last decade. Some of these systems can, however, be used in applications that are ranked as High Risk by the European Commission in the AI act—for instance, online job candidate recommendations. When used in the European Union, commercial AI systems in such applications will be required to have proper statistical properties with regard to the potential discrimination they could engender. This motivated our contribution. We present a novel optimal transport strategy to mitigate undesirable algorithmic biases in multi-class neural network classification. Our strategy is model agnostic and can be used on any multi-class classification neural network model. To anticipate the certification of recommendation systems using textual data, we used it on the Bios dataset, for which the learning task consists of predicting the occupation of female and male individuals, based on their LinkedIn biography. The results showed that our approach can reduce undesired algorithmic biases in this context to lower levels than a standard strategy."
FairGAT: Fairness-Aware Graph Attention Networks,2024,Öykü Deniz Köse; Yanning Shen,ACM Transactions on Knowledge Discovery from Data,5,W4391753841,10.1145/3645096,https://openalex.org/W4391753841,https://dl.acm.org/doi/pdf/10.1145/3645096,Computer science; Graph; Theoretical computer science,article,True,"Graphs can facilitate modeling various complex systems such as gene networks and power grids as well as analyzing the underlying relations within them. Learning over graphs has recently attracted increasing attention, particularly graph neural network (GNN)–based solutions, among which graph attention networks (GATs) have become one of the most widely utilized neural network structures for graph-based tasks. Although it is shown that the use of graph structures in learning results in the amplification of algorithmic bias, the influence of the attention design in GATs on algorithmic bias has not been investigated. Motivated by this, the present study first carries out a theoretical analysis in order to demonstrate the sources of algorithmic bias in GAT-based learning for node classification. Then, a novel algorithm, FairGAT, which leverages a fairness-aware attention design, is developed based on the theoretical findings. Experimental results on real-world networks demonstrate that FairGAT improves group fairness measures while also providing comparable utility to the fairness-aware baselines for node classification and link prediction."
Representation Bias in Data: A Survey on Identification and Resolution Techniques,2023,Nima Shahbazi; Lin Yin; Abolfazl Asudeh; H. V. Jagadish,ACM Computing Surveys,62,W4327743697,10.1145/3588433,https://openalex.org/W4327743697,https://doi.org/10.1145/3588433,Computer science; Representation (politics); Categorization; Identification (biology); Data science,review,True,"Data-driven algorithms are only as good as the data they work with, while data sets, especially social data, often fail to represent minorities adequately. Representation Bias in data can happen due to various reasons ranging from historical discrimination to selection and sampling biases in the data acquisition and preparation methods. Given that ""bias in, bias out"", one cannot expect AI-based solutions to have equitable outcomes for societal applications, without addressing issues such as representation bias. While there has been extensive study of fairness in machine learning models, including several review papers, bias in the data has been less studied. This paper reviews the literature on identifying and resolving representation bias as a feature of a data set, independent of how consumed later. The scope of this survey is bounded to structured (tabular) and unstructured (e.g., image, text, graph) data. It presents taxonomies to categorize the studied techniques based on multiple design dimensions and provides a side-by-side comparison of their properties. There is still a long way to fully address representation bias issues in data. The authors hope that this survey motivates researchers to approach these challenges in the future by observing existing work within their respective domains."
"Identifying sources of bias when testing three available algorithms for quantifying white matter lesions: BIANCA, LPA and LGA",2024,Tatiana Miller; Nóra Bittner; Susanne Moebus; Svenja Caspers,GeroScience,1,W4401433955,10.1007/s11357-024-01306-w,https://openalex.org/W4401433955,https://doi.org/10.1007/s11357-024-01306-w,Algorithm; Dementia; Cohort; Hyperintensity; Population,article,True,"Abstract Brain magnetic resonance imaging frequently reveals white matter lesions (WMLs) in older adults. They are often associated with cognitive impairment and risk of dementia. Given the continuous search for the optimal segmentation algorithm, we broke down this question by exploring whether the output of algorithms frequently used might be biased by the presence of different influencing factors. We studied the impact of age, sex, blood glucose levels, diabetes, systolic blood pressure and hypertension on automatic WML segmentation algorithms. We evaluated three widely used algorithms (BIANCA, LPA and LGA) using the population-based 1000BRAINS cohort ( N = 1166, aged 18–87, 523 females, 643 males). We analysed two main aspects. Firstly, we examined whether training data (TD) characteristics influenced WML estimations, assessing the impact of relevant factors in the TD. Secondly, algorithm’s output and performance within selected subgroups defined by these factors were assessed. Results revealed that BIANCA’s WML estimations are influenced by the characteristics present in the TD. LPA and LGA consistently provided lower WML estimations compared to BIANCA’s output when tested on participants under 67 years of age without risk cardiovascular factors. Notably, LPA and LGA showed reduced accuracy for these participants. However, LPA and LGA showed better performance for older participants presenting cardiovascular risk factors. Results suggest that incorporating comprehensive cohort factors like diverse age, sex and participants with and without hypertension in the TD could enhance WML-based analyses and mitigate potential sources of bias. LPA and LGA are a fast and valid option for older participants with cardiovascular risk factors."
Adaptations in the simulated annealing algorithm to generate D-optimal mixing experiments both in the absence and presence of biases in the specification of the proportions,2024,Marcelo Ângelo Cirillo; Fortunato Silva de Menezes,Communications in Statistics Case Studies Data Analysis and Applications,1,W4396769378,10.1080/23737484.2024.2350393,https://openalex.org/W4396769378,,Simulated annealing; Algorithm; Mixing (physics); Annealing (glass); Computer science,article,False,"The mixture designs are represented in terms of variables (xi, i=1,·,q) called by components. The components display an experimental region described in a simplex space, where each variable has values between 0 and 1, portraying the specification of proportions. With these characteristics, the practice of the designs both in several industrial and in pharmaceutical sectors becomes justifiable. The issue is that defining a structure with N experimental points becomes complex since we can generate infinite configurations. Therefore, we used D-optimal mixture designs, which, in summary, guarantee more accurate confidence regions for parameter estimates as we develop these designs conditioned to a statistical criterion. For this reason, we have undertaken this approach to fulfill the aim of this study. We propose a modification in the simulated annealing algorithm combined with the Monte Carlo procedure in the generation of D-optimal mixture designs classified both by the presence and by the absence of bias in the proportion specifications. We conclude that the proposed modification in the simulated annealing algorithm provides the search with generation of D-optimal designs, supporting the control of the distribution of experimental points either near or far from the edges of the observed region limited by the simplex space. The optimal strategies generated in a biased way are less sensitive in relation to the number of components."
The life cycle of large language models in education: A framework for understanding sources of bias,2024,Jinsook Lee; Yann Hicke; Renzhe Yu; Christopher Brooks; René F. Kizilcec,British Journal of Educational Technology,28,W4400585758,10.1111/bjet.13505,https://openalex.org/W4400585758,,Computer science; Psychology,article,False,"Abstract Large language models (LLMs) are increasingly adopted in educational contexts to provide personalized support to students and teachers. The unprecedented capacity of LLM‐based applications to understand and generate natural language can potentially improve instructional effectiveness and learning outcomes, but the integration of LLMs in education technology has renewed concerns over algorithmic bias, which may exacerbate educational inequalities. Building on prior work that mapped the traditional machine learning life cycle, we provide a framework of the LLM life cycle from the initial development of LLMs to customizing pre‐trained models for various applications in educational settings. We explain each step in the LLM life cycle and identify potential sources of bias that may arise in the context of education. We discuss why current measures of bias from traditional machine learning fail to transfer to LLM‐generated text (eg, tutoring conversations) because text encodings are high‐dimensional, there can be multiple correct responses, and tailoring responses may be pedagogically desirable rather than unfair. The proposed framework clarifies the complex nature of bias in LLM applications and provides practical guidance for their evaluation to promote educational equity. Practitioner notes What is already known about this topic The life cycle of traditional machine learning (ML) applications which focus on predicting labels is well understood. Biases are known to enter in traditional ML applications at various points in the life cycle, and methods to measure and mitigate these biases have been developed and tested. Large language models (LLMs) and other forms of generative artificial intelligence (GenAI) are increasingly adopted in education technologies (EdTech), but current evaluation approaches are not specific to the domain of education. What this paper adds A holistic perspective of the LLM life cycle with domain‐specific examples in education to highlight opportunities and challenges for incorporating natural language understanding (NLU) and natural language generation (NLG) into EdTech. Potential sources of bias are identified in each step of the LLM life cycle and discussed in the context of education. A framework for understanding where to expect potential harms of LLMs for students, teachers, and other users of GenAI technology in education, which can guide approaches to bias measurement and mitigation. Implications for practice and/or policy Education practitioners and policymakers should be aware that biases can originate from a multitude of steps in the LLM life cycle, and the life cycle perspective offers them a heuristic for asking technology developers to explain each step to assess the risk of bias. Measuring the biases of systems that use LLMs in education is more complex than with traditional ML, in large part because the evaluation of natural language generation is highly context‐dependent (eg, what counts as good feedback on an assignment varies). EdTech developers can play an important role in collecting and curating datasets for the evaluation and benchmarking of LLM applications moving forward."
Algorithmic Bias and Discrimination in India: A Looming Crisis,2025,Dharish David; B. Rajeshwari; S. Timhna,Journal of Development Policy and Practice,0,W4412857165,10.1177/24551333251343358,https://openalex.org/W4412857165,,Looming; Economics; Political science; Keynesian economics; Development economics,article,False,"This article critically examines algorithmic bias and discrimination in the Indian context, focusing on the intersection of rapid artificial intelligence (AI) adoption and existing societal inequities rooted in historical injustices. It explores how AI systems, developed without deliberation and representation, risk perpetuating biases related to caste, religion, gender and socio-economic status, posing significant dangers to marginalised communities. Using a multi-theoretical framework, including algorithmic bias theory, sociotechnical systems theory and feminist theories of structural injustice, the study highlights AI’s entanglement with societal norms and power dynamics. The qualitative research involved in-depth interviews with five experts from academia, law, science and policy to examine AI’s sociotechnical landscape in India. Key findings include underrepresentation of marginalised communities in AI datasets, lack of ethical AI guidelines, risks of surveillance misuse and limited awareness of AI ethics among developers and policymakers. A novel insight is the danger of addressing AI bias reactively, as this could entrench further harm. The study advocates for public policy solutions to promote data equity, foster responsibility among AI developers, establish independent oversight and encourage public discourse on AI ethics. Prioritising equity and ethical considerations is crucial to harnessing AI’s potential while protecting the rights and dignity of all citizens."
Demonstration of Eight-Sensor Sagnac Fiber-Optic Hydrophone Array with Alternative Quadrature Phase Bias and Response Equalization Demodulation Algorithm,2025,Delong Zhao; Ke Wang; Shuolin Yang; Wenjie Xie; Yuzhong Chen; Jie Yang; Zhangqi Song; Zhaohua Sun,Photonics,1,W4406023846,10.3390/photonics12010034,https://openalex.org/W4406023846,https://doi.org/10.3390/photonics12010034,Demodulation; Equalization (audio); Quadrature (astronomy); Optics; Hydrophone,article,True,"The Sagnac interferometer-based fiber-optic hydrophone (S-FOH) exhibits a frequency-dependent response, causing the output signal to deviate from the original acoustic signal, with severe cases leading to signal distortion. A response equalization demodulation algorithm is demonstrated to recover high-fidelity acoustic signals from interference phase signals. An eight-sensor S-FOH array featuring an alternative quadrature phase bias scheme is demonstrated, and experimental verification of the response equalization demodulation algorithm is performed. The temporal relationship of phase modulation pulses and sampling light pulses is analyzed, and a demodulation algorithm is introduced to obtain the phase difference of the Sagnac interferometer. The acoustic pressure sensitivity is equalized to be flat with an average of −135.0 ± 0.4 dB from 10 to 2032 Hz. The pulse response of the S-FOHA after the equalization algorithm is highly similar to the PZT hydrophone output signal, with a correlation coefficient of 0.987."
The Future of AI in Education,2024,Mustafa Kayyali,Advances in educational technologies and instructional design book series,15,W4405106411,10.4018/979-8-3693-7220-3.ch013,https://openalex.org/W4405106411,,Computer science; Learning analytics; Personalized learning; Data science; Analytics,book-chapter,False,"This chapter explores the expanding role of artificial intelligence (AI) in education, focusing on future projections and emerging trends shaping the next generation of learning. AI has already transformed educational landscapes, fostering personalized learning experiences, adaptive technologies, and automated administrative processes. This chapter examines upcoming AI-driven innovations poised to enhance educational outcomes further. Key trends, including intelligent tutoring systems, AI-powered assessments, and data-driven learning analytics, are analyzed for their potential impact. The chapter also addresses the ethical and practical challenges posed by AI in education, such as algorithmic bias and privacy concerns. By understanding these evolving dynamics, educators and policymakers can better prepare for a future where AI contributes positively to global learning environments."
Improving Delivery App User Experience with Tailored Search Features,2023,Archit Joshi; Murali Mohana Krishna Dandu; Vanitha Sivasankaran; A Renuka; Om Goel,Universal Research Reports,18,W4402814535,10.36676/urr.v10.i2.1373,https://openalex.org/W4402814535,,Computer science; World Wide Web; User experience design; Mobile apps; Human–computer interaction,article,False,"In an increasingly digital world, enhancing user experience for delivery applications has become a critical factor for success. This paper explores the impact of tailored search features on improving user experience within delivery apps. Traditional search functionalities often fall short by providing generic results that may not align with users' specific preferences or needs. Tailored search features, driven by personalized algorithms and user behaviour analysis, offer a more refined approach to search results. By leveraging data analytics, machine learning, and user profiling, these features can significantly enhance the relevance and accuracy of search outcomes. This study examines various strategies for implementing tailored search functionalities, such as context-aware search, predictive search, and user-specific recommendations. The research highlights how these features contribute to a more intuitive and satisfying user experience, thereby increasing user engagement and retention. Furthermore, the paper discusses the challenges associated with developing and integrating these advanced search features, including data privacy concerns and algorithmic biases. The findings suggest that while tailored search features can greatly improve user satisfaction and operational efficiency, careful consideration must be given to ethical implications and user consent. Overall, this study provides a comprehensive overview of how tailored search features can transform the delivery"
Detecting algorithmic bias in medical AI-models,2023,Jeffrey C. Smith; Andre L. Holder; Rishikesan Kamaleswaran; Yao Xie,arXiv (Cornell University),0,W4389422181,10.48550/arxiv.2312.02959,https://openalex.org/W4389422181,https://arxiv.org/abs/2312.02959,Computer science; Artificial intelligence; Machine learning; Clinical decision support system; Context (archaeology),preprint,True,"With the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. This paper presents an innovative framework for detecting areas of algorithmic bias in medical-AI decision support systems. Our approach efficiently identifies potential biases in medical-AI models, specifically in the context of sepsis prediction, by employing the Classification and Regression Trees (CART) algorithm. We verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. The effectiveness of the concept is further validated by experiments using electronic medical records from Grady Memorial Hospital in Atlanta, Georgia. These tests demonstrate the practical implementation of our strategy in a clinical environment, where it can function as a vital instrument for guaranteeing fairness and equity in AI-based medical decisions."
The Impact of Artificial Intelligence on Social Media Content,2024,Elsir Ali Saad Mohamed; Murtada Elbashir Osman; Badur Algasim Mohamed,Journal of Social Sciences,13,W4395680208,10.3844/jssp.2024.12.16,https://openalex.org/W4395680208,https://thescipub.com/pdf/jssp.2024.12.16.pdf,Content (measure theory); Social media; Psychology; Media content; Social psychology,article,True,"Artificial Intelligence (AI) has become a powerful tool for creating and managing social media content. Social media platforms have integrated AI technology into their algorithms to optimize the user experience. However, the impact of AI on social media is a topic of debate. This research aims to explore the effects of AI and its implications for content creators and consumers. It is recommended that social media platforms ensure that the use of AI is transparent and ethical to maintain user trust. The impact of AI on social media content is significant and multifaceted, enabling personalized content recommendations, automated content generation, and real-time content analysis. However, there are also concerns about algorithmic bias and the potential for job displacement. As AI technology continues to evolve, it is essential to ensure that ethical considerations and social responsibility are prioritized in the development and use of AI in social media marketing. The impact of AI on social media content is a complex issue, with both positive and negative effects. While AI algorithms can enhance the user experience by providing personalized content, there are concerns that they may also lead to the spread of misinformation and the creation of filter bubbles. To mitigate these potential negative effects, it is important to promote transparency, media literacy, and human moderation, in order to ensure that social media content is accurate, diverse, and informative."
"Artificial intelligence for diabetes: Enhancing prevention, diagnosis, and effective management",2024,Mohamed Khalifa; Mona Albadawy,Computer Methods and Programs in Biomedicine Update,35,W4391749765,10.1016/j.cmpbup.2024.100141,https://openalex.org/W4391749765,https://doi.org/10.1016/j.cmpbup.2024.100141,Diabetes mellitus; Diabetes management; Medicine; Type 2 diabetes; Endocrinology,article,True,"Diabetes, a major cause of premature mortality, affects millions globally, with its prevalence increasing due to lifestyle factors and aging populations. This systematic review explores the role of Artificial Intelligence (AI) in enhancing the prevention, diagnosis, and management of diabetes, highlighting the potential for personalised and proactive healthcare. A structured four-step method was used, including extensive literature searches, specific inclusion and exclusion criteria, data extraction from selected studies focusing on AI's role in diabetes, and thorough analysis to identify specific domains and functions where AI contributes significantly. Through examining 43 experimental studies, AI has been identified as a transformative force across eight key domains in diabetes care: 1) Diabetes Management and Treatment, 2) Diagnostic and Imaging Technologies, 3) Health Monitoring Systems, 4) Developing Predictive Models, 5) Public Health Interventions, 6) Lifestyle and Dietary Management, 7) Enhancing Clinical Decision-Making, and 8) Patient Engagement and Self-Management. Each domain showcases AI's potential to revolutionise care, from personalising treatment plans and improving diagnostic accuracy to enhancing patient engagement and predictive healthcare. AI's integration into diabetes care offers personalised, efficient, and proactive solutions. It enhances care accuracy, empowers patients, and provides better understanding of diabetes management. However, the successful implementation of AI requires continued research, data security, interdisciplinary collaboration, and a focus on patient-centred solutions. Education for healthcare professionals and regulatory frameworks are also crucial to address challenges like algorithmic bias and ethics. AI in diabetes care promises improved health outcomes and quality of life through personalised and proactive healthcare. Future efforts should focus on continued investment, ensuring data security, fostering interdisciplinary collaboration, and prioritising patient-centred solutions. Regular monitoring and evaluation are essential to adjust strategies and understand long-term impacts, ensuring AI's ethical and effective integration into healthcare."
Evaluating Zero-Shot Large Language Models Recommenders on Popularity Bias and Unfairness: A Comparative Approach to Traditional Algorithms,2024,Gloria Ortega; Rodrigo Ferrari de Souza; Marcelo Garcia Manzato,,1,W4403389206,10.5753/webmedia_estendido.2024.244310,https://openalex.org/W4403389206,https://sol.sbc.org.br/index.php/webmedia_estendido/article/download/30475/30281,Popularity; Computer science; Zero (linguistics); Shot (pellet); Algorithm,article,True,"Large Language Models (LLMs), such as ChatGPT, have transcended technological boundaries and are now widely used across various domains to enhance productivity. This widespread application highlights their versatility, with a notable presence as recommender systems. Existing literature already showcases their capabilities in this area. In this paper, we present a detailed empirical evaluation of the effectiveness of Zero-Shot LLMs, specifically ChatGPT 3.5 Turbo, without special settings, in calibrating popularity bias and ensuring fairness in movie and TV show recommendations when prompted. We particularly focus on how these models adapt their output, comparing them to traditional post-processing algorithms. Our findings indicate that LLMs, evaluated through metrics such as Mean Average Precision (MAP) and Mean Rank Miscalibration (MRMC), not only perform well but also have the potential to surpass conventional recommender systems models like Singular Value Decomposition (SVD) when paired with calibration methods. The results underscore the advantages of using LLMs in more advanced scenarios due to their ease of implementation and performance."
AI-Driven Clinical Decision Support Systems: An Ongoing Pursuit of Potential,2024,Malek Elhaddad; Sara Hamam,Cureus,114,W4394009485,10.7759/cureus.57728,https://openalex.org/W4394009485,https://assets.cureus.com/uploads/review_article/pdf/244463/20240406-13366-1bzxzru.pdf,Clinical decision support system; Workflow; Interpretability; Usability; Artificial intelligence,article,True,"Clinical Decision Support Systems (CDSS) are essential tools in contemporary healthcare, enhancing clinicians' decisions and patient outcomes. The integration of artificial intelligence (AI) is now revolutionizing CDSS even further. This review delves into AI technologies transforming CDSS, their applications in healthcare decision-making, associated challenges, and the potential trajectory toward fully realizing AI-CDSS's potential. The review begins by laying the groundwork with a definition of CDSS and its function within the healthcare field. It then highlights the increasingly significant role that AI is playing in enhancing CDSS effectiveness and efficiency, underlining its evolving prominence in shaping healthcare practices. It examines the integration of AI technologies into CDSS, including machine learning algorithms like neural networks and decision trees, natural language processing, and deep learning. It also addresses the challenges associated with AI integration, such as interpretability and bias. We then shift to AI applications within CDSS, with real-life examples of AI-driven diagnostics, personalized treatment recommendations, risk prediction, early intervention, and AI-assisted clinical documentation. The review emphasizes user-centered design in AI-CDSS integration, addressing usability, trust, workflow, and ethical and legal considerations. It acknowledges prevailing obstacles and suggests strategies for successful AI-CDSS adoption, highlighting the need for workflow alignment and interdisciplinary collaboration. The review concludes by summarizing key findings, underscoring AI's transformative potential in CDSS, and advocating for continued research and innovation. It emphasizes the need for collaborative efforts to realize a future where AI-powered CDSS optimizes healthcare delivery and improves patient outcomes."
Mitigating Nonlinear Algorithmic Bias in Binary Classification,2023,Wendy Hui; John W. Lau,arXiv (Cornell University),0,W4389649970,10.48550/arxiv.2312.05429,https://openalex.org/W4389649970,https://arxiv.org/abs/2312.05429,Computer science; Binary number; Nonlinear system; Focus (optics); Binary classification,preprint,True,"This paper proposes the use of causal modeling to detect and mitigate algorithmic bias that is nonlinear in the protected attribute. We provide a general overview of our approach. We use the German Credit data set, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on age bias and the problem of binary classification. We show that the probability of getting correctly classified as ""low risk"" is lowest among young people. The probability increases with age nonlinearly. To incorporate the nonlinearity into the causal model, we introduce a higher order polynomial term. Based on the fitted causal model, the de-biased probability estimates are computed, showing improved fairness with little impact on overall classification accuracy. Causal modeling is intuitive and, hence, its use can enhance explicability and promotes trust among different stakeholders of AI."
A Guide to Cross-Validation for Artificial Intelligence in Medical Imaging,2023,Tyler Bradshaw; Zachary Huemann; Junjie Hu; Arman Rahmim,Radiology Artificial Intelligence,92,W4377989430,10.1148/ryai.220232,https://openalex.org/W4377989430,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10388213,Hyperparameter; Convolutional neural network; Artificial intelligence; Machine learning; Field (mathematics),review,True,"Artificial intelligence (AI) is being increasingly used to automate and improve technologies within the field of medical imaging. A critical step in the development of an AI algorithm is estimating its prediction error through cross-validation (CV). The use of CV can help prevent overoptimism in AI algorithms and can mitigate certain biases associated with hyperparameter tuning and algorithm selection. This article introduces the principles of CV and provides a practical guide on the use of CV for AI algorithm development in medical imaging. Different CV techniques are described, as well as their advantages and disadvantages under different scenarios. Common pitfalls in prediction error estimation and guidance on how to avoid them are also discussed. Keywords: Education, Research Design, Technical Aspects, Statistics, Supervised Learning, Convolutional Neural Network (CNN) Supplemental material is available for this article. © RSNA, 2023"
Mitigating Algorithmic Bias in Healthcare AI for Equitable Care,2024,,,0,W4404410152,10.1002/9781394263752.ch6,https://openalex.org/W4404410152,,Health care; Computer science; Data science; Political science; Law,other,False,"The healthcare industry is exploring the use of artificial intelligence (AI) to improve patient outcomes, but the datasets used to train these systems raise concerns about algorithmic bias and its potential to exacerbate racial disparities in care. This chapter explores the issue of algorithmic bias in healthcare AI and its potential to exacerbate racial disparities in patient care. The chapter begins by explaining how AI systems, particularly those using machine learning (ML), rely on datasets that may be incomplete or under-representative, leading to systematic errors and racial bias. It then discusses the current legal landscape and challenges in regulating algorithmic bias, emphasizing the need for collaboration among stakeholders to identify, prevent, and mitigate bias. The chapter also explores strategies for enhancing data quality and diversity, the role of regulatory oversight and transparency, and the importance of diverse teams and inclusive datasets in addressing algorithmic bias. Finally, the chapter concludes by discussing the challenges faced by developers in gathering representative datasets, the need for legal frameworks to keep pace with technological advancements, and the importance of funding and accountability in creating a more equitable healthcare AI landscape."
Variable Step-Size Diffusion Bias-Compensated APV Algorithm Over Networks,2024,Fuyi Huang; Shuting Yang; Sheng Zhang; Haiqiang Chen; Pengwei Wen,IEEE Transactions on Signal and Information Processing over Networks,1,W4404238455,10.1109/tsipn.2024.3496255,https://openalex.org/W4404238455,,Algorithm; Variable (mathematics); Diffusion; Computer science; Statistical physics,article,False,
"AI for Robustness and Fairness: Addressing bias, fairness and robustness in machine learning algorithms",2025,Gaurav Kashyap,Journal of Artificial Intelligence Machine Learning and Data Science,1,W4407056922,10.51219/jaimld/gaurav-kashyap/443,https://openalex.org/W4407056922,,Robustness (evolution); Computer science; Machine learning; Artificial intelligence; Algorithm,article,False,
"A Solution to Co-occurrence Bias in Pedestrian Attribute Recognition: Theory, Algorithms, and Improvements",2025,Yibo Zhou; Hai‐Miao Hu; Jinzuo Yu; Haotian Wu; Shiliang Pu; Hanzi Wang,International Journal of Computer Vision,1,W4408559974,10.1007/s11263-025-02405-7,https://openalex.org/W4408559974,,Pattern recognition (psychology); Artificial intelligence; Computer science; Pedestrian detection; Pedestrian,article,False,
Optimizing public transport system using biased random-key genetic algorithm,2024,João Luiz Alves Oliveira; André L. L. Aquino; Rian G. S. Pinheiro; Bruno Nogueira,Applied Soft Computing,1,W4394566100,10.1016/j.asoc.2024.111578,https://openalex.org/W4394566100,,Computer science; Key (lock); Genetic algorithm; Algorithm; Machine learning,article,False,
AI in Education,2024,Ashok Singh Gaur; Hari Om Sharan; Rajeev Kumar,Advances in computational intelligence and robotics book series,13,W4392432031,10.4018/979-8-3693-2964-1.ch003,https://openalex.org/W4392432031,,Computer science; Geology,book-chapter,False,"As artificial intelligence (AI) continues to advance, its integration into the field of education presents both promising opportunities and ethical challenges. This chapter explores the multifaceted landscape of AI in education, examining the ethical considerations associated with its implementation. The opportunities encompass personalized learning experiences, adaptive assessment tools, and efficient administrative processes. However, ethical concerns arise regarding data privacy, algorithmic bias, accountability, and the potential exacerbation of educational inequalities. Artificial intelligence is a field of study that combines the applications of machine learning, algorithm production, and natural language processing. Applications of AI transform the tools of education. AI has a variety of educational applications, such as personalized learning platforms to promote students' learning, automated assessment systems to aid teachers, and facial recognition systems to generate insights about learners' behaviors."
AI and Employment Discrimination: AIHR's Algorithmic Bias,2025,Yuzhe Zhu,Applied and Computational Engineering,0,W4407587235,10.54254/2755-2721/2025.20842,https://openalex.org/W4407587235,https://doi.org/10.54254/2755-2721/2025.20842,Economics; Computer science; Psychology; Artificial intelligence; Political science,article,True,"Today, artificial intelligence (AI) has developed rapidly to permeate every aspect of people's lives. Some AI has already replaced humans to start working in factories or companies, in addition to AI in the field of employment and AIHR under the recruitment path based on big data algorithms for resume screening and employee interviews. The extension of AI to employment recruitment raises the issue of potential algorithmic discrimination, which manifests itself in discrimination in hiring data extrapolation, hiring data interpretation, and hiring data applications. The studys shows that employment equity and artificial intelligence in the current recruitment path, it should be combined with employment algorithm discrimination and legal challenges to explore the solution path: overcome the root algorithm bias at the technical level, clarify the responsibility of the recruitment subject, improve the laws and regulations on AI in the employment field, and set up supervision and evaluation institutions. Taking into account industrial development and employment development, we will promote the development and progress of AI recruitment and ensure that the right to fair opportunities for employees is not infringed."
"Human–Algorithmic Bias: Source, Evolution, and Impact",2025,Xiyang Hu; Yan Huang; Beibei Li; Tian Lu,Management Science,0,W4414199338,10.1287/mnsc.2022.03862,https://openalex.org/W4414199338,,Counterfactual thinking; Computer science; Context (archaeology); Econometrics; Profit (economics),article,False,"Prior work on human-algorithmic bias has seen difficulty in empirically identifying the underlying mechanisms of bias because in a typical “one-time” decision-making scenario, different mechanisms generate the same patterns of observable decisions. In this study, leveraging a unique repeat decision-making setting in a high-stakes microlending context, we aim to uncover the underlying source, evolution dynamics, and associated impacts of bias. We first develop a structural econometric model of the decision dynamics to understand the source and evolution of bias in human evaluators in microloan granting. We find that both preference-based and belief-based biases exist in human decisions and are in favor of female applicants. Our counterfactual simulations show that the elimination of either of the two biases improves the fairness in financial resource allocation as well as the platform profits. The profit improvement mainly stems from the increased approval probability for male borrowers, especially those who would eventually pay back loans. Furthermore, to examine how human biases evolve when being inherited by machine learning (ML) algorithms, we train state-of-the-art ML algorithms for default risk prediction on both real-world data sets with human biases encoded within and counterfactual data sets with human biases partially or fully removed. We find that even fairness-unaware ML algorithms can reduce bias in human decisions. Interestingly, although removing both types of human bias from the training data can further improve ML fairness, the fairness-enhancing effects vary significantly between new and repeat applicants. Based on our findings, we discuss how to reduce decision bias most effectively in a human-ML pipeline. This paper has been accepted by D. J. Wu for the Special Issue on the Human-Algorithm Connection. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.03862 ."
"Fairness and Bias in Artificial Intelligence: A Brief Survey of Sources, Impacts, and Mitigation Strategies (Preprint)",2023,Emilio Ferrara,,81,W4366757677,10.2196/preprints.48399,https://openalex.org/W4366757677,,Preprint; Transparency (behavior); Accountability; Computer science; Artificial intelligence,preprint,False,"<sec> <title>BACKGROUND</title> The significant advancements in applying Artificial Intelligence (AI) to healthcare decision-making, medical diagnosis, and other domains have simultaneously raised concerns about the fairness and bias of AI systems, particularly in areas like healthcare, employment, criminal justice, and credit scoring. Such systems can lead to unfair outcomes and perpetuate existing inequalities. This survey paper offers a succinct, comprehensive overview of fairness and bias in AI, addressing their sources, impacts, and mitigation strategies. </sec> <sec> <title>OBJECTIVE</title> We review sources of bias, such as data, algorithm, and human decision biases, and assess the societal impact of biased AI systems, focusing on the perpetuation of inequalities and the reinforcement of harmful stereotypes. We explore various proposed mitigation strategies, discussing the ethical considerations of their implementation and emphasizing the need for interdisciplinary collaboration to ensure effectiveness. </sec> <sec> <title>METHODS</title> Through a systematic literature review spanning multiple academic disciplines, we present definitions of AI bias and its different types, and discuss the negative impacts of AI bias on individuals and society. We also provide an overview of current approaches to mitigate AI bias, including data pre-processing, model selection, and post-processing. </sec> <sec> <title>RESULTS</title> Addressing bias in AI requires a holistic approach, involving diverse and representative datasets, enhanced transparency and accountability in AI systems, and the exploration of alternative AI paradigms that prioritize fairness and ethical considerations. </sec> <sec> <title>CONCLUSIONS</title> This survey contributes to the ongoing discussion on developing fair and unbiased AI systems by providing an overview of the sources, impacts, and mitigation strategies related to AI bias. </sec>"
A Multi-objective Biased Random-Key Genetic Algorithm for the Siting of Emergency Vehicles,2023,Francesca Da Ros; Luca Di Gaspero; David La Barbera; Vincenzo Della Mea; Kevin Roitero; Laura Deroma; Sabrina Licata; Francesca Valent,Lecture notes in computer science,1,W4321504914,10.1007/978-3-031-26504-4_32,https://openalex.org/W4321504914,,Key (lock); Computer science; Genetic algorithm; Percentile; Event (particle physics),book-chapter,False,
Artificial intelligence integration in personalised learning for employee growth: a game-changing strategy,2023,Aaradhana Rukadikar; Komal Khandelwal,Strategic HR Review,14,W4386111475,10.1108/shr-08-2023-0046,https://openalex.org/W4386111475,,Originality; Knowledge management; Value (mathematics); Computer science; Artificial intelligence,article,False,"Purpose This viewpoint paper aims to explore the adoption of artificial intelligence (AI) in personalized learning for employees and its potential, challenges and ethical consideration to transform the landscape of employee’s performance improvement. Design/methodology/approach A review of relevant research papers, articles and case studies is done to highlight developments in research and practice. Findings Personalised learning powered by AI will transform employee education and training as AI evolves, helping individuals and organisations succeed in an ever-changing, knowledge-driven world. Concerns like data privacy, algorithmic bias and human engagement must be addressed to achieve ethical and fair implementation. Originality/value This paper discusses the overall influence of AI adoption on personalised learning for employees. It examines how AI technologies have the ability to disrupt traditional learning methodologies, modify organisational learning cultures and encourage an environment that fosters ongoing learning and innovation."
Evaluating Machine Learning Models and Their Diagnostic Value,2023,Gaël Varoquaux; Olivier Colliot,Neuromethods,105,W4286829109,10.1007/978-1-0716-3195-9_20,https://openalex.org/W4286829109,https://doi.org/10.1007/978-1-0716-3195-9_20,Machine learning; Computer science; Artificial intelligence; Training set; Cross-validation,book-chapter,True,"Abstract This chapter describes model validation, a crucial part of machine learning whether it is to select the best model or to assess performance of a given model. We start by detailing the main performance metrics for different tasks (classification, regression), and how they may be interpreted, including in the face of class imbalance, varying prevalence, or asymmetric cost–benefit trade-offs. We then explain how to estimate these metrics in an unbiased manner using training, validation, and test sets. We describe cross-validation procedures—to use a larger part of the data for both training and testing—and the dangers of data leakage—optimism bias due to training data contaminating the test set. Finally, we discuss how to obtain confidence intervals of performance metrics, distinguishing two situations: internal validation or evaluation of learning algorithms and external validation or evaluation of resulting prediction models."
Investigating Algorithmic Bias on Spotify,2024,Marianne Lumeau; François Moreau; Samuel Coavoux; Ramadan Jose Aly-Tovar,AEA Randomized Controlled Trials,0,W4403222505,10.1257/rct.14495-1.1,https://openalex.org/W4403222505,,Computer science; Econometrics; Economics,dataset,False,
Algorithmic Bias in Hiring Algorithms: A Kenyan Perspective,2024,Kazungu Mrashui,Strathmore Law Review,0,W4408867032,10.52907/slr.v9i1.480,https://openalex.org/W4408867032,,Kenya; Perspective (graphical); Computer science; Algorithm; Artificial intelligence,article,False,"The use of machine learning algorithms has permeated into nearly all aspects of life. With this steady integration, tasks previously handled by humans are increasingly falling into the ‘hands’ of machines. Ideally this would be celebrated as a great improvement for mankind. Tasks that were previously riddled with human bias such as hiring would now be performed by an ‘omniscient algorithm’ that could harbor no bias therefore resulting in fair outcomes for the previously oppressed. However, this is not the case. The integration of machine learning algorithms in the hiring process risks further exacerbating existing bias that was prevalent or introducing new data-driven bias. The question then is how to contend with this novel form of discrimination: algorithmic discrimination. The answer to combating algorithmic discrimination is algorithmic fairness. The goal should not be to create ‘fair’ algorithms but rather to detect and mitigate fairness-related harms as much as possible. By doing so, a balance can be struck between the competing interests of innovation and employee rights. This article demonstrates that algorithmic discrimination during hiring is a real threat to the Kenyan jobseeker. Although this form of discrimination can be addressed by Kenyan law, more needs to be done to detect and mitigate fairness-related harms as much as possible."
Algorithmic biases and the discoverability of digital cultural content,2025,Jonathan Paquette,Journal of Cultural Management and Cultural Policy / Zeitschrift für Kulturmanagement und Kulturpolitik,0,W4414338836,10.1177/27018466251366274,https://openalex.org/W4414338836,https://doi.org/10.1177/27018466251366274,Discoverability; Computer science; Content (measure theory); Information retrieval; World Wide Web,article,True,"The digital era has transformed how the production of culture is accessed, how it circulates, and how it is organized. In this article, I wish to discuss the notion of discoverability. This notion, I argue, is one of the most recent cultural policy instruments that has emerged in the digital era. Discoverability implies creating conditions under which the public can easily encounter (be proposed or offered) cultural content that is culturally relevant. In other words, the notion of discoverability includes the capacity to encounter local cultural content and content that is made in languages other than English. Discoverability, however, tends to function on algorithmic biases that privilege English-language cultural content and content produced by large global corporations. From a cultural policy perspective, discoverability is rooted in two basic dimensions: the regulation of culture and the accessibility of culture. This article emphasizes the place of French-language content and touches on two dimensions: the accessibility of French-language digital content in general, and the issue of cultural content from French-speaking minorities. In doing so, it also sheds light on strategies and policies that are pertinent to other languages and to other linguistic minorities."
"Análisis de retos y dilemas que deberá afrontar la bioética del siglo xxi, en la era de la salud digital",2024,Robert Panadés Zafra; Noemí Amorós Parramon; Marc Albiol-Perarnau; Oriol Yuguero Torres,Atención Primaria,15,W4392529727,10.1016/j.aprim.2024.102901,https://openalex.org/W4392529727,https://doi.org/10.1016/j.aprim.2024.102901,Bioethics; Digital health; Confidentiality; Health care; Telemedicine,article,True,"The medical history underscores the significance of ethics in each advancement, with bioethics playing a pivotal role in addressing emerging ethical challenges in digital health (DH). This article examines the ethical dilemmas of innovations in DH, focusing on the healthcare system, professionals, and patients. Artificial Intelligence (AI) raises concerns such as confidentiality and algorithmic biases. Mobile applications (Apps) empower but pose challenges of access and digital literacy. Telemedicine (TM) democratizes and reduces healthcare costs but requires addressing the digital divide and interconsultation dilemmas; it necessitates high-quality standards with patient information protection and attention to equity in access. Wearables and the Internet of Things (IoT) transform healthcare but face ethical challenges like privacy and equity. 21st-century bioethics must be adaptable as DH tools demand constant review and consensus, necessitating health science faculties' preparedness for the forthcoming changes."
Algorithmic Bias as Ultimately Fixable,2024,Elisabeth Kelan,Routledge eBooks,0,W4401973221,10.4324/9781003427100-4,https://openalex.org/W4401973221,https://doi.org/10.4324/9781003427100-4,Computer science,book-chapter,True,
Feminism and Algorithmic Bias in the Media,2025,Sibongile Mpofu,Routledge eBooks,0,W4414607224,10.4324/9781003674498-5,https://openalex.org/W4414607224,,Feminism; Art; Sociology; Gender studies,book-chapter,False,
Robustly Improving Bandit Algorithms with Confounded and Selection Biased Offline Data: A Causal Approach,2024,Wen Huang; Xintao Wu,Proceedings of the AAAI Conference on Artificial Intelligence,1,W4393160650,10.1609/aaai.v38i18.30027,https://openalex.org/W4393160650,https://ojs.aaai.org/index.php/AAAI/article/download/30027/31806,Selection (genetic algorithm); Computer science; Algorithm; Artificial intelligence; Machine learning,article,True,"This paper studies bandit problems where an agent has access to offline data that might be utilized to potentially improve the estimation of each arm’s reward distribution. A major obstacle in this setting is the existence of compound biases from the observational data. Ignoring these biases and blindly fitting a model with the biased data could even negatively affect the online learning phase. In this work, we formulate this problem from a causal perspective. First, we categorize the biases into confounding bias and selection bias based on the causal structure they imply. Next, we extract the causal bound for each arm that is robust towards compound biases from biased observational data. The derived bounds contain the ground truth mean reward and can effectively guide the bandit agent to learn a nearly-optimal decision policy. We also conduct regret analysis in both contextual and non-contextual bandit settings and show that prior causal bounds could help consistently reduce the asymptotic regret."
Investigating Algorithmic Bias on Spotify,2024,Marianne Lumeau; François Moreau; Samuel Coavoux; Ramadan Jose Aly-Tovar,AEA Randomized Controlled Trials,0,W4403175398,10.1257/rct.14495,https://openalex.org/W4403175398,,Computer science; Econometrics; Economics,dataset,False,
Investigating Algorithmic Bias on Spotify,2024,Marianne Lumeau; François Moreau; Samuel Coavoux; Ramadan Jose Aly-Tovar,AEA Randomized Controlled Trials,0,W4403175400,10.1257/rct.14495-1.0,https://openalex.org/W4403175400,,Computer science; Data science,dataset,False,
QUANTIFYING ALGORITHMIC BIAS IN NEWS RECOMMENDATIONS: METHODOLOGIES AND CASE STUDIES,2024,Roman Tsekhmeistruk,Academia Polonica.,0,W4405790524,10.23856/6627,https://openalex.org/W4405790524,https://doi.org/10.23856/6627,Transparency (behavior); Personalization; Empowerment; Journalism; Accountability,article,False,"This study investigates algorithmic bias in news recommendations, a critical issue in today’s digital media landscape. As recommendation algorithms curate personalized content, they can also perpetuate systematic biases that distort information access and public discourse. The research begins with a literature review, identifying key themes and gaps in understanding algorithmic bias. A robust methodology is developed, incorporating user-centric analyses, content diversity assessments, and fairness evaluations to quantify the impact of bias in news recommendations. Through detailed case studies, the study highlights how biased algorithms shape user experiences, limit exposure to diverse perspectives, and contribute to societal polarization. The findings emphasize the urgent need for ethical considerations in algorithm design and provide actionable recommendations for media organizations, technology companies, and policymakers. By advocating for transparency, accountability, and user empowerment, this research aims to foster a more equitable digital information environment. Ultimately, the study contributes to the discourse on algorithmic bias, promoting a media landscape where diverse voices are heard and the integrity of journalism is maintained in the age of personalization."
"Role and challenges of ChatGPT, Gemini, and similar generative artificial intelligence in human resource management",2024,Nitin Liladhar Rane,Studies in Economics and Business Relations,17,W4392774084,10.48185/sebr.v5i1.1001,https://openalex.org/W4392774084,https://sabapub.com/index.php/sebr/article/download/1001/550,Generative grammar; Artificial intelligence; Knowledge management; Computer science; Environmental resource management,article,True,"The integration of generative artificial intelligence (AI) systems, such as ChatGPT, into Human Resource Management (HRM) has marked the beginning of a groundbreaking era in innovative workforce management and employee engagement. This research investigates the pivotal role played by ChatGPT and analogous generative AI technologies in HRM, underscoring their significance in the realms of recruitment, employee training, and organizational communication. Leveraging their natural language processing abilities, these AI systems streamline the recruitment process, ensuring unbiased candidate selection and enhancing the overall efficiency of HR departments. Moreover, in training and development initiatives, ChatGPT facilitates tailored learning experiences, adjusting content to meet individual employee needs, thereby fostering skill enhancement and professional growth. Nevertheless, the widespread adoption of ChatGPT in HRM is not without its challenges. Ethical concerns, such as data privacy and algorithmic bias, necessitate thorough examination to prevent discriminatory practices and guarantee equitable treatment of employees. Additionally, the requirement for continuous monitoring and refinement of AI algorithms to align with evolving organizational cultures and goals presents a significant hurdle. Moreover, striking a harmonious balance between AI-driven automation and human intervention is imperative to preserve the human touch in HRM processes, safeguarding the empathetic and intuitive elements that are essential for effective employee management. This research delves into these complex dynamics, shedding light on the transformative potential of ChatGPT and akin generative AI technologies in HRM, while also emphasizing the need for vigilance and strategic planning to address the associated challenges. Through an exhaustive analysis of real-world case studies and ethical frameworks, this study offers valuable insights for HR professionals, policymakers, and researchers endeavoring to navigate the intricate landscape of AI-powered HRM.&#x0D;"
Challenging the appearance of machine intelligence: Cognitive bias in LLMs and Best Practices for Adoption,2023,Alaina N. Talboy; Elizabeth Fuller,arXiv (Cornell University),9,W4362655418,10.48550/arxiv.2304.01358,https://openalex.org/W4362655418,https://arxiv.org/abs/2304.01358,Cognition; Set (abstract data type); Cognitive bias; Ethnic group; Psychology,preprint,True,"Assessments of algorithmic bias in large language models (LLMs) are generally catered to uncovering systemic discrimination based on protected characteristics such as sex and ethnicity. However, there are over 180 documented cognitive biases that pervade human reasoning and decision making that are routinely ignored when discussing the ethical complexities of AI. We demonstrate the presence of these cognitive biases in LLMs and discuss the implications of using biased reasoning under the guise of expertise. We call for stronger education, risk management, and continued research as widespread adoption of this technology increases. Finally, we close with a set of best practices for when and how to employ this technology as widespread adoption continues to grow."
Mandating Transparency: A Regulatory Framework for Algorithmic Bias and Opacity,2025,Kaichen Zhang,Applied and Computational Engineering,0,W4412495075,10.54254/2755-2721/2025.po25142,https://openalex.org/W4412495075,,Transparency (behavior); Opacity; Computer science; Computer security; Physics,article,False,"Algorithmic bias and opacity pose threats to the global population by damaging social equity, as demonstrated by racial disparities in facial recognition accuracy and healthcare prediction algorithms. This is mainly because most algorithms are black boxes---few know how they operate; sometimes this is intentional to maximize company profits. While ethical principles lack enforceability, current regulatory approaches like the EU AI Act focus primarily on risk and privacy but fail to identify and address biases directly. A disclosure-based regulatory framework adapted from the US FDA's Total Product Lifecycle (TPLC), originally designed for medical algorithms, is a favorable alternative. Extended TPLC has already been proposed, which lists out potential biases throughout the entire implementation of medical algorithms. As a successfully applied visual diagnostic AI, the IDx-DR demonstrates the effectiveness of companies referencing the TPLC to identify, address, and publicly disclose their handling of biases throughout an algorithm's lifecycle. This shows the potential of this framework in resolving algorithmic bias and corporate secrecy (""black boxes"") in all fields. However, biases and disclosure requirements of companies need to be generalized for wide applications. The extended TPLC should thus be simplified, involving only three stages: design, development, and deployment. Its effectiveness is demonstrated through three examples: flawed teacher evaluation algorithms (design), specialized legal AI development (development), and biased healthcare risk score monitoring (deployment). While this framework promotes algorithm transparency and accountability without stifling innovation, its effectiveness can be enhanced if governments mandate disclosure accordingly and coordinate globally to prevent corporate regulatory arbitrage."
Harnessing Artificial Intelligence,2024,Imogen Cooper,"Advances in educational marketing, administration, and leadership book series",12,W2898347082,10.4018/979-8-3693-7190-9.ch003,https://openalex.org/W2898347082,,Computer science; Artificial intelligence,book-chapter,False,"This provides a comprehensive exploration of Artificial Intelligence (AI) and its implications for education. Beginning with an introduction to the foundational principles of AI, the chapter delves into various aspects of AI, including machine learning, neural networks, and deep learning. It examines how AI is transforming industries and societies across healthcare, finance, and education, highlighting its potential to drive innovation and efficiency. Furthermore, the chapter addresses the ethical considerations associated with AI education, emphasizing the importance of data privacy, algorithmic bias, transparency, and accountability. By promoting ethical awareness and responsible decision-making, educators can empower students to navigate the AI-driven world ethically and responsibly."
Brain-age prediction: A systematic comparison of machine learning workflows,2023,Shammi More; Georgios Antonopoulos; Felix Hoffstaedter; Julian Caspers; Simon B. Eickhoff; Kaustubh R. Patil,NeuroImage,78,W4321073670,10.1016/j.neuroimage.2023.119947,https://openalex.org/W4321073670,https://doi.org/10.1016/j.neuroimage.2023.119947,Workflow; Computer science; Artificial intelligence; Machine learning; Database,article,True,"The difference between age predicted using anatomical brain scans and chronological age, i.e., the brain-age delta, provides a proxy for atypical aging. Various data representations and machine learning (ML) algorithms have been used for brain-age estimation. However, how these choices compare on performance criteria important for real-world applications, such as; (1) within-dataset accuracy, (2) cross-dataset generalization, (3) test-retest reliability, and (4) longitudinal consistency, remains uncharacterized. We evaluated 128 workflows consisting of 16 feature representations derived from gray matter (GM) images and eight ML algorithms with diverse inductive biases. Using four large neuroimaging databases covering the adult lifespan (total N = 2953, 18-88 years), we followed a systematic model selection procedure by sequentially applying stringent criteria. The 128 workflows showed a within-dataset mean absolute error (MAE) between 4.73-8.38 years, from which 32 broadly sampled workflows showed a cross-dataset MAE between 5.23-8.98 years. The test-retest reliability and longitudinal consistency of the top 10 workflows were comparable. The choice of feature representation and the ML algorithm both affected the performance. Specifically, voxel-wise feature spaces (smoothed and resampled), with and without principal components analysis, with non-linear and kernel-based ML algorithms performed well. Strikingly, the correlation of brain-age delta with behavioral measures disagreed between within-dataset and cross-dataset predictions. Application of the best-performing workflow on the ADNI sample showed a significantly higher brain-age delta in Alzheimer's and mild cognitive impairment patients compared to healthy controls. However, in the presence of age bias, the delta estimates in the patients varied depending on the sample used for bias correction. Taken together, brain-age shows promise, but further evaluation and improvements are needed for its real-world application."
"The Role of Artificial Intelligence in Nursing Education, and Practice: An Umbrella Review (Preprint)",2024,Rabie Adel El Arab; Omayma Abdulaziz Al Moosa; Fuad H. Abuadas; Joel Somerville,Journal of Medical Internet Research,19,W4408361589,10.2196/69881,https://openalex.org/W4408361589,https://doi.org/10.2196/69881,Preprint; Psychology; Nursing; Computer science; Medicine,review,True,"Artificial intelligence (AI) is rapidly transforming healthcare, offering significant advancements in patient care, clinical workflows, and nursing education. While AI has the potential to enhance health outcomes and operational efficiency, its integration into nursing practice and education raises critical ethical, social, and educational challenges that must be addressed to ensure responsible and equitable adoption. This umbrella review aims to evaluate the integration of AI into nursing practice and education, with a focus on ethical and social implications, and to propose evidence-based recommendations to support the responsible and effective adoption of AI technologies in nursing. A comprehensive literature search was conducted in PubMed, CINAHL, Web of Science, Embase, and IEEE Xplore to identify relevant review articles (systematic, scoping, narrative, etc.) on AI integration in nursing, published up to October 2024 (with an updated search in January 2025). Eligibility was determined using the SPIDER framework to include reviews addressing AI in any nursing context (practice or education). Two reviewers independently screened studies, extracted data, and assessed the quality of each review using ROBIS and an adapted AMSTAR 2 tool. The findings were synthesized using thematic analysis to identify key recurring themes across the included studies. Eighteen reviews met the inclusion criteria, encompassing diverse nursing domains (clinical practice, education, and research). Three overarching themes emerged: (1) Ethical and Social Implications - widespread concerns about data privacy, algorithmic bias, transparency in AI decision-making, accountability, and equitable access; (2) Transformation of Nursing Education - the need for curriculum reform to integrate AI literacy, the use of AI-driven educational tools, and training to address ethical and interpersonal skills in an AI-enabled environment; and (3) Strategies for Integration - the importance of scalable implementation plans, development of ethical governance frameworks, promoting equity in AI access, and fostering interdisciplinary collaboration. Critical barriers identified across studies include algorithmic bias, data privacy concerns, resistance to AI adoption among nursing professionals, lack of standardized AI education (highlighting the need for curriculum updates), and disparities in access to AI technologies. AI offers significant promise to transform nursing practice and education, but realizing these benefits requires proactive strategies to address the identified challenges. This review recommends implementing robust ethical AI governance frameworks and regulatory guidelines, integrating AI literacy and ethics into nursing curricula, and encouraging interdisciplinary collaboration between healthcare and technology professionals. Such measures will help ensure that AI technologies are adopted in nursing practice in an ethical and equitable manner. Further research is needed to develop standardized implementation strategies and to evaluate the long-term impacts of AI integration on patient care and professional nursing practice."
Algorithm Bias and Perceived Fairness: A Comprehensive Scoping Review,2024,Amirhossein Hajigholam Saryazdi,,0,W4399141338,10.1145/3632634.3655848,https://openalex.org/W4399141338,https://dl.acm.org/doi/pdf/10.1145/3632634.3655848,Fairness measure; Computer science; Telecommunications; Wireless; Throughput,article,True,"Artificial intelligence (AI)-based algorithms are playing an increasingly prominent role in shaping daily life. However, these algorithms can exhibit biases that exacerbate societal injustices. Such biases have a substantial impact on people's perceptions of algorithmic fairness, yet the precise mechanisms and scope of this phenomenon remain relatively understudied. To address this research gap, a comprehensive scoping literature review is conducted, providing an overview of current research in the field. Subsequently, a novel theoretical model is developed that synthesizes key themes, including algorithm bias, algorithm fairness, perceived fairness, individual characteristics, social characteristics, task characteristics, and technology characteristics. The paper contributes proposing a set of propositions that underscore the critical gaps in the existing literature, contribute to a deeper comprehension of the relationships among the identified themes and their constituent elements, and offer a roadmap for future research in the domain."
Implications of AI Bias in HRI,2023,Tom Hitron; Noa Morag Yaar; Hadas Erel,,11,W4323688010,10.1145/3568162.3576977,https://openalex.org/W4323688010,https://doi.org/10.1145/3568162.3576977,Preference; Robot; Psychology; Human–robot interaction; Object (grammar),article,False,"Social robotic behavior is commonly designed using AI algorithms which are trained on human behavioral data. This training process may result in robotic behaviors that echo human biases and stereotypes. In this work, we evaluated whether an interaction with a biased robotic object can increase participants' stereotypical thinking. In the study, a gender-biased robot moderated debates between two participants (man and woman) in three conditions: (1) The robot's behavior matched gender stereotypes (Pro-Man); (2) The robot's behavior countered gender stereotypes (Pro-Woman); (3) The robot's behavior did not reflect gender stereotypes and did not counter them (No-Preference). Quantitative and qualitative measures indicated that the interaction with the robot in the Pro-Man condition increased participants' stereotypical thinking. In the No-Preference condition, stereotypical thinking was also observed but to a lesser extent. In contrast, when the robot displayed counter-biased behavior in the Pro-Woman condition, stereotypical thinking was eliminated. Our findings suggest that HRI designers must be conscious of AI algorithmic biases, as interactions with biased robots can reinforce implicit stereotypical thinking and exacerbate existing biases in society. On the other hand, counter-biased robotic behavior can be leveraged to support present efforts to address the negative impact of stereotypical thinking."
Automatic detection and classification of lung cancer CT scans based on deep learning and ebola optimization search algorithm,2023,Tehnan I. A. Mohamed; Olaide N. Oyelade; Absalom E. Ezugwu,PLoS ONE,66,W4385932710,10.1371/journal.pone.0285796,https://openalex.org/W4385932710,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0285796&type=printable,Lung cancer; Algorithm; Computer science; Cancer; Artificial intelligence,article,True,"Recently, research has shown an increased spread of non-communicable diseases such as cancer. Lung cancer diagnosis and detection has become one of the biggest obstacles in recent years. Early lung cancer diagnosis and detection would reliably promote safety and the survival of many lives globally. The precise classification of lung cancer using medical images will help physicians select suitable therapy to reduce cancer mortality. Much work has been carried out in lung cancer detection using CNN. However, lung cancer prediction still becomes difficult due to the multifaceted designs in the CT scan. Moreover, CNN models have challenges that affect their performance, including choosing the optimal architecture, selecting suitable model parameters, and picking the best values for weights and biases. To address the problem of selecting optimal weight and bias combination required for classification of lung cancer in CT images, this study proposes a hybrid metaheuristic and CNN algorithm. We first designed a CNN architecture and then computed the solution vector of the model. The resulting solution vector was passed to the Ebola optimization search algorithm (EOSA) to select the best combination of weights and bias to train the CNN model to handle the classification problem. After thoroughly training the EOSA-CNN hybrid model, we obtained the optimal configuration, which yielded good performance. Experimentation with the publicly accessible Iraq-Oncology Teaching Hospital / National Center for Cancer Diseases (IQ-OTH/NCCD) lung cancer dataset showed that the EOSA metaheuristic algorithm yielded a classification accuracy of 0.9321. Similarly, the performance comparisons of EOSA-CNN with other methods, namely, GA-CNN, LCBO-CNN, MVO-CNN, SBO-CNN, WOA-CNN, and the classical CNN, were also computed and presented. The result showed that EOSA-CNN achieved a specificity of 0.7941, 0.97951, 0.9328, and sensitivity of 0.9038, 0.13333, and 0.9071 for normal, benign, and malignant cases, respectively. This confirms that the hybrid algorithm provides a good solution for the classification of lung cancer."
"The 4 As: Ask, Adapt, Author, Analyze: AI Literacy Framework for Families",2023,Stefania Druga; Jason Yip; Michael Preston; Devin Dillon,The MIT Press eBooks,11,W3173590633,10.7551/mitpress/13654.003.0014,https://openalex.org/W3173590633,https://direct.mit.edu/books/oa-edited-volume/chapter-pdf/2140402/c007100_9780262374316.pdf,Literacy; Ask price; Perspective (graphical); Order (exchange); Computer science,book-chapter,True,"Families’ interactions with various forms of AI technologies have recently attracted significant attention. Since these technologies do not support developmentally adaptable and family-friendly interactions, we recognize an opportunity to create a framework that supports family AI literacy. Our novel framework is composed of four main dimensions (4As): ask, adapt, author, and analyze. We believe that in order to ensure algorithmic fairness, this framework can be used by families for developing a critical understanding of smart technologies embedded in their lives. We define our AI literacy dimensions building on prior work and through a series of co-design and AI learning sessions with families. Our current findings show how children perceive algorithmic bias differently from adults and how families engage in collaborative sense-making by probing, tricking, and authoring AI applications in playful ways. We discuss the implications of AI literacy from the broader perspective of technology development, public policy, and algorithmic justice. We argue that AI literacy is a fundamental right for families and propose a series of learning activities and guidelines in order to support and protect this right."
AI in Education,2024,Gurwinder Singh; Anshika Thakur,Advances in computational intelligence and robotics book series,9,W4392431983,10.4018/979-8-3693-2964-1.ch002,https://openalex.org/W4392431983,,Computer science; Psychology,book-chapter,False,"Artificial intelligence (AI) offers transformative opportunities in education, promising personalized learning and streamlined tasks. However, its integration raises ethical concerns like algorithmic bias and data privacy. AI can revolutionize education by tailoring learning experiences and optimizing administrative tasks, yet it risks eroding student autonomy and perpetuating biases. To mitigate these challenges, transparent algorithms, robust data governance, and educator training are essential. Successful AI implementations demonstrate improved engagement and outcomes. Moving forward, collaborative efforts are crucial to navigate ethical complexities and ensure AI enhances education responsibly, fostering equity and inclusivity."
Analyzing metaheuristic algorithms performance and the causes of the zero-bias problem: a different perspective in benchmarks,2025,Bernardo Morales-Castañeda; Marco Pérez‐Cisneros; Erik Cuevas; Daniel Zaldívar; Miguel Islas Toski; Alma Rodríguez,Evolutionary Intelligence,1,W4408005167,10.1007/s12065-025-01024-y,https://openalex.org/W4408005167,,Computer science; Perspective (graphical); Metaheuristic; Algorithm; Zero (linguistics),article,False,
Examining the impact of bias mitigation algorithms on the sustainability of ML-enabled systems: A benchmark study,2025,Vincenzo De Martino; Gianmario Voria; Ciro Troiano; Gemma Catolino; Fabio Palomba,Journal of Systems and Software,1,W4410773810,10.1016/j.jss.2025.112458,https://openalex.org/W4410773810,https://doi.org/10.1016/j.jss.2025.112458,Benchmark (surveying); Sustainability; Computer science; Algorithm; Geography,article,True,
Spatial Algorithmic Bias in Socio-Economic Clustering of Russian Regions,2024,В. И. Блануца,Spatial Economics,0,W4400338082,10.14530/se.2024.2.071-092,https://openalex.org/W4400338082,https://doi.org/10.14530/se.2024.2.071-092,Cluster analysis; Computer science; Geography; Economic geography; Artificial intelligence,article,True,"Decision-making based on complex human-machine algorithms can lead to discrimination of citizens based on gender, race and other grounds. However, in world science there is no idea of algorithmically conditioned discrimination of citizens by their place of residence. This also applies to the adoption of algorithmic decisions on the socio-economic development of regions. Therefore, the purpose of our study was to detect algorithmic bias in the results of socio-economic clustering of Russian regions. To achieve this goal, it was necessary to identify sensitive operations in cluster analysis that could lead to spatial injustice, form an array of articles on socio-economic clustering of subjects (regions) of the Russian Federation, analyze all articles for the possibility of algorithmic bias and identify Russian regions with potentially biased attitudes towards them as a result of clustering. The term ‘spatial algorithmic bias’ is proposed. Using the author’s semantic search algorithm in bibliographic databases, six hundred articles with empirical results of cluster analysis of Russian regions by socio-economic indicators were identified. The characteristics of the identified articles are given. The analysis of all the articles showed that algorithmic bias is most evident in the four operations of the clustering algorithm – deploying a conceptual model into an optimal set of indicators, selecting regions, choosing a way to combine regions into clusters and determining the number of clusters. Examples of discriminated Russian regions are presented for each operation. Three directions of further research are indicated. Practical significance may be associated with the adoption of unbiased decisions on regional socio-economic development based on fair clustering of the Russian Federation’s subjects"
A Biased Random Key Genetic Algorithm for Solving the Longest Common Square Subsequence Problem,2024,Jaume Reixach; Christian Blum; Marko Djukanović; Günther R. Raidl,IEEE Transactions on Evolutionary Computation,1,W4399571756,10.1109/tevc.2024.3413150,https://openalex.org/W4399571756,,Longest common subsequence problem; Key (lock); Algorithm; Longest increasing subsequence; Genetic algorithm,article,False,
A Biased Random Key Genetic Algorithm for Solving the Longest Common Square Subsequence Problem,2023,Jaume Reixach; Christian Blum; Marko Djukanović; Guenther Raidl,,1,W4383648249,10.2139/ssrn.4504431,https://openalex.org/W4383648249,,Longest common subsequence problem; Concatenation (mathematics); String (physics); Algorithm; Context (archaeology),preprint,False,
Robustly Improving Bandit Algorithms with Confounded and Selection Biased Offline Data: A Causal Approach,2023,Wen Huang; Xintao Wu,arXiv (Cornell University),1,W4390091977,10.48550/arxiv.2312.12731,https://openalex.org/W4390091977,https://arxiv.org/abs/2312.12731,Regret; Observational study; Computer science; Machine learning; Causal inference,preprint,True,"This paper studies bandit problems where an agent has access to offline data that might be utilized to potentially improve the estimation of each arm's reward distribution. A major obstacle in this setting is the existence of compound biases from the observational data. Ignoring these biases and blindly fitting a model with the biased data could even negatively affect the online learning phase. In this work, we formulate this problem from a causal perspective. First, we categorize the biases into confounding bias and selection bias based on the causal structure they imply. Next, we extract the causal bound for each arm that is robust towards compound biases from biased observational data. The derived bounds contain the ground truth mean reward and can effectively guide the bandit agent to learn a nearly-optimal decision policy. We also conduct regret analysis in both contextual and non-contextual bandit settings and show that prior causal bounds could help consistently reduce the asymptotic regret."
Calibration of the SMAP soil moisture retrieval algorithm to reduce bias over the Amazon rainforest,2023,Kyeungwoo Cho; Robinson Negrón‐Juárez; Andreas Colliander; Eric G. Cosio; Norma Salinas; A LETCIA PONTES DE ARAUJO; Jeffrey Q. Chambers; Jingfeng Wang,Zenodo (CERN European Organization for Nuclear Research),1,W4386795566,10.5281/zenodo.8350626,https://openalex.org/W4386795566,https://zenodo.org/records/8350626/files/JSTARS-2023-00998_Proof_hi.pdf,Amazon rainforest; Rainforest; Environmental science; Calibration; Remote sensing,article,True,
